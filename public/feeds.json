[
  {
    "name": "C++ Team Blog",
    "category": "기업",
    "posts": [
      {
        "creator": "Michael Price",
        "title": "GitHub Copilot app modernization for C++ is now in Public Preview",
        "link": "https://devblogs.microsoft.com/cppblog/github-copilot-app-modernization-cpp-public-preview/",
        "pubDate": "Tue, 27 Jan 2026 22:31:57 +0000",
        "content:encodedSnippet": "With the launch of Visual Studio 2026, we announced a Private Preview of GitHub Copilot app modernization for C++, which reduces the cost of adopting the latest version of the MSVC Build Tools. We used the feedback we received from our many Private Preview participants to make improvements that benefit all our users. After receiving feedback, we added support for CMake projects, reduced hallucinations, removed several critical failures, improved Copilot’s behavior when encountering an internal compiler error, and reinforced Copilot’s understanding of when project files need to be modified to do the upgrade.\nHere’s what one of our Private Preview participants said about their experience:\n“Having Copilot guide the upgrade flow and surface suggested changes in context has made that process smoother than doing it entirely by hand or by another agent.” – Private Preview participant\n\nWe are happy to announce that this feature is now available to all C++ users as a Public Preview in Visual Studio 2026 Insiders.\nTo get started, check out the documentation on Microsoft Learn.\nWhat to expect\nAfter launching GitHub Copilot app modernization, Copilot will examine your project to see if there are any steps to take to update your project settings to move to the newer MSVC version. If so, it’ll assist you in making those changes.\nAssessment\nAfter the settings have been updated, Copilot will do an initial build to assess if there are any issues blocking your upgrade, such as stricter conformance, warnings whose warning level has changed, or non-standard extensions that have been deprecated or removed. After the assessment is complete, Copilot checks with you to confirm accuracy, and gives you a chance to give it further instructions like ignoring specific or entire categories of issues.\n\nPlanning\nAfter you and Copilot agree on the assessment, it will move into the planning stage, where Copilot will propose solutions to all the issues that need to be addressed. Again, it will produce a detailed description of these solutions and its reasoning for applying them, and it will check with you for any additional information. If you don’t like the proposed solution, you can direct it down another path.\n\nExecution\nOnce the plan is set, Copilot will break the plan down into concrete tasks for execution. You can direct it to approach the implementation in ways that fit your organization’s processes, such as by keeping similar changes in the same commit or using a particular style guideline when editing the code. Copilot will execute the tasks and initiate another build to check that all issues are resolved. If they aren’t, it will iterate until it has resolved the issues for you.\n\nYou are in control\nAt every step of the way, you can shape Copilot’s behavior, guiding it towards solutions that fit your own expectations, saving you time researching, diagnosing issues, designing solutions, and implementing those solutions. It can take a multi-person, multi-week task of upgrading your build tools and turn it into something you do on the same day as the release of the new tools.\nTalk to us!\nWe are excited for you to try out this feature. Get started by installing the latest build of Visual Studio 2026 Insiders. Let us know how well this feature is working for you and how we can make it even better. If you have any questions or general comments about the feature, feel free to leave a comment on this blog post. If you want to suggest an improvement, you can use the Help > Send Feedback menu directly in Visual Studio to post on Developer Community.\nThe post GitHub Copilot app modernization for C++ is now in Public Preview appeared first on C++ Team Blog.",
        "dc:creator": "Michael Price",
        "comments": "https://devblogs.microsoft.com/cppblog/github-copilot-app-modernization-cpp-public-preview/#respond",
        "content": "<p>With the launch of Visual Studio 2026, we announced a Private Preview of GitHub Copilot app modernization for C++, which reduces the cost of adopting the latest version of the MSVC Build Tools. We used the feedback we received from our many Private Preview participants to make improvements that benefit all our users. After receiving [&#8230;]</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/cppblog/github-copilot-app-modernization-cpp-public-preview/\">GitHub Copilot app modernization for C++ is now in Public Preview</a> appeared first on <a href=\"https://devblogs.microsoft.com/cppblog\">C++ Team Blog</a>.</p>\n",
        "contentSnippet": "With the launch of Visual Studio 2026, we announced a Private Preview of GitHub Copilot app modernization for C++, which reduces the cost of adopting the latest version of the MSVC Build Tools. We used the feedback we received from our many Private Preview participants to make improvements that benefit all our users. After receiving […]\nThe post GitHub Copilot app modernization for C++ is now in Public Preview appeared first on C++ Team Blog.",
        "guid": "https://devblogs.microsoft.com/cppblog/?p=36249",
        "categories": [
          "Announcement",
          "C++",
          "Copilot",
          "VC++ Migration Documentation",
          "Visual Studio"
        ],
        "isoDate": "2026-01-27T22:31:57.000Z"
      },
      {
        "creator": "Sinem Akinci",
        "title": "Visual Studio Code CMake Tools 1.22: Target bookmarks and better CTest output",
        "link": "https://devblogs.microsoft.com/cppblog/visual-studio-code-cmake-tools-1-22-target-bookmarks-and-better-ctest-output/",
        "pubDate": "Tue, 27 Jan 2026 16:36:46 +0000",
        "content:encodedSnippet": "We’re excited to announce the latest 1.22 release of the CMake Tools extension for Visual Studio Code. This update brings a host of new additions, including project outline updates for filtering and bookmarking CMake targets in large CMake projects and expanded CTest support to customize this output. To view the full list of updates with this release, please look at our CHANGELOG.\nThis release features the following contributions from our open-source community. Thank you for your continued support!\nAdd bookmarks and filtering of outline view by @bradphelan\nAdd pre-fill project name using current folder name by @ho-cooh\nAdd API v5 which adds presets api by @OrkunTokdemir\nAdd output parser for include-what-you-use by @malsyned\nIn test explorer, associated CTest tests with outermost function or macro invocation that calls add_test() instead of with the add_test() call itself by @malsyned\nBetter support of cmake v4.1 and its error index files in cmake-file-api replies by @STMicroelectronics\nFix bug in which clicking “Run Test” for filtered tests executed all tests instead by @hippo91\nFix auto-focusing the “Search” input field in the CMake cache view by @simhof-basyskom\nProject Outline view updates: Filter and bookmark your CMake Targets\nNavigating through large CMake projects with many nested targets can sometimes be difficult. The Project Outline view has been updated to have filtering and bookmarking support, making it easier to manage your CMake targets.\nFilter through complex target outlines\nYou can now filter the Project Outline view to quickly locate specific targets in large projects. This is especially useful when working with projects that generate dozens of targets across multiple subdirectories.\n\nFor example, you might filter targets by a feature area or naming convention.\n\nBookmark commonly used CMake targets\nCMake Tools now supports bookmarking commonly used targets so they appear in a dedicated Bookmarks section in the CMake sidebar. This provides quick access to targets you build, debug, or run most often, without having to repeatedly search through the full project hierarchy.\nTo bookmark a target, navigate to the desired target in the Project Outline view and select Toggle Bookmark.\n\nThis will add the selected target to the separate Bookmarks section in the CMake sidebar. From here, commonly used targets can be built, debugged, or ran in the terminal.\nImproved CTest failure output.\nThis release also improves the CTest experience by adding support for configurable failure patterns. With the new Failure Patterns setting, you can tell CMake Tools how to interpret test output so failures surface more useful and structured information.\n\nThis is particularly helpful for test frameworks where important failure details, such as diffs or assertion outputs, are embedded in test logs. Instead of manually digging through raw output, you can define patterns that extract and highlight the relevant information directly after a test fails.\nFor example, you can define how CHECK_EQUAL shows diffing straight to the user.\nThis allows the user to have a transparent view of their test failures and quickly debug any test output.\n\nWhat do you think?\nDownload Visual Studio Code and our C++ extensions (CMake Tools and C/C++) and let us know what you think. We would love to see what you contribute to our repo. Please create an issue if there’s anything you’d like to see and upvote/downvote any existing issues. Comment below or reach us via email at visualcpp@microsoft.com, via X at , or via Bluesky at @msftcpp.bsky.social.\n \n \nThe post Visual Studio Code CMake Tools 1.22: Target bookmarks and better CTest output appeared first on C++ Team Blog.",
        "dc:creator": "Sinem Akinci",
        "comments": "https://devblogs.microsoft.com/cppblog/visual-studio-code-cmake-tools-1-22-target-bookmarks-and-better-ctest-output/#respond",
        "content": "<p>We&#8217;re excited to announce the latest 1.22 release of the CMake Tools extension for Visual Studio Code. This update brings a host of new additions, including project outline updates for filtering and bookmarking CMake targets in large CMake projects and expanded CTest support to customize this output. To view the full list of updates with [&#8230;]</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/cppblog/visual-studio-code-cmake-tools-1-22-target-bookmarks-and-better-ctest-output/\">Visual Studio Code CMake Tools 1.22: Target bookmarks and better CTest output</a> appeared first on <a href=\"https://devblogs.microsoft.com/cppblog\">C++ Team Blog</a>.</p>\n",
        "contentSnippet": "We’re excited to announce the latest 1.22 release of the CMake Tools extension for Visual Studio Code. This update brings a host of new additions, including project outline updates for filtering and bookmarking CMake targets in large CMake projects and expanded CTest support to customize this output. To view the full list of updates with […]\nThe post Visual Studio Code CMake Tools 1.22: Target bookmarks and better CTest output appeared first on C++ Team Blog.",
        "guid": "https://devblogs.microsoft.com/cppblog/?p=36235",
        "categories": [
          "C++",
          "CMake",
          "Visual Studio Code"
        ],
        "isoDate": "2026-01-27T16:36:46.000Z"
      }
    ]
  },
  {
    "name": "Facebook Engineering",
    "category": "기업",
    "posts": [
      {
        "creator": "",
        "title": "Rust at Scale: An Added Layer of Security for WhatsApp",
        "link": "https://engineering.fb.com/2026/01/27/security/rust-at-scale-security-whatsapp/",
        "pubDate": "Tue, 27 Jan 2026 15:00:09 +0000",
        "content:encodedSnippet": "WhatsApp has adopted and rolled out a new layer of security for users – built with Rust – as part of its effort to harden defenses against malware threats.\nWhatsApp’s experience creating and distributing our media consistency library in Rust to billions of devices and browsers proves Rust is production ready at a global scale.\nOur Media Handling Strategy\nWhatsApp provides default end-to-end encryption for over 3 billion people to message securely each and every day. Online security is an adversarial space, and to continue ensuring users can keep messaging securely, we’re constantly adapting and evolving our strategy against cyber-security threats – all while supporting the WhatsApp infrastructure to help people connect. \nFor example, WhatsApp, like many other applications, allows users to share media and other types of documents. WhatsApp helps protect users by warning about dangerous attachments like APKs, yet rare and sophisticated malware could be hidden within a seemingly benign file like an image or video. These maliciously crafted files might target unpatched vulnerabilities in the operating system, libraries distributed by the operating system, or the application itself.\nTo help protect against such potential threads, WhatsApp is increasingly using the Rust programming language, including in our media sharing functionality. Rust is a memory safe language offering numerous security benefits. We believe that this is the largest rollout globally of any library written in Rust.\nTo help explain why and how we rolled this out, we should first look back at a key OS-level vulnerability that sent an important signal to WhatsApp around hardening media-sharing defenses.\n2015 Android Vulnerability: A Wake-up Call for Media File Protections\nIn 2015, Android devices, and the applications that ran on them, became vulnerable to the “Stagefright” vulnerability. The bug lay in the processing of media files by operating system-provided libraries, so WhatsApp and other applications could not patch the underlying vulnerability. Because it could often take months for people to update to the latest version of their software, we set out to find solutions that would keep WhatsApp users safe, even in the event of an operating system vulnerability. \nAt that time, we realized that a cross-platform C++ library already developed by WhatsApp to send and consistently format MP4 files (called “wamedia”) could be modified to detect files which do not adhere to the MP4 standard and might trigger bugs in a vulnerable OS library on the receiver side – hence putting a target’s security at risk. We rolled out this check and were able to protect WhatsApp users from the Stagefright vulnerability much more rapidly than by depending on users to update the OS itself.\nBut because media checks run automatically on download and process untrusted inputs, we identified early on that wamedia was a prime candidate for using a memory safe language. \n\nOur Solution: Rust at Scale\nRather than an incremental rewrite, we developed the Rust version of wamedia in parallel with the original C++ version. We used differential fuzzing and extensive integration and unit tests to ensure compatibility between the two implementations.\nTwo major hurdles were the initial binary size increase due to bringing in the Rust standard library and the build system support required for the diverse platforms supported by WhatsApp. WhatsApp made a long-term bet to build that support. In the end, we replaced 160,000 lines of C++ (excluding tests) with 90,000 lines of Rust (including tests). The Rust version showed performance and runtime memory usage advantages over the C++. Given this success, Rust was fully rolled out to all WhatsApp users and many platforms: Android, iOS, Mac, Web, Wearables, and more. With this positive evidence in hand, memory safe languages will play an ever increasing part in WhatsApp’s overall approach to application and user security.\nOver time, we’ve added more checks for non-conformant structures within certain file types to help protect downstream libraries from parser differential exploit attempts. Additionally, we check higher risk file types, even if structurally conformant, for risk indicators. For instance, PDFs are often a vehicle for malware, and more specifically, the presence of embedded files and scripting elements within a PDF further raise risks. We also detect when one file type masquerades as another, through a spoofed extension or MIME type. Finally, we uniformly flag known dangerous file types, such as executables or applications, for special handling in the application UX. Altogether, we call this ensemble of checks “Kaleidoscope.” This system protects people on WhatsApp from potentially malicious unofficial clients and attachments. Although format checks will not stop every attack, this layer of defense helps mitigate many of them.\nEach month, these libraries are distributed to billions of phones, laptops, desktops, watches, and browsers running on multiple operating systems for people on WhatsApp, Messenger, and Instagram. This is the largest ever deployment of Rust code to a diverse set of end-user platforms and products that we are aware of. Our experience speaks to the production-readiness and unique value proposition of Rust on the client-side.\nHow Rust Fits In To WhatsApp’s Approach to App Security\nThis is just one example of WhatsApp’s many investments in security. It’s why we built default end-to-end encryption for personal messages and calls, offer end-to-end encrypted backups, and use key transparency technology to verify a secure connection, provide additional calling protections, and more.\nWhatsApp has a strong track record of being loud when we find issues and working to hold bad actors accountable. For example, WhatsApp reports CVEs for important issues we find in our applications, even if we do not find evidence of exploitation. We do this to give people on WhatsApp the best chance of protecting themselves by seeing a security advisory and updating quickly.\nTo ensure application security, we first must identify and quantify the sources of risk. We do this through internal and external audits like NCC Group’s public assessment of WhatsApp’s end-to-end encrypted backups, fuzzing, static analysis, supply chain management, and automated attack surface analysis. We also recently expanded our Bug Bounty program to introduce the WhatsApp Research Proxy – a tool that makes research into WhatsApp’s network protocol more effective.\nNext, we reduce the identified risk. Like many others in the industry, we found that the majority of the high severity vulnerabilities we published were due to memory safety issues in code written in the C and C++ programming languages. To combat this we invest in three parallel strategies:\nDesign the product to minimize unnecessary attack surface exposure.\nInvest in security assurance for the remaining C and C++ code.\nDefault the choice of memory safe languages, and not C and C++, for new code.\nWhatsApp has added protections like CFI, hardened memory allocators, safer buffer handling APIs, and more. C and C++ developers have specialized security training, development guidelines, and automated security analysis on their changes. We also have strict SLAs for fixing issues uncovered by the risk identification process.\nAccelerating Rust Adoption to Enhance Security\nRust enabled WhatsApp’s security team to develop a secure, high performance, cross-platform library to ensure media shared on the platform is consistent and safe across devices. This is an important step forward in adding additional security behind the scenes for users and part of our ongoing defense-in-depth approach. Security teams at WhatsApp and Meta are highlighting opportunities for high impact adoption of Rust to interested teams, and we anticipate accelerating adoption of Rust over the coming years. \nThe post Rust at Scale: An Added Layer of Security for WhatsApp appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>WhatsApp has adopted and rolled out a new layer of security for users – built with Rust – as part of its effort to harden defenses against malware threats. WhatsApp’s experience creating and distributing our media consistency library in Rust to billions of devices and browsers proves Rust is production ready at a global scale. [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2026/01/27/security/rust-at-scale-security-whatsapp/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2026/01/27/security/rust-at-scale-security-whatsapp/\">Rust at Scale: An Added Layer of Security for WhatsApp</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "WhatsApp has adopted and rolled out a new layer of security for users – built with Rust – as part of its effort to harden defenses against malware threats. WhatsApp’s experience creating and distributing our media consistency library in Rust to billions of devices and browsers proves Rust is production ready at a global scale. [...]\nRead More...\nThe post Rust at Scale: An Added Layer of Security for WhatsApp appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=23541",
        "categories": [
          "Security & Privacy",
          "WhatsApp"
        ],
        "isoDate": "2026-01-27T15:00:09.000Z"
      }
    ]
  },
  {
    "name": "eBay Tech Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Twitter Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Netflix TechBlog",
    "category": "기업",
    "posts": [
      {
        "creator": "Netflix Technology Blog",
        "title": "The AI Evolution of Graph Search at Netflix",
        "link": "https://netflixtechblog.com/the-ai-evolution-of-graph-search-at-netflix-d416ec5b1151?source=rss----2615bd06b42e---4",
        "pubDate": "Mon, 26 Jan 2026 19:01:27 GMT",
        "content:encodedSnippet": "The AI Evolution of Graph Search at Netflix: From Structured Queries to Natural Language\nBy Alex Hutter and Bartosz Balukiewicz\nOur previous blog posts (part 1, part 2, part 3) detailed how Netflix’s Graph Search platform addresses the challenges of searching across federated data sets within Netflix’s enterprise ecosystem. Although highly scalable and easy to configure, it still relies on a structured query language for input. Natural language based search has been possible for some time, but the level of effort required was high. The emergence of readily-available AI, specifically Large Language Models (LLMs), has created new opportunities to integrate AI search features, with a smaller investment and improved accuracy.\nWhile Text-to-Query and Text-to-SQL are established problems, the complexity of distributed Graph Search data in the GraphQL ecosystem necessitates innovative solutions. This is the first in a three-part series where we will detail our journey: how we implemented these solutions, evaluated their performance, and ultimately evolved them into a self-managed platform.\nThe Need for Intuitive Search: Addressing Business and Product Demands\nNatural language search is the ability to use everyday language to retrieve information as opposed to complex, structured query languages like the Graph Search Filter Domain Specific Language (DSL). When users interact with 100’s of various UIs within the suite of Content and Business Products applications, a frequent task is filtering a data table like the one below:\nExample Content and Business Products application view\nIdeally, a user simply wants to satisfy a query like “I want to see all movies from the 90s about robots from the US.” Because the underlying platform operates on the Graph Search Filter DSL, the application acts as an intermediary. Users input their requirements through UI elements — toggling facets or using query builders — and the system programmatically converts these interactions into a valid DSL query to filter the data.\nThe Complexity of filtering and DSL generation\nThis process presents a few issues.\nToday, many applications have bespoke components for collecting user input — the experience varies across them and they have inconsistent support for the DSL. Users need to “learn” how to use each application to achieve their goals.\nAdditionally, some domains have hundreds of fields in an index that could be faceted or filtered by. A subject matter expert (SME) may know exactly what they want to accomplish, but be bottlenecked by the inefficient pace of filling out a large scale UI form and translating their questions in order to encode it in a representation Graph Search needs.\nMost importantly, users think and operate using natural language, not technical constructs like query builders, components, or DSLs. By requiring them to switch contexts, we introduce friction that slows them down or even prevents their progress.\nWith readily-available AI components, our users can now interact with our systems through natural language. The challenge now is to make sure our offering, searching Netflix’s complex enterprise state with natural language, is an intuitive and trustworthy experience.\nNatural language queries translated into Graph Search Filter DSL\nWe’ve made a decision to pursue generating Graph Search Filter statements from natural language to meet this need. Our intention is to augment and not replace existing applications with retrieval augmented generation (RAG), providing tooling and capabilities so that applications in our ecosystem have newly accessible means of processing and presenting their data in their distinct domain flavours. It should be noted that all the work here has direct application to building a RAG system on top of Graph Search in the future.\nUnder the Hood: Our Approach to Text-to-Query\nThe core function of the text-to-query process is converting a user’s (often ambiguous) natural language question into a structured query. We primarily achieve this through the use of an LLM.\nBefore we dive deeper, let’s quickly revisit the structure of Graph Search Filter DSL. Each Graph Search index is defined by a GraphQL query, made up of a collection of fields. Each field has a type e.g. boolean, string, and some have their permitted values governed by controlled vocabularies — a standardized and governed list of values (like an enumeration, or a foreign key). The names of those fields can be used to construct expressions using comparison (e.g. > or ==) or inclusion/exclusion operators (e.g. IN). In turn those expressions can be combined using logical operators (e.g. AND) to construct complex statements.\nGraph Search Filter DSL\nWith that understanding, we can now more rigorously define the conversion process. We need the LLM to generate a Graph Search Filter DSL statement that is syntactically, semantically, and pragmatically correct.\nSyntactic correctness is easy — does it parse? To be syntactically correct, the generated statement must be well formed i.e. follow the grammar of the Graph Search Filter DSL.\nSemantic correctness adds some additional complexity as it requires more knowledge of the index itself. To be semantically correct:\n\nit must respect the field types i.e. only use comparisons that make sense given the underlying type;\nit must only use fields that are actually present in the index, i.e. does not hallucinate;\nwhen the values of a field are constrained to a controlled vocabulary, any comparison must only use values from that controlled vocabulary.\n\nPragmatic correctness is much more difficult. It asks the question: does the generated filter actually capture the intent of the user’s query?\nThe following sections will detail how we pre-process the user’s question to create appropriate context for the instructions that we will provide to the LLM — both of which are fundamental to LLM interaction — as well as post-processing we perform on the generated statement to validate it, and help users understand and trust the results they receive.\nAt a high level that process looks like this:\nGraph Search FIlter DSL generation process\nContext Engineering\nPreparation for the filter generation task is predominantly engineering the appropriate context. The LLM will need access to the fields of an index and their metadata in order to construct semantically correct filters. As the indices are defined by GraphQL queries, we can use the type information from the GraphQL schema to derive much of the required information. For some fields, there is additional information we can provide beyond what’s available in the schema as well, in particular permissible values that pull from controlled vocabularies.\nEach field in the index is associated with metadata as seen below, and that metadata is provided as part of the context.\nGraph Search index representation\nThe field is derived from the document path as characterized by the GraphQL query.\nThe description is the comment from the GraphQL schema for the field.\nThe type is derived from the GraphQL schema for the field e.g. Boolean, String, enum. We also support an additional controlled vocabulary type we will discuss more of shortly.\nThe valid values are derived from enum values for the enum type or from a controlled vocabulary as we will now discuss.\n\nA controlled vocabulary is a specific field type that consists of a finite set of allowed values, which are defined by a SMEs or domain owners. Index fields can be associated with a particular controlled vocabulary, e.g. countries with members such as Spain and Thailand, and any usage of that field within a generated statement must refer to values from that vocabulary.\nNaively providing all the metadata as context to the LLM worked for simple cases but did not scale. Some indices have hundreds of fields and some controlled vocabularies have thousands of valid values. Providing all of those, especially the controlled vocabulary values and their accompanying metadata, expands the context; this proportionally increases latency and decreases the correctness of generated filter statements. Not providing the values wasn’t an option as we needed to ground the LLMs generated statements- without them, the LLM would frequently hallucinate values that did not exist.\nCurating the context to an appropriate subset was a problem we addressed using the well known RAG pattern.\nField RAG\nAs mentioned previously, some indices have hundreds of fields, however, most user’s questions typically refer only to a handful of them. If there was no cost in including them all, we would, but as mentioned prior, there is a cost in terms of the latency of query generation as well as the correctness of the generated query (e.g. needle-in-the-hackstack problem) and non-deterministic results.\nTo determine which subset of fields to include in the context, we “match” them against the intent of the user’s question.\n\nEmbeddings are created for index fields and their metadata (name, description, type) and are indexed in a vector store\nAt filter generation time, the user’s question is chunked with an overlapping strategy. For each chunk, we perform a vector search to identify the top K most relevant values and the fields to which they belong.\nDeduplication: The top K fields from each chunk are both consolidated and deduplicated before being provided as context to the system instructions.\nField RAG process (chunking, merge, deduplicate)\nControlled Vocabularies RAG\nIndex fields of the controlled vocabulary type are associated with a particular controlled vocabulary, again, countries are one example. Given a user’s question, we can infer whether or not it refers to values of a particular controlled vocabulary. In turn, by knowing which controlled vocabulary values are present, we can identify additional, related index fields that should be included in the context that may not have been identified by the field RAG step.\nEach controlled vocabulary value has:\n\na unique identifier within its type;\na human readable display name;\na description of the value;\nalso-known-as values or AKA display names, e.g. “romcom” for “Romantic Comedy”.\n\nTo determine which subset of values to include in the context for controlled vocabulary fields (and also possibly infer additional fields), we “match” them against the user’s question.\n\nEmbeddings are created for controlled vocabulary values and their metadata, and these are indexed in a vector store. The controlled vocabularies are available via GraphQL and are regularly fetched and reindexed so this system stays up to date with any changes in the domain.\nAt filter generation time, the user’s question is chunked. For each chunk, we perform a vector search to identify the top K most relevant values (but only for the controlled vocabularies that are associated with fields in the index)\nThe top K values from each chunk are deduplicated by their controlled vocabulary type. The associated field definition is then injected into the context along with the matched values.\nControlled Vocabularies RAG\nCombining both approaches, the RAG of fields and controlled vocabularies, we end up with the solution that each input question resolves in available and matched fields and values:\nField and CV RAG\nThe quality of results generated by the RAG tool can be significantly enhanced by tuning its various parameters, or “levers.” These include strategies for reranking, chunking, and the selection of different embedding generation models. The careful and systematic evaluation of these factors will be the focus of the subsequent parts of this series.\nThe Instructions\nOnce the context is constructed, it is provided to the LLM with a set of instructions and the user’s question. The instructions can be summarised as follows: “Given a natural language question, generate a syntactically, semantically, and pragmatically correct filter statement given the availability of the following index fields and their metadata.”\n\nIn order to generate a syntactically correct filter statement, the instructions include the syntax rules of the DSL.\nIn order to generate a semantically correct filter statement, the instructions tell the LLM to ground the generated statement in the provided context.\nIn order to generate a pragmatically correct filter statement, so far we focus on better context engineering to ensure that only the most relevant fields and values are provided. We haven’t identified any instructions that make the LLM just “do better” at this aspect of the task.\nGraph Search Filter DSL generation\nAfter the filter statement is generated by the LLM, we deterministically validate it prior to returning the values to the user.\nValidation\nSyntactic Correctness\nSyntactic correctness ensures the LLM output is a parsable filter statement. We utilize an Abstract Syntax Tree (AST) parser built for our custom DSL. If the generated string fails to parse into a valid AST, we know immediately that the query is malformed and there is a fundamental issue with the generation.\nThe other approach to solve this problem could be using the structured outputs modes provided by some LLMs. However, our initial evaluation yielded mixed results, as the custom DSL is not natively supported and requires further work.\nSemantic Correctness\nDespite careful context engineering using the RAG pattern, the LLM sometimes hallucinates both fields and available values in the generated filter statement. The most straightforward way of preventing this phenomenon is validating the generated filters against available index metadata. This approach does not impact the overall latency of the system, as we are already working with an AST of the filter statement, and the metadata is freely available from the context engineering stage.\nDSL verification & hallucinations\nIf a hallucination is detected it can be returned as an error to a user, indicating the need to refine the query, or can be provided back to the LLM in the form of a feedback loop for self correction.\nThis increases the filter generation time, so should be used cautiously with a limited number of retries.\nBuilding Confidence\nYou probably noticed we are not validating the generated filter for pragmatic correctness. That task is the hardest challenge: The filter parses (syntactic) and uses real fields (semantic), but is it what the user meant? When a user searches for “Dark”, do they mean the specific German sci-fi series Dark, or are they browsing for the mood category “dark TV shows”?\nThe gap between what a user intended and the generated filter statement is often caused by ambiguity. Ambiguity stems from the compression of natural language. A user says “German time-travel mystery with the missing boy and the cave” but the index contains discrete metadata fields like releaseYear, genreTags, and synopsisKeywords.\nHow do we ensure users aren’t inadvertently led to wrong answers or to answers for questions they didn’t ask?\nShowing Our Work\nOne way we are handling ambiguity is by showing our work. We visualise the generated filters in the UI in a user-friendly way allowing them to very clearly see if the answer we’re returning is what they were looking for so they can trust the results..\nWe cannot show a raw DSL string (e.g., origin.country == ‘Germany’ AND genre.tags CONTAINS ‘Time Travel’ AND synopsisKeywords LIKE ‘*cave*’) to a non-technical user. Instead, we reflect its underlying AST into UI components.\nAfter the LLM generates a filter statement, we parse it into an AST, and then map that AST to the existing “Chips” and “Facets” in our UI (see below). If the LLM generates a filter for origin.country == ‘Germany’, the user sees the “Country” dropdown pre-selected to “Germany.” This gives users immediate visual feedback and the ability to easily fine-tune the query using standard UI controls when the results need improvement or further experimentation.\nGenerated filters visualisation\nExplicit Entity Selection\nAnother strategy we’ve developed to remove ambiguity happens at query time. We give users the ability to constrain their input to refer to known entities using “@mentions”. Similar to Slack, typing @ lets them search for entities directly from our specialized UI Graph Search component, giving them easy access to multiple controlled vocabularies (plus other identifying metadata like launch year) to feel confident they’re choosing the entity they intend.\nIf a user types, “When was @dark produced”, we explicitly know they are referring to the Series controlled vocabulary, allowing us to bypass the RAG inference step and hard-code that context, significantly increasing pragmatic correctness (and building user trust in the process).\nExample @mentions usage in the UI\nEnd-to-end architecture\nAs mentioned previously, the solution architecture is divided into pre-processing, filter statement generation, and then post-processing stages. The pre-processing handles context building and involves a RAG pattern for similarity search, while the post-processing validation stage checks the correctness of the LLM-generated filter statements and provides visibility into the results for end users. This design strategically balances LLM involvement with more deterministic strategies.\nEnd-to-end architecture\nThe end-to-end process is as follows:\n\nA user’s natural language question (with optional `@mentions` statements) are provided as input, along with the Graph Search index context\nThe context is scoped by using the RAG pattern on both fields and possible values\nThe pre-processed context and the question are fed into the LLM with an instruction asking for a syntactically and semantically correct filter statement\nThe generated filer statement DSL is verified and checked for hallucinations\nThe final response contains the related AST in order to build “Chips” and “Facets”\n\nSummary\nBy combining our existing Graph Search infrastructure with the power and flexibility of LLMs, we’ve bridged the gap between complex filter statements and user intent. We moved from requiring users to speak our language (DSL) to our systems understanding theirs.\nThe initial challenge for our users was successfully addressed. However, our next steps involve transforming this system into a comprehensive and expandable platform, rigorously evaluating its performance in a live production environment, and expanding its capabilities to support GraphQL-first user interfaces. These topics, and others, will be the focus of the subsequent installments in this series. Be sure to follow along!\nYou may have noticed that we have a lot more to do on this project, including named entity recognition and extraction, intent detection so we can route questions to the appropriate indices, and query rewriting among others. If this kind of work interests you, reach out! We’re hiring in our Warsaw office, check for open roles here.\nCredits\nSpecial thanks to Alejandro Quesada, Yevgeniya Li, Dmytro Kyrii, Razvan-Gabriel Gatea, Orif Milod, Michal Krol, Jeff Balis, Charles Zhao, Shilpa Motukuri, Shervine Amidi, Alex Borysov, Mike Azar, Bernardo Gomez Palacio, Haoyuan He, Eduardo Ramirez, Cynthia Xie.\n\nThe AI Evolution of Graph Search at Netflix was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Netflix Technology Blog",
        "guid": "https://medium.com/p/d416ec5b1151",
        "categories": [
          "search-engines",
          "graphql",
          "software-engineering",
          "ai",
          "llm"
        ],
        "isoDate": "2026-01-26T19:01:27.000Z"
      }
    ]
  },
  {
    "name": "JetBrains: Developer Tools for Professionals and Teams – Company Blog | JetBrains",
    "category": "기업",
    "posts": [
      {
        "creator": "Sasha Ivanova",
        "title": "ReSharper and Rider 2025.3.2 Updates Out Now!",
        "link": "https://blog.jetbrains.com/dotnet/2026/01/29/resharper-and-rider-2025-3-2/",
        "pubDate": "Thu, 29 Jan 2026 15:44:43 +0000",
        "content:encodedSnippet": "We’ve just released another update for ReSharper, Rider and the .NET tools version 2025.3. You can update to this version from inside those tools, using the Toolbox app or our website. \nLet’s take a look at the latest updates. \nRider 2025.3.2\nAgent Client Protocol (ACP) agent registry support\nYou can now browse and install ACP-compatible AI agents directly from a curated registry in the AI Chat tool window. ACP defines a standard protocol for AI coding agents, letting Rider connect to any compatible agent without custom integrations. Users can add agents from the registry and use them immediately in AI Chat.\nMore information in this blog post. \nNotable fixes\nPlugin suggestions based on project dependencies are shown again. [RIDER-134557]\nGo to Implementation and Ctrl+Click navigation no longer scrolls the editor to an unexpected position after opening the target file. [RIDER-133722]\nRemove (Ctrl+X) in the Find tool window now removes the selected entry from the results list again. [RIDER-132377]\nInitializeComponent calls in .NET MAUI projects are no longer incorrectly flagged as errors by code analysis. [RSRP-502255]\n.NET MAUI apps can now be deployed and run on physical Android devices when a RuntimeIdentifier is explicitly specified in the project file. [RIDER-133803]\nSolutions with multi-targeted .NET projects (e.g. MAUI projects targeting both platform-specific and plain .NET frameworks) now build correctly from a clean state, including projects that reference them. [RIDER-132476]\nHot Reload now works correctly in projects that use the C# interceptors feature. [RIDER-126990] \nHot Reload now works correctly for Blazor WebAssembly standalone projects targeting .NET 10. [RIDER-133277]\nUnreal Engine code vision inlays are now vertically centered relative to the line they annotate, restoring correct alignment in the editor. [RIDER-132075]\nSolution-Wide Error Analysis now reuses precalculated diagnostics from document analysis, reducing update latency while typing. [RIDER-133684]\nThe ConstantExpected inspection now respects the [ConstantExpected] attribute transitively and no longer raises warnings when values are passed through annotated parameters. [RSRP-502500]\nThe OutParameterValuesAlwaysDiscarded inspection no longer reports false positives for out parameters used in C# 14 extension member syntax. [RSRP-502536]\nRecursive calls are now correctly indicated in the gutter for C# extension members. [RSRP-502225]\nFor the complete list of issues resolved, please see our issue tracker. \nDownload Rider 2025.3.2\n                                                    \nReSharper 2025.3.2\nThese are the most notable fixes included in this update:\nInitializeComponent calls in .NET MAUI projects are no longer incorrectly flagged as errors by code analysis. [RSRP-502255]\nThe ConstantExpected inspection now respects the [ConstantExpected] attribute transitively and no longer raises warnings when values are passed through annotated parameters. [RSRP-502500]\nThe OutParameterValuesAlwaysDiscarded inspection no longer reports false positives for out parameters used in C# 14 extension member syntax. [RSRP-502536]\nRecursive calls are now correctly indicated in the gutter for C# extension members. [RSRP-502225]\nThe bundled Unity plugin now works correctly in ReSharper Command Line Tools. [RIDER-134769]\nFixed an issue that could prevent memory profiling from starting on systems where the Microsoft-Windows-RPC ETW provider is already in use (for example, by security software).\nFor the full list of resolved issues included in this build, please refer to our issue tracker.\nDownload ReSharper 2025.3.2\n                                                    \nYou can download the latest builds from our website (Rider, ReSharper) or via the Toolbox App. You can also update Rider as a snap.\nThis is our last update of the year, and we want to close with a simple message: thank you. From the .NET team at JetBrains, we wish you a happy, warm, and safe holiday season. We’ll see you in the new year!",
        "dc:creator": "Sasha Ivanova",
        "content": "We’ve just released another update for ReSharper, Rider and the .NET tools version 2025.3. You can update to this version from inside those tools, using the Toolbox app or our website.&#160; Let’s take a look at the latest updates.&#160; Rider 2025.3.2 Agent Client Protocol (ACP) agent registry support You can now browse and install ACP-compatible [&#8230;]",
        "contentSnippet": "We’ve just released another update for ReSharper, Rider and the .NET tools version 2025.3. You can update to this version from inside those tools, using the Toolbox app or our website.  Let’s take a look at the latest updates.  Rider 2025.3.2 Agent Client Protocol (ACP) agent registry support You can now browse and install ACP-compatible […]",
        "guid": "https://blog.jetbrains.com/?post_type=dotnet&p=677156",
        "categories": [
          "net-tools",
          "releases",
          "resharper",
          "rider"
        ],
        "isoDate": "2026-01-29T15:44:43.000Z"
      },
      {
        "creator": "Anna Ruban",
        "title": "Game Dev in 2025: Excerpts From the State of Game Development Report",
        "link": "https://blog.jetbrains.com/dotnet/2026/01/29/game-dev-in-2025-excerpts-from-the-state-of-game-development-report/",
        "pubDate": "Thu, 29 Jan 2026 10:00:00 +0000",
        "content:encodedSnippet": "As we approach the midpoint of the decade, game developers face an evolving landscape shaped by shifting job security, technology choices, platform strategies, and practical AI adoption. Our State of Game Development 2025 report reveals critical insights that provide a clearer picture of where the industry is heading.\nExplore the report\n                                                    \nLayoffs and job security concerns\nThe game development sector has experienced significant turbulence. More than half of industry professionals reported experiencing layoffs within their organizations. This figure is notably higher than in previous years. Job security has sharply declined, placing game developers among the least confident in the tech industry. Studios may need to revisit their organizational strategies, hiring practices, and retention policies to foster stability.\n\n\n\n\nEngine preferences: Unity still leads, but Godot is rising\nUnity remains dominant among indie and mid-sized studios due to its versatility and ease of use. Unreal Engine remains a strong competitor, especially for graphics-intensive projects. However, the most notable trend is the rapid growth of Godot, an open-source engine that has become increasingly popular among indie developers and hobbyists for its flexibility, openness, and community-driven nature.\n\n\n\n\n\n\n\n\n\nPlatform priorities: Mobile and desktop dominate\nIndie developers maintain their focus primarily on mobile and desktop platforms, with Android being the most targeted, closely followed by Windows and iOS. This aligns with the industry’s shift toward platforms that offer wider user bases and easier market entry points compared to traditional consoles.\n\n\n\n\nIDEs and developer tools: JetBrains Rider emerges as a favorite\nIn 2025, JetBrains Rider emerged as the top IDE for indie developers, surpassing both Visual Studio and VS Code. Rider’s robust feature set and seamless workflow integration have made it the preferred tool for daily coding.\n\n\n\n\nPractical AI adoption: From experimental to essential\nAI technology has moved beyond experimentation to become a standard part of game development workflows. Nearly half of developers regularly use AI for feature implementation, and many leverage it for streamlined code reviews. ChatGPT, GitHub Copilot, and JetBrains AI Assistant have become staples in developers’ toolkits, while Junie, JetBrains’ AI coding agent released in April 2025, is quickly gaining popularity. AAA studios show a strong interest in incorporating AI coding agents into their pipelines.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck out the full report to dive deeper into these game-changing trends  and leverage our data to benchmark your studio’s strategies and decisions against current industry standards.\nExplore the report",
        "dc:creator": "Anna Ruban",
        "content": "As we approach the midpoint of the decade, game developers face an evolving landscape shaped by shifting job security, technology choices, platform strategies, and practical AI adoption. Our State of Game Development 2025 report reveals critical insights that provide a clearer picture of where the industry is heading. Layoffs and job security concerns The game [&#8230;]",
        "contentSnippet": "As we approach the midpoint of the decade, game developers face an evolving landscape shaped by shifting job security, technology choices, platform strategies, and practical AI adoption. Our State of Game Development 2025 report reveals critical insights that provide a clearer picture of where the industry is heading. Layoffs and job security concerns The game […]",
        "guid": "https://blog.jetbrains.com/?post_type=dotnet&p=675222",
        "categories": [
          "net-tools",
          "ai",
          "research",
          "teamcity-2",
          "game-development",
          "gamedev",
          "report",
          "rider"
        ],
        "isoDate": "2026-01-29T10:00:00.000Z"
      },
      {
        "creator": "Jan-Niklas Wortmann",
        "title": "ACP Agent Registry Is Live: Find and Connect AI Coding Agents in Your JetBrains IDE",
        "link": "https://blog.jetbrains.com/ai/2026/01/acp-agent-registry/",
        "pubDate": "Wed, 28 Jan 2026 14:56:05 +0000",
        "content:encodedSnippet": "AI coding agents are multiplying fast. Some of the most common ones include Gemini CLI, Claude Code, Auggie, OpenCode, and Copilot, and more are being released every day. Each comes with its own unique strengths, specific setups, and varying levels of editor support. Keeping track of what’s out there, let alone getting it running in your IDE, hasn’t been easy.\nWell, until now, that is.\nTogether with Zed (Zed’s announcement), we’ve launched the official ACP Agent Registry: a directory of AI coding agents, integrated directly into JetBrains IDEs and Zed. Browse what’s available, click Install, and start working right away. This beta release is just the beginning.\nWhat is the ACP (and why does it matter)?\nThe Agent Client Protocol is an open standard that lets any AI coding agent work in any supporting editor. Think of it like the Language Server Protocol, but for AI agents. The LSP lets any editor support any language through a shared standard. The ACP does the same for coding agents. You only need to implement it once, and then it will work in your JetBrains IDE, Zed, or any other editor that supports the protocol.\nThis means you get to pick your preferred agent and editor, and they will then work together seamlessly – no vendor lock-in and no waiting for someone to build a specific integration.\nACP has been, since we started integrating it to Mistral Vibe, a real joy to use: thoughtfully designed from the ground up, community-driven, and evolving rapidly. We’ve found it not only simplifies integration, but also fits our focus on open and flexible tools. It’s really great to see a standard that puts developer choice first.\nMichel Thomazo, Software Engineer @ Mistral AI\nWhat the registry adds\nThe ACP made agent interoperability technically possible. The registry makes it convenient.\nInstead of manually configuring agents, you can now:\nDiscover agents that fit your workflow \nInstall with a single click, no config files required\nSwitch between agents based on the task at hand\nStay current as agents release new versions\nWhat’s in the registry?\nAt launch, you’ll find a wide array of different agents:\nAgent\nDescription\nAuggie CLI\nFull-featured coding assistant optimized for large-scale refactors\nFactory Droid\nSpecialized agent for automated code generation workflows\nGemini CLI\nGoogle’s agent with deep codebase understanding and multimodal capabilities\nGitHub Copilot\nGitHub’s AI pair programmer, now available via the ACP\nMistral Vibe\nLightweight, fast agent built on Mistral’s models\nOpenCode\nCommunity-driven, fully open-source agent\nQwen Code\nAlibaba’s coding agent with strong multilingual support\nInnovation in software agents is moving at an unbelievable pace. The Agent Registry and ACP makes it simple for developers to use the best agents in their favorite tools. \nChris Kelly, Product @ Augment Code\n\nWhy would you want multiple agents?\nIn general, it’s less about having multiple agents than about enabling you to pick and choose the ones that work well in your workflow. Different agents come with different benefits. Some provide a more attractive pricing structure for your business, some provide a user experience that you simply enjoy more than others’, and some embody the ideas of open-source development that just resonate with you.\nThe Agent Client Protocol registry lets you experiment freely. Try a few, see what clicks for your workflow, and then keep the ones that help. You’re not locked into a single vendor’s vision of what AI-assisted development should look like.\nWe’re excited to support the ACP Agent Registry as a step toward a more open agent ecosystem where Droids can integrate seamlessly across all IDEs.\nFrancesca LaBianca, VP of Operations @ Factory\n\nHow to use it\nIn any JetBrains IDE (2025.3.2+) with JetBrains AI (253.30387.147):\nOpen Settings | Tools | AI Assistant | Agents or select  “Install From ACP Registry…” in the agent picker menu\n\nFind an agent that interests you\nClick Install\nThat’s it. The agent is configured and ready to use in the AI Chat tool window.\nQuick note: agents typically come with their own subscription. That’s between you and them. You won’t need a JetBrains AI subscription to use ACP agents.\nWant to try something concrete? Install OpenCode, open a project, and ask it to explain an unfamiliar module. OpenCode also lets you swap between different LLMs, so you can experiment with what works best for you. \nIf you prefer manual configuration, that option is still there, too. Just edit the acp.json directly. This is useful for agents that aren’t in the registry yet or for custom setups.\nFor agent builders: Get listed\nIf you’re building an ACP-compatible agent, the registry is now the fastest way to reach developers across JetBrains IDEs and Zed.\nTo list your agent:\nHead to the ACP Registry repository and check out the CONTRIBUTING.md for the full submission process and metadata requirements. Please note that, for now, we are only featuring agents that support Agent Auth or Terminal Auth. Full details of requirements and conditions can be found here. \nThis is an open registry. If you’re building an ACP-compatible agent, you’re welcome to submit it. The registry exists to serve the ecosystem, not to gatekeep it.\nWhat this means\nFor developers: More choice and zero lock-in. Use any agent you want in the IDE you love. \nFor agent builders: Instant distribution to millions of JetBrains and Zed users. Implement the ACP once and reach everyone.\nFor the ecosystem: Competition on quality, not on who controls the integration. The best agents win because they’re the best, not because they have exclusive deals.\nWe’re building this openly with Zed because we believe AI-assisted development shouldn’t be locked inside any single vendor’s ecosystem. Developers deserve to pick their tools freely.\nThe registry is one more step toward that future.\nTry it today\nThe ACP Registry is available now in JetBrains IDE versions 2025.3 and later. Update your IDE and the JetBrains AI plugin, open Settings, and start exploring.\nHave feedback? Found a bug? The registry repo is open for issues and PRs. And if you’re building something interesting with ACP, we’d love to hear about it!",
        "dc:creator": "Jan-Niklas Wortmann",
        "content": "AI coding agents are multiplying fast. Some of the most common ones include Gemini CLI, Claude Code, Auggie, OpenCode, and Copilot, and more are being released every day. Each comes with its own unique strengths, specific setups, and varying levels of editor support. Keeping track of what&#8217;s out there, let alone getting it running in your [&#8230;]",
        "contentSnippet": "AI coding agents are multiplying fast. Some of the most common ones include Gemini CLI, Claude Code, Auggie, OpenCode, and Copilot, and more are being released every day. Each comes with its own unique strengths, specific setups, and varying levels of editor support. Keeping track of what’s out there, let alone getting it running in your […]",
        "guid": "https://blog.jetbrains.com/?post_type=ai&p=677216",
        "categories": [
          "ai-assistant",
          "news",
          "acp"
        ],
        "isoDate": "2026-01-28T14:56:05.000Z"
      },
      {
        "creator": "Dmitrii Korovin",
        "title": "Vendor Lock-In vs. Strategic Partnership for Your CI/CD",
        "link": "https://blog.jetbrains.com/teamcity/2026/01/vendor-lock-in-vs-strategic-partnership/",
        "pubDate": "Wed, 28 Jan 2026 12:19:21 +0000",
        "content:encodedSnippet": "This article was brought to you by Jakkie Koekemoer & Shweta, draft.dev.\nTraditional wisdom in the dev world is that once you opt for proprietary APIs or vendor-specific tooling, migrating to alternative platforms is costly or impractical to adopt in the future.\nThat’s why engineering teams have considered vendor lock-in a trap. You end up paying more, limiting technical flexibility, losing bargaining power, and cramming your processes into someone else’s roadmap. And if you still want control and flexibility, you need to be on your toes with proactive planning, regular exports, and negotiating strict contractual exit terms.\nBut this binary thinking of lock-in vs. freedom is misleading.\nWhenever you spend significant time investing in any technology or architecture, you’re locked in. It might not seem like it on paper, but your reliance on that system grows the more you use it. Switching to something else requires a significant amount of work, even if there aren’t any other costs involved.\nInstead of trying to escape the unavoidable, consider to what extent any tool or technology meets your needs without enforcing limitations that hold you back.\nIn the context of CI/CD, that means a tool that accelerates delivery without sacrificing control.\nYour dependencies in CI/CD run across integrations, pricing tiers, hosted vs. self-hosted trade-offs, supported SLAs, and ecosystem workflows. A CI/CD vendor that provides open APIs, clear export paths, predictable pricing, and collaborative roadmapping has the potential to become a strategic partner.\nThis guide highlights the potential technical, financial, operational, and strategic risks CI/CD solutions can pose for your organization. Based on this framework, we’ll walk you through a practical questionnaire for evaluating potential solutions against your organization’s needs.\nUnderstanding your organization’s risk profile\nAny CI/CD platform can present you with a list of features, and that’s enough if you only want to choose a product.\nHowever, if you want to make a long-term commitment, you need to stop asking “What features does this solution have?” and instead ask “What risks does this vendor introduce?” By considering risks, you can assess whether a vendor will be a good strategic partner for you in the long term.\nLet’s look at the most important risks a CI/CD platform can introduce for an organization.\nTechnical risks\nThe foundation of any CI/CD platform is its portability. Consider whether the platform locks you into proprietary languages or relies on custom plugins for critical integrations.\nMore importantly, can pipelines, configurations, build artifacts, and logs be exported in a reusable format? If not, you run the risk of significant rework whenever there’s a need to reproduce builds in a different environment, be it a new data center, cloud region, or entirely different infrastructure provider.\nReproducing the same build reliability elsewhere requires substantial rewrites or custom tooling, potentially turning what should be a straightforward migration into a months-long engineering effort.\nFinancial risks\nHidden costs can accumulate quickly if you’re not careful. Consider data-transfer fees that you pay every time your team downloads a large build artifact, storage overages, and unpredictable API pricing when you’re building the financial case for adoption.\nAlso consider what happens when it’s time to leave. Do contracts lock in long-term minimums? Are there punitive exit clauses? A single large migration with consulting fees, rework, and downtime costs can make switching prohibitively expensive.\nOperational risks\nDay-to-day dependence on a vendor creates hidden fragility. If SLAs are slow, outages linger, or documentation falls out of sync with the product, you become increasingly reliant on vendor support just to keep critical systems running.\nOver time, you may end up locking into a specific UX and workflow, making it painful to change platforms even when better alternatives emerge.\nStrategic risks\nA platform that serves your current needs may not support your future growth, so consider alignment. Does the vendor’s product vision match your evolving direction? If the vendor deprioritizes features you rely on, undergoes a strategic shift in pricing or focus, or is acquired by a competitor with different priorities, will you be forced to compromise your own product goals?\nWithout formal channels to influence the vendor’s roadmap, you’re betting that the vendor’s interests will remain aligned with yours, which is a risky assumption over a multiyear relationship.\nEvaluating CI/CD platforms\nThe ideal CI/CD solution reduces risk, accelerates delivery, and empowers you to evolve your pipelines and processes. Which vendor or solution meets these requirements will depend on your organizational requirements and risk factors.\nThe following questionnaire can help you with your evaluation.\nUse the questions and suggested importance (Must, Should, or Nice) as a starting point, but feel free to adapt the questions and importance based on your organization’s needs.\nComplete the questionnaire for each platform or vendor you’re assessing, and rate questions from 0–2:\n0 = not available/unacceptable gap (indicates a real blocker)\n1 = partial/mitigatable (the capability exists but has limits or caveats)\n2 = fully available/meets requirements (the vendor delivers the capability as needed)\n\nThe question and the risk it mitigatesTypeVendor A (0–2)Vendor B (0–2)\nCan you export all configs, artifacts, and logs in usable, open formats?Red flag: Proprietary formats hold your operational data hostage.Must\nAre core APIs open, versioned, and comprehensively documented?Red flag: Private APIs prevent you from building essential custom automations.Must\nIs core functionality built-in or reliant on a third-party plugin ecosystem?Red flag: A plugin-first model creates architectural fragility and a massive, unmanaged security risk.Must\nDoes the vendor offer a self-hosted or portable runner option?Red flag: A single hosting model can lock you into an infrastructure that doesn’t meet your security or cost needs.Must\nIs pricing published, transparent, and predictable as you scale?Red flag: Opaque, usage-based pricing makes budgeting impossible and exposes you to surprise costs.Must\nDoes the contract include a clear support SLA and fair termination terms?Red flag: Without a guaranteed SLA, you have no support in a crisis. Punitive exit clauses are a major lock-in tactic.Must\nDoes the platform provide an intuitive UI that empowers the entire team, not just experts?Red flag: A clunky, expert-only UI creates a “guru” bottleneck and dramatically increases onboarding costs.Should\nCan you easily extract the raw data needed to track performance metrics (like DORA) in your own tools?Red flag: A “data black box” platform prevents you from measuring what matters and achieving data-driven improvement.Should\nIs there an active and transparent customer feedback channel that influences the roadmap?Red flag: A closed roadmap signals that the vendor’s priorities, not yours, will drive the product’s future.Should\nDoes the vendor have a clear and credible vision for incorporating AI-driven capabilities?Red flag: No AI vision signals strategic stagnation. You’re buying into a platform that is already behind the innovation curve.Nice\nDoes the platform have built-in, first-class features for performance (e.g. test parallelization)?Red flag: Lacking built-in performance features means you will spend engineering time building workarounds.Nice\n\n\n\n\n\nScore yourself as follows:\nAdd each vendor’s Must scores (max 11). Multiply the sum by 2 (to factor in its criticality).\nAdd the raw sums for Should (max 8) and Nice (max 6).\nTotal score = (Must sum × 2) + Should sum + Nice sum.\nMax possible = 36. Convert to a percentage if you want.\nInterpret the results as follows:\n>60 percent: Strong candidate (confirm that there are no instances where Must = 0)\n30–60 percent: Proceed with caution\n<30 percent: High risk, don’t commit\nIf any Must row is scored 0 for a vendor, it should be treated as a major deal-breaker, and you ought to remove the vendor from the shortlist. If you’re considering any option that scores poorly on export, automation, or billing transparency, running a short portability proof of concept (POC) is recommended.\nIf you’ve identified any red flags, categorize them to decide on your next steps:\nBlockers: Are there critical gaps that are deal-breakers (for example, no data export or punitive exit clauses)? This should remove the vendor from consideration.\nMajor risks: Are there issues that could be a deal-breaker (for example, API limitations)? Do a POC before making a decision.\nConcerns: Are there any concerning issues that aren’t critical (for example, uncertainty about the product roadmap)? Contact the vendor to see if they can shed some light.\nLastly, remember to escalate anything affecting compliance, SLAs, or costs to SRE, procurement, and/or legal teams.\nDIY vs. integrated CI/CD platforms: Risks, rewards, and decision criteria\nPartnering with a vendor isn’t your only option for a CI/CD solution. You can also build your own.\nA do-it-yourself (DIY) CI/CD gives you full control to tailor every component to niche needs and stitch together the best available tools. This can be useful for unique architectures or strict compliance demands.\nHowever, that control comes with costs. You must own maintenance, upgrades, security, and scaling, and the glue code between tools often becomes brittle technical debt.\n\nDIY prosDIY cons\nYou have full control of every component; optimize for niche requirements.You own maintenance, upgrades, and security for the entire stack.\nChoose best-of-breed tools for each stage (VCS, runner, artifact store).Integration and cross-tool orchestration add complexity and brittle glue code.\nLower vendor dependency; avoids many proprietary lock-in vectors.Hidden TCO from ops time, custom plugins, and scaling pain.\n\n\n\n\n\nChoose DIY if you need extreme customization, have seasoned DevOps capacity, and want full control over cost levers.\nHowever, if you value developer productivity, shorter time to market, and vendor-managed reliability, opt for an integrated solution.\n\nIntegrated platform prosIntegrated platform cons\nUnified UX, fewer integration points, and faster onboarding for engineers.Some feature gaps or vendor-specific behaviors can be costly to reproduce.\nVendor accountability for reliability, scaling, and parts of security.Pricing models and metering can be hard to forecast as usage grows.\nBuilt-in productivity features (e.g. parallelization, flaky test detection, and visual pipelines).Partial lock-in remains unless the vendor supports portability and exports.\n\n\n\n\n\nOther resources\nTwo-thirds of engineers lose 20%+ of their time to inefficiencies. Check out our ROI of Developer Experience blog post to see how TeamCity’s dev-centric CI/CD platform helps reclaim productivity through intuitive UIs, deep VCS integration, and unified monitoring that eliminates context switching.\nThinking about the practical implications of migration? Our Migration Planning Kit for DevOps engineers and engineering managers includes a migration readiness checklist and a phased rollout plan.",
        "dc:creator": "Dmitrii Korovin",
        "content": "This article was brought to you by Jakkie Koekemoer &#38; Shweta, draft.dev. Traditional wisdom in the dev world is that once you opt for proprietary APIs or vendor-specific tooling, migrating to alternative platforms is costly or impractical to adopt in the future. That&#8217;s why engineering teams have considered vendor lock-in a trap. You end up [&#8230;]",
        "contentSnippet": "This article was brought to you by Jakkie Koekemoer & Shweta, draft.dev. Traditional wisdom in the dev world is that once you opt for proprietary APIs or vendor-specific tooling, migrating to alternative platforms is costly or impractical to adopt in the future. That’s why engineering teams have considered vendor lock-in a trap. You end up […]",
        "guid": "https://blog.jetbrains.com/?post_type=teamcity&p=677353",
        "categories": [
          "best-practices",
          "teamcity-2",
          "devopspains",
          "jenkins"
        ],
        "isoDate": "2026-01-28T12:19:21.000Z"
      },
      {
        "creator": "Andrey Belyaev",
        "title": "Spring Data JDBC Made Easy with IntelliJ IDEA",
        "link": "https://blog.jetbrains.com/idea/2026/01/spring-data-jdbc-made-easy-with-intellij-idea/",
        "pubDate": "Wed, 28 Jan 2026 09:59:06 +0000",
        "content:encodedSnippet": "IntelliJ IDEA is well-known for its Spring Framework support. The IDE helps with bean navigation, on-the-go bean injection, Spring Data methods completion, and assistance with generating HTTP requests for controllers. One of the areas that IDEA is good at is dealing with JPA. You, as a developer, can:\nGenerate entities based on DB tables.\nCreate diff scripts for Liquibase and Flyway on model changes.\nCompare two DB versions and generate a diff script.\nBefore IntelliJ IDEA 2025.3, dealing with Spring Data JDBC required more effort. After creating a table or adding a field, you had to update the code manually or invoke an AI. Finally, we introduce first-class support for Spring Data JDBC. This means the IDE now understands your JDBC entities just as well as it understands JPA and provides the same functionality. Whether you prefer creating a DB schema first or building a proper object model, you can leverage IDEA’s features for your work.\nIn this article, we’ll try to answer the most common questions that usually come up in different daily tasks. We’ll use the following domain model containing Courses, Students and Enrollments as an example.\n\n\n\n\ncreate table courses\n(\n   id          int primary key auto_increment,\n   name        varchar(255) not null,\n   description text\n);\n\ncreate table students\n(\n   id    int primary key auto_increment,\n   name  varchar(255) not null,\n   email varchar(255) not null unique\n);\n\ncreate table enrollments\n(\n   student_id  int       not null,\n   course_id   int       not null,\n   enrolled_at timestamp not null,\n   primary key (student_id, course_id),\n   foreign key (student_id) references students (id),\n   foreign key (course_id) references courses (id)\n);\nGetting Started\nQ: How do I connect IntelliJ IDEA to my database for reverse engineering and diff?\nA: Click on the Database tool window → Click on “+” icon → Data Source → MySQL\nProvide database connection properties and click on Test Connection to verify the connectivity. Click OK to connect to your database.\n\n\n\n\nIf you have datasource properties configured in your application.properties/yml file then you can register DataSource by simply clicking on the gutter icon.\n\n\n\n\nOn the other hand, if you have DataSource registered in the Database tool window, then you can simply drag the connection onto the application.properties/yml file to configure the datasource properties.\n\n\n\n\nQ: How does IntelliJ IDEA detect Spring Data JDBC entities and repositories?\nA: IntelliJ IDEA detects classes with @Id (org.springframework.data.annotation.Id) annotated properties as Spring Data JDBC entities. \nAny interface that extends either org.springframework.data.repository.Repository or any of its sub-interfaces(CrudRepository, ListCrudRepository, PagingAndSortingRepository, etc.) will be automatically detected as Spring Data Repositories.\nQ: IDEA does not detect my entity, what should I do?\nA: In case the automatic detection of entities is not working (for example, currently records with @Id properties without @Table annotation are not automatically detected), then you can explicitly mark them as Spring Data JDBC entities by adding @Table annotation on the class/record or by adding them to Mapping Context as follows:\n\n\n\n\nQ: DB-first or code-first: which workflow should I choose for Spring Data JDBC?\nA: From Spring Data Documentation:\nAll Spring Data modules are inspired by the concepts of “repository”, “aggregate”, and “aggregate root” from Domain Driven Design. These are possibly even more important for Spring Data JDBC, because they are, to some extent, contrary to normal practice when working with relational databases.\nWhile using Spring Data JDBC, it is recommended to follow DDD and start with domain models/entities first.\nQ: When should I use IntelliJ IDEA’s built-in Spring Data JDBC features instead of asking an AI to generate code?\nA: You can use AI planning mode to brainstorm the design and use IntelliJ IDEA’s built-in code generation feature to deterministically generate code. \nIf you are following the code-first approach, you can generate the models using AI and use IntelliJ IDEA to generate Flyway/Liquibase migrations from the entities. \nConversely, if you are following the database-first approach, then you can generate Entities from the existing schema deterministically without any AI and its associated cost.\nQ: What are the current limitations vs JPA support (and where does the IDE intentionally stay hands-off)?\nA: IntelliJ IDEA provides all the necessary support for working with Spring Data JPA and Spring Data JDBC. So, it boils down to the comparison of Spring Data JDBC vs Spring Data JPA features.\nIntelliJ IDEA provides support for both Spring Data JPA and Spring Data JDBC with the following:\nGenerating entities from an existing database schema\nInitializing Flyway/Liquibase migrations from entities\nGenerating database migrations from code changes\nSynchronizing database changes into entities\nAutocompletion of Spring Data Repository methods\nOn-demand creation of Spring Data Repository methods\nSchema Evolution and Migration\nQ: How do I generate @Table entities from an existing database schema?\nA: In the Database tool window, right-click on the DataSource connection and select “Create JDBC Entities from DB…”, select the Target package, select the tables you want to generate entities for, and click OK.\n\n\n\n\nQ: How does IntelliJ IDEA handle composite keys during reverse engineering?\nA: While generating Spring Data JDBC entities from the existing schema, for composite keys IntelliJ IDEA generates a class for the ComposeKey and uses @Embedded annotation to define the primary key.\nIn our example, the enrollments table composite key EnrollmentId is generated as follows:\npublic class EnrollmentId {\n   private Integer studentId = 0;\n   private Integer courseId = 0;\n\n   // setters and getters\n}\n\n@Table(name = \"enrollments\")\npublic class Enrollment {\n   @Id\n   @Embedded.Nullable\n   private EnrollmentId id;\n\n   private Instant enrolledAt = Instant.now();\n\n   public Enrollment(@Nullable EnrollmentId id, Instant enrolledAt) {\n       this.id = id;\n       this.enrolledAt = enrolledAt;\n   }\n\n   // setters and getters\n}\nQ: I started with the code-first approach. Can IntelliJ IDEA generate migrations from entities?\nA: Yes. You can generate Flyway or Liquibase Migrations from the entities. Using the entity gutter icon, invoke Flyway/Liquibase Init Migration action, choose the entities and DataSource and generate the migrations.\n\n\n\n\nQ: Can IntelliJ IDEA generate migrations from my code changes?\nA: IntelliJ IDEA can generate Flyway and Liquibase migrations from the code changes (adding new entities, changing or adding entity properties).\n\n\n\n\nQ: What changes should I review manually before applying generated migrations?\nA: In short, ALL. While IntelliJ IDEA detects the database dialect and generates the appropriate database migration scripts, it is better to review before applying.\nYou should also pay attention to the migration change color codes (green, yellow, red) that represent safe, needs attention and dangerous types. The safe operations mostly create new resources(tables, columns, etc) that can be easily reverted. The needs attention type changes alter the schema (rename tables or columns, change datatypes, etc) that could have a significant impact on the application. The dangerous type changes are irreversible, such as dropping a column or table, which should be carefully evaluated and performed with the utmost care.\n\n\n\n\nQ: How does the “Diff & migration scripts” feature handle complex changes, such as renaming a column or changing a data type, between my @Table model and the DB?\nA: When generating migration scripts, IntelliJ IDEA shows the different types of operation in different color codes(green, yellow, red) to represent safe, needs attention and dangerous types.\nFor migrations like renaming a column name or modifying data types, IntelliJ IDEA offers suggestion to rename the column name or update datatype instead of creating a new column and dropping the old column.\n\n\n\n\nWorking with Spring Data\nQ: In DDD, we focus on “Aggregate Roots.” How does IDEA help me stay within those boundaries?\nA: According to DDD principles, an entity should be associated with only one aggregate root. In IntelliJ IDEA, if you try to associate the same entity with multiple aggregates, it will show an inspection warning and provide quick fixes.\n\n\n\n\nQ: How do I create a Spring Data JDBC repository?\nA: You can create Spring Data JDBC Repository by using right-click → Spring Component (Java) → RepositoryName.\n\n\n\n\nProductivity Hacks\nQ: Can I add multiple missing columns to an entity in one go?\nA: You can add the missing columns to an entity by right-clicking on Table → Create Entity Attributes From DB and select all the missing columns.\n\n\n\n\nYou can also add one column at a time using column auto completion support.\n\n\n\n\nQ: How do I quickly create a repository and inject it into a service (end-to-end flow)?\nA: A simpler way is to just start typing the {entityName}Repo and IntelliJ IDEA will show an option to create Repository. It will create the Repository and automatically inject in the current bean. Similarly, you can inject any existing bean and start using it by just start typing the Spring bean name.\n\n\n\n\nQ: How “smart” is the derived method completion?\nA: IntelliJ IDEA provides autocompletion for finder methods based on the field names of entity classes and Spring Data JDBC supported keywords.\n\n\n\n\nYou don’t even need to create the Repository methods ahead of time. Wherever you are using the repository, you can start typing the finder method using the autocomplete feature, and IntelliJ IDEA will create that method automatically in the repository.\n\n\n\n\nConclusion\nFirst-class Spring Data JDBC support in IntelliJ IDEA reduces manual work in everyday JDBC tasks. If you prefer a code-first approach, the IDE helps you keep your aggregates and domain model consistent and can generate migration scripts for Flyway or Liquibase that you can review and apply. If you work DB-first, reverse engineering helps you bring database changes back into code with fewer steps. \nAI can definitely help you draft migrations or mappings, but it should not be the source of truth. IntelliJ IDEA inspections and database tools help you verify changes early, before they ship.\nGive it a try in your project, and let us know what works well and what you want to see next.",
        "dc:creator": "Andrey Belyaev",
        "content": "IntelliJ IDEA is well-known for its Spring Framework support. The IDE helps with bean navigation, on-the-go bean injection, Spring Data methods completion, and assistance with generating HTTP requests for controllers. One of the areas that IDEA is good at is dealing with JPA. You, as a developer, can: Before IntelliJ IDEA 2025.3, dealing with Spring [&#8230;]",
        "contentSnippet": "IntelliJ IDEA is well-known for its Spring Framework support. The IDE helps with bean navigation, on-the-go bean injection, Spring Data methods completion, and assistance with generating HTTP requests for controllers. One of the areas that IDEA is good at is dealing with JPA. You, as a developer, can: Before IntelliJ IDEA 2025.3, dealing with Spring […]",
        "guid": "https://blog.jetbrains.com/?post_type=idea&p=675359",
        "categories": [
          "idea",
          "java",
          "jpa",
          "spring-boot",
          "spring-data",
          "spring-data-jdbc",
          "spring-data-jpa"
        ],
        "isoDate": "2026-01-28T09:59:06.000Z"
      },
      {
        "creator": "Ilia Afanasiev",
        "title": "Google Colab Support Is Now Available in PyCharm 2025.3.2",
        "link": "https://blog.jetbrains.com/pycharm/2026/01/google-colab-support-is-now-available-in-pycharm-2025-3-2/",
        "pubDate": "Wed, 28 Jan 2026 09:33:49 +0000",
        "content:encodedSnippet": "PyCharm is designed to support the full range of modern Python workflows, from web development to data and ML/AI work, in a single IDE. An essential part of these workflows is Jupyter notebooks, which are widely used for experimentation, data exploration, and prototyping across many roles.\nPyCharm provides first-class support for Jupyter notebooks, both locally and when connecting to external Jupyter servers, with IDE features such as refactoring and navigation available directly in notebooks. Meanwhile, Google Colab has become a key tool for running notebook-based experiments in the cloud, especially when local resources are insufficient.\nWith PyCharm 2025.3.2, we’re bringing local IDE workflows and Colab-hosted notebooks together. Google Colab support is now available for free in PyCharm as a core feature, along with basic Jupyter notebook support. If you already use Google Colab, you can now bring your notebooks into PyCharm and work with them using IDE features designed for larger projects and longer development sessions.\nDownload PyCharm\n                                                    \nGetting started with Google Colab in PyCharm\nConnecting PyCharm to Colab is quick and straightforward:\nOpen a Jupyter notebook in PyCharm.\nSelect Google Colab (Beta) from the Jupyter server menu in the top-right corner.\nSign in to your Google account.\nCreate and use a Colab-backed server for the notebook.\nOnce connected, your notebook behaves as usual, with navigation, inline outputs, tables, and visualizations rendered directly in the editor.\n\n\n\n\nWorking with data and files \nWhen your Jupyter notebook depends on files that are not yet available on the Colab machine, PyCharm helps you handle this without interrupting your workflow. If a file is missing, you can upload it directly from your local environment. The remote file structure is also visible in the Project tool window, so you can browse directories and inspect files as you work.\nWhether you’re experimenting with data, prototyping models, or working with notebooks that outgrow local resources, this integration makes it easier to move between local work, remote execution, and cloud resources without changing how you work in PyCharm.\nIf you’d like to try it out:\nDownload PyCharm 2025.3.2\nLearn more about Google Colab",
        "dc:creator": "Ilia Afanasiev",
        "content": "PyCharm is designed to support the full range of modern Python workflows, from web development to data and ML/AI work, in a single IDE. An essential part of these workflows is Jupyter notebooks, which are widely used for experimentation, data exploration, and prototyping across many roles. PyCharm provides first-class support for Jupyter notebooks, both locally [&#8230;]",
        "contentSnippet": "PyCharm is designed to support the full range of modern Python workflows, from web development to data and ML/AI work, in a single IDE. An essential part of these workflows is Jupyter notebooks, which are widely used for experimentation, data exploration, and prototyping across many roles. PyCharm provides first-class support for Jupyter notebooks, both locally […]",
        "guid": "https://blog.jetbrains.com/?post_type=pycharm&p=677006",
        "categories": [
          "releases",
          "jupyter-notebooks"
        ],
        "isoDate": "2026-01-28T09:33:49.000Z"
      },
      {
        "creator": "Maciej Gorywoda",
        "title": "IntelliJ Scala Plugin in 2025",
        "link": "https://blog.jetbrains.com/scala/2026/01/27/intellij-scala-plugin-in-2025/",
        "pubDate": "Tue, 27 Jan 2026 16:32:55 +0000",
        "content:encodedSnippet": "In December 2024, we published a year-in-review summary of our work on the Scala Plugin. By that time the following year, we were busy releasing IntelliJ IDEA and the Scala Plugin 2025.3, as well as preparing for the upcoming Scala 3.8 release. This was followed by a long holiday break.\nBut as they say, not all who wander are lost. Now, without further ado, let’s take a look back at the main Scala Plugin highlights of 2025.\n\n\n\n\nMain changes to the Scala Plugin\nScala 3 support\nSupporting Scala 3 sometimes feels like chasing a rabbit: While we’re working on delivering support for existing features, the compiler team is building new ones. But we’re closing the gap. Here are some of the features we added support for in 2025:\nNamed tuples\n“Better fors” and refutable patterns in for-comprehensions\nOpaque types\nContext bounds and givens\nThe new into modifier\n\n\n\n\nThe last point (support for into) is part of a larger project to ensure support for Scala 3.8 by the time it is released. Additionally, we introduced several improvements to code resolution and type inference, and addressed some “good code is red” issues in Compiler-based highlighting, making both Built-in and Compiler-based highlighting modes more reliable.\nNamed tuples and their support in Scala Plugin\n\n\n\nsbt\nThe new sbt modules layout is now enabled by default. We made several improvements to the new separate production/test modules and resolved many edge cases discovered since the feature was first released in 2024.3. We also directed a portion of our efforts toward supporting sbt 2.0, as its release is right around the corner.\n\n\n\n\nX-Ray mode\nX-Ray mode enhancements include displaying apply calls, name hints (including for all parameters, not just literals), and clearer indicators of when the apply method is being used instead of a regular constructor. We’ve also improved type hints and info for variable patterns, generators, literal parameters, underscore parameters, and kind projector syntax.\n\n\n\n\nPerformance improvements\nPerformance improvements include faster editor actions and fixes for occasional UI freezes during actions like Show Type Info and Show Implicit Argument. You can disable built-in inspections in Settings for better performance in cases where compiler highlighting will suffice. Tips and contact instructions are also available under Settings. But we’ve got plenty more where that came from and will be announcing further improvements in 2026 – stay tuned!\n\n\n\n\nYou can read about all these changes and many more in detail in our release notes:\nRelease Notes 2025.1\nRelease Notes 2025.2\nRelease Notes 2025.3\n\n\n\n\nAnd in a blog post about our support for Scala 3.8.\n\n\n\n\nState of Developer Ecosystem Survey 2025\nIn what has now become an annual tradition, JetBrains conducted a survey of users of our products. Here are a few key Scala-related insights:\n\n\n\n\nScala 3 popularity\nAmong the 446 people who answered this question, 274 (59%) answered that they use Scala 3 regularly. “Regularly” is a keyword here. We also asked about your usage of different Scala versions at home and at work. Although versions 2.13 and 2.12 are still in use, the percentage of developers who have already switched to Scala 3 but still use Scala 2 at work adds another 15% to the result.\n\n\n\n\nUnit testing frameworks\nScalaTest maintains its top position at 70% (down from 71% last year), but MUnit and ZIOTest are on the rise, coming in at 22% and 17%, respectively (as opposed to 19% and 8% last year).\n\n\n\n\nFrameworks and libraries\nCats, ZIO, and FS2 remain popular, with ZIO having the biggest increase in popularity among them, jumping from 27% in 2024 to 32% in 2025. Akka usage, on the other hand, is trending downward – it was at 35% two years ago, then fell to 30% last year, and is now at 28%. It’s clearly losing users to Pekko, which has risen from 15% to 22%.\n\n\n\n\nHead over to this page to compare this year’s numbers with those of last year. And feel free to check out The State of Developer Ecosystem Report 2025.\n\n\n\n\nYouTube videos about the Scala Plugin\nIn December 2024, we released the first video in a new YouTube series about how IntelliJ IDEA supports Scala. Each video is approximately 10 minutes long and discusses one main feature, along with a few smaller, closely related ones. In 2025, we released seven more of these videos, covering various topics from enum support to the debugger. For our mobile users, we also whipped up a few YouTube Shorts addressing the same topics. You can watch them all here.\n\n\n\n\nConferences\nLast but not least, in 2025, we were Gold sponsors at two major Scala conferences: Scalar in March (Warsaw, Poland) and Scala Days in August (Lausanne, Switzerland). At Scalar, we gave a lightning talk on two closely connected Scala Plugin features: inlay hints and X-Ray mode. At Scala Days, we delivered two 40-minute talks. The first was a sort of two-for-one: It addressed how to stay productive with the Scala Plugin and gave a quick status update on Scala 3 and the contributions made to it thus far. The other talk was a walkthrough of AI tooling in IntelliJ IDEA.\n\n\n\n\n\nThat’s all for now! We hope your 2026 is off to a productive start.\n\n\n\n\nThe Scala team at JetBrains",
        "dc:creator": "Maciej Gorywoda",
        "content": "In December 2024, we published a year-in-review summary of our work on the Scala Plugin. By that time the following year, we were busy releasing IntelliJ IDEA and the Scala Plugin 2025.3, as well as preparing for the upcoming Scala 3.8 release. This was followed by a long holiday break. But as they say, not [&#8230;]",
        "contentSnippet": "In December 2024, we published a year-in-review summary of our work on the Scala Plugin. By that time the following year, we were busy releasing IntelliJ IDEA and the Scala Plugin 2025.3, as well as preparing for the upcoming Scala 3.8 release. This was followed by a long holiday break. But as they say, not […]",
        "guid": "https://blog.jetbrains.com/?post_type=scala&p=676660",
        "categories": [
          "scala",
          "scala-programming",
          "2025",
          "intellij-idea"
        ],
        "isoDate": "2026-01-27T16:32:55.000Z"
      },
      {
        "creator": "Irina Mihajlovic",
        "title": "Rust vs JavaScript & TypeScript: performance, WebAssembly, and developer experience",
        "link": "https://blog.jetbrains.com/rust/2026/01/27/rust-vs-javascript-typescript/",
        "pubDate": "Tue, 27 Jan 2026 11:22:01 +0000",
        "content:encodedSnippet": "TL;DR\nRust and JavaScript/TypeScript aren’t competitors – they excel in different domains and are increasingly used together.\nJS/TS offers an unmatched ecosystem depth and rapid iteration, while Rust delivers performance, memory safety, and reliability.\nWebAssembly bridges the two, letting Rust power performance-critical logic inside JS/TS apps.\nHybrid Rust + JS/TS architectures are now common across major companies and open-source projects.\nRust adoption is accelerating, especially in backend, infrastructure, and Wasm-heavy workloads; JS/TS remains dominant for UI and full-stack flexibility.\nModern teams often use JS/TS for the product layer and Rust for the engine beneath it, combining speed with flexibility.\n\n\n\n\nRust and JavaScript/TypeScript (JS/TS) are often perceived as rivals, but in reality, they’re complementary languages that serve different purposes and excel across different domains. They were designed with different goals in mind, which dictate the best use cases for each.\nJavaScript/TypeScript offers benefits such as unmatched flexibility, a massive ecosystem, and rapid iteration. Meanwhile, Rust shines in projects that demand top-tier performance, memory safety, and system-level reliability. \nAnd thanks to technologies like WebAssembly, Rust code can now integrate seamlessly with JS/TS environments, effectively serving as a high-performance engine under a flexible frontend. Such hybrid projects combining Rust with JavaScript/TypeScript are on the rise, reflecting a broader trend of language collaboration. A good example of that is a Rust and TypeScript project using WebAssembly, where Rust handles performance-critical logic, and TS manages the web-facing side.\nThis article dives into the current adoption and use cases of Rust and JavaScript/TypeScript to help you explore how these two languages can work together, including real-world commercial applications.\nRust and JavaScript: Where they stand in 2025\n\n\n\n\nToday, Rust and JavaScript/TypeScript stand out as two languages shaping very different yet complementary domains. According to the 2025 Developer Ecosystem Survey, 10% of developers plan to adopt Rust next year, while TypeScript’s real-world adoption has been higher than any other language over the past five years. The survey’s Language Promise Index indicates that TypeScript, Rust, and Go have the highest perceived growth potential, while JavaScript, PHP, and SQL appear to have reached a plateau. \nLet’s take a closer look at Rust and JavaScript/TypeScript to understand their place in the current software development landscape. \n\nCategoryRustJavaScript / TypeScript\nPrimary strengthPerformance and memory safetyFlexibility and massive ecosystem\nTypical use casesSystems programming, backends, WebAssembly modules, tooling enginesFrontend apps, full stack via Node, prototyping, product layer logic\nExecution modelCompiled, strongly typed, predictable performanceInterpreted or JIT compiled, dynamic or gradually typed\nWebAssembly supportFirst class, highly optimizedWorks as host and integration layer\nDeveloper experienceSteeper learning curve but excellent tooling through Cargo and RustRoverVery fast iteration, strong tooling in VS Code and WebStorm\nEcosystemGrowing crates ecosystem and strong system level toolingLargest library ecosystem in the world\nIdeal role in hybrid stackPerformance engine under JS/TSUI layer, product logic, integration, DX\n\n\n\n\n\nRust\nIn 2024, Rust achieved a significant milestone, reaching an estimated 2.27 million developers, with 709,000 identifying it as their primary programming language. This year, Rust continues to gain traction in areas where performance and safety matter. Its commercial usage increased substantially – between 2021 and 2024, the share of Rust developers using it in production grew by 68.75%.\nRust’s ecosystem is thriving across industries, powering everything from operating systems and browsers to cloud infrastructure, WebAssembly apps, embedded devices, and blockchain platforms. Leaders like Microsoft, Google, Amazon, and Meta rely on Rust for its unmatched blend of speed, safety, and scalability – whether in the Linux kernel or the AWS Firecracker microVMs. \nThere’s another rising use case of Rust that points to its natural fit alongside JavaScript/TypeScript: web development. According to JetBrains ecosystem data from 2024, Rust for web development is the third most popular use case, showing how well Rust integrates with the web dev ecosystem.\nJavaScript / TypeScript\nJavaScript remains a ubiquitous choice among developers – it has the largest usage share among professional developers (68.8% in 2025). TypeScript’s tooling and large ecosystem adoption make it a strong “next step” for JS teams. To illustrate the rise of TS: in August 2025, TypeScript overtook both JavaScript and Python in GitHub contributor usage.\n\n\n\n\nThe reasons behind this expanding adoption are clear: the JavaScript and TypeScript ecosystems offer remarkable flexibility, making them one of the most powerful environments for modern software development. \nDevelopers can use both languages to build dynamic front-end interfaces and robust back-end services using frameworks like Node.js. This opens the door to developing full-stack applications with a single language. The speed and adaptability of JS/TS make them ideal for rapid prototyping, while the massive plugin and library ecosystems provide ready-made solutions for virtually any challenge, from UI frameworks to complex data handling. \n“Rust is undoubtedly the big star in JavaScript and TypeScript tooling and infrastructure. It addresses the most significant issue in the current tooling ecosystem: performance. And I’m 100% convinced that the safeguards from language and compiler make it easier to create successful tools to begin with.”\n\n            \nStefan Baumgartner\n                                                                Author of the TypeScript Cookbook and TypeScript in 50 Lessons\n                                    \nCommon Use Cases and Projects\nRust’s growing collection of web frameworks now mirrors many of the patterns familiar in JavaScript ecosystems, making it easier for teams to blend both worlds. Actix Web, Axum, Rocket, Warp, Loco, Cot, and full-stack tools like Leptos offer routing, middleware patterns, request handling, and component models that map conceptually to popular JS frameworks. \nFor example, Actix Web and Axum align with the role that Express or Fastify play in the Node.js ecosystem, while Leptos feels closer to React or Svelte in how it structures reactive UI logic, except it’s compiled to WebAssembly for performance.\nHow Rust Web Frameworks Map to Familiar JavaScript Patterns\nRust’s modern web frameworks feel approachable for developers coming from the JavaScript ecosystem. While they are different in typing and execution models, the core concepts like routing, middleware, handlers, and extractors follow patterns that JS developers already use daily.\nActix Web is known for its speed and mature async runtime. It uses an actor model that let’s you build highly concurrent services without complex coordination logic. For teams experienced with high-throughput Node servers or frameworks like Fastify, Actix Web offers a similar emphasis on efficiency, but with Rust’s predictability and stronger guarantees around memory safety and performance.\nAxum takes a modular, middleware-driven approach. It’s built on top of the Tower ecosystem, and it promotes clear separation between routing, request handling, and service composition. Developers who appreciate the flexibility of Koa or modern frameworks in the Deno ecosystem often find Axum intuitive because it leans on composable primitives and a clean, type-first design. If you want to see this approach in action, this short Axum tutorial walks through a real example. \n\n\n\n\n\n\n\nRocket focuses on ergonomics. Its declarative routing and macro-powered syntax reduce boilerplate and make handler definitions straightforward. This resonates with developers who enjoy the expressiveness of decorator-driven frameworks such as NestJS, but want the reliability of a compiled language. With Rocket 1.0 stabilized, the framework has become a dependable option for production APIs and prototypes alike.\n\n\n\n\nTogether, these frameworks show how Rust’s server-side ecosystem has evolved into a space where JS/TS developers can transition smoothly, adopting familiar patterns with the added benefit of stronger safety guarantees and more predictable performance\nWhy These Frameworks Matter in Hybrid Architectures\nAs more teams blend Rust with JavaScript or TypeScript, frameworks like Actix Web, Axum, and Rocket become essential building blocks for the backend or core logic layer. They address the limits that Node services often encounter, especially under high load, CPU-heavy operations, or strict latency requirements. \nRust’s concurrency model and predictable throughput make these frameworks a strong fit for performance-sensitive backend logic, while JS/TS continues to drive the UI and application layer.\nThey also integrate cleanly with modern API tooling, making it easy to expose REST or gRPC interfaces that TypeScript clients consume reliably. For teams working with WebAssembly, these frameworks complement workflows where browser modules and backend services share Rust-based components. \nThe result is a hybrid architecture where JavaScript drives the user experience and rapid iteration, while Rust powers the critical paths that require precision and speed.\nRust’s WebAssembly Capabilities and JavaScript Integration\nWebAssembly serves as the practical bridge that allows Rust and JavaScript to collaborate within the browser. You can compile Rust directly to Wasm, allowing JavaScript to call into high-performance Rust functions whenever a task demands more speed or predictability than JS alone can provide. \nThis model is increasingly common as more teams use Rust-compiled Wasm for image processing, data visualization pipelines, physics simulations, cryptography, and other computation-intensive routines that would slow down or block a JS app if implemented purely in JavaScript.\nThe workflow is straightforward: JavaScript or TypeScript continues to handle the majority of the application, like UI rendering, event handling, state management, and routing, while Rust takes on focused, performance-critical modules. \nWhat you get is a JS/TS application with specialized Rust components quietly accelerating the hardest parts. \nMany teams now rely on this pattern, using Rust-powered WebAssembly modules to offload complex calculations while keeping the rest of the codebase in familiar JavaScript/TypeScript. It’s a complementary approach that strengthens modern web workflows without replacing established frontend practices (and causing lots of headaches!).\nReal-World Patterns for Introducing Rust Web Frameworks into JS/TS Workflows\nTeams adopting Rust alongside JavaScript and TypeScript often begin with incremental changes, using Rust frameworks as clean entry points into existing systems.\nA common starting point is replacing a single Node microservice with an Actix Web or Axum service, especially for tasks like data processing, authentication, or analytics where Node struggles under load. TypeScript clients generated from OpenAPI help keep the integration smooth.\nAnother pattern is using Rocket or Axum to build backend services for React or Next.js applications. These Rust services offer predictable latency under high concurrency, while the JS/TS frontend maintains flexibility and fast iteration cycles.\nFor teams working with WebAssembly, Rust’s web frameworks pair naturally with workflows where browser modules and server components share Rust-based logic.\nTogether, these patterns show how Rust frameworks extend JS/TS workflows by handling performance critical components without disrupting the broader development process.\nRust vs JavaScript: Ecosystem, Tooling, and Developer Experience\nEcosystem\nFor developers coming from the JavaScript world, the Rust ecosystem can feel both familiar and refreshingly different. JavaScript thrives on its extensive library ecosystem, rapidly evolving frameworks, and package managers. Rust offers a more streamlined environment, with its standard toolchain centered around Cargo, which serves as a unified build system, dependency manager, test runner, and project scaffolder. \nTooling\nOn the tooling side, JS developers often rely on WebStorm or VS Code, while Rust developers increasingly use RustRover, IntelliJ with a Rust plugin or CLion with Rust support for a more integrated development experience. \nAccording to the 2024 State of Rust Survey, VS Code remains the most commonly used editor for Rust, followed by Neovim and IntelliJ based IDEs, highlighting a split between lightweight editors and full featured IDEs depending on workflow preferences and project complexity.\nGet started with RustRover\n                                    \nCommunities\nWhen comparing communities, you can expect JavaScript’s scale to be unparalleled – reflecting decades of growth, numerous tutorials, and widespread industry adoption. Rust’s community is smaller but expanding rapidly, especially among developers seeking stronger safety guarantees or exploring alternatives to traditional Java-based backend stacks (this is why comparing Rust to Java makes so much sense!).\nDeveloper experience\nWhen surveyed using the JetBrains Language Promise Index, JavaScript sits firmly in the “mature and stable” category: widely adopted, battle-tested, and unlikely to fade. Rust, in contrast, scores high in terms of long-term potential and upward momentum. Its promise comes from a combination of safety, performance, and growing enterprise interest. These are traits that appeal to teams modernizing infrastructure or boosting their existing Java or JS stacks.\nIn practice, developers don’t have to choose between Rust and JavaScript. Rust brings predictability, performance, and modern tooling, such as Cargo and RustRover; JavaScript offers flexibility, a broad ecosystem, and a massive support network. Together, they form a complementary set of tools that let teams build faster, safer, and more maintainable software.\nDeveloper Adoption of Rust vs JavaScript\nRust learning curve and resources \nRust adoption today spans a wide range of organizations – from cloud platforms to fast-moving startups – giving the programming language solid credibility in real-world production systems. \nCompanies we all know, like Cloudflare, Figma, Discord, Amazon, and Dropbox, use Rust for performance-critical services, security-sensitive components, or infrastructure tooling. Startups building developer tools, edge runtimes, and data platforms increasingly choose Rust for its blend of speed and safety. \nIn the open-source space, many modern projects combine Rust and JavaScript, using Rust for core engines and JavaScript/TypeScript for the surrounding UI or integration layers. Examples include frameworks, linters, build tools, and WebAssembly-powered libraries that offload heavy tasks from JS/TS to Rust.\nIf you’re curious about how to get started after seeing these real-world use cases, our blog post How to Learn Rust walks through a practical learning path. \nStill, Rust’s learning curve can be steeper than JavaScript’s – especially in terms of the ownership and borrowing model. But once you understand these concepts, Rust tends to produce more predictable and maintainable systems. High-quality resources support this ramp-up: comprehensive documentation like Rust Book, courses and Rust exercises, and robust tooling such as RustRover. \nThis combination of industry adoption, open-source momentum, and strong educational materials makes Rust a reliable long-term skill for developers looking to expand beyond traditional JavaScript workflows.\nLearning curve and resources for JS/TS developers\nIf you’re a seasoned JavaScript developer looking to dip your toes into the world of Rust, you can use plenty of beginner-friendly resources, such as in-depth Rust guides or community-driven guides focused on WebAssembly and JavaScript integration. For experienced engineers who love to learn right inside the IDE, RustRover is a strong solution. You get a faster and more structured way to learn the language, thanks to its intelligent tooling, code insights, and smooth project setup.\nThe Rust community itself is a major advantage: active forums, Discord servers, meetups, and strong tooling support create a welcoming environment for newcomers. \nWhen exploring Rust, many JS/TS developers start small. For example, you can add a single Rust module to an existing JavaScript/TypeScript project to speed up a computationally intensive task, image transformation, or parsing operation – and then expand your Rust footprint as you gain confidence. \nThis incremental approach works well, especially in a job market where both junior developers (61%) and senior developers (54%) report challenges; expanding into Rust can open new opportunities without abandoning familiar JavaScript workflows. For JS/TS developers, experimenting with Rust isn’t about replacing what they know but enhancing their toolkit with a language that fits naturally alongside their existing skills.\nHow Rust and JS/TS Work Together\nModern teams are increasingly combining Rust and JavaScript/TypeScript in the same workflow, especially when performance is a key consideration. A common pattern is to compile Rust to WebAssembly and call it directly from JS/TS. \nThis is ideal for tasks such as data parsing, image processing, simulation code, or crypto routines, where Rust WebAssembly‘s performance offers a clear advantage. The approach complements the JS/TS side, which continues to manage UI, routing, and application logic. \nAs much as 85% of developers are already using AI assistants. The openness to adopt new tools opens the doors to experimenting with Rust and JavaScript hybrid architectures. \nJS/TS handles frontend and routine development tasks, while Rust modules (including those implemented via WebAssembly) take on performance-intensive tasks or critical backend services. This illustrates complementary usage without claiming one language is superior.\nExample: Calling a Rust-compiled Wasm module from TypeScript\nimport init, { compute_heavy_task } from \"./pkg/my_rust_module.js\";\n\nasync function run() {\n  await init(\"./pkg/my_rust_module_bg.wasm\");\n  const result = compute_heavy_task(500000);\n  console.log(\"Result:\", result);\n}\n\nrun();\nThis approach gives teams the best of both worlds: JS/TS drives the product experience, while Rust takes over for the parts that require speed, safety, or system-level control.\nCommercial usage of Rust and JS/TS: 3 Examples\nFigma\nFigma is a web-based design platform that pairs Rust and WebAssembly with a TypeScript- and React-driven interface. Its core graphics rendering and heavy computational tasks are compiled from Rust to Wasm, giving the browser near-native performance for operations like vector math, canvas manipulation, and real-time collaboration updates. \nMeanwhile, the application’s main interface, interaction logic, and overall user experience are built with TypeScript and React, allowing rapid iteration and a rich, responsive UI layer. This blend of Rust/Wasm for performance-critical work and JS/TS for product development speed is a central reason Figma feels both powerful and fluid inside the browser.\nBiome\nBiome is a performant toolchain for web projects. It provides developer tools that help maintain the health and consistency of those projects. It is designed to be used through LSP integrations in editors and the CLI to format and lint code.\nBiome is designed to be used through a CLI and editor integrations via the Language Server Protocol. While it supports both JavaScript and TypeScript projects, Biome itself does not rely on JavaScript or TypeScript internally. Instead, it replaces traditional JavaScript based tooling like Prettier and ESLint with a Rust implemented alternative focused on speed, reliability, and developer experience.\n“Biome aims to replace existing JavaScript tooling like Prettier and ESLint with a Rust-implemented toolchain that offers better performance and a more consistent developer experience.”\n\n            \nDenis Bezrukov, Core Contributor in Biome\n                                                        \nCloudflare\nCloudflare’s edge computing platform relies heavily on Rust to power its secure, high-performance serverless runtimes, including the foundation of Cloudflare Workers. Rust handles the low-level execution engine, network operations, isolation layers, and overall runtime reliability. \nOn top of this Rust-powered infrastructure, developers write their actual edge logic in JavaScript or TypeScript, which Cloudflare compiles and runs across its global network. This pairing enables Cloudflare to provide a fast and secure edge platform built on Rust while offering developers the familiarity and flexibility of the JavaScript/TypeScript ecosystem.\nConclusion\nAs you can see, Rust and JavaScript/TypeScript work best not as alternatives but as complementary layers of the same stack. Bringing together Rust’s speed, safety, and accuracy with the adaptability and rapid development of JS/TS provides teams with a well-rounded method that works for everything from web applications to backend systems. Hybrid workflows – whether through WebAssembly, shared services, or modular backends – let you select the appropriate tool for each part of your architecture. \nFor anyone exploring this path, experimenting with mixed Rust + JS/TS projects is one of the most effective ways to understand their combined strengths. And with tools like RustRover streamlining the Rust development experience, adopting these workflows has never been more accessible.\nIf you’re experimenting with Rust and JavaScript integration, RustRover provides a streamlined environment for managing your Rust toolchain, inspecting WASM, and working across hybrid projects.\nTry RustRover",
        "dc:creator": "Irina Mihajlovic",
        "content": "TL;DR Rust and JavaScript/TypeScript (JS/TS) are often perceived as rivals, but in reality, they’re complementary languages that serve different purposes and excel across different domains. They were designed with different goals in mind, which dictate the best use cases for each. JavaScript/TypeScript offers benefits such as unmatched flexibility, a massive ecosystem, and rapid iteration. Meanwhile, [&#8230;]",
        "contentSnippet": "TL;DR Rust and JavaScript/TypeScript (JS/TS) are often perceived as rivals, but in reality, they’re complementary languages that serve different purposes and excel across different domains. They were designed with different goals in mind, which dictate the best use cases for each. JavaScript/TypeScript offers benefits such as unmatched flexibility, a massive ecosystem, and rapid iteration. Meanwhile, […]",
        "guid": "https://blog.jetbrains.com/?post_type=rust&p=671574",
        "categories": [
          "comparison",
          "javascript",
          "rustrover",
          "rust"
        ],
        "isoDate": "2026-01-27T11:22:01.000Z"
      },
      {
        "creator": "Boris Ageev",
        "title": "A Year of Creator Wins: Highlights from the JetBrains Content Creators Program 2025",
        "link": "https://blog.jetbrains.com/blog/2026/01/27/content-creators-wins/",
        "pubDate": "Tue, 27 Jan 2026 09:44:39 +0000",
        "content:encodedSnippet": "With more than 200 new members joining the JetBrains Content Creator community, we were bound to hear some exciting new things from them in 2025. Our members were steadily increasing in number, honing their YouTube videos, tirelessly keeping up with the posting schedule, and providing JetBrains perks directly to their audiences. But the real results of their work made us especially proud, so we decided to dedicate an entire article to our creators and their year!\nOur wins start with folks who improved their lives thanks to their unstoppable creativity:\n“One of my biggest accomplishments in 2025 has been taking my personal brand to the next level and turning it into something that now works very much like a company. This year I’ve been able to fully live off my content…largely thanks to the quality of the content and the steady growth of my community,”\n\n            \nsays Carlos Belenguer Jover, a TikTok creator and Twitch streamer working under the name @melenitasdev.\nThe growth, according to Carlos, is the result of people’s genuine desire to learn programming through accessible and fun content. To us, it was inspiring to hear how JetBrains products were a key part of that journey:\n“I use them daily in my own work and naturally across my videos and livestreams, and they’ve become deeply integrated into how I create, code, and teach.”\n\n            \nAn unexpected tool switch\nSome creator stories consisted of happy accidents, as was the case for YouTube creator Miguel Teheran:\n“My story started three months ago when my Windows laptop showed me a strange white screen and after that it couldn’t turn on again…I decided to buy a used MacBook and use the Apple ecosystem since I use macOS [at work] as well. I installed Rider [and] all of my content in my channel is now recorded with Rider and I am motivating people to try this tool.”\n\n            \nHitting massive reach and recognition\nFor all successful creators, personal highlights are connected to growth, and JetBrains creators grew big time in 2025. Anton Martynyuk’s newsletter at https://antondevtips.com has grown to more than 20K subscribers and Anton himself was recognized as a Microsoft MVP.\n“JetBrains tools have helped a lot on this journey, as I use Rider, DataGrip, AI Assistant, and Junie every day.”\n\n            \nThis wasn’t the first time Rider was mentioned to us. Adam Myhre frequently features it on the git-amend YouTube channel. As far as highlights go, Adam won Best Tutorial Series at Unity Awards 2025 and surpassed 2.5M views across his entire channel.\n“Rider continues to be a major part of every video! Thank you for your support!”\n\n            \nSome of our champions almost tripled their growth targets, like Luciano Souza’s YouTube channel and other social media:\n“In 2025 I’ve reached 140M views across networks (Instagram, X, and LinkedIn) and my goal was 50M through the year. I’m very happy with those results and all the effects that this brings to my career as a Software Engineer and Content Creator, and obviously JetBrains was with me on that.”\n\n            \nBeyond content: Books, OSS, and more\nTwo of our creators shone with projects outside of the content space: Jens Oliver Meiert wrote an entire book dedicated to web development, as did Ashley Allen, who published his Web Dev’s Guide to Freelancing.\n“It’s been a productive and impactful year, and I’m very happy about the results – thanks in good part to WebStorm!”\n\n            \n— Jens\n                                                        \n“I wrote the entire book in PhpStorm, using Markdown files for content, then wrote/ran scripts to generate the PDF and EPUB files. Being able to do all this inside PhpStorm was a massive help for my workflow because I could get instant feedback on the book as I was writing it, all without leaving my IDE. JetBrains also played a part in this book by providing a discount code for all the book’s readers, which I am incredibly grateful for!”\n\n            \n— Ashley\n                                                        \nHighlights for Ashley included using PhpStorm to maintain open-source Laravel/PHP packages totaling 3.1M downloads. For Jens, the entire year was dedicated to writing articles – more than a hundred of them – as well as various refactorings for Frontend Dogma and meiert.com.\nThese stories highlight something developers rarely talk about: using an IDE to manage the entire production – content, scripts, builds, and revisions – without constantly switching tools.\nLearning through community\nSeveral creators highlighted not just tools, but the broader ecosystem around them – from plugins and marketplaces to how new technologies reach developers. Saving the best for last, we decided to include Serena Sensini’s quote in full:\n“I’ve been using JetBrains tools for a long, long time, and I’m super happy to be a partner, because this gave me the opportunity to make our followers [explore] the different products you develop, but especially to give people [a chance] to explore different and innovative ways to code. Bringing emerging technologies to the community through the implementation of a huge marketplace and its plugins, your work with the external communication and community engagement, is incredible and I really need to thank you for that.”\n\n            \nThese stories go beyond individual wins. Creators’ workflows, feedback, and experiments shape how we think about the developer experience, tooling, and community. We’re grateful to everyone who shared their stories with us this year, with a special shoutout to Gerson Azabache Martinez, Ali Bouali, Artem Kondranin, and Faisal Memon.\nInspired by these stories?\nApplications to the JetBrains Content Creator Program are open. If you create developer-focused content and want support, tools, and a community that grows with you, we’d love to hear from you!",
        "dc:creator": "Boris Ageev",
        "content": "With more than 200 new members joining the JetBrains Content Creator community, we were bound to hear some exciting new things from them in 2025. Our members were steadily increasing in number, honing their YouTube videos, tirelessly keeping up with the posting schedule, and providing JetBrains perks directly to their audiences. But the real results [&#8230;]",
        "contentSnippet": "With more than 200 new members joining the JetBrains Content Creator community, we were bound to hear some exciting new things from them in 2025. Our members were steadily increasing in number, honing their YouTube videos, tirelessly keeping up with the posting schedule, and providing JetBrains perks directly to their audiences. But the real results […]",
        "guid": "https://blog.jetbrains.com/?post_type=blog&p=676046",
        "categories": [
          "community",
          "community-support",
          "content-creators-program",
          "rider"
        ],
        "isoDate": "2026-01-27T09:44:39.000Z"
      },
      {
        "creator": "Dmitrii Korovin",
        "title": "The Jenkins Migration Planning Kit",
        "link": "https://blog.jetbrains.com/teamcity/2026/01/jenkins-migration-planning-kit/",
        "pubDate": "Mon, 26 Jan 2026 18:25:53 +0000",
        "content:encodedSnippet": "This article was brought to you by Cameron Pavey, draft.dev.\nJenkins has served the development community well for over a decade, but it was designed for a different era of software development. Developers who are tired of fighting with plugin compatibility issues, slow builds, and brittle configurations are exploring alternatives.\nBut is your organization ready for the move? What is the actual work involved? And how do you communicate the benefits of a more modern CI/CD solution to leadership to get their buy-in?\nOur migration planning kit answers these questions:\nThe migration-readiness assessment scores and plots your organization’s readiness against the pain you’re experiencing with Jenkins to determine whether you are an ideal migration candidate, need to address foundational issues first, or ought to focus on other priorities.\nYou’ll see how Jenkins patterns compare to TeamCity to assess the effort required and plan for an effective migration.\nThe safe migration plan guides you through the five phases of a successful migration: discovery & assessment, pilot setup, incremental migration, optimization, and full cutover.\nYou’ll get templates and guidance on how to communicate the benefits of migration to management.\nThis guide contains a lot of information, so feel free to use the hyperlinks above to navigate to the information that’s most relevant for you right now.\nMigration-readiness checklist: Assessing your team’s preparation\nBefore considering any migration, you need an honest assessment of your current CI/CD maturity and readiness for change. The following self-evaluation will help you determine whether migration is the right next step and identify areas that need attention.\nOrganizational-readiness assessment\nStart by evaluating how your organization manages Jenkins infrastructure and pipeline development.\nDo you manage Jenkins centrally, or is it distributed across teams? Centralized management typically indicates better standardization and makes migration coordination easier. Distributed management may require more planning but often means teams have stronger pipeline ownership.\nDo you have standardized pipeline practices across your organization? If you have consistent patterns, shared libraries, and documented standards, you are typically better positioned for migration because you’ve already solved many of the organizational challenges. If your pipelines vary dramatically between projects, consider standardizing approaches before migrating.\nAre your builds reliable, or do you regularly experience flaky failures? This question reveals both technical debt and team practices. Reliable builds point to good testing practices and stable infrastructure, while frequent flakiness could be due to underlying issues that migration alone won’t solve.\nTechnical-inventory questions\nUnderstanding your current Jenkins footprint is essential for planning.\nHow many Jenkinsfiles do you maintain across all repositories? This gives you a scope estimate for the migration effort. Count both declarative pipelines and freestyle jobs that would need conversion.\nWhich Jenkins plugins are actively used in your pipelines? Create a comprehensive list including build tools, deployment integrations, notification systems, and reporting plugins. Research TeamCity equivalents for critical plugins, and identify any that might require custom development or workflow changes.\nDo you have custom integrations, scripts, or Jenkins extensions? Custom code represents the highest migration risk because it cannot be automatically translated to TeamCity: Each integration must be completely rebuilt using different APIs, often without clear documentation of the original business requirements or dependencies. Document any custom plugins, shared libraries, or external integrations that directly interact with Jenkins APIs.\nCI/CD pain point analysis\nConsider and identify the specific problems migration should solve for your team.\nAre your builds slow, or do you experience frequent bottlenecks? Common issues include slow build agents, inefficient artifact management, poor parallelization, and resource contention during peak hours. Having a clear goal is important as it will inform decisions throughout the migration process.\nDo you face debugging challenges regularly? Consider how much time your team spends investigating pipeline failures, tracking down build-environment issues, or understanding why tests behave differently in CI versus local environments.\nDo you have scalability issues that limit your current setup? Look for signs like build queues during busy periods, difficulty adding new projects, or infrastructure costs that grow faster than your team size.\nDo you want more control over your pipeline logic and infrastructure? If you are frequently working around Jenkins limitations or investing significant time in custom solutions, you would likely benefit greatly from TeamCity’s more flexible architecture.\nDo you need better visibility into flaky or slow tests? If your team struggles to identify unreliable tests or understand performance bottlenecks, TeamCity’s built-in test intelligence can provide immediate value.\nAre you spending more time maintaining CI infrastructure than building products? This is often the clearest signal that your current platform has become a productivity drain rather than an enabler.\nInterpreting your migration assessment\nLet’s now determine your readiness for migration.\nYou can find a printable version of this readiness checklist in the supporting files.\nFirst, calculate two separate scores:\nReadiness score:\nGive yourself 1 point for each “Yes”:\nCentralized Jenkins management\nStandardized pipeline practices across teams\nReliable builds with minimal flaky failures\nClear inventory of Jenkinsfiles and plugins\nDocumented custom integrations and dependencies\nPain score:\nGive yourself 1 point for each “Yes”:\nSlow builds or frequent bottlenecks\nRegular debugging challenges consuming team time\nScalability issues limiting growth\nNeed for more pipeline control and flexibility\nPoor visibility into test performance and failures\nMore time spent maintaining than building\nMigration recommendation\n\n\n\n\nLow readiness (0–3 points) + high pain (4–6 points): Address foundational issues first, but consider that TeamCity’s better tooling can help with cleanup. Start with organizational improvements and a small pilot.\nLow readiness (0–3 points) + low pain (0–3 points): Migration is not a priority. Focus on other improvements to your development process first.\nWhy choose TeamCity?\nKnowing when it’s time to migrate is only part of the puzzle. You also need to have a solid platform to migrate to. TeamCity addresses the core problems that drive teams away from Jenkins while providing capabilities that simply don’t exist in older CI/CD platforms. Consider the following, to list just a few:\nKotlin DSL for maintainable pipeline logic\nTeamCity’s Kotlin DSL provides type safety, IDE support, and compile-time validation. You can create reusable templates, refactor configurations with confidence, and catch errors before committing changes. Complex pipeline logic becomes maintainable code rather than fragile scripts that break unexpectedly.\nBuilt-in test intelligence\nFlaky test detection, automatic failure assignment, and a comprehensive test history are native platform features. TeamCity identifies which tests fail inconsistently, tracks patterns across builds, and assigns investigations based on recent changes. You’re not parsing logs or building custom solutions to understand test failures; the platform tells you what’s wrong and who should fix it.\nEnterprise orchestration without enterprise complexity\nBuild chains, artifact dependencies, and parallel execution work without extensive configuration or specialized expertise. Visual pipeline editors make complex workflows immediately comprehensible to new team members. You gain sophisticated orchestration capabilities without the operational overhead typically associated with enterprise platforms.\nHow do Jenkins pipelines compare to TeamCity?\nUnderstanding how your existing Jenkins patterns translate to TeamCity will help you determine what kinds of workflows you need to create as part of planning for an effective migration.\nPipelines vs. Build Chains\nTeamCity offers two complementary approaches for defining build workflows: Build Chains and Pipelines. Build Chains is an established method for modeling dependencies and complex workflows, while Pipelines, a new feature as of version 2025.07, provides a modern pipeline experience. While Pipelines is a newer feature that does not yet support as many use cases as the more battle-hardened Build Chains, it offers a more familiar and intuitive interface for teams migrating from Jenkins.\nThe specifics of the CI/CD processes you’re looking to implement will determine which option is best for you.\nOpt for Build Chains if:\nYou need mature, production-ready orchestration with advanced features like artifact dependencies, custom triggers, and snapshot isolation.\nYour workflows involve large monorepos or multiproject builds where many components must be built, tested, and deployed in a coordinated fashion.\nStability and long-term support matter more than having the latest pipeline syntax.\nOpt for Pipelines if:\nYou want a modern, YAML-based pipeline-as-code approach that feels closer to Jenkins, GitHub Actions, or GitLab CI.\nYour workflows are relatively straightforward (build → test → deploy) and don’t require deep artifact or snapshot dependency modeling.\nYou value developer-friendly ergonomics and repo-first configuration over enterprise-grade complexity.\nSample patterns\nBoth Build Chains and Pipelines support Kotlin DSL for configuration (Kotlin DSL support is coming to Pipelines after 2025.11), which offers significant advantages over traditional Jenkinsfile syntax.\nThe following side-by-side examples compare common Jenkins pipeline patterns with TeamCity’s Kotlin DSL, showing syntax differences and TeamCity’s enhanced native capabilities.\nDeclarative pipeline definition\nJenkins’s declarative pipelines use a YAML-like syntax within Groovy that can become verbose and error-prone:\n// Jenkinsfile\n\npipeline {\n\n    agent any\n\n    stages {\n\n        stage('Build') {\n\n            steps {\n\n                sh 'echo \"Building application...\"'\n\n                sh './gradlew build'\n\n            }\n\n        }\n\n        stage('Test') {\n\n            steps {\n\n                sh './gradlew test'\n\n            }\n\n        }\n\n        stage('Deploy') {\n\n            steps {\n\n                sh './deploy.sh'\n\n            }\n\n        }\n\n    }\n\n}\nTeamCity’s Kotlin DSL provides type safety and better IDE support:\n// .teamcity/settings.kts\n\nimport jetbrains.buildServer.configs.kotlin.*\n\nimport jetbrains.buildServer.configs.kotlin.buildSteps.script\n\nimport jetbrains.buildServer.configs.kotlin.triggers.vcs\n\nobject Build : BuildType({\n\n    name = \"Build and Deploy\"\n\n    vcs {\n\n        root(DslContext.settingsRoot)\n\n    }\n\n    steps {\n\n        script {\n\n            name = \"Build\"\n\n            scriptContent = \"\"\"\n\n                echo \"Building application...\"\n\n                ./gradlew build\n\n            \"\"\".trimIndent()\n\n        }\n\n        script {\n\n            name = \"Test\"\n\n            scriptContent = \"./gradlew test\"\n\n        }\n\n        script {\n\n            name = \"Deploy\"\n\n            scriptContent = \"./deploy.sh\"\n\n        }\n\n    }\n\n    triggers {\n\n        vcs {\n\n            branchFilter = \"+:*\"\n\n        }\n\n    }\n\n})\nTriggering builds on VCS commits\nJenkins requires either polling or webhook configuration with plugins:\npipeline {\n\n    triggers {\n\n        pollSCM('H/5 * * * *')  // Poll every 5 minutes\n\n        // OR\n\n        githubPush()  // Requires GitHub plugin\n\n    }\n\n    stages {\n\n        stage('Build on Commit') {\n\n            when {\n\n                anyOf {\n\n                    branch 'main'\n\n                    branch 'develop'\n\n                    changeRequest()\n\n                }\n\n            }\n\n            steps {\n\n                sh './build.sh'\n\n            }\n\n        }\n\n    }\n\n}\nTeamCity provides native VCS integration with sophisticated branch filtering:\nobject BuildOnCommit : BuildType({\n\n    name = \"Build on VCS Commit\"\n\n    vcs {\n\n        root(DslContext.settingsRoot)\n\n        branchFilter = \"\"\"\n\n            +:refs/heads/main\n\n            +:refs/heads/develop\n\n            +:refs/heads/feature/*\n\n        \"\"\".trimIndent()\n\n    }\n\n    triggers {\n\n        vcs {\n\n            branchFilter = \"+:*\"\n\n            enableQueueOptimization = false\n\n        }\n\n    }\n\n    steps {\n\n        script {\n\n            name = \"Build Application\"\n\n            scriptContent = \"./build.sh\"\n\n        }\n\n    }\n\n})\nEnvironment variables and configuration\nJenkins handles environment variables through pipeline syntax:\npipeline {\n\n    environment {\n\n        DATABASE_URL = 'jdbc:postgresql://localhost:5432/mydb'\n\n        API_KEY = credentials('api-key')\n\n    }\n\n    stages {\n\n        stage('Deploy') {\n\n            environment {\n\n                DEPLOY_ENV = 'staging'\n\n            }\n\n            steps {\n\n                sh 'echo \"Deploying to ${DEPLOY_ENV}\"'\n\n            }\n\n        }\n\n    }\n\n}\nTeamCity provides more flexible parameter management with type safety:\nobject Deploy : BuildType({\n\n    name = \"Deploy Application\"\n\n    params {\n\n        text(\"database.url\", \"jdbc:postgresql://localhost:5432/mydb\")\n\n        password(\"api.key\", \"credentialsJSON:api-key\")\n\n        select(\"deploy.environment\", \"staging\", options = listOf(\"staging\", \"production\"))\n\n    }\n\n    steps {\n\n        script {\n\n            scriptContent = \"\"\"\n\n                echo \"Deploying to %deploy.environment%\"\n\n                ./deploy.sh --env %deploy.environment%\n\n            \"\"\".trimIndent()\n\n        }\n\n    }\n\n})\nTest reporting integration\nJenkins requires plugins for comprehensive test reporting:\npipeline {\n\n    stages {\n\n        stage('Test') {\n\n            steps {\n\n                sh './gradlew test'\n\n            }\n\n            post {\n\n                always {\n\n                    publishTestResults([\n\n                        testResultsFiles: 'build/test-results/test/*.xml',\n\n                        allowEmptyResults: false\n\n                    ])\n\n                    publishHTML([\n\n                        allowMissing: false,\n\n                        alwaysLinkToLastBuild: true,\n\n                        keepAll: true,\n\n                        reportDir: 'build/reports/tests/test',\n\n                        reportFiles: 'index.html',\n\n                        reportName: 'Test Report'\n\n                    ])\n\n                }\n\n            }\n\n        }\n\n    }\n\n}\nTeamCity provides built-in test intelligence with automatic failure assignment:\nobject TestWithReporting : BuildType({\n\n    name = \"Test with Reporting\"\n\n    steps {\n\n        script {\n\n            name = \"Run Tests\"\n\n            scriptContent = \"./gradlew test\"\n\n        }\n\n    }\n\n    features {\n\n        xmlReport {\n\n            reportType = XmlReport.XmlReportType.JUNIT\n\n            rules = \"build/test-results/test/*.xml\"\n\n        }\n\n        htmlReport {\n\n            reportDir = \"build/reports/tests/test\"\n\n            startPage = \"index.html\"\n\n            reportName = \"Test Results\"\n\n        }\n\n        investigationsAutoAssigner {\n\n            users = \"teamlead\"\n\n            assignOnSecondFailure = true\n\n            assignOnNewFailure = true\n\n        }\n\n    }\n\n    failureConditions {\n\n        executionTimeoutMin = 30\n\n        testFailure = false  // Don't fail build on test failures, just report\n\n    }\n\n})\nConditional stages and matrix builds\nJenkins’s conditional logic can become complex and hard to maintain:\npipeline {\n\n    stages {\n\n        stage('Deploy to Production') {\n\n            when {\n\n                branch 'main'\n\n                environment name: 'DEPLOY_PROD', value: 'true'\n\n            }\n\n            steps {\n\n                sh './deploy-prod.sh'\n\n            }\n\n        }\n\n    }\n\n    strategy {\n\n        matrix {\n\n            axes {\n\n                axis {\n\n                    name 'JAVA_VERSION'\n\n                    values '11', '17', '21'\n\n                }\n\n                axis {\n\n                    name 'OS'\n\n                    values 'ubuntu-latest', 'windows-latest'\n\n                }\n\n            }\n\n        }\n\n        stages {\n\n            stage('Test Matrix') {\n\n                steps {\n\n                    sh './test-java-${JAVA_VERSION}.sh'\n\n                }\n\n            }\n\n        }\n\n    }\n\n}\nTeamCity’s approach is more explicit and maintainable, with better reusability:\nobject ProductionDeploy : BuildType({\n\n    name = \"Production Deploy\"\n\n    steps {\n\n        script {\n\n            name = \"Deploy to Production\"\n\n            scriptContent = \"./deploy-prod.sh\"\n\n            conditions {\n\n                equals(\"teamcity.build.branch\", \"main\")\n\n                equals(\"deploy.environment\", \"production\")\n\n            }\n\n        }\n\n    }\n\n})\n\n// Matrix builds as separate build configurations with reusable functions\n\nfun createTestBuild(javaVersion: String, os: String): BuildType {\n\n    return BuildType({\n\n        name = \"Test Java $javaVersion on $os\"\n\n        params {\n\n            text(\"java.version\", javaVersion)\n\n            text(\"agent.os\", os)\n\n        }\n\n        steps {\n\n            script {\n\n                scriptContent = \"./test-java-%java.version%.sh\"\n\n            }\n\n        }\n\n        requirements {\n\n            equals(\"system.os\", os)\n\n        }\n\n    })\n\n}\n\n// Create matrix builds programmatically\n\nval testBuilds = listOf(\n\n    createTestBuild(\"11\", \"Linux\"),\n\n    createTestBuild(\"17\", \"Linux\"),\n\n    createTestBuild(\"21\", \"Linux\"),\n\n    createTestBuild(\"11\", \"Windows\"),\n\n    createTestBuild(\"17\", \"Windows\"),\n\n    createTestBuild(\"21\", \"Windows\")\n\n)\nArtifact management\nJenkins’s artifact handling requires careful configuration and lacks sophisticated dependency management:\npipeline {\n\n    stages {\n\n        stage('Build') {\n\n            steps {\n\n                sh './gradlew build'\n\n            }\n\n            post {\n\n                success {\n\n                    archiveArtifacts([\n\n                        artifacts: 'build/libs/*.jar,build/distributions/*.zip',\n\n                        allowEmptyArchive: false,\n\n                        fingerprint: true\n\n                    ])\n\n                }\n\n            }\n\n        }\n\n        stage('Deploy') {\n\n            steps {\n\n                // Copy artifacts from upstream build\n\n                copyArtifacts([\n\n                    projectName: 'upstream-job',\n\n                    selector: lastSuccessful(),\n\n                    target: 'artifacts/'\n\n                ])\n\n                sh 'deploy.sh artifacts/*.jar'\n\n            }\n\n        }\n\n    }\n\n}\nTeamCity provides sophisticated artifact management, with dependency tracking and automatic cleanup:\nobject BuildWithArtifacts : BuildType({\n\n    name = \"Build and Archive\"\n\n    steps {\n\n        script {\n\n            name = \"Build Application\"\n\n            scriptContent = \"./gradlew build\"\n\n        }\n\n    }\n\n    artifactRules = \"\"\"\n\n        build/libs/*.jar => libs/\n\n        build/distributions/*.zip => distributions/\n\n        build/reports/** => reports/\n\n    \"\"\".trimIndent()\n\n    cleanup {\n\n        keepRule {\n\n            id = \"keep_successful_builds\"\n\n            keepAtLeast = days(30)\n\n            applyToBuilds {\n\n                inBranches {\n\n                    branchFilter = \"+:refs/heads/main\"\n\n                }\n\n                withStatus = BuildStatus.SUCCESSFUL\n\n            }\n\n            preserveArtifacts = PreserveArtifacts.ALL\n\n        }\n\n    }\n\n})\n\nobject DeployWithArtifacts : BuildType({\n\n    name = \"Deploy Application\"\n\n    dependencies {\n\n        artifacts(BuildWithArtifacts) {\n\n            buildRule = lastSuccessful()\n\n            artifactRules = \"\"\"\n\n                libs/*.jar => app/\n\n                distributions/*.zip => packages/\n\n            \"\"\".trimIndent()\n\n        }\n\n    }\n\n    steps {\n\n        script {\n\n            name = \"Deploy\"\n\n            scriptContent = \"\"\"\n\n                echo \"Deploying artifacts...\"\n\n                ./deploy.sh app/*.jar\n\n            \"\"\".trimIndent()\n\n        }\n\n    }\n\n})\nBenefits of Kotlin DSL\nThe advantages of TeamCity’s Kotlin DSL approach become clear when you’re managing complex pipelines:\nType safety and IDE support: Your IDE can provide autocompletion, refactoring tools, and compile-time error checking for pipeline configurations. This eliminates the trial-and-error cycles common with Jenkinsfile development.\nReusability and modularity: You can create reusable functions, templates, and shared configuration objects. This reduces duplication and makes it easier to maintain consistent patterns across multiple projects.\nVersion-control integration: Kotlin DSL configurations are easier to review in pull requests, track changes over time, and understand the impact of modifications. The type system makes it clearer what each change affects.\nWhat does a safe migration plan look like?\nSuccessfully migrating from Jenkins to a more modern system requires a structured, phased approach that minimizes risk while incrementally proving value as you go.\nYou’ll get the most value out of your migration if you don’t treat it as a direct-translation exercise but as an opportunity to modernize and improve your CI/CD workflows.\nPhase 1: Discovery and assessment\nGoal: Understand your current Jenkins environment and identify the biggest challenges you want to overcome through migration.\nKey question: What are the most significant problems we want to solve by migrating, and how will you measure success?\nThe discovery phase involves conducting a comprehensive audit of your Jenkins instances and documenting all active jobs, pipelines, and their dependencies. Create an inventory that includes Jenkinsfiles, freestyle jobs, shared libraries, custom plugins, and external integrations. Pay special attention to identifying critical pipelines that cannot afford downtime and any custom functionality that might require special handling.\nThis audit should go beyond simple documentation. Ask what business goal each Jenkins pipeline was originally designed to accomplish. Many complex Jenkins configurations exist as workarounds for platform limitations rather than optimal solutions. Similarly, some pipelines may have evolved organically, accumulating complexity that can be eliminated in a fresh implementation.\nDuring this phase, engage with development teams to understand their pain points with the current system. Document specific examples of maintenance overhead, debugging challenges, and workflow inefficiencies. This information will guide your migration priorities and help you measure the success of your TeamCity implementation.\nPhase 2: Pilot setup\nGoal: Set up a low-risk TeamCity environment, and demonstrate core capabilities with a representative workload.\nKey question: Can you prove measurable value at a small scale before committing to a broader migration?\nChoose a pilot project that represents typical patterns in your organization but won’t impact critical business operations if problems occur. This might be an internal tool, a staging-environment pipeline, or a new feature-branch workflow. The goal is to validate TeamCity’s capabilities with real work while building team confidence.\nResist the temptation to simply recreate your Jenkins pipeline as is in TeamCity. Treat the pilot as an opportunity to implement best practices from the start. Use TeamCity’s visual build chains to clarify pipeline dependencies, leverage Kotlin DSL for maintainable configuration, and take advantage of built-in features like test parallelization and flaky test detection.\nSet up side-by-side comparisons where possible. Run the same builds in both Jenkins and TeamCity to compare performance, reliability, and developer experience. Document specific improvements, such as reduced build times, clearer failure diagnostics, or easier configuration management.\nCollect detailed feedback from the development team using the pilot system. Focus on daily workflow impact. Is it easier to understand build failures? Are configuration changes simpler to make? Does the system provide better visibility into test results?\nPhase 3: Incremental migration\nGoal: Migrate remaining pipelines based on complexity and business criticality while maintaining Jenkins as a fallback option.\nKey question: How can you migrate safely without disrupting team productivity or deployment capabilities?\nDevelop a migration priority matrix based on pipeline complexity, business criticality, and team readiness. Start with simpler, less critical pipelines to build expertise and confidence. Save the most complex or business-critical systems for later when your team has developed migration best practices.\nFor each pipeline migration, follow a consistent modernization approach. Review the original Jenkins implementation to understand its intended purpose. Then, design a TeamCity solution that accomplishes the same goals using native platform capabilities. This often results in simpler, more maintainable configurations.\nMaintain Jenkins pipelines in parallel during this phase so you can do quick rollbacks if issues arise. Use feature flags or branch-based routing to gradually shift traffic to TeamCity while keeping Jenkins as a safety net. This parallel operation also allows you to validate that TeamCity implementations produce identical results.\nUse Kotlin DSL consistently for new TeamCity configurations. This investment in type-safe, maintainable pipeline definitions pays dividends as your migration scales and more teams adopt the platform.\nPhase 4: Optimization\nGoal: Take full advantage of TeamCity’s advanced features to achieve benefits beyond basic pipeline migration.\nKey question: How can you unlock capabilities that weren’t possible with Jenkins to improve development velocity and quality?\nThis phase focuses on using TeamCity’s built-in intelligence and optimization features. Enable flaky test detection to automatically identify unreliable tests that waste developer time. Configure test parallelization to reduce build times and provide faster feedback on changes.\nImplement visual build chains to make complex workflows easier to understand and maintain. Use TeamCity’s dependency management to optimize resource utilization and reduce unnecessary work. Take advantage of TeamCity’s superior observability features. Set up dashboards that provide clear visibility into build performance, test results, and system health. Configure meaningful notifications that help developers focus on actionable issues rather than noise.\nConsider implementing TeamCity’s integration features with development tools like IDEs, issue trackers, and deployment platforms. These integrations can significantly improve developer workflow efficiency when working with TeamCity builds.\nPhase 5: Full cutover\nGoal: Complete the migration by retiring Jenkins and ensuring all teams are successfully operating on TeamCity.\nKey question: Are you confident enough in your TeamCity implementation to retire Jenkins and scale this success to other teams?\nEstablish comprehensive monitoring for your TeamCity environment before retiring Jenkins. This includes system performance metrics, build success rates, and developer satisfaction indicators. You want to catch any issues quickly and have data to demonstrate migration success.\nOnce you’re ready, create a formal Jenkins retirement plan that includes data archival, access revocation, and infrastructure decommissioning. Ensure you maintain access to historical build data and artifacts as needed for compliance or debugging purposes.\nDocument migration outcomes and lessons learned. Consider creating case studies that demonstrate measurable improvements in build performance, developer productivity, and system maintainability. This documentation will be valuable for expanding TeamCity adoption to other teams and for ongoing system optimization.\nIt may also be worth establishing an internal guild focused on CI/CD best practices with TeamCity. This group can help new teams adopt the platform effectively and continue evolving your implementation based on emerging needs.\nExplaining the migration to management\nTechnical migrations often fail not because of technology issues but because of inadequate stakeholder communication and buy-in. Leadership needs to understand both the business case for migration and the risk-mitigation strategies that make it a safe investment.\nKey benefits for leadership\nPresent migration benefits in terms of business impact rather than technical features.\nLower CI/CD maintenance overhead translates to engineering time that can be redirected from infrastructure firefighting to product development. Quantify this where possible: If senior engineers currently spend 10 percent of their time managing Jenkins, that represents a significant opportunity cost.\nFaster builds and deployment cycles directly impact time to market and customer responsiveness. Demonstrate how TeamCity’s optimizations can reduce feedback loops and enable more frequent releases. This is particularly compelling for organizations pursuing competitive advantages through rapid innovation.\nBetter test insights and quality visibility reduce production incidents and customer-impacting defects. TeamCity’s flaky test detection and failure-analysis capabilities help teams identify quality issues before they reach customers, reducing support burden and protecting brand reputation.\nRisk-mitigation talking points\nAddress leadership concerns about migration risk directly and specifically.\nTeamCity supports parallel CI environments, which means you can validate the new system thoroughly without disrupting current operations. This isn’t a risky “big bang” migration that puts business continuity at stake.\nNo full cutover is required until teams are ready, which allows for controlled, incremental adoption. Teams can migrate when they have capacity and confidence rather than being forced into arbitrary timelines that create unnecessary stress and risk.\nThe phased rollout approach means early phases can be reversed easily if issues arise, while later phases build on proven success. This creates multiple decision points where leadership can evaluate progress and adjust strategy based on actual results rather than theoretical projections.\nBusiness-case template\nBelow is a template you can adapt for your organization’s specific situation and leadership communication style. You can find an editable copy of this template in the supporting files.\nProposal: TeamCity Migration to Reduce CI/CD Maintenance and Improve Development Velocity\nExecutive summary\nOur engineering team is requesting approval to migrate from Jenkins to TeamCity for our CI/CD infrastructure. This migration will reduce maintenance overhead currently consuming [X hours per week] of senior engineering time, improve build performance by an estimated [Y percent], and provide better visibility into code-quality issues before they reach production.\nCurrent challenges\nOur [your team name] team currently experiences [current pain point] with our Jenkins infrastructure. For example, [pipeline example] requires [specific maintenance burden or performance issue]. These issues are consuming approximately [time estimate] of engineering capacity that could be better spent on [strategic initiatives].\nProposed solution\nTeamCity offers a modern CI/CD platform with built-in capabilities that address our current pain points:\nReduced maintenance: The Kotlin DSL configuration eliminates plugin-compatibility issues and provides IDE support for pipeline development.\nImproved performance: Built-in test parallelization and intelligent caching can reduce build times by [estimated percentage].\nBetter quality insights: Automatic flaky test detection and comprehensive failure analysis help teams identify issues faster.\nRisk mitigation\nThis migration uses a proven, low-risk approach:\nParallel operation: TeamCity will run alongside Jenkins during transition, allowing immediate rollback if issues arise.\nIncremental adoption: We’ll migrate one pipeline at a time, starting with noncritical systems to validate the approach.\nPilot validation: Initial implementation will focus on [specific low-risk project] to prove value before broader adoption.\nSuccess metrics\nWe will measure migration success through these metrics:\nEngineering efficiency: Reduction in CI/CD maintenance time from [current] to [target].\nBuild performance: Average build-time improvement of [estimated percentage].\nQuality indicators: Faster identification of test issues and deployment problems.\nTimeline and investment\nPhase 1 (discovery): [timeframe] – current system audit and TeamCity environment setup.\nPhase 2 (pilot): [timeframe] – single project migration and validation.\nPhase 3 (rollout): [timeframe] – incremental migration based on pilot results.\nRecommendation\nWe recommend proceeding with Phase 1 (discovery) and a pilot implementation. This low-risk initial investment of [resource estimate] will provide concrete data about TeamCity’s benefits for our specific use cases and inform decisions about broader adoption.\nNext steps\nWith your approval, we will:\nConduct a comprehensive Jenkins environment audit.\nSet up a TeamCity evaluation environment.\nExecute pilot migration with [specific project].\nReport our results and recommendations for Phase 3.\nPlease let me know if you have questions about this proposal or would like additional details about any aspect of the migration plan.\nConclusion: Making migration a strategic success\nJenkins isn’t an inherently bad tool: It served the industry well during the early days of continuous integration and enabled countless teams to adopt automated build and deployment practices. However, like many tools from that era, Jenkins was designed for constraints and expectations that were different from what modern software development teams face today.\nIf your team is spending more time maintaining CI/CD infrastructure than building products, experiencing frequent pipeline-reliability issues, or lacking visibility into build and test performance, migration to a modern platform like TeamCity represents a strategic investment in developer productivity and system reliability.\nThe key to successful migration is approaching it as a systematic, risk-managed process rather than a disruptive technology replacement. With the right planning framework, you can migrate safely while improving your development workflows and team efficiency.\nStart with the migration-readiness checklist to perform an honest assessment of your current situation and organizational readiness. This self-evaluation will help you determine whether migration makes sense for your team and identify areas that need attention regardless of platform choice.\nExperiment with Kotlin DSL samples to understand how TeamCity’s approach differs from Jenkins and what benefits it offers for your specific use cases. The type safety and IDE support alone often convince teams that the learning investment is worthwhile.\nExecute a focused pilot project to validate TeamCity’s benefits in your environment with real workloads and development patterns. This proof-of-concept approach allows you to make data-driven decisions about broader adoption while building team confidence and expertise.\nTeamCity is a modern, scalable CI/CD platform that supports powerful pipeline authoring via Kotlin DSL, built-in test intelligence, and visual pipeline chains. Whether you’re a team of five or five hundred developers, TeamCity helps you ship software with confidence through better tooling, clearer visibility, and reduced maintenance overhead.\nThe migration frameworks and templates in this guide provide the structure you need to execute a successful transition. However, every organization’s situation is unique, so keep in mind that you’ll likely adapt these approaches based on your specific constraints and requirements.\nReady to explore how TeamCity can improve your development workflow? Visit the TeamCity website to learn more about our platform capabilities, or contact our team to discuss your specific migration requirements and get expert guidance for your transition planning.",
        "dc:creator": "Dmitrii Korovin",
        "content": "This article was brought to you by Cameron Pavey, draft.dev. Jenkins has served the development community well for over a decade, but it was designed for a different era of software development. Developers who are tired of fighting with plugin compatibility issues, slow builds, and brittle configurations are exploring alternatives. But is your organization ready [&#8230;]",
        "contentSnippet": "This article was brought to you by Cameron Pavey, draft.dev. Jenkins has served the development community well for over a decade, but it was designed for a different era of software development. Developers who are tired of fighting with plugin compatibility issues, slow builds, and brittle configurations are exploring alternatives. But is your organization ready […]",
        "guid": "https://blog.jetbrains.com/?post_type=teamcity&p=677066",
        "categories": [
          "best-practices",
          "teamcity-2",
          "devopspains",
          "jenkins"
        ],
        "isoDate": "2026-01-26T18:25:53.000Z"
      },
      {
        "creator": "Fatimazahra El Akkary",
        "title": "Building AI Agents in Kotlin – Part 5: Teaching Agents to Forget",
        "link": "https://blog.jetbrains.com/ai/2026/01/building-ai-agents-in-kotlin-part-5-teaching-agents-to-forget/",
        "pubDate": "Mon, 26 Jan 2026 16:09:12 +0000",
        "content:encodedSnippet": "Previously in this series:\nBuilding AI Agents in Kotlin – Part 1: A Minimal Coding Agent\nBuilding AI Agents in Kotlin – Part 2: A Deeper Dive Into Tools\nBuilding AI Agents in Kotlin – Part 3: Under Observation\nBuilding AI Agents in Kotlin – Part 4: Delegation and Sub-Agent\nAgents eventually run out of context. When they do, they crash, and you lose everything mid-task.\nWe’ve been running GPT-5 Codex since Part 1. It scores 0.58 on SWE-bench Verified. We tried Claude Sonnet 4.5 next, which scored 0.6 and ran faster on most tasks. But complex problems hit Claude’s 200K context window faster. \nYou’ll probably find yourself switching models too, for better performance, lower cost, or to run locally. Sometimes that means smaller context windows, especially for local models limited by expensive memory. But even the biggest context windows fail on complex and long tasks. You can’t just keep buying more context.\nThe problem is that agents hold onto everything: every file, every command output, every search result, every user message. Eventually, there’s no room left.\nThat’s where compression comes in. But not the lazy kind that just drops old messages when you run out of space. Think about handing off a task to another developer. You don’t give them a transcript of everything you did. You tell them the goal, what files you changed, what worked, and what didn’t. That’s smart compression: Keep the context needed to continue; drop the verbose history.\nLet’s figure out how to implement this in Koog. First, we need to understand what strategy = singleRunStrategy() has been doing since Part 1. This is where you see how strategies control your agent’s loop and how you can modify them to create your own flow. We’ll examine singleRunStrategy(), and then build a version that compresses automatically. \nWhat that strategy line does\nIn the previous parts, you built a coding agent. You gave it tools, along with this line:\nstrategy = singleRunStrategy()\nHere, a strategy is the code that runs your agent loop. All strategies share the same core elements: call the LLM, execute tools, send results back, repeat. But they differ in when they stop and what they do between iterations.\nsingleRunStrategy() is the simplest possible version. It keeps iterating as long as the LLM returns tool calls. In other words: call the LLM → return tool call? → execute → call again → return text? → done.\n\n\n\n\nIt works fine for simple tasks. But on complex problems, the history keeps growing. Every command output, every file read, every search result stays in context. Eventually you hit the limit and crash mid-task.\nWhat we need instead is a strategy that runs the same loop but also checks the history size and compresses it when it grows too large.\nAdding compression\nWe’re swapping this:\nstrategy = singleRunStrategy()\nFor this:\nstrategy = singleRunStrategyWithHistoryCompression()\nThe loop is the same, but there’s now a checkpoint between Execute Tool and Send Tool Result. After each tool execution, the strategy asks:\nCheck: Is the history greater than the threshold?\nIf yes? Compress it: Extract the important facts. Drop the rest.\nIf no? Continue as usual.\n\n\n\n\nThat checkpoint is what lets your agent complete long-running tasks within tighter token budgets to avoid the error “context window exceeded”.\nBut you have to configure it. The strategy can’t guess when the history is too big or what facts matter for your task. You have to tell it two things: when to compress, and what to keep.\nWhen to compress \nFirst, you set thresholds – how many messages or characters you allow before compression kicks in:\nval CODE_AGENT_HISTORY_TOO_BIG = { prompt ->\n   prompt.messages.size > 200 || prompt.messages.sumOf { it.content.length } > 200_000}\nWhy these numbers? We experimented with different thresholds after seeing where errors appeared.\n\n\n\n\nThis agent hit 220K tokens and crashed. Claude’s limit is 200K tokens. We needed to compress before reaching that point.\nWe set compression at 200 messages or 200,000 characters, whichever comes first. Note that the code measures characters, not tokens. They’re different, but this threshold keeps us under the token limit – high enough to avoid over-compression, low enough to prevent hitting the limit.\nThese numbers aren’t fixed. If your agent hits the limit earlier, lower the threshold. Still completing tasks past these thresholds? Raise them. The choice depends on your use case: file sizes, message length, how verbose your tool outputs are, task complexity. Experiment and find what works.\nWhat to keep\nYou’ve set when compression triggers. Now choose what to keep.\nThere are two options: Either trust the LLM to decide what’s important, or tell it exactly what to extract.\nOption 1: Trust the LLM to summarize\nThe LLM decides what’s important using WholeHistory:\ncompressionStrategy = WholeHistory\nWhen compression triggers, Koog asks the LLM to create a TL;DR summary of the entire conversation history.\nBefore compression:\nAll messages up to the threshold: system prompt, user messages, assistant responses, tool calls, tool results.\nAfter compression:\nSystem prompt (preserved).\nFirst user message (preserved, so the agent remembers the original goal). \nOne TL;DR summary message (written by the LLM).\nThe tradeoff: This approach is simple and fast, but you’re trusting the LLM to decide what matters. Sometimes it keeps exactly the right details. Sometimes it drops something critical.\nOption 2: Tell the LLM exactly what to extract\nInstead of telling the model to summarize everything, you specify exactly what facts to extract using RetrieveFactsFromHistory:\ncompressionStrategy = RetrieveFactsFromHistory(\n   Concept(...),\n   Concept(...),\n   ...\n)\nHow it works:\nYou define Concept objects: specific questions about your task that the agent must remember. When compression triggers, Koog makes one LLM call per concept, sending the full conversation history each time and asking just that single question.\n\n\n\n\nWhy separate calls? LLMs get worse at answering when you ask multiple questions at once. We saw this in testing: Bundle eight concepts into one prompt, and some answers come back vague or incomplete. Ask them one at a time, and each response is more reliable.\nDefining a concept\nEach Concept instance has three parts:\nkeyword: a label for logs \ndescription: the actual question or instruction the LLM should answer \nfactType: the expected format of the answer\n\nMULTIPLE for lists\nSINGLE for single value\nConcept(\n    keyword = \"project-structure\",\n    description = \"What is the project structure?\",\n    factType = FactType.MULTIPLE\n)\nChoosing the right concepts\nFor our coding agent, the key question is: What information, if lost, would force the agent to start over? \nHere’s what happens when compression drops critical information:\nThe agent opens the same file twice as though it’s never seen it before.\nIt rewrites tests that already exist.\nIt drifts away from the task you originally gave it.\nEach failure shows what must survive compression:\nRe-exploring files → You need a project-structure concept.\nRedoing finished work → You need an important-achievements concept.\nLosing direction → You need an agent-goal concept.\nOur coding agent concepts\nWhen we tested on SWE-bench-Verified, we ended up with eight concepts. Here are three of them:\nval CODE_AGENT_COMPRESSION_STRATEGY = RetrieveFactsFromHistory(\n    Concept(\n        \"project-structure\",\n        \"What is the structure of this project?\",\n        FactType.MULTIPLE\n    ),\n    Concept(\n        \"important-achievements\",\n        \"What has been achieved during the execution of this current agent?\",\n        FactType.MULTIPLE\n    ),\n    Concept(\n        \"agent-goal\",\n        \"What is the primary goal or task the agent is trying to accomplish in this session?\",\n        FactType.SINGLE\n    ),\n    ...\nYour agent may need different concepts. The goal isn’t to copy the list; it’s to identify what state your agent needs to continue working and define concepts that preserve that information.\nYou can check out the full implementation of the eight concepts on GitHub. \nWhich model to use \nJust like with sub-agents (Part 4), you can use different models for different parts of the process. The retrievalModel parameters lets you specify which LLM handles the history compression. This parameter is optional – if not specified, compression uses your agent’s main model.\nretrievalModel = OpenAIModels.Chat.GPT4_1Mini\nHere’s the complete configuration for the coding-agent strategy:\nstrategy = singleRunStrategyWithHistoryCompression(\n   config = HistoryCompressionConfig(\n       isHistoryTooBig = CODE_AGENT_HISTORY_TOO_BIG,\n       compressionStrategy = CODE_AGENT_COMPRESSION_STRATEGY,\n       retrievalModel = OpenAIModels.Chat.GPT4_1Mini\n   )\n)\nThree parameters: when to compress (isHistoryTooBig), what to keep (compressionStrategy), and which model does the work (retrievalModel).\nConclusion\nAt this point, your agent can run longer tasks without hitting context limits. The compression problem is solved. Instead of crashing when it runs out of space, the agent compresses its history, keeping decisions and outcomes while dropping verbose outputs, and continues working within whatever token budget you have.\nIn this series, we started with a basic coding agent in Part 1. Since then, we’ve added tools, observability, sub-agents, and history compression. These five pieces give you what you need to build working AI agents in Kotlin that can operate within the constraints of real models.\nIf you want to keep building and practicing these patterns, planning and reasoning are interesting areas to explore: how agents decide what to do across multiple turns and how they break down complex problems. We didn’t cover those here, but they’re good practice once you’ve got these pieces working. \nThe full code’s on GitHub. If you run into any issues, leave a comment. We’re happy to help 😊",
        "dc:creator": "Fatimazahra El Akkary",
        "content": "Previously in this series: Agents eventually run out of context. When they do, they crash, and you lose everything mid-task. We&#8217;ve been running GPT-5 Codex since Part 1. It scores 0.58 on SWE-bench Verified. We tried Claude Sonnet 4.5 next, which scored 0.6 and ran faster on most tasks. But complex problems hit Claude&#8217;s 200K [&#8230;]",
        "contentSnippet": "Previously in this series: Agents eventually run out of context. When they do, they crash, and you lose everything mid-task. We’ve been running GPT-5 Codex since Part 1. It scores 0.58 on SWE-bench Verified. We tried Claude Sonnet 4.5 next, which scored 0.6 and ran faster on most tasks. But complex problems hit Claude’s 200K […]",
        "guid": "https://blog.jetbrains.com/?post_type=ai&p=676707",
        "categories": [
          "kotlin",
          "tutorials",
          "ai",
          "ai-agents"
        ],
        "isoDate": "2026-01-26T16:09:12.000Z"
      },
      {
        "creator": "Sasha Korepanov",
        "title": "How We Made Variable Inspections 87 Times Faster for Unreal Engine in Rider",
        "link": "https://blog.jetbrains.com/dotnet/2026/01/26/how-we-made-variable-inspections-87-times-faster-for-unreal-engine-in-rider/",
        "pubDate": "Mon, 26 Jan 2026 13:08:53 +0000",
        "content:encodedSnippet": "If you’ve ever expanded a complex Unreal Engine variable in Rider’s debugger and had time to contemplate your life choices, this post is for you.\nWe’ve rewritten our expression evaluator, and the results are dramatic: Variable inspection is up to 87 times faster on warm runs and 16 times faster on cold ones. The debugger memory usage has dropped to just over a third of what it was. And there’s even a bonus benefit, but more on that at the end.\nThese improvements are already available for you to test in Rider 2026.1 EAP 1. But how did we get here? Buckle up – things are about to get very technical, but the payoff will be worth it.\nTest it in Rider 2026.1 EAP 1\n                                                    \nThe Natvis of it all\nVariable visualization is one of the most important features of any debugger. Without it, debuggers show just raw class fields – no array elements, no map contents, just raw pointers to memory, trees, and hash tables. There are many ways to tell debuggers how to display types nicely: pretty printers, data formatters, Natvis, and others.\nIf you use the MSVC C++ toolchain to build your programs, there’s a 99% chance you’re using Natvis. Natvis allows you to describe how to visualize your types. You can specify which items to show and describe that a type should be visualized as an array. Or, if you have a hash table, you can write complex expressions with temporary local variables, invisible to the debugged program, for extracting elements. Natvis also supports intrinsics – user-defined functions that prevent code duplication and make expressions more structured.\nTo understand what Natvis does, let’s look at a simple example with std::string using the standard stl.natvis:\n\n\n\n\nAs you can see, the debugger must evaluate at least 6 expressions when you expand a simple string object.\nThe LLDB challenge\nRider supports Natvis, and uses it when you debug native C++ applications on Windows. But here’s the tricky part: Rider uses LLDB as its native C++ debugger, and LLDB doesn’t support Natvis natively. Instead, LLDB supports data formatters, which allow writing arbitrary Python code.\nWe thought we had it all figured out when we developed a special data formatter that implements Natvis support under Rider’s hood. The formatter parses Natvis files – matching types, creating items, generating summary strings, and building child hierarchies… Except we ran into some problems. \nWe had to use LLDB’s expression evaluator to evaluate Natvis expressions. This evaluator uses Clang under the hood, which is powerful but brings significant issues:\nMaintenance pains: Clang parsers are notoriously difficult to maintain. Sometimes, valid Natvis expressions simply wouldn’t compile, and debugging these failures was extremely challenging.\nPerformance problems: It’s slow – very slow.\nLimited intrinsic support: There’s no simple way to register and call intrinsics.\nMemory pollution: Declaring temporary variables for expressions injects those variables into the debugged process.\n\n\n\n\nTo work around these issues, we accumulated hacks everywhere. We had hacks in Clang, tried inlining intrinsics, and used top-level function declarations (an LLDB feature) for intrinsics that couldn’t be inlined. To improve performance, we cached Clang parsing results. But all these workarounds were unreliable and unstable.\nIf you use Rider for Unreal Engine, you’ve probably noticed that after updating the Unreal Engine version, TMap or TArray visualization sometimes stops working. This happens because Unreal.natvis becomes more complex with every update, and LLDB’s expression evaluator can’t handle the new expressions properly. Additionally, using LLDB’s expression evaluator within LLDB data formatters is fundamentally problematic.\nBuilding our own solution\nTo eliminate these issues, we decided to rethink how we work with Natvis entirely. Our first step: develop an alternative expression evaluator for Natvis without Clang. We knew about lldb-eval – another custom expression evaluator for LLDB that’s fast and powerful – but we wanted complete control, so we implemented our own evaluator from scratch.\nYes, we threw out an entire time-tested system and rewrote everything. Before you question our sanity, let us show you why it was worth it.\nThe results\nWe didn’t create artificial synthetic projects for testing. Instead, we used a standard Lyra game demo and created a script that recursively dumps variable children, emulating the scenario where you expand variables to see their contents. The script processes 6,000 values with two measurement scenarios: cold debugger (the very first evaluation, like a fresh debug session) and warm debugger (with all caches warmed up).\n\nColdWarmDebugger RAM usage\nStandard LLDB evaluator115 seconds7 seconds9 GB\nOur new evaluator7 seconds0.08 seconds3.5 GB\n\n\n\n\n\nThat’s 16 times faster on cold starts and 87 times faster on warm runs! Plus, it takes just over a third of the memory as before. The difference is immediately noticeable during debugging. On massive Unreal Engine projects, the improvement is even more dramatic.\nThe bonus benefit we promised you\nOur new expression evaluator delivered two additional benefits we did not foresee.\n🎉 Core dumps finally work properly . The standard LLDB evaluator performs poorly with core dumps – almost every expression from Unreal.natvis fails, resulting in terrible visualization. With our new evaluator, Natvis works completely and reliably. The difference is striking:\nWhat Rider 2025.3 with the standard LLDB expression evaluator looks like in comparison to Rider 2026.1 EAP 1 with the alternative LLDB expression evaluator.\n\n\n\nPerformance is better across the board. Since this is a general expression evaluator, we also use it for:\nNormal expression evaluation (Add to Watches actions)\nConditional breakpoints\nWhile the performance boost here isn’t as dramatic and depends on the specific expression, even simple expressions make conditional breakpoints at least 20% faster. For complex expressions, the difference can be even larger!\nWhat do you think?\nThese improvements are already available for you to test in the Early Access Program builds for Rider 2026.1. Give one of the EAP builds a try (they’re free!) and let us know how much faster your debugging sessions become. \nTest it in Rider 2026.1 EAP 1\n                                                    \nIf you enjoyed this story, perhaps you’d also like a to know how we made LLDB’s stepping time 50 times faster in Rider.  Cheers!",
        "dc:creator": "Sasha Korepanov",
        "content": "If you&#8217;ve ever expanded a complex Unreal Engine variable in Rider&#8217;s debugger and had time to contemplate your life choices, this post is for you. We&#8217;ve rewritten our expression evaluator, and the results are dramatic: Variable inspection is up to 87 times faster on warm runs and 16 times faster on cold ones. The debugger [&#8230;]",
        "contentSnippet": "If you’ve ever expanded a complex Unreal Engine variable in Rider’s debugger and had time to contemplate your life choices, this post is for you. We’ve rewritten our expression evaluator, and the results are dramatic: Variable inspection is up to 87 times faster on warm runs and 16 times faster on cold ones. The debugger […]",
        "guid": "https://blog.jetbrains.com/?post_type=dotnet&p=676804",
        "categories": [
          "net-tools",
          "rider",
          "eap",
          "game-debugging",
          "game-development",
          "lldb",
          "unreal-engine"
        ],
        "isoDate": "2026-01-26T13:08:53.000Z"
      }
    ]
  },
  {
    "name": "Airbnb Engineering & Data Science",
    "category": "기업",
    "posts": [
      {
        "creator": "Lauren Mackevich",
        "title": "My Journey to Airbnb: Peter Coles",
        "link": "https://medium.com/airbnb-engineering/my-journey-to-airbnb-peter-coles-0e8efd01c5a8?source=rss----53c7c27702d5---4",
        "pubDate": "Wed, 28 Jan 2026 18:04:20 GMT",
        "content:encodedSnippet": "Public school to PhD\nThe story of Airbnb’s Head Economist for Policy and Director of Data Science involves geology, co-teaching with a Nobel Prize winner, and CSI. (No, not the hit TV franchise.)\nPeter Coles was born and raised in Milwaukee, Wisconsin. He studied math at Princeton, earned his PhD in economics at Stanford, and taught at Harvard Business School before joining eBay and becoming a Data Science leader at Airbnb.\nAs you’ll see from his story, Peter has a deep interest in how marketplaces work. By transitioning from academia to the business world, he not only gets to study first-hand data about millions of guests and hosts, but also to influence product and policy decisions. And he still gets to hang out with academics. Check out all the research Peter and his team are doing here.\n\nMy fascination with marketplaces goes back a long time.\nSometime around second grade in Milwaukee, Wisconsin (where I grew up), my friends and I had the great idea to run a rock stand. It was like a lemonade stand, but instead we would sell rocks. Rocks we found in the street. Neighborhood kids could find their own rocks and sell them, and we’d take 25%. Nobody got any sales. Fortunately I’ve learned a bit more about marketplaces since then — more about that shortly.\nFrom kindergarten through high school, I was a public school kid. My parents valued education, and helping others — my father was a doctor, my mother a nutritionist — and those are values I still hold dear.\nWhile I played soccer and tennis and was moderately social, this period was probably best defined by an obsession with competitions. Math, Chess, Science Olympiad, Quiz Bowl, Academic Decathlon, puzzle races with my younger brother — there was hardly a nerdy competition offered where I didn’t compete. Time well spent? Let’s just say I missed a lot of high school parties while studying to become the five-time Wisconsin State Rocks, Minerals, and Fossil Identification champion — so you can be the judge.\nBy the time college came around, it seemed time to rebel. I wanted to be done with the nerdy stuff. I applied and was accepted to Princeton, and started studying ancient history. After one semester and (in my view) an underappreciated essay on the Hittites, I was back to majoring in math. At least I was good at that! I figured I could work on practical skills later.\nAfter graduating, I accepted a fellowship in Germany and continued to Stanford for a PhD in economics — a somewhat more applied science, though I focused on game theory, at the intersection of math and strategy. I had the good fortune there to be the second graduate student of Jon Levin, now the President of Stanford University, who taught me the importance of simplification in research — even when the subject matter itself is complex.\nEven while in this still-theoretical space, I kept my feet on the ground — or at least on the pedals. During a monthlong break in my classes in Germany, I biked around Europe, crashing with friends and family members of my classmates — people I had never met before staying with them. In a sense, I was prototyping Airbnb well before it existed!\nMy time in graduate school was Silicon Valley in the 2000s, after the dot-com crash, so tech was in the midst of a renaissance. Many of my friends were at growing companies like Google and Amazon. It was very tempting to stay in California to be a part of this, but I ended up with one more stop in academia.\nMarkets in theory, markets in practice\nHarvard Business School, known for its focus on managerial science, was perhaps the most compelling place in the academic world that would allow me to stay close to the tech industry. I got a double stroke of good fortune: not only was I offered an assistant professorship there, but I also got to co-teach with Al Roth, a founder of the field of Market Design. Al is still an important mentor to me, and later won a Nobel Prize!\nIn my time researching and teaching graduate students, I was exposed to many examples of market design, conducting research on the topic of “Matching”; that is, mechanisms to pair users from two groups, often when price cannot be used to clear the market. This covered strategy of participants, signaling in markets, and I even had a chance to improve the market for PhD economists. I also wrote a number of case studies, including on Zillow, Microsoft, Craigslist, and more. The teaching and writing was a lot of fun, but I also came to realize I wasn’t a fit for academia in the long term. My attention span was too short to dedicate most of my time to research papers (and especially peer reviews), but I was enormously appreciative of this phase of my career.\nBy this point it was 2013, and two simultaneous and interrelated phenomena were exploding in tech: mobile and the sharing economy. It was a perfect time to head back west and finally enter the tech world.\nI landed at eBay, which for a student of marketplaces was an ideal company: just about everything is for sale, and it was ripe for market design. Steve Tadelis, a mentor from my Stanford days, had created one of the first economics teams inside a tech company, which I took over when Steve left. At the same time, eBay was getting on the data science train — this was before every company had a DS team — and my group joined another to form eBay’s Data Labs. One of my favorite projects there was a project called “What’s it Worth” (which I worked on with Airbnb colleague Dean Chen), where we developed a methodology for determining the fair market value of items. Some hands-on practical work, some modeling — this was just what I was hoping for.\nIn 2015 Riley Newman, one of Airbnb’s first employees and then its Head of Data Science, presented an even more enticing opportunity. The Airbnb platform was growing quickly, and for the first time attracting substantial regulatory attention. They needed an economics team to partner with the growing policy team, to jointly address the question of Airbnb’s relationship to cities. This was a new way for me to apply economics. I was all in.\nAddressing Airbnb’s critical questions with data\nWhen I think back to my eight and a half years so far at Airbnb, I view this as entailing three “phases.” In the first, I worked to address economic questions by establishing a global team of data scientists and economists to analyze the relationship of short-term rentals to the world.\nMeanwhile, as Airbnb continued to grow, execs were asking big questions that couldn’t be answered by any specific data science team. They needed a group with visibility across the whole organization. So in this second phase, Jackson Wang and I founded a team called Central Strategy & Insights, or CSI.\nThe acronym was no coincidence: we saw ourselves as forensic investigators, piecing together stories as we collected evidence. One important period of CSI’s work addressed changes brought on by the pandemic — in particular a major adjustment in where guests were looking to stay, and the supply we’d need to accommodate them. We also led the company’s business reviews, and generated analyses to describe the business to shareholders ahead of the IPO.\nMy third phase at Airbnb started a lot like the first, but supersized: developing models to inform a well-considered approach to policy considerations, this time as travel rebounded after the pandemic and governments were no longer fully occupied with a public health emergency. Our newly expanded group of economics PhDs and analysts also came up with ways to evaluate Airbnb’s impact on guests, hosts, and society, including via our US Economic Impact Report.\nA great balance of academic and applied science\nAlmost all of my first several years at Airbnb had been internally facing. That’s changed in recent times, as we’ve spun up and expanded a program to collaborate with academic researchers to analyze Airbnb’s data and improve the experience for users.\nThe first step was to figure out how to collaborate with external researchers, while respecting privacy and legal limitations. Collaboration interest then came quickly. We have now published well-received papers with professors from MIT, Berkeley, Stanford, UCLA, NYU and more, with others in progress. One paper I wrote with colleagues and academic partners develops foundations for what “quality” means in platforms, from an economic perspective.\nWe’ve also launched a monthly seminar where we invite our academic collaborators to discuss research with Airbnb data scientists and technologists. Developing research is great, but there’s nothing like live discussions to cross-pollinate and foster ideas. This builds on a strong collaborative learning tradition at Airbnb, with internal classes and reading groups to grow our skills and keep up with tech developments.\nAlongside engaging with academia, I’m so excited my data science colleagues and I have a mandate to be innovative and proactive. We have the space and encouragement to work on big ideas, even if they might take a year or two to prove out — and perhaps more importantly, even if some of the ideas fail. But nothing is more important than the people. I am proud of the students, scientists, and even professors I have hired here over the years, and love seeing them grow and find success.\nA license to tackle big topics, continual education, research on the product as well as its relationship to the outside world, amazing colleagues, and a direct connection to academia all make Airbnb a unique place to be a market designer, economist, and data scientist. Whether or not you spent your free time in eighth grade studying rocks.\nIf you want to learn more about the research happening at Airbnb you can read our published papers here. If this type of work interests you, check out our open roles.\n\nMy Journey to Airbnb: Peter Coles was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Lauren Mackevich",
        "guid": "https://medium.com/p/0e8efd01c5a8",
        "categories": [
          "people",
          "data-science",
          "economics",
          "technology"
        ],
        "isoDate": "2026-01-28T18:04:20.000Z"
      }
    ]
  },
  {
    "name": "PayPal Engineering",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Visual Studio Blog",
    "category": "기업",
    "posts": [
      {
        "creator": "Simona Liao",
        "title": "Visual Studio January Update — Enhanced Editor Experience",
        "link": "https://devblogs.microsoft.com/visualstudio/visual-studio-january-update-enhanced-editor-experience/",
        "pubDate": "Tue, 27 Jan 2026 18:30:12 +0000",
        "content:encodedSnippet": "Productivity Improvements\nThis month, we are bringing you a series of small yet long requested and popular features to let you better control and customize your editor.\nFast scrolling: Hold down the Alt key while scrolling the mouse wheel to move quickly when reviewing code or reading documentation. You can adjust the fast-scrolling speed in Tools > Options > Text Editor > Advanced > Touchpad and mouse wheel scrolling sensitivity.\nMiddle click scroll: Press down on your scroll wheel and move the mouse to quickly scroll through your document, making it easier to explore large files. This feature is off by default in 18.3 Insiders 2 and needs to be enabled via Tools > Options > Text Editor > Advanced > Middle click to scroll.\nHTML-rich copy paste: Visual Studio now supports HTML clipboard format when cutting or copying code from the editor. You can paste colorized code into web versions of Office apps, Azure DevOps work items, or other HTML-based controls. This feature is available in the latest Insiders and will be shipped in the Release channel soon.\nSyntactic line compression: Lines without letters or numbers are compressed by 25%, letting you see more code at once. Regular lines stay the same height. Enable this feature in Tools > Options > Text Editor > Advanced by checking Compress blank lines and Compress lines that do not have any alphanumeric characters.\nSlimmer left margin: We’ve heard you that you would like to see a slimmer left margin to have more horizontal space for your code! To save an extra column, we moved the Quick Actions icon (lightbulb or screwdriver) from the margin into the editor, where it appears inline with your code when fixes or refactoring are available. To try out the experience now, go to Tools > Options > Text Editor > Advanced and check Show Quick Actions icon inside the editor. We also have more customized margin controls coming, so stay tuned!\nColorized Code Completions\nCode completions are now colorized with syntax highlighting to help you quickly parse suggested code between variables, functions, and other elements! To try out this experience, go to Tools > Options > Text Editor > Code Completions and check “Use colorized text for code completions.”\nTo differentiate suggestions from actual code, colorized completions use lower opacity and italic styling. If you want to further customize the look, go to Tools > Options > Environment > Fonts and Colors and select “Code Completions” from the Display items list. From there, you can customize the font, size, foreground/background colors, and styles (bold, italicized…).\nClick to Accept Code Completions Partially\nHave you ever wanted to accept a code completion partially with a single click? With this new feature, you can click directly inside a suggestion to accept it up to the cursor position. As you hover over the suggestion, each segment highlights to show exactly what will be accepted.\n\nThis feature gives you more control over how much of a completion you want to accept. If you prefer using the keyboard, you can still press Ctrl + Right Arrow to accept one word at a time, or Ctrl + Down Arrow to accept one line at a time.\nStreamlined Markdown Preview Controls\nVisual Studio Markdown editor now gives you faster, clearer access to preview options:\nSwitch between preview modes: Split Preview shows the editor and preview side by side, Open Preview shows only the preview, and Edit Markdown shows just the editor. The preview-only mode helps you focus on rendered content, especially for large images or complex Mermaid diagrams.\nWhen previewing a Mermaid diagram, use the zoom controls in the top-left corner to zoom in or out for better readability.\nWhen Copilot Chat generates markdown content, click the Preview button at the top right corner of the chat window to see a rendered view. You can then edit and save the content directly. No need to copy and paste manually.\n \nTo try all this latest goodness, please update your Visual Studio regularly and join us in the Insiders Channel if you haven’t yet! (Note: All features shared in this blog post are available in Visual Studio 2026 only.) Please give them a try and let us know any feedback you have or the next thing you would like to see!\nThe post Visual Studio January Update — Enhanced Editor Experience appeared first on Visual Studio Blog.",
        "dc:creator": "Simona Liao",
        "comments": "https://devblogs.microsoft.com/visualstudio/visual-studio-january-update-enhanced-editor-experience/#comments",
        "content": "<p>Productivity Improvements This month, we are bringing you a series of small yet long requested and popular features to let you better control and customize your editor. Fast scrolling: Hold down the Alt key while scrolling the mouse wheel to move quickly when reviewing code or reading documentation. You can adjust the fast-scrolling speed in [&#8230;]</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/visualstudio/visual-studio-january-update-enhanced-editor-experience/\">Visual Studio January Update — Enhanced Editor Experience</a> appeared first on <a href=\"https://devblogs.microsoft.com/visualstudio\">Visual Studio Blog</a>.</p>\n",
        "contentSnippet": "Productivity Improvements This month, we are bringing you a series of small yet long requested and popular features to let you better control and customize your editor. Fast scrolling: Hold down the Alt key while scrolling the mouse wheel to move quickly when reviewing code or reading documentation. You can adjust the fast-scrolling speed in […]\nThe post Visual Studio January Update — Enhanced Editor Experience appeared first on Visual Studio Blog.",
        "guid": "https://devblogs.microsoft.com/visualstudio/?p=255468",
        "categories": [
          "GitHub Copilot",
          "Productivity",
          "Developer Productivity",
          "Editor"
        ],
        "isoDate": "2026-01-27T18:30:12.000Z"
      }
    ]
  },
  {
    "name": "Joshua",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권재명",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김석기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김선영",
    "category": "개인",
    "posts": [
      {
        "creator": "sunyzero",
        "title": "LargeSystemCache 설정에 대한 오해 (윈11, 윈10 최적화)",
        "link": "https://sunyzero.tistory.com/324",
        "pubDate": "Fri, 23 Jan 2026 19:52:55 +0900",
        "author": "sunyzero",
        "comments": "https://sunyzero.tistory.com/324#entry324comment",
        "content": "<p data-ke-size=\"size16\">윈도11 최적화 팁 중에 <span style=\"background-color: #f6e199;\">LargeSystemCache</span>와 <span style=\"background-color: #9feec3;\">DisablePagingExecutive</span>가 있는데, 이건 옛날에나 쓰던 방식이고, SSD를 사용하는 최근 2010년 이후의 시스템에는 오히려 문제가 되기 때문에 쓰면 안되는 설정이다. 하지만 여러 곳에서 잘못된 정보가 유통되기에 이 글을 쓰게 되었다. 실제로 주변인이 이걸 설정해서 PC와 랩탑이 이상 작동하는 문제가 있었고, 고쳐준 다음에 쓴 글이다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">1. 역사적 배경 (알아둬도 큰 쓸모는 없지만...)</h2>\n<p data-ke-size=\"size16\"><span style=\"color: #333333; text-align: start;\">LargeSystemCache와<span>&nbsp;</span></span>DisablePagingExecutive 설정은 과거 HDD라고 불리는 하드 디스크를 장착한 십수년전 시스템에서 사용되던 설정으로서, 커널이라고 불리는 시스템 코어 부분이 사용하는 메모리 영역을 큰 값으로 확보해서 시스템 영역을 메모리에 상주시키는 효과가 있었다. 그렇다면 왜 시스템 영역을 메모리에 상주되는 기능이 있는지 살펴봐야 한다. 이를 위해 우린 스왑 영역에 대해서 조금 알아둬야만한다.</p>\n<p data-ke-size=\"size16\">우선 램은 프로그램이 실행되기 위에서 디스크에 있는 정보를 읽어서 올려두는 주 메모리 공간(main memory)이다. 모든 프로그램은 램을 사용한다는 사실부터 기억해두자.</p>\n<p data-ke-size=\"size16\">2000년대 중반만 하더라도 인터넷 열풍으로 PC 시장이 빠르게 성장했는데, 그에 비해 메모리(RAM) 용량은 발전이 더뎌서 1GB정도 밖에 안되는 경우가 많았다. 2026년 기준으로 보면 PC의 램 최소사양이 8~16GB이고, 게임을 하는 사람은 보통 32GB를 쓰는 것에 비하면 1GB는 엄청 작은 값이었다. 따라서 Windows에서는 여러 프로그램을 원활하게 실행하기 위해 최대한 램을 아껴써야만 했다. 이를 위해 스왑 영역을 사용하는 기법이 도입되었는데, 이는 OS(운영체제)론을 배운 학생이라면 다 알것이다. 혹시나 모르는 사람을 위해 설명하자면, 운영체제가 메모리 부족을 해결하기 위해 디스크의 특정 영역에 메모리 공간의 일부를 이동(copy and remove)시켰다가 나중에 필요할때 다시 메모리로 가져오는 기법이다. 이렇게 하면 RAM이 1GB여도 2GB인 것처럼 뻥튀기를 할 수 있었다.&nbsp;운영체제론에서는 이때 사용되는 디스크 공간을 스왑공간(swap space)이라고 부르는데, 윈도에서는 페이지 파일(pagefile) 이라고 다르게 부르며 실제 위치는 C:\\pagefile.sys 에 만들어지는 경우가 많다.[1]</p>\n<p data-ke-size=\"size16\">참고로 메모리의 영역을 pagefile로 이동시키는 과정을 <span style=\"background-color: #f6e199;\">스왑 아웃(swap out)</span>이라고 부르고, 반대로 pagefile에서 RAM으로 이동하는 과정을 <span style=\"background-color: #f6e199;\">스왑 인(swap in)</span>이라고 부른다. 아래 그림을 보면 이해가 쉬울 것이다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Windows11_ram_hdd_swap.png\" data-origin-width=\"1051\" data-origin-height=\"807\"><span data-url=\"https://blog.kakaocdn.net/dn/26DOu/dJMcaajMma1/Af7ipBkFXPYypOv9gEXfAk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/26DOu/dJMcaajMma1/Af7ipBkFXPYypOv9gEXfAk/img.png\" data-alt=\"Windows, Swap Out, Swap In (Gemini 생성)\"><img src=\"https://blog.kakaocdn.net/dn/26DOu/dJMcaajMma1/Af7ipBkFXPYypOv9gEXfAk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F26DOu%2FdJMcaajMma1%2FAf7ipBkFXPYypOv9gEXfAk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1051\" height=\"807\" data-filename=\"Windows11_ram_hdd_swap.png\" data-origin-width=\"1051\" data-origin-height=\"807\"/></span><figcaption>Windows, Swap Out, Swap In (Gemini 생성)</figcaption>\n</figure>\n</p>\n<p data-ke-size=\"size16\">그런데 스왑 아웃이나 스왑 인에는 심각한 문제가 있다. 그건 바로 딜레이가 발생한다는 점이다. 어떤 프로그램을 수행하려고 하는데, 필요한 램 공간이 500MB정도 부족하다고 가정하자. 그럼 운영체제는 최소 500MB 이상의 메모리를 확보하기 위해 메모리 일부분을 디스크로 복사하는 과정, 즉 swap out이 발생한다. 문제는 500MB이상의 데이터를 디스크로 복사하는 과정에서 심각한 딜레이가 발생한다는 점이다(당시 하드 디스크의 속도는 대체로 초당 50~100MB수준 이었다). 따라서 500MB이상의 메모리를 swap out하려면 최소 6~10초 정도는 딜레이가 생길 것이다. 그런데 프로그램 실행시 이 정도 딜레이가 발생하면 악영향을 줄 것이다. 그렇다면 딜레이를 줄이기 위해 어떤 방법이 있을까? 그건 바로 미리미리 복사하던지, 아니면 메모리를 비워서 빈 메모리 공간을 만들어두는 것이다.</p>\n<p data-ke-size=\"size16\">그런데 이렇게 미리 공간을 확보하는 것이 효과가 좋은 원인을 생각해보면 HDD(하드 디스크)의 복사 속도가 매우 느렸기 때문에 발생하는 딜레이였다. 그리고 공간을 확보하기 위해 디스크를 자주 쓰거나 읽게되면 디스크로부터 데이터를 주로 많이 읽어들이는 데이터베이스나 서버들은 악영향이 생긴다. 그래서 메모리를 미리 확보하는 기법은 반대로 커널의 응답속도가 떨어뜨리는 문제가 생긴다. 그래서 주변 장치를 많이 사용하거나 혹은 커널이 대용량 데이터를 처리하는 경우에 반대로 커널(시스템) 영역이 디스크로 swap out되지 않도록 하는 기능이 필요해진 것이었다. 이를 돕는 기능이 바로 LargeSystemCache, DisablePagingExecutive 라 부르는 것이었다. 두가지 기능에 대해서는 아래에 표로 정리해두었다.</p>\n<table style=\"border-collapse: collapse; width: 100%;\" border=\"1\" data-ke-align=\"alignLeft\">\n<tbody>\n<tr>\n<td style=\"width: 23.0233%;\"><span style=\"background-color: #f6e199;\"> LargeSystemCache </span></td>\n<td style=\"width: 76.9767%;\">대용량 캐시를 사용하도록 하여, 커널이 파일을 읽거나 메모리 영역을 사용 할 때 캐싱을 최대한 하도록 하는 기법이다. 즉 빈 공간을 남기지 말고 최대한 파일을 위한 캐시로 쓰게 한다. 윈도 서버에서 데이터베이스나 네트워크 서비스를 사용할 때 반응 속도를 올리는데 도움이 되던 설정이다. 게임이나 문서 작업을 하는 일반인들에게는 필요가 없는 설정이다.[2]</td>\n</tr>\n<tr>\n<td style=\"width: 23.0233%;\"><span style=\"color: #333333; text-align: start; background-color: #f6e199;\"> DisablePagingExecutive&nbsp; </span></td>\n<td style=\"width: 76.9767%;\">커널이 사용하는 영역의 메모리나 디바이스 메모리를 swap out을 하지 않고 메인 메모리에 유지하도록 하는 것이다. 위와 마찬가지로 윈도 시스템의 반응 속도를 올려주는 장점이 있었다. 혹은 디바이스 장치를 개발하는 경우에 주로 사용한다. [3]</td>\n</tr>\n</tbody>\n</table>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">이들은 위에 정리한대로 메인 메모리(RAM)를 소모해서 윈도 시스템 자체의 서비스와 장치들의 반응 속도를 올리는게 핵심 목적이므로 주소 Windows Server에서 사용되는 기능이었다. 물론 개인용 PC에서 사용하던 Windows 7 (2009년)이나 Windows 8 (2012년)에는 4GB이상의 메모리를 가지고 있고, HDD 디스크를 사용하던 경우에는 이 설정이 효과가 있는 경우가 많았었다. 문제는 2010년대 중반에는 SSD가 대거 보급되었고, 8GB이상의 대용량 메모리를 가진 시스템이 늘어나면서 해당 설정이 무의미해졌다는 점이다. <span style=\"color: #ee2323;\">지금 대부분의 시스템들은 보통 16GB이상의 메모리를 가지고 있고, 고성능의 SSD를 가지고 있기에 전혀 쓸 이유가 없는 기능이다</span>.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Tiger_smoking_season.png\" data-origin-width=\"1415\" data-origin-height=\"1287\"><span data-url=\"https://blog.kakaocdn.net/dn/bdRLi2/dJMcacomG2g/1gI7JgTcUZzFRqmd8oEEfK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bdRLi2/dJMcacomG2g/1gI7JgTcUZzFRqmd8oEEfK/img.png\" data-alt=\"호랑이 담배 피우던 시절에 쓰는 PC설정 (Gemini 생성)\"><img src=\"https://blog.kakaocdn.net/dn/bdRLi2/dJMcacomG2g/1gI7JgTcUZzFRqmd8oEEfK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbdRLi2%2FdJMcacomG2g%2F1gI7JgTcUZzFRqmd8oEEfK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1415\" height=\"1287\" data-filename=\"Tiger_smoking_season.png\" data-origin-width=\"1415\" data-origin-height=\"1287\"/></span><figcaption>호랑이 담배 피우던 시절에 쓰는 PC설정 (Gemini 생성)</figcaption>\n</figure>\n</p>\n<p data-ke-size=\"size16\">그럼에도 불구하고 이 설정을 하면 빨라진다는 호랑이 담배피던 시절 괴담이 늘어나고 있는데, Win10이나 Win11에서&nbsp; LargeSystemCache나 DisablePagingExecutive 를 설정해봐야 효과도 없고, 도리어 시스템을 불안정하게 만드는 경우도 많다. 특히 랩탑(노트북)은 이 설정을 켜면 절전모드에서 깨어날때 문제가 생기거나 몇몇 블루투스나 와이파이 장치들이 문제를 일으키기도 한다. PC 데스크탑에서는 랩탑보다는 안정적이지만 간혹 USB 장치나 PCIe 장치가 문제를 일으킬 수도 있다. 따라서 현재는 쓰면 안되는 기능이다. 그러면 이 기능을 어떻게 켜고 끄는지 살펴보자.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">2. LargeSystemCache, DisablePagingExecutive<span style=\"color: #333333; text-align: start;\"><span>&nbsp;</span></span> 끄기</h2>\n<p data-ke-size=\"size16\">혹시라도 이 설정을 켠 사람들은 아래 방법으로 끄면 된다. 먼저 regedit를 실행해서 레지스트리 편집을 시행한다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Windows11_LargeSystemCache_DisablePagingExecutive.png\" data-origin-width=\"1366\" data-origin-height=\"627\"><span data-url=\"https://blog.kakaocdn.net/dn/bZWgt5/dJMcagjZYXx/w2pq8MvntRmIKTGpVVwBM0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bZWgt5/dJMcagjZYXx/w2pq8MvntRmIKTGpVVwBM0/img.png\" data-alt=\"LargeSystemCache와 DisablePagingExecutive 설정\"><img src=\"https://blog.kakaocdn.net/dn/bZWgt5/dJMcagjZYXx/w2pq8MvntRmIKTGpVVwBM0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbZWgt5%2FdJMcagjZYXx%2Fw2pq8MvntRmIKTGpVVwBM0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1366\" height=\"627\" data-filename=\"Windows11_LargeSystemCache_DisablePagingExecutive.png\" data-origin-width=\"1366\" data-origin-height=\"627\"/></span><figcaption>LargeSystemCache와 DisablePagingExecutive 설정</figcaption>\n</figure>\n</p>\n<p data-ke-size=\"size16\">여기서 HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Memory Management 설정으로 이동하면 <span style=\"color: #333333; text-align: start;\"><span style=\"background-color: #f6e199;\">LargeSystemCache</span>나<span>&nbsp;</span></span><span style=\"color: #333333; text-align: start;\"><span style=\"background-color: #9feec3;\">DisablePagingExecutive</span><span> 설정이 보일 것이다. 이 값들은 on하는 것이 <span style=\"background-color: #f6e199;\">1</span>이고, off하는 것이 <span style=\"background-color: #ffc1c8;\">0</span>이다. 당연히 default값은 off인 0이다. 이 2가지 설정 값이 1이라면 0으로 수정하면 된다.</span></span></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">혹시 이 값을 설정한 뒤에 페이징 파일(pagefile)을 0바이트로 설정했다면 원상복구하는 것을 추천한다. 원복을 위해서는 다음과 같이 sysdm.cpl 프로그램에 설정하는 것이 편하다. 먼저 Win + R 키를 누르면 나타나는 좌측 하단 입력창에 <span style=\"background-color: #9feec3;\">sysdm.cpl</span> 을 타이핑해서 실행한다. 그리고 \"<span style=\"background-color: #f6e199;\">고급</span>\" 탭에 \"<span style=\"background-color: #c0d1e7;\">성능</span>\" 설정을 눌러서 \"<span style=\"background-color: #c1bef9;\">성능 옵션</span>\" 창을 띄운다. 여기서 다시 \"<span style=\"background-color: #f6e199;\">고급</span>\" 탭으로 이동해서 \"<span style=\"background-color: #f6e199;\">가상 메모리</span>\"의 변경 버튼을 누르면 아래 그림처럼 가상 메모리 설정을 볼 수 있다. 가상메모리 설정을 \"<b><span style=\"color: #ee2323;\">시스템이 관리하는 크기</span></b>\"로 설정하면 원상복구가 된다. 간혹 가상메모리 크기를 고정값으로 \"<b><span style=\"color: #ee2323;\">사용자 지정 크기</span></b>\" 로 16GB로 해두는게 좋다는 글도 있는데, 다 십수년전 옛날 설정 이야기다. 2026년 기준으로 보면 쓸데없는 설정이다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Windows11_LargeSystemCache_DisablePagingExecutive_PagingFile.png\" data-origin-width=\"677\" data-origin-height=\"977\"><span data-url=\"https://blog.kakaocdn.net/dn/bMicnY/dJMcacaQV6d/TWkDLxE48q4quBKY6opES0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bMicnY/dJMcacaQV6d/TWkDLxE48q4quBKY6opES0/img.png\" data-alt=\"sysdm.cpl의 성능옵션, 가상 메모리 설정\"><img src=\"https://blog.kakaocdn.net/dn/bMicnY/dJMcacaQV6d/TWkDLxE48q4quBKY6opES0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbMicnY%2FdJMcacaQV6d%2FTWkDLxE48q4quBKY6opES0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"677\" height=\"977\" data-filename=\"Windows11_LargeSystemCache_DisablePagingExecutive_PagingFile.png\" data-origin-width=\"677\" data-origin-height=\"977\"/></span><figcaption>sysdm.cpl의 성능옵션, 가상 메모리 설정</figcaption>\n</figure>\n</p>\n<p data-ke-size=\"size16\">윈도10이나 윈도11은 옛날처럼 최적화 하는 기법이 대부분 작동하지 않는다. 오히려 요새 최적화 기법은 발열이 심각한 몇몇 랩탑의 CPU나 올인원 PC들의 CPU 성능을 80% 정도로 낮추는 기능이 유용하고, 알림을 꺼서 최적화 하는 정도만 제대로 쓸모가 있다. 그 외에는 그냥 디폴트 값을 쓰는게 좋다. 괜히 만져서 더 느려지거나 문제가 생길 수 있으니 주의하도록 하자.</p>\n<p data-ke-size=\"size16\">그러나 굳이 꼭 쓰고 싶다면 위에 레지스트리를 고쳐서 LargeSystemCache 정도만 쓰는 것을 추천한다. 간혹 엄청난 문서 파일들을 작업하거나 동영상 같은 것을 편집하는 경우에는 아주 조금 유용할 수는 있을 듯 하다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">레퍼런스</h2>\n<p data-ke-size=\"size16\">[1] 페이징 파일 소개, Microsoft Learn, <a href=\"https://learn.microsoft.com/ko-kr/troubleshoot/windows-client/performance/introduction-to-the-page-file?source=recommendations\" target=\"_blank\" rel=\"noopener&nbsp;noreferrer\">https://learn.microsoft.com/ko-kr/troubleshoot/windows-client/performance/introduction-to-the-page-file?source=recommendations</a></p>\n<p data-ke-size=\"size16\">[2] LargeSystemCache에 대한 설명, <a href=\"https://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2003/cc784562(v=ws.10)\" target=\"_blank\" rel=\"noopener&nbsp;noreferrer\">https://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2003/cc784562(v=ws.10)</a></p>\n<p data-ke-size=\"size16\">[3] DisablePagingExecutive 에 대한 설명, <a href=\"https://learn.microsoft.com/en-us/windows-hardware/test/wpt/wpr-command-line-options\" target=\"_blank\" rel=\"noopener&nbsp;noreferrer\">https://learn.microsoft.com/en-us/windows-hardware/test/wpt/wpr-command-line-options</a></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">히스토리</h2>\n<p data-ke-size=\"size16\">2026.01.23 문서를 처음으로 작성</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "윈도11 최적화 팁 중에 LargeSystemCache와 DisablePagingExecutive가 있는데, 이건 옛날에나 쓰던 방식이고, SSD를 사용하는 최근 2010년 이후의 시스템에는 오히려 문제가 되기 때문에 쓰면 안되는 설정이다. 하지만 여러 곳에서 잘못된 정보가 유통되기에 이 글을 쓰게 되었다. 실제로 주변인이 이걸 설정해서 PC와 랩탑이 이상 작동하는 문제가 있었고, 고쳐준 다음에 쓴 글이다.\n \n1. 역사적 배경 (알아둬도 큰 쓸모는 없지만...)\nLargeSystemCache와 DisablePagingExecutive 설정은 과거 HDD라고 불리는 하드 디스크를 장착한 십수년전 시스템에서 사용되던 설정으로서, 커널이라고 불리는 시스템 코어 부분이 사용하는 메모리 영역을 큰 값으로 확보해서 시스템 영역을 메모리에 상주시키는 효과가 있었다. 그렇다면 왜 시스템 영역을 메모리에 상주되는 기능이 있는지 살펴봐야 한다. 이를 위해 우린 스왑 영역에 대해서 조금 알아둬야만한다.\n우선 램은 프로그램이 실행되기 위에서 디스크에 있는 정보를 읽어서 올려두는 주 메모리 공간(main memory)이다. 모든 프로그램은 램을 사용한다는 사실부터 기억해두자.\n2000년대 중반만 하더라도 인터넷 열풍으로 PC 시장이 빠르게 성장했는데, 그에 비해 메모리(RAM) 용량은 발전이 더뎌서 1GB정도 밖에 안되는 경우가 많았다. 2026년 기준으로 보면 PC의 램 최소사양이 8~16GB이고, 게임을 하는 사람은 보통 32GB를 쓰는 것에 비하면 1GB는 엄청 작은 값이었다. 따라서 Windows에서는 여러 프로그램을 원활하게 실행하기 위해 최대한 램을 아껴써야만 했다. 이를 위해 스왑 영역을 사용하는 기법이 도입되었는데, 이는 OS(운영체제)론을 배운 학생이라면 다 알것이다. 혹시나 모르는 사람을 위해 설명하자면, 운영체제가 메모리 부족을 해결하기 위해 디스크의 특정 영역에 메모리 공간의 일부를 이동(copy and remove)시켰다가 나중에 필요할때 다시 메모리로 가져오는 기법이다. 이렇게 하면 RAM이 1GB여도 2GB인 것처럼 뻥튀기를 할 수 있었다. 운영체제론에서는 이때 사용되는 디스크 공간을 스왑공간(swap space)이라고 부르는데, 윈도에서는 페이지 파일(pagefile) 이라고 다르게 부르며 실제 위치는 C:\\pagefile.sys 에 만들어지는 경우가 많다.[1]\n참고로 메모리의 영역을 pagefile로 이동시키는 과정을 스왑 아웃(swap out)이라고 부르고, 반대로 pagefile에서 RAM으로 이동하는 과정을 스왑 인(swap in)이라고 부른다. 아래 그림을 보면 이해가 쉬울 것이다.\nWindows, Swap Out, Swap In (Gemini 생성)\n\n\n그런데 스왑 아웃이나 스왑 인에는 심각한 문제가 있다. 그건 바로 딜레이가 발생한다는 점이다. 어떤 프로그램을 수행하려고 하는데, 필요한 램 공간이 500MB정도 부족하다고 가정하자. 그럼 운영체제는 최소 500MB 이상의 메모리를 확보하기 위해 메모리 일부분을 디스크로 복사하는 과정, 즉 swap out이 발생한다. 문제는 500MB이상의 데이터를 디스크로 복사하는 과정에서 심각한 딜레이가 발생한다는 점이다(당시 하드 디스크의 속도는 대체로 초당 50~100MB수준 이었다). 따라서 500MB이상의 메모리를 swap out하려면 최소 6~10초 정도는 딜레이가 생길 것이다. 그런데 프로그램 실행시 이 정도 딜레이가 발생하면 악영향을 줄 것이다. 그렇다면 딜레이를 줄이기 위해 어떤 방법이 있을까? 그건 바로 미리미리 복사하던지, 아니면 메모리를 비워서 빈 메모리 공간을 만들어두는 것이다.\n그런데 이렇게 미리 공간을 확보하는 것이 효과가 좋은 원인을 생각해보면 HDD(하드 디스크)의 복사 속도가 매우 느렸기 때문에 발생하는 딜레이였다. 그리고 공간을 확보하기 위해 디스크를 자주 쓰거나 읽게되면 디스크로부터 데이터를 주로 많이 읽어들이는 데이터베이스나 서버들은 악영향이 생긴다. 그래서 메모리를 미리 확보하는 기법은 반대로 커널의 응답속도가 떨어뜨리는 문제가 생긴다. 그래서 주변 장치를 많이 사용하거나 혹은 커널이 대용량 데이터를 처리하는 경우에 반대로 커널(시스템) 영역이 디스크로 swap out되지 않도록 하는 기능이 필요해진 것이었다. 이를 돕는 기능이 바로 LargeSystemCache, DisablePagingExecutive 라 부르는 것이었다. 두가지 기능에 대해서는 아래에 표로 정리해두었다.\n LargeSystemCache \n대용량 캐시를 사용하도록 하여, 커널이 파일을 읽거나 메모리 영역을 사용 할 때 캐싱을 최대한 하도록 하는 기법이다. 즉 빈 공간을 남기지 말고 최대한 파일을 위한 캐시로 쓰게 한다. 윈도 서버에서 데이터베이스나 네트워크 서비스를 사용할 때 반응 속도를 올리는데 도움이 되던 설정이다. 게임이나 문서 작업을 하는 일반인들에게는 필요가 없는 설정이다.[2]\n\n\n DisablePagingExecutive  \n커널이 사용하는 영역의 메모리나 디바이스 메모리를 swap out을 하지 않고 메인 메모리에 유지하도록 하는 것이다. 위와 마찬가지로 윈도 시스템의 반응 속도를 올려주는 장점이 있었다. 혹은 디바이스 장치를 개발하는 경우에 주로 사용한다. [3]\n\n\n\n \n이들은 위에 정리한대로 메인 메모리(RAM)를 소모해서 윈도 시스템 자체의 서비스와 장치들의 반응 속도를 올리는게 핵심 목적이므로 주소 Windows Server에서 사용되는 기능이었다. 물론 개인용 PC에서 사용하던 Windows 7 (2009년)이나 Windows 8 (2012년)에는 4GB이상의 메모리를 가지고 있고, HDD 디스크를 사용하던 경우에는 이 설정이 효과가 있는 경우가 많았었다. 문제는 2010년대 중반에는 SSD가 대거 보급되었고, 8GB이상의 대용량 메모리를 가진 시스템이 늘어나면서 해당 설정이 무의미해졌다는 점이다. 지금 대부분의 시스템들은 보통 16GB이상의 메모리를 가지고 있고, 고성능의 SSD를 가지고 있기에 전혀 쓸 이유가 없는 기능이다.\n호랑이 담배 피우던 시절에 쓰는 PC설정 (Gemini 생성)\n\n\n그럼에도 불구하고 이 설정을 하면 빨라진다는 호랑이 담배피던 시절 괴담이 늘어나고 있는데, Win10이나 Win11에서  LargeSystemCache나 DisablePagingExecutive 를 설정해봐야 효과도 없고, 도리어 시스템을 불안정하게 만드는 경우도 많다. 특히 랩탑(노트북)은 이 설정을 켜면 절전모드에서 깨어날때 문제가 생기거나 몇몇 블루투스나 와이파이 장치들이 문제를 일으키기도 한다. PC 데스크탑에서는 랩탑보다는 안정적이지만 간혹 USB 장치나 PCIe 장치가 문제를 일으킬 수도 있다. 따라서 현재는 쓰면 안되는 기능이다. 그러면 이 기능을 어떻게 켜고 끄는지 살펴보자.\n \n2. LargeSystemCache, DisablePagingExecutive  끄기\n혹시라도 이 설정을 켠 사람들은 아래 방법으로 끄면 된다. 먼저 regedit를 실행해서 레지스트리 편집을 시행한다.\nLargeSystemCache와 DisablePagingExecutive 설정\n\n\n여기서 HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Memory Management 설정으로 이동하면 LargeSystemCache나 DisablePagingExecutive 설정이 보일 것이다. 이 값들은 on하는 것이 1이고, off하는 것이 0이다. 당연히 default값은 off인 0이다. 이 2가지 설정 값이 1이라면 0으로 수정하면 된다.\n \n혹시 이 값을 설정한 뒤에 페이징 파일(pagefile)을 0바이트로 설정했다면 원상복구하는 것을 추천한다. 원복을 위해서는 다음과 같이 sysdm.cpl 프로그램에 설정하는 것이 편하다. 먼저 Win + R 키를 누르면 나타나는 좌측 하단 입력창에 sysdm.cpl 을 타이핑해서 실행한다. 그리고 \"고급\" 탭에 \"성능\" 설정을 눌러서 \"성능 옵션\" 창을 띄운다. 여기서 다시 \"고급\" 탭으로 이동해서 \"가상 메모리\"의 변경 버튼을 누르면 아래 그림처럼 가상 메모리 설정을 볼 수 있다. 가상메모리 설정을 \"시스템이 관리하는 크기\"로 설정하면 원상복구가 된다. 간혹 가상메모리 크기를 고정값으로 \"사용자 지정 크기\" 로 16GB로 해두는게 좋다는 글도 있는데, 다 십수년전 옛날 설정 이야기다. 2026년 기준으로 보면 쓸데없는 설정이다.\nsysdm.cpl의 성능옵션, 가상 메모리 설정\n\n\n윈도10이나 윈도11은 옛날처럼 최적화 하는 기법이 대부분 작동하지 않는다. 오히려 요새 최적화 기법은 발열이 심각한 몇몇 랩탑의 CPU나 올인원 PC들의 CPU 성능을 80% 정도로 낮추는 기능이 유용하고, 알림을 꺼서 최적화 하는 정도만 제대로 쓸모가 있다. 그 외에는 그냥 디폴트 값을 쓰는게 좋다. 괜히 만져서 더 느려지거나 문제가 생길 수 있으니 주의하도록 하자.\n그러나 굳이 꼭 쓰고 싶다면 위에 레지스트리를 고쳐서 LargeSystemCache 정도만 쓰는 것을 추천한다. 간혹 엄청난 문서 파일들을 작업하거나 동영상 같은 것을 편집하는 경우에는 아주 조금 유용할 수는 있을 듯 하다.\n \n레퍼런스\n[1] 페이징 파일 소개, Microsoft Learn, https://learn.microsoft.com/ko-kr/troubleshoot/windows-client/performance/introduction-to-the-page-file?source=recommendations\n[2] LargeSystemCache에 대한 설명, https://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2003/cc784562(v=ws.10)\n[3] DisablePagingExecutive 에 대한 설명, https://learn.microsoft.com/en-us/windows-hardware/test/wpt/wpr-command-line-options\n \n \n히스토리\n2026.01.23 문서를 처음으로 작성",
        "guid": "https://sunyzero.tistory.com/324",
        "categories": [
          "컴퓨터 관련/윈도 패밀리",
          "DisablePagingExecutive",
          "LargeSystemCache",
          "regedit",
          "sysdm.cpl",
          "Windows",
          "windows11",
          "운영체제",
          "윈도 페이징 파일 설정",
          "최적화"
        ],
        "isoDate": "2026-01-23T10:52:55.000Z"
      }
    ]
  },
  {
    "name": "강대명",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권정혁",
    "category": "개인",
    "posts": []
  },
  {
    "name": "줌구",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수보",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김시은",
    "category": "개인",
    "posts": []
  },
  {
    "name": "곽민수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김범진",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민서",
    "category": "개인",
    "posts": []
  },
  {
    "name": "I am not Okay",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권창현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권기호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수민",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강태욱",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권용진",
    "category": "개인",
    "posts": [
      {
        "creator": "권용진",
        "title": "스테이블코인 실전 강의",
        "link": "https://brunch.co.kr/@@H9i/93",
        "pubDate": "Wed, 28 Jan 2026 05:35:33 GMT",
        "author": "권용진",
        "content": "저는 오프라인 강의를 좋아합니다. 코로나 전에 퀀트 강의도 몇번 하였는데, 온라인 강의에 비해서 함께 집중하고 실습을 함께 하는 과정은 비교 할수 없을 정도로 장점이 많은 것 같습니다.  스테이블코인에 대한 수많은 자료들이 있습니다. 스테이블코인의 원리, 서클이 뭔지, 테더가 뭔지, 글로벌에서 어떤 일들이 일어나는지, 수박 겉핥기 식 이야기들은 많지만  실",
        "contentSnippet": "저는 오프라인 강의를 좋아합니다. 코로나 전에 퀀트 강의도 몇번 하였는데, 온라인 강의에 비해서 함께 집중하고 실습을 함께 하는 과정은 비교 할수 없을 정도로 장점이 많은 것 같습니다.  스테이블코인에 대한 수많은 자료들이 있습니다. 스테이블코인의 원리, 서클이 뭔지, 테더가 뭔지, 글로벌에서 어떤 일들이 일어나는지, 수박 겉핥기 식 이야기들은 많지만  실",
        "guid": "https://brunch.co.kr/@@H9i/93",
        "isoDate": "2026-01-28T05:35:33.000Z"
      }
    ]
  },
  {
    "name": "김민수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김승호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "RSS feed for boyism Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성준의 린스타트업과 디자인씽킹",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권혁우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김준형",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강동혁",
    "category": "개인",
    "posts": []
  },
  {
    "name": "고종범",
    "category": "개인",
    "posts": []
  },
  {
    "name": "cheese10yun",
    "category": "개인",
    "posts": []
  },
  {
    "name": "구자철",
    "category": "개인",
    "posts": []
  },
  {
    "name": "FSS",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권동준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김용일",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김도균",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민석",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김동우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권윤학",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강성훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김만수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "엘키",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김슬기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김광현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성빈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강명훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민장",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성중",
    "category": "개인",
    "posts": []
  },
  {
    "name": "구교준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김덕기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "고명환",
    "category": "개인",
    "posts": [
      {
        "creator": "고명환",
        "title": "R&amp;D 합격은 성과지표(KPI)에서 갈린다. - 창업(스타트업)",
        "link": "https://brunch.co.kr/@@LOc/327",
        "pubDate": "Fri, 23 Jan 2026 03:32:32 GMT",
        "author": "고명환",
        "content": "1. 왜 R&amp;D 지원사업은 '성과지표'에서 합격이 갈릴까  처음 R&amp;D를 신청하는 스타트업 대표님들이 가장 많이 놓치는 지점은 한 가지입니다. 정부 R&amp;D는 '좋은 기술' 자체보다 예산 투입의 결과를 '증명 가능한 형태'로 설계했는지를 봅니다.  평가위원 입장에서 사업계획서는 결국 아래 질문에 대한 답입니다.  이 과제는 무엇을 만들고(산출물) 어떤 기준으<img src= \"https://img1.kakaocdn.net/thumb/R1280x0/?fname=http%3A%2F%2Ft1.kakaocdn.net%2Fbrunch%2Fservice%2Fuser%2FLOc%2Fimage%2F_FRck0zdinWyIxb2nnpeUYPVaj0.jpg\" width=\"500\" />",
        "contentSnippet": "1. 왜 R&D 지원사업은 '성과지표'에서 합격이 갈릴까  처음 R&D를 신청하는 스타트업 대표님들이 가장 많이 놓치는 지점은 한 가지입니다. 정부 R&D는 '좋은 기술' 자체보다 예산 투입의 결과를 '증명 가능한 형태'로 설계했는지를 봅니다.  평가위원 입장에서 사업계획서는 결국 아래 질문에 대한 답입니다.  이 과제는 무엇을 만들고(산출물) 어떤 기준으",
        "guid": "https://brunch.co.kr/@@LOc/327",
        "isoDate": "2026-01-23T03:32:32.000Z"
      }
    ]
  },
  {
    "name": "강성희",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강병수",
    "category": "개인",
    "posts": [
      {
        "creator": "bahamoth",
        "title": "일관성 있는 Agentic AI Workflow를 팀 프로젝트에 적용하는 법 #2",
        "link": "https://01010011.blog/2026/01/30/how-to-build-a-consistent-agentic-ai-workflow-for-team-projects-2/",
        "pubDate": "Fri, 30 Jan 2026 01:07:34 +0000",
        "content:encodedSnippet": "전편에서 제안한 원칙들을 어떻게 팀 프로젝트에 적용하는지에 대해 Claude Code Plugin / Skills / Hooks를 이용하여 구체적인 예시를 살펴본다.\n케이스: LLM의 작업 진행사항을 이슈 트래커와 동기화하기\n언젠가 AI의 개발 산출물이 인간의 수준을 아득히 뛰어넘는 날이 오면 더 이상 작업 이력을 추적할 필요가 없어지는 시점이 올지도 모른다.\n하지만 인간지능과 인공지능이 공동의 산출물을 함께 작업하는 현시점에서는 기존의 방식대로 작업 계획을 수립하고, 이력을 추적하는 방식이 아직까지는 유효하다. 다만, 기존의 협업 방식이 코딩에이전트의 신속한 작업흐름의 병목이 될 소지가 있으므로 인간과 AI가 조화롭게 협업할 수 있는 방안을 모색해야 한다.\n이 글에서는 기존 작업 이력 추적 방식을 유지하면서도 코딩 에이전트의 병목을 최소화하는 방법을 Claude Code의 Plugin, Skills, Hooks 활용한 예제를 통해 살펴본다.\n기존 SW 개발 방식.\n일감 등록 → 공유/회의 → 업무 배분 → 개발\n일반적인 소프트웨어 개발에서 협업은 업무 관계자 간 공유를 전제로 한다. 온/오프라인에서 컨텍스트를 공유, 업무 이해를 바탕으로 일감을 배분한다.\n커뮤니케이션은 코스트가 큰 작업이지만 업무의 시작점과 범위를 명확히 하고 팀 전체의 추적성을 높이기 위해 필수적이다.\n그래서 협업 문화가 고도화된 조직에서는 비동기적으로 컨텍스트를 공유, 커뮤니케이션 비용을 최소화한다.\n코딩 에이전트와의 SW 개발 방식.\n목적 공유 → 스펙 구체화 → 구현 계획 수립 → 개발\n반면 코딩 에이전트와의 개발은 Local Session 내에서 독자적으로 이뤄진다. 개발자는 에이전트에게 목적을 전달하고, 대화를 통해 스펙을 구체화하며, 구현 계획을 수립한 뒤 바로 개발에 들어간다. 기존 협업 방식에서 팀 회의나 이슈 트래커가 담당하던 기획과 계획 수립이, 개발자와 에이전트 사이의 로컬 세션으로 대체된다.\n문제는 이 과정이 외부로 공유되지 않는다는 것이다. “왜 이런 설계를 선택했는지”, “어떤 대안을 검토했는지”, “구현 중 어떤 제약을 만났는지” 등등의 모든 컨텍스트가 대화창 안에만 존재한다. 세션이 종료되면 컨텍스트는 휘발되며 다른 팀원은 최종 결과물만 볼 수 있을 뿐, 결과물에 이르기까지의 의사결정 과정은 알 수 없다.\n협업과 지속 가능성이 없는 프로젝트라면 컨텍스트의 공유는 불필요하기에 별 문제가 되지 않을 것이다. 하지만 지속적인 유지보수와 개선이 필요한 팀 프로젝트라면 과정의 가시성을 갖추어야 한다.\n두 방식의 충돌\n협업 문화가 고도화되지 않은 조직의 워크플로우에 코딩 에이전트를 도입한다면 AI의 압도적인 구현 속도를 프로세스가 따라가지 못할 것이다. 일감등록, 업무배분, 회의 등 모든 단계가 병목이 된다.\n반면, 코딩 에이전트와의 개발 방식대로 Local Session 내에서 모든 작업을 계획하고 처리, 결과물만 공유하도록 내버려둔다면 팀단위의 의사결정을 파편화된 Local Session 들에게 위탁하는 결과를 낳게 될 것이다.\n해결책 모색: 이슈 트래커-코딩 에이전트 동기화\n워크플로우 개선에 앞서 가장 먼저 할 일은 협업 문화를 비동기적으로 개선하는 것이다.\nGitHub Engineering의 비동기 커뮤니케이션 원칙처럼, 문서와 이슈 기반으로 의사결정이 이루어지는 문화가 전제되어야 한다. 정례적, 정기적으로 열리는 회의에서 상정되는 아이템이 일감이 되는 워크플로우는 코딩에이전트의 역량에 병목이 된다.\n비동기 협업 문화가 갖춰졌다면, 다음 단계는 로컬 세션에 갇힌 코딩 에이전트의 작업 이력을 이 흐름에 자연스럽게 합류시키는 것이다.\nLinear 이슈 트래커\n이 글에서는 이슈 트래커로 Linear를 사용한다. Linear를 선택한 이유는 Opinionated issue tracker이기 때문이다. Opinionated software란 특정 use case에 맞게 purpose-built된 소프트웨어를 의미한다. Linear의 공동창업자 Jori Lallo는 Figma 블로그 인터뷰에서 이렇게 설명한다:\n“We design it so that there’s one really good way of doing things. Flexible software lets everyone invent their own workflows, which eventually creates chaos as teams scale.”\n자유도가 높은 도구는 모든 사람이 각자의 워크플로우를 만들어내고, 팀이 커질수록 혼란이 가중된다. Linear는 이슈 상태, 워크플로우, 브랜치 네이밍(gitBranchName), GitHub 연동 방식이 사전 정의되어 있고, 사용자가 이 흐름을 따르도록 유도한다. 이런 설계는 코딩 에이전트와의 통합에 유리하다. 워크플로우가 명확하게 정의되어 있으면, 에이전트에게 “Linear 방식대로 해”라고 지시하는 것만으로 일관된 결과를 기대할 수 있다. 선택지가 많으면 에이전트도 혼란스럽다.\nGitHub-Linear Integration\nLinear와 GitHub 연동은 물 흐르듯 매끄럽다. 몇 가지 규칙만 따르면 별도의 작업 없이 자동으로 동기화된다. GitHub + Linear integration을 설정하면 PR 자동 연결과 머지 시 이슈 상태 동기화가 활성화된다.\n이슈 ID 기반 브랜치 생성\n\nLinear 이슈 화면에서 Copy git branch name 액션을 쓰면, username + issue ID + title 처럼 Linear – GitHub 간 연동을 위한 ID 정보를 포함한 브랜치 이름을 바로 만들어 GitHub에서 쓸 수 있다. 이 포맷은 GitHub 인테그레이션 설정에서 팀 규칙에 맞게 변경 가능하다.\nPR 관리 (이슈  PR 자동 링크)\n\n브랜치 이름이나 PR 제목에 이슈 ID를 포함하면 PR 생성 시 자동으로 Linear 이슈와 연결되고, PR 본문/커밋 메시지에 Fixes ENG-123 같은 “magic word + 이슈 ID”를 쓰면 여러 PR·커밋을 하나의 이슈에 링크할 수 있다.\nMagic Words\n\nPR 본문이나 커밋 메시지에 Fixes ENG-123 같은 형식으로 이슈 ID 앞에 Fixes, Closes, Resolves 같은 magic word를 쓰면 해당 PR/커밋이 머지될 때 Linear 이슈가 자동으로 Done 상태로 변경된다. 여러 이슈를 한꺼번에 닫을 수도 있다.분류Magic words (대소문자 무시)Linear에서의 효과사용처예시Closing (머지 후 Issue 자동 완료)close, closes, closed, closing, fix, fixes, fixed, fixing, resolve, resolves, resolved, resolving, complete, completes, completed, completingPR/MR 머지 시 ‘Done’으로 이슈 상태 변경PR/MR descriptionFixes ENG-123Non-closing / Contributing (참조 링크)ref, references, part of, related to, contributes to, towards머지 후에도 이슈 상태 변경 안됨PR/MR descriptionRelated to ENG-123\n이슈 상태 자동 관리\n\nGitHub PR이 draft → open → merged 로 이동할 때, 팀별 워크플로 설정에 따라 Linear 이슈 상태를 In Progress / Done 등으로 자동으로 옮길 수 있다. PR revert 시 이슈를 다시 여는 동작도 지원한다.\nGitHub Issues 동기화(선택)\n\n필요하면 특정 리포지토리의 GitHub Issues를 Linear 팀과 연결해, 새로 생성되는 이슈의 제목·설명·상태·라벨·assignee·댓글 등을 양방향으로 동기화할 수 있다.\n시도 1: linear mcp를 이용한 이슈 트래커 동기화\nMCP(Model Context Protocol)는 “로컬 세션에서 만든 계획을 Linear에 바로 반영”하는 가장 직관적인 방법이다. 개발자는 Claude Code와 대화하며 구현 계획(체크리스트/작업 단위)을 만들고, 그 내용을 바탕으로 Linear MCP(Multi-Client/Model Context Protocol) 툴 호출을 명시적으로 지시해서 이슈를 생성/갱신한다.\n진행 흐름\n1) 목적/스펙 합의: Claude Code와 대화로 요구사항과 범위를 정리한다.\n2) 구현 계획 작성: 구현 계획을 TODO/체크리스트 형태로 만든다.\n3) Linear 이슈 반영(수동 트리거): linear mcp를 이용하여 지금까지의 구현 계획을 바탕으로 Linear에 이슈를 만들고 업데이트할 것을 지시한다.\n4) 개발 진행 중 동기화(반복): 진행 상황이 바뀔 때마다(작업 착수/PR 생성/막힌 점 발생/완료) 다시 MCP 호출을 시켜 상태/코멘트/체크리스트를 업데이트한다.\nLinear MCP 사용례\nMCP 사용법은 어렵지 않다. 아래와 같이 “어떤 이슈를 어떤 내용으로 만들지/바꿀지”를 구체적으로 기술하여 Tool call을 유도하면 된다.\n이슈 생성: 제목/설명/우선순위/라벨/담당자/팀/프로젝트 등을 포함해 생성\n이슈 업데이트: 상태(In Progress/Done), 설명(계획 상세), 코멘트(결정사항/제약/진행상황) 등을 갱신\n이슈 생성 예시\n지금까지 정리한 구현 계획을 Linear에 반영해줘.\n- 팀: ENG\n- 프로젝트: Agent Workflow\n- 이슈 제목: \"Claude Code Hooks로 Linear 동기화 자동화\"\n- 설명에는 아래 계획을 체크리스트로 포함\n- 우선순위 P2, 라벨: agenticai, tooling\n\n\n\n\n이슈 업데이트 예시\n방금 PR을 만들었어. Linear 이슈에 PR 링크를 코멘트로 남기고 상태를 In Progress로 바꿔줘.\n그리고 문제점(권한 이슈)을 'Blocker' 코멘트로 기록해줘.\n\n\n\n\n이 방식의 문제는 대화를 통해 명시적으로 이슈트래커 동기화 지시를 해야만 한다는 점이다. 개발자는 매 변경점 마다 이슈를 갱신하기 위해 코딩 에이전트와 대화를 해야 한다.\n문제점\n작업 병목: 계획을 세우는 흐름과, Linear에 반영하는 흐름이 분리되어 있어 매 작업마다 “등록/업데이트” 지시가 요구된다.\n누락 가능성: 깜박하고 작업 내역을 동기화하지 않는 등 인간의 실수 개입 여지가 있다.\n일관성 부족: 코딩 에이전트와 대화를 통해 이슈 본문/제목/prefix 등을 기술하다 보면 일관성을 보장하기 어렵게 된다.\n시도 2: CLAUDE.md 및 Sub-Agent 를 이용한 이슈 트래커 동기화 자동화\n이번에는 매번 Linear 업데이트를 지시하는 대신, 계획 단계에서 정리한 내용을 자동으로 등록/업데이트하도록 CLAUDE.md에 규칙을 기술한다.\nCLAUDE.md: 구현 전에 반드시 이슈 트래커를 업데이트하도록 명시하는 규칙 문서\nSub-Agent: 이슈 작성 형식과 예제를 프롬프트에 고정해, 항상 같은 템플릿으로 작성하게 하는 전담 에이전트\n진행 흐름\n1) CLAUDE.md에 동기화 규칙 명시: 구현에 들어가기 전에, 계획 내용을 이슈 트래커에 등록/업데이트하도록 규칙을 박아둔다.\n2) Sub-Agent에 작성 형식 기술: 제목 규칙, 본문 섹션, 라벨/우선순위 등을 Sub-Agent 프롬프트로 템플릿 화한다.\n3) 계획 수립: Claude Code와 대화로 스펙을 구체화하고 구현 계획을 체크리스트로 정리한다.\n4) 구현: CLAUDE.md에 명시된 규칙에 따라 구현에 들어가기 전 이슈트래커에 계획과 진행사항을 갱신한다.\n사용례\nCLAUDE.md 예시\n# CLAUDE.md\n\n## Issue Tracker Sync\n- 계획이 확정되면 항상 \"구현 계획\"을 체크리스트로 만든다.\n- 구현을 시작하기 전에는 항상 구현 계획을 기반으로 Linear 이슈를 생성한다.\n\n\n\n\nSub-Agent 예시\n[You are Issue Tracker Agent]\n- Convert the current plan into a Linear issue update.\n- Title rule: <PREFIX>: <verb> <object>\n- Body sections: Background / Decisions / Alternatives / Checklist\n- Default labels: agenticai, tooling\n- Default priority: P2\n\n\n\n\n문제점\n누락: CLAUDE.md에 규칙을 명시해도, 그 내용이 반드시 실행됨을 보장하지는 않는다. 대화 흐름이 길어지거나 컨텍스트가 여러 번 바뀌면, 계획 → 구현 전환 시점에서 동기화 스텝이 건너뛰어질 수 있다.\n진행사항 반영: CLAUDE.md 만으로는 구현 도중에 발생하는 모든 이력을 이슈 트래커에 반영하도록 제어하기 어렵다. 마찬가지로 누락이 발생한다.\n유연성 부족: 이미 논의된 이슈를 구현하거나, 존재하는 브랜치를 체크아웃 하는 등의 유연성을 워크플로우에 반영하기 어렵다.\nClaude Code Hooks\n위와 같이 특정 작업에 대해 반드시 실행을 보장해야 하는 경우, Claude Code의 Hooks 기능이 해결책이 될 수 있다.\nHook은 Claude Code의 특정 생명주기 이벤트에서 자동으로 실행되는 사용자 정의 셸 명령이다. 대화형으로 LLM에게 요청하거나 사전에 프롬프트로 기재하여 지시하는 방식과는 달리, Hook은 특정 상황에서 특정 동작을 강제할 수 있다.\n다시 말해 Hook은 반드시 실행된다.\n이벤트 발생 → Hook 트리거 → 스크립트 실행 → Exit Code에 따라 진행/차단\nHook은 다음과 같은 문제들을 해결할 수 있다.\n1. 반복 작업 자동화: 파일 수정 후 진행사항 요약, 구현 전에 이슈 및 브랜치 생성 등\n2. 프로젝트 규칙 강제: 특정 명령 차단, 파일 경로 검증, 네이밍 컨벤션 준수\n3. 컨텍스트 자동 주입: 세션 시작 사전에 체크리스트 로드\n예를 들어 PreToolUse 이벤트에 Write|Edit matcher를 설정하면 Claude가 파일을 수정하려 할 때마다 스크립트가 실행된다. 또한 PreCompact 이벤트를 활용하면 컨텍스트 윈도우가 압축되기 전에 현재까지의 작업 내용을 Linear 이슈에 코멘트로 남길 수 있다.\nClaude Code Hook 이벤트 종류\n\n이벤트발생 시점matcher 지원주요 용도\n\nUserPromptSubmit사용자 프롬프트 제출 직후, Claude 처리 전X프롬프트 검증, 컨텍스트 주입, 보안 필터링\nPreToolUse도구 파라미터 생성 후, 실행 전O특정 명령 차단, 민감 파일 보호, 입력 수정\nPermissionRequest권한 다이얼로그 표시 시O특정 도구 자동 승인/거부\nPostToolUse도구 실행 성공 직후O자동 포매팅, 로깅, 결과 검증\nStop메인 에이전트 응답 완료 시X품질 게이트 (lint, test), 작업 완료 검증\nSubagentStop서브에이전트 작업 완료 시X서브태스크 완료 검증, 추가 작업 강제\nPreCompact컨텍스트 윈도우 압축 전X트랜스크립트 백업, 진행 상황 저장\nSessionStart세션 시작 또는 재개 시X개발 컨텍스트 로드, 환경 초기화\nSessionEnd세션 종료 시X정리 작업, 세션 통계 로깅\nNotificationClaude Code가 알림 전송 시O데스크톱 알림, 사운드 재생\n\n\n\n\n\n이때 실행된 스크립트에서 반환하는 Exit Code에 따라 이후 진행을 제어할 수 있다.\n예를 들어 main 브랜치에서 직접 파일을 수정하려는 시도를 감지하면 스크립트가 exit 2를 반환하고, stderr에 “main 브랜치에서는 직접 수정할 수 없습니다. Linear 이슈를 생성하고 feature 브랜치를 만드세요.”라는 메시지를 출력한다.\n이 메시지는 Claude에게 전달되어, Claude가 스스로 Linear 이슈 생성 → 브랜치 생성 → 체크아웃 순서로 작업을 수정하게 된다.\nExit Code에 따른 흐름 제어\n\nExit Code의미Claude에게 전달동작\n\n0성공stdout (JSON)정상 진행. JSON 출력 시 decision 등 정교한 제어 가능\n2차단 에러stderr해당 동작 차단. stderr 내용이 Claude에게 자동 피드백되어 수정 유도\n기타비차단 에러Xstderr가 verbose mode에서만 표시. 실행은 계속됨\n\n\n\n\n\n시도 3: Claude Code Hooks을 이용하여 누락 없이 원하는 동작을 일관성 있게 수행\n앞서 살펴본 Hook의 특성을 활용하면 반드시 실행을 보장하여야 하는 Linear 워크플로우를 강제할 수 있다.\n워크플로우는 다음과 같다.\n1. 사용자 프롬프트 제출\n   └─ [UserPromptSubmit] check_linear_env.py → 환경 설정(Linear Team/Project 정보 등) 확인, 미설정 시 안내\n\n2. Claude가 Plan Mode에서 구현 계획 수립\n\n3. 구현 시도 (파일 수정)\n   └─ [PreToolUse: Write|Edit] ensure_linear_branch.py\n      ├─ main 브랜치? → exit 2, \"Linear 이슈를 생성하세요\"\n      ├─ 이슈 없음? → exit 2, \"이슈를 먼저 생성하세요\"  \n      └─ 정상 브랜치 → exit 0, 진행\n\n4. 구현 진행 중 컨텍스트 압축 필요\n   └─ [PreCompact] prompt → Linear 이슈에 진행 상황 코멘트\n\n5. git commit 시도\n   └─ [PreToolUse: Bash] validate_commit_msg.py\n      ├─ 형식 불일치? → exit 2, \"feat(scope): description 형식을 사용하세요\"\n      └─ 형식 일치 → exit 0, 커밋 진행\n\n6. PR 생성 (Fixes ABC-123 포함) → 머지 시 Linear 이슈 자동 완료\n\n\n\n\n다음과 같이 hooks.json 을 구성하면 Event/Matcher 별 스크립트를 실행할 수 있다. 물론 작업에 따라 스크립트 대신 프롬프트를 직접 기술할 수도 있다.\nhooks.json 구성\n{\n  \"hooks\": {\n    \"UserPromptSubmit\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 \\\"${CLAUDE_PLUGIN_ROOT}/scripts/check_linear_env.py\\\"\"\n          }\n        ]\n      }\n    ],\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Write|Edit|ExitPlanMode\",\n        \"hooks\": [\n          { \"type\": \"command\", \"command\": \"...check_linear_env.py\" }\n        ]\n      },\n      {\n        \"matcher\": \"Write|Edit|ExitPlanMode\",\n        \"hooks\": [\n          { \"type\": \"command\", \"command\": \"...ensure_linear_branch.py\" }\n        ]\n      },\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          { \"type\": \"command\", \"command\": \"...validate_commit_msg.py\" }\n        ]\n      }\n    ],\n    \"PreCompact\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Pre-Compact: Sync progress to Linear...\"\n          }\n        ]\n      }\n    ]\n  }\n}\n\n\n\n\nHook-스크립트의 역할\n\n스크립트이벤트matcher역할Exit Code\n\ncheck_linear_env.pyUserPromptSubmit–환경변수 설정 확인. 미설정 시 안내 메시지 출력항상 0 (advisory)\ncheck_linear_env.pyPreToolUseWrite|Edit|ExitPlanMode파일 수정 전 환경 재확인항상 0 (advisory)\nensure_linear_branch.pyPreToolUseWrite|Edit|ExitPlanModeLinear 이슈/브랜치 존재 확인. 없으면 생성 강제2 (차단)\nvalidate_commit_msg.pyPreToolUseBashgit commit 시 Conventional Commits 형식 검증2 (차단)\n(prompt)PreCompact–컨텍스트 압축 전 Linear 이슈에 진행 상황 코멘트–\n\n\n\n\n\nClaude Code Plugin: 패키지 가능한 AI Tool Use Best Practice\nClaude Code 의 Hook, Skill, MCP 설정을 패키징하는 방법을 살펴보자. Claude Code Plugin은 사용자가 구성한 Tool Use Best Practice를 재사용하고 공유할 수 있도록 제공되는 패키지이다.\nPlugin 구조\nPlugin은 다음과 같은 표준 디렉토리 구조를 따른다:\nplugin-name/\n├── .claude-plugin/\n│   └── plugin.json       # 플러그인 메타데이터 (필수)\n├── hooks/\n│   └── hooks.json        # Hook 설정\n├── scripts/              # Hook에서 실행할 스크립트\n├── skills/               # Skill 정의 (SKILL.md)\n├── agents/               # 특화된 에이전트 정의\n├── commands/             # 슬래시 명령어\n├── .mcp.json             # MCP 서버 설정\n└── README.md             # 문서화\n\n\n\n\nMarketplace로 배포하기\n이러한 Plugin을 팀이나 커뮤니티와 공유하려면 Marketplace에 등록한다. Marketplace는 Plugin 카탈로그로, 사용자가 다양한 Plugin을 탐색, 설치, 관리할 수 있도록 한다.\n참고 자료\nLinear 이슈 트래커 – 코딩 에이전트 간의 워크플로우를 일관성 있게 유지하는 방법을 어떻게 Claude Code Plugin으로 작성하고 배포하는지에 대한 예제 프로젝트를 아래에 공개한다.\nLinear Workflow Plugin: https://github.com/bahamoth/claude-linear-workflow/\nMarketplace: https://github.com/bahamoth/claude-marketplace/",
        "dc:creator": "bahamoth",
        "comments": "https://01010011.blog/2026/01/30/how-to-build-a-consistent-agentic-ai-workflow-for-team-projects-2/#respond",
        "content": "전편에서 제안한 원칙들을 어떻게 팀 프로젝트에 적용하는지에 대해 Claude Code Plugin / Skills / Hooks를 이용하여 구체적인 예시를 살펴본다. 케이스: LLM의 작업 진행사항을 이슈 트래커와 동기화하기 언젠가 AI의 개발 산출물이 인간의 수준을 아득히 뛰어넘는 날이 오면 더 이상 작업 이력을 추적할 필요가 없어지는 시점이 올지도 모른다.하지만 인간지능과 인공지능이 공동의 산출물을 함께 작업하는 현시점에서는 기존의 방식대로 [&#8230;]",
        "contentSnippet": "전편에서 제안한 원칙들을 어떻게 팀 프로젝트에 적용하는지에 대해 Claude Code Plugin / Skills / Hooks를 이용하여 구체적인 예시를 살펴본다. 케이스: LLM의 작업 진행사항을 이슈 트래커와 동기화하기 언젠가 AI의 개발 산출물이 인간의 수준을 아득히 뛰어넘는 날이 오면 더 이상 작업 이력을 추적할 필요가 없어지는 시점이 올지도 모른다.하지만 인간지능과 인공지능이 공동의 산출물을 함께 작업하는 현시점에서는 기존의 방식대로 […]",
        "guid": "https://01010011.blog/?p=2540",
        "categories": [
          "programming"
        ],
        "isoDate": "2026-01-30T01:07:34.000Z"
      }
    ]
  },
  {
    "name": "김봉현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강형석",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수로",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강미경",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강진우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권민재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권태관",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김도곤",
    "category": "개인",
    "posts": []
  },
  {
    "name": "칡토스의 게임 개발",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김선철",
    "category": "개인",
    "posts": []
  },
  {
    "name": "프리웨어 이야기",
    "category": "개인",
    "posts": [
      {
        "creator": "어떤오후의 프리웨어 이야기",
        "title": "집중력 저하 시대, NotebookLM으로 '느린 읽기' 습관 재부팅하는 비법",
        "link": "https://muzbox.tistory.com/483705",
        "pubDate": "Thu, 29 Jan 2026 16:03:54 +0900",
        "author": "어떤오후의 프리웨어 이야기",
        "comments": "https://muzbox.tistory.com/483705#entry483705comment",
        "content": "<div style=\"font-family: 'Noto Sans KR', sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; font-size: 16px; box-sizing: border-box; color: #3c4043;\">\n<div style=\"background-color: #e8f4fd; padding: 15px; border-radius: 8px; font-style: italic; margin-bottom: 25px; font-size: 15px;\">&nbsp;넘쳐나는 정보의 홍수 속에서 우리는 점점 더 짧은 집중력을 갖게 됩니다. 길고 복잡한 글 앞에서 쉽게 좌절하고, 피상적인 이해에 만족하곤 하죠. 이제는 달라져야 합니다. 이 글은 구글 NotebookLM을 활용해 '느린 읽기' 습관을 되찾고, 정보의 깊이를 완전히 내 것으로 만드는 혁신적인 방법을 제안합니다. 단순한 요약을 넘어, 진정한 사고의 파트너로서 NotebookLM이 여러분의 지적 성장에 어떻게 기여하는지 함께 탐구해 보세요.</div>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"download.jpg\" data-origin-width=\"1200\" data-origin-height=\"1200\"><span data-url=\"https://blog.kakaocdn.net/dn/cFRitZ/dJMcagdgOWp/1RVSJP8rmXPvZ3VHSFiQg1/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/cFRitZ/dJMcagdgOWp/1RVSJP8rmXPvZ3VHSFiQg1/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/cFRitZ/dJMcagdgOWp/1RVSJP8rmXPvZ3VHSFiQg1/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcFRitZ%2FdJMcagdgOWp%2F1RVSJP8rmXPvZ3VHSFiQg1%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"책을 깊이 읽으며 지식을 흡수하는 사람의 모습, NotebookLM을 활용한 느린 읽기를 통한 집중력 향상과 깊은 이해를 상징합니다.\" loading=\"lazy\" width=\"500\" height=\"500\" data-filename=\"download.jpg\" data-origin-width=\"1200\" data-origin-height=\"1200\"/></span></figure>\n\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">혹시 이런 경험 없으신가요? 분명 중요하고 흥미로운 글인데, 몇 문단 읽기도 전에 스크롤을 내리거나 다른 탭으로 넘어가 버리는 경험 말이에요. 텍스트가 조금만 어려워져도 금세 집중력을 잃고, 결국엔 &ldquo;다 읽긴 읽었는데&hellip; 뭘 읽었지?&rdquo; 하는 허무함에 사로잡히곤 하죠. 솔직히 말씀드리면,&nbsp; 저는 이런 현상이 점점 더 심해지는 것 같다고 느껴요. 긴 에세이, 깊이 있는 보도 자료, 연구 기반의 글들을 마치 SNS 피드처럼 쓱 훑어보고 넘기는 습관이 우리에게 너무나도 익숙해져 버린 거죠.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">저 역시 다르지 않았습니다. 수많은 글들을 북마크에 쌓아두고 &ldquo;나중에 읽어야지&rdquo; 하면서도, 결국엔 제대로 소화하지 못하는 날들이 이어졌죠. 그러다 문득 깨달았어요. 익숙함이 곧 이해는 아니라는 사실을요. 단순히 읽는 행위를 넘어, 진정으로 내용을 내 것으로 만들고 싶다는 갈증이 커졌습니다. 처음에는 더 좋은 노트 필기법을 찾아 헤맸고, 다음에는 구글 NotebookLM이라는 도구를 만났습니다. 그리고 이 도구를 &lsquo;느린 읽기&rsquo;의 강력한 동반자로 활용하기 시작했죠. 이미 수많은 글들을 NotebookLM으로 처리하고 있었기에, 이걸 장문 글에 적용하는 것은 저에게 아주 자연스러운 실험이자 전환점이었습니다.</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>  집중력 저하 시대, 왜 &lsquo;느린 읽기&rsquo;가 필요한가요?</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">생각해 보면, 현대 사회는 우리에게 끊임없이 '빠르게'를 요구합니다. 정보의 양은 폭발적으로 증가하고, 우리는 그 속도에 발맞추지 못하면 뒤처질 것 같은 불안감에 시달리죠. 하지만 정말 역설적으로, 이럴 때일수록 '느리게' 읽는 습관이 더욱 중요해진다고 저는 생각합니다. 빠르게 훑고 지나가버리는 것은 결코 진정한 학습이나 사고의 확장을 가져다주지 못해요. 오히려 미묘한 맥락, 숨겨진 전제, 그리고 깊은 통찰을 놓치게 만들 뿐입니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">느린 읽기는 단순히 속도를 늦추는 것을 넘어선, 하나의 <b>사고방식의 전환</b>입니다. 텍스트와 진정으로 대화하고, 저자의 의도를 파고들며, 나아가 나 자신만의 관점을 형성하는 과정이죠. 이 과정에서 필요한 것이 바로 '인내심'입니다. 그런데 이 인내심, 혼자서 기르기 정말 어렵지 않나요? 그래서 저는 NotebookLM을 활용해 이 여정을 함께할 파트너를 만들기로 했습니다.</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>  NotebookLM, 요약 도구를 넘어선 &lsquo;사고의 동반자&rsquo;로</b></h2>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px;\" data-ke-size=\"size23\">  요약 기계에서 벗어나 깊이 있는 이해로</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">많은 분들이 NotebookLM을 <b>&lsquo;빠르게 답을 얻기 위한 요약 도구&rsquo;</b>로 활용하실 거예요. 문서를 업로드하고, 요약을 요청한 다음, 핵심만 쓱 훑어보는 방식 말이죠. 물론 이런 활용법도 유용하지만, 솔직히 이건 우리가 다른 콘텐츠를 소비하는 성급한 방식과 크게 다르지 않다고 봅니다. 저 역시 그랬고요. 그러다 어느 순간, 이렇게 얻은 지식은 뇌리에 오래 남지 않는다는 좌절감을 느끼기 시작했습니다. 이 경험이 저를 변화시켰어요.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">저는 NotebookLM에 전체 문서를 요약해 달라는 요청을 멈췄습니다. 대신, <b>제가 인내심을 가지고 장문의 글을 읽는 동안 함께 걸어가는 &lsquo;읽기 파트너&rsquo;</b>로 대하기 시작했죠. 지름길을 찾는 대신, 목적지에 도달하는 여정 자체를 중요하게 여기는 방식으로요. 목표는 더 이상 속도가 아니었습니다. 요약 과정에서 사라질 수 있는 모든 미묘한 뉘앙스, 숨겨진 의미까지 완벽하게 이해할 만큼 충분히 콘텐츠와 함께 머무는 것. 그것이 저의 새로운 목표가 되었어요.</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"download.jpg\" data-origin-width=\"1408\" data-origin-height=\"768\"><span data-url=\"https://blog.kakaocdn.net/dn/bBCqvG/dJMcaioCFHY/vDtbuk7gOJbyZKoCIklKJK/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/bBCqvG/dJMcaioCFHY/vDtbuk7gOJbyZKoCIklKJK/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/bBCqvG/dJMcaioCFHY/vDtbuk7gOJbyZKoCIklKJK/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbBCqvG%2FdJMcaioCFHY%2FvDtbuk7gOJbyZKoCIklKJK%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"빠르게 훑어보는 습관에서 벗어나 NotebookLM을 활용해 깊이 있는 느린 읽기로 전환하는 모습을 추상적으로 표현한 이미지.\" loading=\"lazy\" width=\"1408\" height=\"768\" data-filename=\"download.jpg\" data-origin-width=\"1408\" data-origin-height=\"768\"/></span></figure>\n\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px;\" data-ke-size=\"size23\">  텍스트 중심 접근: 하나의 글로 하나의 노트를!</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">이러한 새로운 읽기 및 학습 시스템을 구축하기 위해, 저는 길거나 까다로운 텍스트 하나당 <b>별도의 NotebookLM 노트북을 하나씩 만들어요.</b> 예를 들어, 경제학 논문 하나를 읽을 때 그 논문만을 위한 노트북을 따로 만드는 식이죠. 이 <b>&lsquo;단일 텍스트 중심 접근&rsquo;</b>은 생각보다 훨씬 중요합니다. 왜냐하면 이렇게 해야만 글의 맥락을 온전히 유지할 수 있고, 아직 충분히 이해되지 않은 아이디어가 마구잡이로 섞여 혼란이 가중되는 것을 막을 수 있거든요. 하나의 기사나 PDF가 NotebookLM에서 저의 연구와 학습을 위한 지식 기반을 구축하는 출발점이 되는 셈이죠.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px;\" data-ke-size=\"size23\">⚙️ 느린 읽기를 강제하는 나만의 맞춤 지침</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">제가 NotebookLM을 &lsquo;느린 읽기 파트너&rsquo;로 활용할 수 있었던 핵심적인 이유 중 하나는 바로 <b>간단한 &lsquo;사용자 지정 지침(Custom Instruction)&rsquo;</b> 덕분입니다. NotebookLM의 <b>&lsquo;Configure notebook &gt; Custom&rsquo;</b> 메뉴에서 자신만의 지침을 입력할 수 있는데, 제 설정은 대략 이렇습니다.</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"1224\" data-origin-height=\"624\"><span data-url=\"https://blog.kakaocdn.net/dn/bJmelz/dJMcagj2lad/fwIk92pJoKRziOTe2y8nf0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bJmelz/dJMcagj2lad/fwIk92pJoKRziOTe2y8nf0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bJmelz/dJMcagj2lad/fwIk92pJoKRziOTe2y8nf0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbJmelz%2FdJMcagj2lad%2FfwIk92pJoKRziOTe2y8nf0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1224\" height=\"624\" data-origin-width=\"1224\" data-origin-height=\"624\"/></span></figure>\n\n<div style=\"background-color: #e8f4fd; border-left: 4px solid #1a73e8; padding: 15px; margin: 20px 0; border-radius: 0 8px 8px 0; color: #3c4043;\">  <b>나만의 NotebookLM 맞춤 지침 예시</b><br />\n<ul style=\"list-style-type: disc; padding-left: 20px; margin-top: 10px;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 8px;\">너는 나의 느린 읽기 파트너다. 내가 명시적으로 요청하지 않는 한, 전체 섹션을 요약하지 말 것.</li>\n<li style=\"margin-bottom: 8px;\">내가 문단을 붙여넣으면, 그것을 신중하게 바꿔 말해 주고, 전제(assumptions)를 드러내며, 모호한 부분을 강조해 달라.</li>\n<li style=\"margin-bottom: 8px;\">결론보다 질문을 우선해 달라.</li>\n</ul>\n</div>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">이 단 한 가지 지침이 NotebookLM과의 모든 대화 톤을 완전히 바꿔 놓았어요. 때로는 NotebookLM의 <b>속도를 늦추는 &lsquo;과속 방지턱&rsquo;</b>처럼 작동해서, 제가 텍스트에 더 오래 머물게 해 줍니다. NotebookLM은 제가 잠시 멈춰 서서, 글이 던지는 질문들을 깊이 곱씹어 보도록 만들어요. 덕분에 저는 피상적인 이해를 넘어, 글의 심층적인 구조와 의미를 탐구할 수 있게 됩니다.</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>  한 문단씩 뜯어보기: &lsquo;혼란&rsquo;을 &lsquo;진전&rsquo;으로 만드는 비법</b></h2>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px;\" data-ke-size=\"size23\">✍️ 지름길을 거부하는 탐구적 프롬프트 활용</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">첫 번째 읽기에서는 NotebookLM이 제안하는 질문들을 활용해 전체적인 맥락을 한번 훑어봅니다. 중요한 것은 <b>두 번째 읽기부터예요.</b> 정말 복잡하고 어려운 글을 만났을 때, 저는 주요 출처를 선택 해제한 상태에서 <b>한 문단 또는 몇 개의 문단만 NotebookLM에 붙여넣습니다.</b> 이처럼 제한적으로 접근하는 것은 의도적인 전략입니다. 어려운 글은 종종 몇 문장 안에 여러 아이디어를 압축해 담고 있는데, 우리가 너무 빠르게 읽으면 피상적인 해석에 머물러 핵심을 놓치기 쉽거든요. 한 문단씩 천천히 읽으면 자연스럽게 속도가 느려질 수밖에 없습니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">NotebookLM의 기본 질문을 그대로 사용하는 대신, 저는 저만의 <b>&lsquo;후속 탐구 질문&rsquo;</b>을 던집니다. 예를 들어, 이런 식이죠.</p>\n<ul style=\"list-style-type: disc; padding-left: 20px; margin-bottom: 20px;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 8px;\">이 문단의 복잡성을 알아차리도록 신중한 독자를 강제로 느리게 만들 다섯 가지 탐구 질문을 생성해 달라.</li>\n<li style=\"margin-bottom: 8px;\">이 문단은 독자나 더 넓은 맥락에 대해 어떤 전제를 깔고 있는가?</li>\n<li style=\"margin-bottom: 8px;\">이 논증의 어느 부분이 앞선 텍스트의 섹션들에 가장 크게 의존하는가?</li>\n<li style=\"margin-bottom: 8px;\">이 문단은 훑어읽을 경우 어떻게 오해될 수 있는가?</li>\n<li style=\"margin-bottom: 8px;\">사려 깊은 비평가는 여기서 무엇을 문제 삼을까?</li>\n</ul>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">이러한 프롬프트들은 제 이해의 엄청난 공백을 여과 없이 드러냅니다. 그런데 이게 바로 목적이에요! 질문과 함께 다시 읽을 때, <b>혼란은 더 이상 실패처럼 느껴지지 않고, 오히려 진정한 진전처럼 느껴지기 시작합니다.</b> 시간이 지나면서 저는 미묘하지만 중요한 변화를 알아차렸어요. 한 문단을 세 번째로 읽을 때는 더 이상 단순히 &lsquo;저자가 무엇을 말하는가?&rsquo;를 묻지 않고, &lsquo;왜 이런 방식으로 말했는가?&rsquo;라는 관점에 초점을 맞추게 된다는 점이죠. 저자의 의도와 배경까지 파고드는 겁니다.</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"download.jpg\" data-origin-width=\"1408\" data-origin-height=\"768\"><span data-url=\"https://blog.kakaocdn.net/dn/bIM1hf/dJMcagqNtgm/nxpk6z5pl3uCqcn9S2wtqk/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/bIM1hf/dJMcagqNtgm/nxpk6z5pl3uCqcn9S2wtqk/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/bIM1hf/dJMcagqNtgm/nxpk6z5pl3uCqcn9S2wtqk/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbIM1hf%2FdJMcagqNtgm%2Fnxpk6z5pl3uCqcn9S2wtqk%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"한 문단을 집중적으로 읽고 NotebookLM의 질문을 통해 깊이 사고하는 과정을 묘사한 이미지.\" loading=\"lazy\" width=\"1408\" height=\"768\" data-filename=\"download.jpg\" data-origin-width=\"1408\" data-origin-height=\"768\"/></span></figure>\n\n<table style=\"width: 100%; border-collapse: collapse; margin: 25px 0; color: #3c4043;\" data-ke-align=\"alignLeft\">\n<thead>\n<tr style=\"background-color: #e8eaed;\">\n<th style=\"border: 1px solid #dadce0; padding: 12px; text-align: center; color: #3c4043; font-weight: bold;\">구분</th>\n<th style=\"border: 1px solid #dadce0; padding: 12px; text-align: center; color: #3c4043; font-weight: bold;\">빠른 훑어읽기 (Superficial Reading)</th>\n<th style=\"border: 1px solid #dadce0; padding: 12px; text-align: center; color: #3c4043; font-weight: bold;\">NotebookLM 활용 느린 읽기 (Deep Reading)</th>\n</tr>\n</thead>\n<tbody>\n<tr style=\"background-color: #f8f9fa;\">\n<td style=\"border: 1px solid #dadce0; padding: 10px; text-align: center; color: #3c4043;\"><b>목표</b></td>\n<td style=\"border: 1px solid #dadce0; padding: 10px; color: #3c4043;\">정보의 핵심 파악, 빠른 소비</td>\n<td style=\"border: 1px solid #dadce0; padding: 10px; color: #3c4043;\">맥락, 뉘앙스, 저자의 의도 심층 이해</td>\n</tr>\n<tr style=\"background-color: #f1f3f4;\">\n<td style=\"border: 1px solid #dadce0; padding: 10px; text-align: center; color: #3c4043;\"><b>활용 방식</b></td>\n<td style=\"border: 1px solid #dadce0; padding: 10px; color: #3c4043;\">요약 기능 위주, 자동 질문 활용</td>\n<td style=\"border: 1px solid #dadce0; padding: 10px; color: #3c4043;\">문단별 탐구 프롬프트, 맞춤 지침</td>\n</tr>\n<tr style=\"background-color: #f8f9fa;\">\n<td style=\"border: 1px solid #dadce0; padding: 10px; text-align: center; color: #3c4043;\"><b>결과</b></td>\n<td style=\"border: 1px solid #dadce0; padding: 10px; color: #3c4043;\">피상적인 이해, 빠른 망각</td>\n<td style=\"border: 1px solid #dadce0; padding: 10px; color: #3c4043;\">깊이 있는 지식 축적, 비판적 사고력 증진</td>\n</tr>\n</tbody>\n</table>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>✅ 능동적 마이크로 드릴: 이해를 위한 작은 근육 운동</b></h2>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px;\" data-ke-size=\"size23\">  수동적 노트 대신 짧고 집중적인 연습으로</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">전통적인 노트 필기 방식, 특히 복잡한 자료 앞에서는 한계를 드러내는 경우가 많습니다. 하이라이트를 아무리 쳐도 쌓이기만 할 뿐, 실제 이해로 이어지지 않는 경우가 허다하죠. 요약 역시 뉘앙스를 죽이고 핵심을 뭉뚱그려 버리기 일쑤고요. 더 나쁜 것은, 단순히 읽거나 훑어보는 것만으로도 <b>&lsquo;내가 이해하고 있다&rsquo;는 착각</b>에 빠지기 쉽다는 점입니다. 저는 이런 함정을 피하기 위해 새로운 방법을 찾았습니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">저는 각 주요 섹션을 읽고 난 뒤, NotebookLM에게 <b>작고 표적화된 &lsquo;마이크로 드릴(Micro Drill)&rsquo;</b>을 생성해 달라고 요청합니다. 이것들은 전통적인 의미의 학습 보조 도구라기보다는, NotebookLM의 강력한 학습 기능을 활용해 <b>저의 약한 이해 포인트를 초기에 드러내는 역할을 해요.</b> 제가 자주 사용하는 프롬프트는 다음과 같습니다.</p>\n<ul style=\"list-style-type: disc; padding-left: 20px; margin-bottom: 20px;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 8px;\">이 논증을 정말로 이해했는지 확인하는 세 문항 퀴즈를 생성해 달라.</li>\n<li style=\"margin-bottom: 8px;\">이 섹션을 주장과 이유를 연결하는 &ldquo;왜&ndash;왜냐하면&rdquo; 사슬로 바꿔 달라.</li>\n<li style=\"margin-bottom: 8px;\">하나의 전제가 제거되면 이 섹션에서 어떤 아이디어가 무너질까?</li>\n</ul>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">이 마이크로 드릴들이 정말 효과적인 이유는 그 <b>&lsquo;범위&rsquo;</b>에 있습니다. 즉시 완료할 수 있을 만큼 짧지만, 동시에 제 이해의 공백을 명확히 드러낼 만큼 충분히 까다롭죠. 만약 막히는 지점이 생긴다면, 저는 억지로 앞으로 밀고 나가지 않습니다. 대신, <b>마찰을 일으킨 바로 그 문단으로 돌아가 다시 읽으며 문제의 핵심을 파고들어요.</b></p>\n<div style=\"background-color: #fce8e6; border-left: 4px solid #d93025; padding: 15px; margin: 20px 0; border-radius: 0 8px 8px 0; color: #3c4043;\">⚠️ <b>주의하세요!</b><br />막히는 부분이 생겼을 때 절대 서둘러 넘어가거나 &lsquo;이해한 척&rsquo;하지 마세요. 그 부분이 바로 여러분의 깊은 이해를 방해하는 지점입니다. 다시 돌아가 꼼꼼히 확인하고 질문하는 습관을 들이는 것이 중요해요.</div>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">제가 직접 만든 퀴즈 프롬프트는 NotebookLM의 기본 퀴즈나 플래시카드 도구와는 다른 결과물을 내놓습니다. 물론 NotebookLM의 기본 도구들도 글 전체를 다 읽은 뒤 최종 점검용으로 사용하기에 가치가 있지만, 제가 만든 마이크로 드릴은 <b>실시간으로 제 이해도를 점검하고 사고를 강제하는 상호작용형 테스트</b>에 가깝습니다. 덕분에 저는 훨씬 더 능동적으로 텍스트와 씨름하며 지식을 쌓아갈 수 있게 되었어요.</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"download.jpg\" data-origin-width=\"1408\" data-origin-height=\"768\"><span data-url=\"https://blog.kakaocdn.net/dn/b00wsA/dJMcahQMCjy/6E4KkGsB6M97RViKB8SKRK/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/b00wsA/dJMcahQMCjy/6E4KkGsB6M97RViKB8SKRK/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/b00wsA/dJMcahQMCjy/6E4KkGsB6M97RViKB8SKRK/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb00wsA%2FdJMcahQMCjy%2F6E4KkGsB6M97RViKB8SKRK%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"마이크로 드릴을 통해 지속적으로 이해도를 점검하고 지식을 심화하는 학습 루프를 표현한 인포그래픽.\" loading=\"lazy\" width=\"1408\" height=\"768\" data-filename=\"download.jpg\" data-origin-width=\"1408\" data-origin-height=\"768\"/></span></figure>\n\n<div style=\"background-color: #f8f9fa; border: 1px solid #dadce0; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); padding: 25px; margin: 40px 0;\">\n<div style=\"font-size: 26px; color: #1a73e8; font-weight: bold; margin-bottom: 15px; padding-bottom: 10px; border-bottom: 2px solid #1a73e8;\">  핵심 요약</div>\n<ul style=\"list-style-type: none; padding-left: 0; margin-bottom: 20px;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 15px; font-size: 17px;\">1. <b>NotebookLM을 요약 도구가 아닌 '느린 읽기 파트너'로 인식</b>하고, 질문을 우선시하는 맞춤 지침을 설정하세요.</li>\n<li style=\"margin-bottom: 15px; font-size: 17px;\">2. <b>복잡한 텍스트는 한 문단씩 집중적으로 읽으며</b>, 지름길 프롬프트를 통해 저자의 숨겨진 전제와 미묘한 뉘앙스를 파악하세요.</li>\n<li style=\"margin-bottom: 15px; font-size: 17px;\">3. <b>각 섹션 뒤에 짧은 '마이크로 드릴'을 활용</b>해 능동적으로 이해도를 점검하고, 약한 부분을 즉시 보완하세요.</li>\n<li style=\"margin-bottom: 0; font-size: 17px;\">4. <b>결정적인 요소는 '인내심'!</b> 작은 단위로 시작하여 점진적으로 느린 읽기 습관을 길러나가세요.</li>\n</ul>\n<div style=\"font-size: 14px; color: #5f6368; margin-top: 20px; padding-top: 15px; border-top: 1px solid #dadce0;\">이러한 전략을 통해 NotebookLM은 단순한 정보 처리 도구를 넘어, 여러분의 사고를 심화시키고 진정한 지식 습득을 돕는 강력한 조력자가 될 것입니다.</div>\n</div>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>✨ 느린 읽기는 하루하루 다시 길러갈 수 있는 기술</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">이런 방식으로 NotebookLM을 활용한다고 해서 글을 더 빨리 끝내게 된 것은 아닙니다. 아니, 정확히 말하면, 더 빨리 끝내기보다는 <b>글 속으로 다시 돌아오게 만드는 힘</b>을 얻었습니다. 유튜브 긴 재생목록으로 학습할 때도 비슷한 접근법을 사용하고 있는데, 확실히 이해도가 남달라요. 속도를 늦추고, 범위를 제한하며, 더 나은 질문을 끊임없이 던지면, 다시 읽는 행위 자체가 호기심을 자극하고 즐거움을 주는 과정이 됩니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">물론, 어떤 AI 도구도 대신해 줄 수 없는 한 가지 필수 요소가 있습니다. 바로 <b>&lsquo;인내심&rsquo;</b>입니다. 처음부터 완벽하게 모든 글에 적용하려 하지 마세요. 어려운 텍스트를 위해 NotebookLM을 사용해 보고 싶다면, <b>작게 시작하는 것이 중요해요.</b> 하나의 긴 기사, 그중에서도 하나의 문단, 딱 한 번의 &lsquo;다시 읽기&rsquo;부터 시작해 보세요. 그것만으로도 2026년 여러분의 깊이 읽는 습관을 재부팅하기에 충분할 겁니다. 새로운 지적 여정을 응원합니다!</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>❓ 자주 묻는 질문 (FAQ)</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\"><b>Q1: NotebookLM의 &lsquo;느린 읽기&rsquo; 방법이 모든 유형의 글에 효과적인가요?</b><br />A1: 아니요, 이 방법은 주로 복잡하거나 깊이 있는 이해가 필요한 장문의 텍스트에 특히 효과적입니다. 뉴스 기사나 짧은 정보성 글에는 일반적인 NotebookLM의 요약 기능을 활용하는 것이 더 효율적일 수 있습니다. 중요한 것은 글의 목적과 자신의 학습 목표에 맞춰 유연하게 적용하는 것입니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\"><b>Q2: NotebookLM 맞춤 지침을 설정하는 것이 정말 큰 차이를 만드나요?</b><br />A2: 네, 예상보다 훨씬 큰 차이를 만듭니다. 맞춤 지침은 NotebookLM이 여러분의 의도에 맞춰 정보를 처리하고 응답하도록 유도하여, 단순한 답변을 넘어 사고를 촉진하는 파트너 역할을 수행하게 합니다. &lsquo;질문을 우선해 달라&rsquo;는 지침이 특히 효과적이에요.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\"><b>Q3: &lsquo;마이크로 드릴&rsquo;이 전통적인 퀴즈와 다른 점은 무엇인가요?</b><br />A3: 마이크로 드릴은 짧고 특정 섹션에 집중하며, 정답을 맞히기보다 이해의 공백을 드러내고 사고를 유도하는 데 중점을 둡니다. 전통적인 퀴즈가 전체적인 지식 점검이라면, 마이크로 드릴은 실시간으로 미묘한 이해 부족을 파고드는 &lsquo;근육 운동&rsquo;에 가깝습니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\"><b>Q4: &lsquo;느린 읽기&rsquo; 습관을 들이는 데 가장 중요한 것은 무엇인가요?</b><br />A4: 가장 중요한 것은 &lsquo;인내심&rsquo;과 &lsquo;작게 시작하는 용기&rsquo;입니다. 처음부터 완벽하려 하기보다, 작은 성공 경험을 통해 점진적으로 습관을 확장해 나가는 것이 중요해요. 혼란을 실패로 여기지 않고, 더 깊이 파고들 기회로 삼는 태도가 필요합니다.</p>\n</div>\n<script type=\"application/ld+json\">\n{\n  \"@context\": \"https://schema.org\",\n  \"@type\": \"FAQPage\",\n  \"mainEntity\": [\n    {\n      \"@type\": \"Question\",\n      \"name\": \"NotebookLM의 ‘느린 읽기’ 방법이 모든 유형의 글에 효과적인가요?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"아니요, 이 방법은 주로 복잡하거나 깊이 있는 이해가 필요한 장문의 텍스트에 특히 효과적입니다. 뉴스 기사나 짧은 정보성 글에는 일반적인 NotebookLM의 요약 기능을 활용하는 것이 더 효율적일 수 있습니다. 중요한 것은 글의 목적과 자신의 학습 목표에 맞춰 유연하게 적용하는 것입니다.\"\n      }\n    },\n    {\n      \"@type\": \"Question\",\n      \"name\": \"NotebookLM 맞춤 지침을 설정하는 것이 정말 큰 차이를 만드나요?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"네, 예상보다 훨씬 큰 차이를 만듭니다. 맞춤 지침은 NotebookLM이 여러분의 의도에 맞춰 정보를 처리하고 응답하도록 유도하여, 단순한 답변을 넘어 사고를 촉진하는 파트너 역할을 수행하게 합니다. ‘질문을 우선해 달라’는 지침이 특히 효과적이에요.\"\n      }\n    },\n    {\n      \"@type\": \"Question\",\n      \"name\": \"‘마이크로 드릴’이 전통적인 퀴즈와 다른 점은 무엇인가요?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"마이크로 드릴은 짧고 특정 섹션에 집중하며, 정답을 맞히기보다 이해의 공백을 드러내고 사고를 유도하는 데 중점을 둡니다. 전통적인 퀴즈가 전체적인 지식 점검이라면, 마이크로 드릴은 실시간으로 미묘한 이해 부족을 파고드는 ‘근육 운동’에 가깝습니다.\"\n      }\n    },\n    {\n      \"@type\": \"Question\",\n      \"name\": \"‘느린 읽기’ 습관을 들이는 데 가장 중요한 것은 무엇인가요?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"가장 중요한 것은 ‘인내심’과 ‘작게 시작하는 용기’입니다. 처음부터 완벽하려 하기보다, 작은 성공 경험을 통해 점진적으로 습관을 확장해 나가는 것이 중요해요. 혼란을 실패로 여기지 않고, 더 깊이 파고들 기회로 삼는 태도가 필요합니다.\"\n      }\n    }\n  ]\n}\n</script>",
        "contentSnippet": "넘쳐나는 정보의 홍수 속에서 우리는 점점 더 짧은 집중력을 갖게 됩니다. 길고 복잡한 글 앞에서 쉽게 좌절하고, 피상적인 이해에 만족하곤 하죠. 이제는 달라져야 합니다. 이 글은 구글 NotebookLM을 활용해 '느린 읽기' 습관을 되찾고, 정보의 깊이를 완전히 내 것으로 만드는 혁신적인 방법을 제안합니다. 단순한 요약을 넘어, 진정한 사고의 파트너로서 NotebookLM이 여러분의 지적 성장에 어떻게 기여하는지 함께 탐구해 보세요.\n\n\n혹시 이런 경험 없으신가요? 분명 중요하고 흥미로운 글인데, 몇 문단 읽기도 전에 스크롤을 내리거나 다른 탭으로 넘어가 버리는 경험 말이에요. 텍스트가 조금만 어려워져도 금세 집중력을 잃고, 결국엔 “다 읽긴 읽었는데… 뭘 읽었지?” 하는 허무함에 사로잡히곤 하죠. 솔직히 말씀드리면,  저는 이런 현상이 점점 더 심해지는 것 같다고 느껴요. 긴 에세이, 깊이 있는 보도 자료, 연구 기반의 글들을 마치 SNS 피드처럼 쓱 훑어보고 넘기는 습관이 우리에게 너무나도 익숙해져 버린 거죠.\n저 역시 다르지 않았습니다. 수많은 글들을 북마크에 쌓아두고 “나중에 읽어야지” 하면서도, 결국엔 제대로 소화하지 못하는 날들이 이어졌죠. 그러다 문득 깨달았어요. 익숙함이 곧 이해는 아니라는 사실을요. 단순히 읽는 행위를 넘어, 진정으로 내용을 내 것으로 만들고 싶다는 갈증이 커졌습니다. 처음에는 더 좋은 노트 필기법을 찾아 헤맸고, 다음에는 구글 NotebookLM이라는 도구를 만났습니다. 그리고 이 도구를 ‘느린 읽기’의 강력한 동반자로 활용하기 시작했죠. 이미 수많은 글들을 NotebookLM으로 처리하고 있었기에, 이걸 장문 글에 적용하는 것은 저에게 아주 자연스러운 실험이자 전환점이었습니다.\n  집중력 저하 시대, 왜 ‘느린 읽기’가 필요한가요?\n생각해 보면, 현대 사회는 우리에게 끊임없이 '빠르게'를 요구합니다. 정보의 양은 폭발적으로 증가하고, 우리는 그 속도에 발맞추지 못하면 뒤처질 것 같은 불안감에 시달리죠. 하지만 정말 역설적으로, 이럴 때일수록 '느리게' 읽는 습관이 더욱 중요해진다고 저는 생각합니다. 빠르게 훑고 지나가버리는 것은 결코 진정한 학습이나 사고의 확장을 가져다주지 못해요. 오히려 미묘한 맥락, 숨겨진 전제, 그리고 깊은 통찰을 놓치게 만들 뿐입니다.\n느린 읽기는 단순히 속도를 늦추는 것을 넘어선, 하나의 사고방식의 전환입니다. 텍스트와 진정으로 대화하고, 저자의 의도를 파고들며, 나아가 나 자신만의 관점을 형성하는 과정이죠. 이 과정에서 필요한 것이 바로 '인내심'입니다. 그런데 이 인내심, 혼자서 기르기 정말 어렵지 않나요? 그래서 저는 NotebookLM을 활용해 이 여정을 함께할 파트너를 만들기로 했습니다.\n  NotebookLM, 요약 도구를 넘어선 ‘사고의 동반자’로\n  요약 기계에서 벗어나 깊이 있는 이해로\n많은 분들이 NotebookLM을 ‘빠르게 답을 얻기 위한 요약 도구’로 활용하실 거예요. 문서를 업로드하고, 요약을 요청한 다음, 핵심만 쓱 훑어보는 방식 말이죠. 물론 이런 활용법도 유용하지만, 솔직히 이건 우리가 다른 콘텐츠를 소비하는 성급한 방식과 크게 다르지 않다고 봅니다. 저 역시 그랬고요. 그러다 어느 순간, 이렇게 얻은 지식은 뇌리에 오래 남지 않는다는 좌절감을 느끼기 시작했습니다. 이 경험이 저를 변화시켰어요.\n저는 NotebookLM에 전체 문서를 요약해 달라는 요청을 멈췄습니다. 대신, 제가 인내심을 가지고 장문의 글을 읽는 동안 함께 걸어가는 ‘읽기 파트너’로 대하기 시작했죠. 지름길을 찾는 대신, 목적지에 도달하는 여정 자체를 중요하게 여기는 방식으로요. 목표는 더 이상 속도가 아니었습니다. 요약 과정에서 사라질 수 있는 모든 미묘한 뉘앙스, 숨겨진 의미까지 완벽하게 이해할 만큼 충분히 콘텐츠와 함께 머무는 것. 그것이 저의 새로운 목표가 되었어요.\n\n\n  텍스트 중심 접근: 하나의 글로 하나의 노트를!\n이러한 새로운 읽기 및 학습 시스템을 구축하기 위해, 저는 길거나 까다로운 텍스트 하나당 별도의 NotebookLM 노트북을 하나씩 만들어요. 예를 들어, 경제학 논문 하나를 읽을 때 그 논문만을 위한 노트북을 따로 만드는 식이죠. 이 ‘단일 텍스트 중심 접근’은 생각보다 훨씬 중요합니다. 왜냐하면 이렇게 해야만 글의 맥락을 온전히 유지할 수 있고, 아직 충분히 이해되지 않은 아이디어가 마구잡이로 섞여 혼란이 가중되는 것을 막을 수 있거든요. 하나의 기사나 PDF가 NotebookLM에서 저의 연구와 학습을 위한 지식 기반을 구축하는 출발점이 되는 셈이죠.\n⚙️ 느린 읽기를 강제하는 나만의 맞춤 지침\n제가 NotebookLM을 ‘느린 읽기 파트너’로 활용할 수 있었던 핵심적인 이유 중 하나는 바로 간단한 ‘사용자 지정 지침(Custom Instruction)’ 덕분입니다. NotebookLM의 ‘Configure notebook > Custom’ 메뉴에서 자신만의 지침을 입력할 수 있는데, 제 설정은 대략 이렇습니다.\n\n\n  나만의 NotebookLM 맞춤 지침 예시\n너는 나의 느린 읽기 파트너다. 내가 명시적으로 요청하지 않는 한, 전체 섹션을 요약하지 말 것.\n내가 문단을 붙여넣으면, 그것을 신중하게 바꿔 말해 주고, 전제(assumptions)를 드러내며, 모호한 부분을 강조해 달라.\n결론보다 질문을 우선해 달라.\n이 단 한 가지 지침이 NotebookLM과의 모든 대화 톤을 완전히 바꿔 놓았어요. 때로는 NotebookLM의 속도를 늦추는 ‘과속 방지턱’처럼 작동해서, 제가 텍스트에 더 오래 머물게 해 줍니다. NotebookLM은 제가 잠시 멈춰 서서, 글이 던지는 질문들을 깊이 곱씹어 보도록 만들어요. 덕분에 저는 피상적인 이해를 넘어, 글의 심층적인 구조와 의미를 탐구할 수 있게 됩니다.\n  한 문단씩 뜯어보기: ‘혼란’을 ‘진전’으로 만드는 비법\n✍️ 지름길을 거부하는 탐구적 프롬프트 활용\n첫 번째 읽기에서는 NotebookLM이 제안하는 질문들을 활용해 전체적인 맥락을 한번 훑어봅니다. 중요한 것은 두 번째 읽기부터예요. 정말 복잡하고 어려운 글을 만났을 때, 저는 주요 출처를 선택 해제한 상태에서 한 문단 또는 몇 개의 문단만 NotebookLM에 붙여넣습니다. 이처럼 제한적으로 접근하는 것은 의도적인 전략입니다. 어려운 글은 종종 몇 문장 안에 여러 아이디어를 압축해 담고 있는데, 우리가 너무 빠르게 읽으면 피상적인 해석에 머물러 핵심을 놓치기 쉽거든요. 한 문단씩 천천히 읽으면 자연스럽게 속도가 느려질 수밖에 없습니다.\nNotebookLM의 기본 질문을 그대로 사용하는 대신, 저는 저만의 ‘후속 탐구 질문’을 던집니다. 예를 들어, 이런 식이죠.\n이 문단의 복잡성을 알아차리도록 신중한 독자를 강제로 느리게 만들 다섯 가지 탐구 질문을 생성해 달라.\n이 문단은 독자나 더 넓은 맥락에 대해 어떤 전제를 깔고 있는가?\n이 논증의 어느 부분이 앞선 텍스트의 섹션들에 가장 크게 의존하는가?\n이 문단은 훑어읽을 경우 어떻게 오해될 수 있는가?\n사려 깊은 비평가는 여기서 무엇을 문제 삼을까?\n이러한 프롬프트들은 제 이해의 엄청난 공백을 여과 없이 드러냅니다. 그런데 이게 바로 목적이에요! 질문과 함께 다시 읽을 때, 혼란은 더 이상 실패처럼 느껴지지 않고, 오히려 진정한 진전처럼 느껴지기 시작합니다. 시간이 지나면서 저는 미묘하지만 중요한 변화를 알아차렸어요. 한 문단을 세 번째로 읽을 때는 더 이상 단순히 ‘저자가 무엇을 말하는가?’를 묻지 않고, ‘왜 이런 방식으로 말했는가?’라는 관점에 초점을 맞추게 된다는 점이죠. 저자의 의도와 배경까지 파고드는 겁니다.\n\n\n\n\n\n구분\n빠른 훑어읽기 (Superficial Reading)\nNotebookLM 활용 느린 읽기 (Deep Reading)\n\n\n\n\n목표\n정보의 핵심 파악, 빠른 소비\n맥락, 뉘앙스, 저자의 의도 심층 이해\n\n\n활용 방식\n요약 기능 위주, 자동 질문 활용\n문단별 탐구 프롬프트, 맞춤 지침\n\n\n결과\n피상적인 이해, 빠른 망각\n깊이 있는 지식 축적, 비판적 사고력 증진\n\n\n\n✅ 능동적 마이크로 드릴: 이해를 위한 작은 근육 운동\n  수동적 노트 대신 짧고 집중적인 연습으로\n전통적인 노트 필기 방식, 특히 복잡한 자료 앞에서는 한계를 드러내는 경우가 많습니다. 하이라이트를 아무리 쳐도 쌓이기만 할 뿐, 실제 이해로 이어지지 않는 경우가 허다하죠. 요약 역시 뉘앙스를 죽이고 핵심을 뭉뚱그려 버리기 일쑤고요. 더 나쁜 것은, 단순히 읽거나 훑어보는 것만으로도 ‘내가 이해하고 있다’는 착각에 빠지기 쉽다는 점입니다. 저는 이런 함정을 피하기 위해 새로운 방법을 찾았습니다.\n저는 각 주요 섹션을 읽고 난 뒤, NotebookLM에게 작고 표적화된 ‘마이크로 드릴(Micro Drill)’을 생성해 달라고 요청합니다. 이것들은 전통적인 의미의 학습 보조 도구라기보다는, NotebookLM의 강력한 학습 기능을 활용해 저의 약한 이해 포인트를 초기에 드러내는 역할을 해요. 제가 자주 사용하는 프롬프트는 다음과 같습니다.\n이 논증을 정말로 이해했는지 확인하는 세 문항 퀴즈를 생성해 달라.\n이 섹션을 주장과 이유를 연결하는 “왜–왜냐하면” 사슬로 바꿔 달라.\n하나의 전제가 제거되면 이 섹션에서 어떤 아이디어가 무너질까?\n이 마이크로 드릴들이 정말 효과적인 이유는 그 ‘범위’에 있습니다. 즉시 완료할 수 있을 만큼 짧지만, 동시에 제 이해의 공백을 명확히 드러낼 만큼 충분히 까다롭죠. 만약 막히는 지점이 생긴다면, 저는 억지로 앞으로 밀고 나가지 않습니다. 대신, 마찰을 일으킨 바로 그 문단으로 돌아가 다시 읽으며 문제의 핵심을 파고들어요.\n⚠️ 주의하세요!\n막히는 부분이 생겼을 때 절대 서둘러 넘어가거나 ‘이해한 척’하지 마세요. 그 부분이 바로 여러분의 깊은 이해를 방해하는 지점입니다. 다시 돌아가 꼼꼼히 확인하고 질문하는 습관을 들이는 것이 중요해요.\n제가 직접 만든 퀴즈 프롬프트는 NotebookLM의 기본 퀴즈나 플래시카드 도구와는 다른 결과물을 내놓습니다. 물론 NotebookLM의 기본 도구들도 글 전체를 다 읽은 뒤 최종 점검용으로 사용하기에 가치가 있지만, 제가 만든 마이크로 드릴은 실시간으로 제 이해도를 점검하고 사고를 강제하는 상호작용형 테스트에 가깝습니다. 덕분에 저는 훨씬 더 능동적으로 텍스트와 씨름하며 지식을 쌓아갈 수 있게 되었어요.\n\n\n\n  핵심 요약\n1. NotebookLM을 요약 도구가 아닌 '느린 읽기 파트너'로 인식하고, 질문을 우선시하는 맞춤 지침을 설정하세요.\n2. 복잡한 텍스트는 한 문단씩 집중적으로 읽으며, 지름길 프롬프트를 통해 저자의 숨겨진 전제와 미묘한 뉘앙스를 파악하세요.\n3. 각 섹션 뒤에 짧은 '마이크로 드릴'을 활용해 능동적으로 이해도를 점검하고, 약한 부분을 즉시 보완하세요.\n4. 결정적인 요소는 '인내심'! 작은 단위로 시작하여 점진적으로 느린 읽기 습관을 길러나가세요.\n이러한 전략을 통해 NotebookLM은 단순한 정보 처리 도구를 넘어, 여러분의 사고를 심화시키고 진정한 지식 습득을 돕는 강력한 조력자가 될 것입니다.\n✨ 느린 읽기는 하루하루 다시 길러갈 수 있는 기술\n이런 방식으로 NotebookLM을 활용한다고 해서 글을 더 빨리 끝내게 된 것은 아닙니다. 아니, 정확히 말하면, 더 빨리 끝내기보다는 글 속으로 다시 돌아오게 만드는 힘을 얻었습니다. 유튜브 긴 재생목록으로 학습할 때도 비슷한 접근법을 사용하고 있는데, 확실히 이해도가 남달라요. 속도를 늦추고, 범위를 제한하며, 더 나은 질문을 끊임없이 던지면, 다시 읽는 행위 자체가 호기심을 자극하고 즐거움을 주는 과정이 됩니다.\n물론, 어떤 AI 도구도 대신해 줄 수 없는 한 가지 필수 요소가 있습니다. 바로 ‘인내심’입니다. 처음부터 완벽하게 모든 글에 적용하려 하지 마세요. 어려운 텍스트를 위해 NotebookLM을 사용해 보고 싶다면, 작게 시작하는 것이 중요해요. 하나의 긴 기사, 그중에서도 하나의 문단, 딱 한 번의 ‘다시 읽기’부터 시작해 보세요. 그것만으로도 2026년 여러분의 깊이 읽는 습관을 재부팅하기에 충분할 겁니다. 새로운 지적 여정을 응원합니다!\n❓ 자주 묻는 질문 (FAQ)\nQ1: NotebookLM의 ‘느린 읽기’ 방법이 모든 유형의 글에 효과적인가요?\nA1: 아니요, 이 방법은 주로 복잡하거나 깊이 있는 이해가 필요한 장문의 텍스트에 특히 효과적입니다. 뉴스 기사나 짧은 정보성 글에는 일반적인 NotebookLM의 요약 기능을 활용하는 것이 더 효율적일 수 있습니다. 중요한 것은 글의 목적과 자신의 학습 목표에 맞춰 유연하게 적용하는 것입니다.\nQ2: NotebookLM 맞춤 지침을 설정하는 것이 정말 큰 차이를 만드나요?\nA2: 네, 예상보다 훨씬 큰 차이를 만듭니다. 맞춤 지침은 NotebookLM이 여러분의 의도에 맞춰 정보를 처리하고 응답하도록 유도하여, 단순한 답변을 넘어 사고를 촉진하는 파트너 역할을 수행하게 합니다. ‘질문을 우선해 달라’는 지침이 특히 효과적이에요.\nQ3: ‘마이크로 드릴’이 전통적인 퀴즈와 다른 점은 무엇인가요?\nA3: 마이크로 드릴은 짧고 특정 섹션에 집중하며, 정답을 맞히기보다 이해의 공백을 드러내고 사고를 유도하는 데 중점을 둡니다. 전통적인 퀴즈가 전체적인 지식 점검이라면, 마이크로 드릴은 실시간으로 미묘한 이해 부족을 파고드는 ‘근육 운동’에 가깝습니다.\nQ4: ‘느린 읽기’ 습관을 들이는 데 가장 중요한 것은 무엇인가요?\nA4: 가장 중요한 것은 ‘인내심’과 ‘작게 시작하는 용기’입니다. 처음부터 완벽하려 하기보다, 작은 성공 경험을 통해 점진적으로 습관을 확장해 나가는 것이 중요해요. 혼란을 실패로 여기지 않고, 더 깊이 파고들 기회로 삼는 태도가 필요합니다.\n\n\n{\n  \"@context\": \"https://schema.org\",\n  \"@type\": \"FAQPage\",\n  \"mainEntity\": [\n    {\n      \"@type\": \"Question\",\n      \"name\": \"NotebookLM의 ‘느린 읽기’ 방법이 모든 유형의 글에 효과적인가요?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"아니요, 이 방법은 주로 복잡하거나 깊이 있는 이해가 필요한 장문의 텍스트에 특히 효과적입니다. 뉴스 기사나 짧은 정보성 글에는 일반적인 NotebookLM의 요약 기능을 활용하는 것이 더 효율적일 수 있습니다. 중요한 것은 글의 목적과 자신의 학습 목표에 맞춰 유연하게 적용하는 것입니다.\"\n      }\n    },\n    {\n      \"@type\": \"Question\",\n      \"name\": \"NotebookLM 맞춤 지침을 설정하는 것이 정말 큰 차이를 만드나요?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"네, 예상보다 훨씬 큰 차이를 만듭니다. 맞춤 지침은 NotebookLM이 여러분의 의도에 맞춰 정보를 처리하고 응답하도록 유도하여, 단순한 답변을 넘어 사고를 촉진하는 파트너 역할을 수행하게 합니다. ‘질문을 우선해 달라’는 지침이 특히 효과적이에요.\"\n      }\n    },\n    {\n      \"@type\": \"Question\",\n      \"name\": \"‘마이크로 드릴’이 전통적인 퀴즈와 다른 점은 무엇인가요?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"마이크로 드릴은 짧고 특정 섹션에 집중하며, 정답을 맞히기보다 이해의 공백을 드러내고 사고를 유도하는 데 중점을 둡니다. 전통적인 퀴즈가 전체적인 지식 점검이라면, 마이크로 드릴은 실시간으로 미묘한 이해 부족을 파고드는 ‘근육 운동’에 가깝습니다.\"\n      }\n    },\n    {\n      \"@type\": \"Question\",\n      \"name\": \"‘느린 읽기’ 습관을 들이는 데 가장 중요한 것은 무엇인가요?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"가장 중요한 것은 ‘인내심’과 ‘작게 시작하는 용기’입니다. 처음부터 완벽하려 하기보다, 작은 성공 경험을 통해 점진적으로 습관을 확장해 나가는 것이 중요해요. 혼란을 실패로 여기지 않고, 더 깊이 파고들 기회로 삼는 태도가 필요합니다.\"\n      }\n    }\n  ]\n}",
        "guid": "https://muzbox.tistory.com/483705",
        "categories": [
          "AI, 미래기술/AI 인사이트",
          "AI활용법",
          "notebooklm",
          "느린읽기",
          "독서습관",
          "디지털독서",
          "생산성도구",
          "심층독서",
          "정보과부하",
          "집중력향상",
          "학습전략"
        ],
        "isoDate": "2026-01-29T07:03:54.000Z"
      },
      {
        "creator": "어떤오후의 프리웨어 이야기",
        "title": "모공, 잡티, 솜털까지 표현! PURE GLOW 앱으로 AI 인물 피부 표현의 리얼리티를 극대화하는 법",
        "link": "https://muzbox.tistory.com/483704",
        "pubDate": "Sat, 24 Jan 2026 22:04:43 +0900",
        "author": "어떤오후의 프리웨어 이야기",
        "comments": "https://muzbox.tistory.com/483704#entry483704comment",
        "content": "<div style=\"font-family: 'Noto Sans KR', sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; font-size: 16px; box-sizing: border-box; color: #3c4043;\">\n<div style=\"background-color: #e8f4fd; padding: 15px; border-radius: 8px; font-style: italic; margin-bottom: 25px; font-size: 15px;\">혹시 AI로 만든 인물 이미지가 너무 부자연스러워서 실망한 적 있으신가요? 오늘은 플라스틱 같은 피부, 인형 같은 눈동자 등 이른바 &lsquo;AI 티&rsquo; 나는 이미지를 피하고, 모공, 잡티, 솜털까지 섬세하게 표현하는 극사실적인 AI 인물 사진을 만드는 방법을 알려드립니다. 혁신적인 PURE GLOW 앱을 통해 조명, 카메라, 피부 질감, 움직임의 4가지 핵심 원칙을 적용하여 실제 사진처럼 보이는 이미지를 만드는 비법을 최신 정보로 공개합니다.</div>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"download.jpg\" data-origin-width=\"1200\" data-origin-height=\"1200\"><span data-url=\"https://blog.kakaocdn.net/dn/bkHtuJ/dJMcacBUIUt/uClZWIpkmmBoNEL7Tlw9L1/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/bkHtuJ/dJMcacBUIUt/uClZWIpkmmBoNEL7Tlw9L1/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/bkHtuJ/dJMcacBUIUt/uClZWIpkmmBoNEL7Tlw9L1/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbkHtuJ%2FdJMcacBUIUt%2FuClZWIpkmmBoNEL7Tlw9L1%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"다양한 인종과 연령대의 사람들이 자연스러운 표정으로 카메라를 응시하는 극사실적인 인물 사진. 모공, 미세한 잡티, 솜털까지 섬세하게 표현된 피부 질감이 돋보이며, 부드러운 자연광이 따뜻한 분위기를 연출한다.\" loading=\"lazy\" width=\"500\" height=\"500\" data-filename=\"download.jpg\" data-origin-width=\"1200\" data-origin-height=\"1200\"/></span></figure>\n\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>✨ AI 인물 이미지, 왜 '진짜' 같지 않을까?</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">AI 기술이 급속도로 발전하면서 이미지 생성 능력도 놀라워졌죠. 그런데 인물 사진만큼은 여전히 어딘가 어색하고 부자연스럽다는 느낌을 지울 수 없습니다. 분명히 &ldquo;사실적인 인물 사진을 생성해줘&rdquo;라고 요청했는데, 결과물은 플라스틱 같은 피부나 인형 같은 눈동자를 가진 이미지일 때가 많아요. 이를 우리는 흔히 <b>&ldquo;AI 티&rdquo;</b>가 난다고 표현하곤 합니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">이런 이미지를 SNS 피드나 블로그 포스팅 섬네일에 사용하면 어떤 일이 벌어질까요? 유저들은 한눈에 &ldquo;아, 이건 그냥 대충 만든 이미지네&rdquo;라는 인상을 받게 되고, 이는 곧 서비스나 콘텐츠에 대한 신뢰도 하락으로 이어지기 쉽습니다. 저도 이런 경험이 참 많았어요. 완벽한 것 같으면서도 어딘가 부족한, 이른바 <b>&lsquo;불쾌한 골짜기(Uncanny Valley)&rsquo;</b> 현상이 바로 그 원인입니다.</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>  제미나이와 함께 찾아낸 극사실 이미지의 비밀</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">그렇다면 어떻게 해야 AI 티 없이 실제 사진처럼 보이는 극사실적인 인물 이미지를 만들 수 있을까요? 저는 이 질문을 제미나이(Gemini)에게 던져서 핵심 인사이트를 얻었습니다. 그리고 그 인사이트를 바탕으로 'PURE GLOW'라는 AI 인물 이미지 생성 앱을 직접 만들어봤어요. 제미나이의 답변은 정말 체계적이고 구체적이었습니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">제미나이는 AI 이미지가 부자연스러운 주된 이유로 너무 <b>매끈한 피부, 완벽한 대칭, 과한 색감</b> 등을 꼽았으며, 이를 해결하기 위한 네 가지 핵심 원칙을 제시했습니다. 바로 <b>구체적인 시간대의 빛 지정, 실제 렌즈 스펙 활용, 피부의 불완전함 구현, 살아있는 움직임 담기</b>입니다. 또한, 앱 개발 시 필요한 조명 설정, 카메라 설정, 피부 질감 옵션, 포즈 라이브러리, 필름 스톡 같은 기능들도 상세하게 제안했죠. 이제 이 인사이트들을 바탕으로 AI 이미지의 10가지 흔한 실수와 고급 테크닉을 자세히 살펴보겠습니다.</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>  AI 티를 내는 10가지 흔한 실수와 개선 방안</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">먼저, 왜 AI 이미지가 이상해 보이는지부터 정확히 아는 것이 중요합니다. 제미나이가 알려준 10가지 결정적인 실수를 파악하고 피하면, 여러분의 AI 이미지 퀄리티가 확연히 달라질 거예요. 제가 직접 이미지를 생성하며 겪었던 문제점들이기도 합니다.</p>\n<ol style=\"list-style-type: decimal; margin-left: 20px; margin-bottom: 20px; color: #3c4043;\" data-ke-list-type=\"decimal\">\n<li style=\"margin-bottom: 10px;\"><b>너무 매끈한 피부:</b> AI는 기본적으로 완벽한 피부를 만들려 하지만, 실제 사람은 모공, 미세한 잡티, 솜털이 있어야 자연스럽습니다. <b>피부의 불완전함을 표현하는 디테일이 핵심이에요.</b></li>\n<li style=\"margin-bottom: 10px;\"><b>완벽한 좌우 대칭:</b> 사람의 얼굴은 완벽하게 대칭적이지 않습니다. 한쪽 눈썹이 살짝 높거나 입꼬리가 약간 다른 등 미묘한 비대칭이 오히려 자연스러움을 더하죠.</li>\n<li style=\"margin-bottom: 10px;\"><b>비현실적인 눈동자:</b> 너무 크거나 과하게 반짝이는 눈동자는 인형처럼 보입니다. 실제 눈은 주변 환경의 빛을 자연스럽게 반사하는 그 느낌이 중요해요.</li>\n<li style=\"margin-bottom: 10px;\"><b>배경이나 간판의 깨진 글자:</b> AI가 생성한 텍스트는 종종 이상하게 깨져 나옵니다. 배경에 간판이나 로고가 있다면, 아예 텍스트를 제외하는 것이 좋습니다.</li>\n<li style=\"margin-bottom: 10px;\"><b>머리카락이 한 덩어리로 뭉쳐있음:</b> 플라스틱 가발처럼 보이는 머리카락은 부자연스럽습니다. '바람에 자연스럽게 흩날리는 머리카락'처럼 디테일을 추가해야 합니다.</li>\n<li style=\"margin-bottom: 10px;\"><b>배경이 너무 비현실적:</b> 지나치게 깔끔하거나 과도하게 흐려진 배경은 오히려 어색해요. 실제 사진처럼 자연스러운 디테일과 보케(아웃포커싱)가 필요합니다.</li>\n<li style=\"margin-bottom: 10px;\"><b>HDR처럼 색감이 과도함:</b> 너무 선명하고 강렬한 색감은 게임 그래픽처럼 보일 수 있습니다. 좀 더 미묘하고 자연스러운 색감이 실제 사진에 가깝습니다.</li>\n<li style=\"margin-bottom: 10px;\"><b>디테일이 지나치게 선명함:</b> 모든 부분이 똑같이 선명하면 부자연스럽습니다. 실제 사진은 초점 맞은 부분만 선명하고 나머지는 살짝 흐릿하게 표현됩니다.</li>\n<li style=\"margin-bottom: 10px;\"><b>포즈가 너무 과도함:</b> 억지스러운 모델 화보 같은 포즈보다는 자연스러운 일상적 순간을 포착한 느낌이 훨씬 리얼합니다.</li>\n<li style=\"margin-bottom: 10px;\"><b>조명이 너무 완벽함:</b> 스튜디오 조명처럼 그림자가 전혀 없고 모든 것이 균일하게 밝으면 인공적으로 보입니다. 실제 환경은 빛과 그림자의 자연스러운 대비가 있어야 합니다.</li>\n</ol>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">이 10가지 실수만 제대로 피해도 AI 이미지의 '티'는 정말 확연히 줄어들 것입니다. 저도 이 점들을 유의하면서 PURE GLOW 앱을 개발하고 사용하고 있습니다. 물론 처음부터 완벽하게 구현하기는 어렵지만, 꾸준히 시도하면 분명 좋은 결과를 얻을 수 있을 거예요.</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"download.jpg\" data-origin-width=\"1408\" data-origin-height=\"768\"><span data-url=\"https://blog.kakaocdn.net/dn/czpGkn/dJMcabbVpgz/GvGmjnmWDLtDTR7lOtPw6k/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/czpGkn/dJMcabbVpgz/GvGmjnmWDLtDTR7lOtPw6k/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/czpGkn/dJMcabbVpgz/GvGmjnmWDLtDTR7lOtPw6k/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FczpGkn%2FdJMcabbVpgz%2FGvGmjnmWDLtDTR7lOtPw6k%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"과도하게 매끄럽고 인형 같은 피부, 부자연스러운 눈동자를 가진 AI 생성 여성 인물 사진. 전반적으로 부자연스럽고 인공적인 느낌을 준다.\" loading=\"lazy\" width=\"1408\" height=\"768\" data-filename=\"download.jpg\" data-origin-width=\"1408\" data-origin-height=\"768\"/></span></figure>\n\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>  극사실 표현을 위한 고급 테크닉: PURE GLOW의 핵심 원리</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">앞서 언급된 실수들을 피하는 것을 넘어, 한 차원 높은 리얼리티를 구현하기 위한 제미나이의 고급 조언들이 있었습니다. PURE GLOW 앱은 이러한 기법들을 옵션으로 제공하여 사용자가 쉽게 적용할 수 있도록 했습니다.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; padding-left: 10px; border-left: 3px solid #1a73e8;\" data-ke-size=\"size23\"><b>1. 필름 룩: 디지털 느낌을 없애고 아날로그 감성 더하기</b></h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">디지털 이미지는 때로 너무 선명하고 색감이 강렬해서 인공적으로 보입니다. 반면 실제 필름 카메라로 찍은 사진은 색이 좀 더 부드럽고 미묘하며, 특유의 감성을 담고 있죠. PURE GLOW는 대표적인 필름 스톡을 모방한 필터를 제공하여 이러한 아날로그적인 느낌을 연출할 수 있습니다.</p>\n<ul style=\"list-style-type: disc; margin-left: 20px; margin-bottom: 20px; color: #3c4043;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 8px;\"><b>코닥 포트라 400:</b> 따뜻하고 부드러운 색감으로, 특히 피부 톤을 아름답게 표현하는 데 탁월합니다.</li>\n<li style=\"margin-bottom: 8px;\"><b>후지 400H:</b> 살짝 차갑고 청량한 느낌으로, 현대적인 감성을 더할 때 유용합니다.</li>\n<li style=\"margin-bottom: 8px;\"><b>코닥 골드 200:</b> 따뜻하고 빈티지한 느낌을 선사하여 아련한 분위기를 연출할 수 있습니다.</li>\n</ul>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">또한, 필름 사진 특유의 미세한 입자감, 즉 <b>그레인(Grain)</b> 효과는 오히려 이미지에 자연스럽고 따뜻한 감성을 불어넣어줍니다. PURE GLOW에서는 이러한 디테일한 옵션까지 조절할 수 있습니다.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; padding-left: 10px; border-left: 3px solid #1a73e8;\" data-ke-size=\"size23\"><b>2. 혼합광: 실제 환경의 빛을 재현하라</b></h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">실제 실내 환경을 생각해보면, 단 하나의 광원만 있는 경우는 거의 없습니다. 창문으로 들어오는 차가운 자연광, 천장의 따뜻한 램프 조명 등 여러 광원이 섞여있는 것이 현실이죠. PURE GLOW의 <b>'아침 창가광'</b> 같은 조명 설정은 이처럼 복합적인 혼합광 효과를 재현하여 이미지에 극도의 사실감을 부여합니다.</p>\n<div style=\"background-color: #e8f4fd; border-left: 4px solid #1a73e8; padding: 15px; margin: 20px 0; border-radius: 0 8px 8px 0;\">  <b>팁:</b> 자연스러운 혼합광은 인물의 입체감을 살리고 그림자를 부드럽게 만들어, AI 이미지가 가진 평면적인 느낌을 효과적으로 줄여줍니다.</div>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; padding-left: 10px; border-left: 3px solid #1a73e8;\" data-ke-size=\"size23\"><b>3. 재질감: 고급스러움과 현실감의 결정적 요소</b></h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">의상 설명을 할 때 단순히 &ldquo;예쁜 원피스&rdquo;라고 하는 대신, <b>&ldquo;실크 소재의 은은하게 빛나는 원피스&rdquo;</b>처럼 재질을 구체적으로 지정하는 것이 중요합니다. 재질마다 빛을 받았을 때 반사되는 방식이 다르기 때문이에요. 실크는 은은한 광택을, 린넨은 거친 반사를, 캐시미어는 부드러운 빛 흡수를 보여주죠. PURE GLOW 앱에서는 이러한 재질감 옵션을 세밀하게 조절하여 의상과 배경의 현실감을 높일 수 있습니다.</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"download.jpg\" data-origin-width=\"1408\" data-origin-height=\"768\"><span data-url=\"https://blog.kakaocdn.net/dn/bi9GKZ/dJMcadtZtAs/2NGKBlMbb8YNNj8h8q41I1/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/bi9GKZ/dJMcadtZtAs/2NGKBlMbb8YNNj8h8q41I1/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/bi9GKZ/dJMcadtZtAs/2NGKBlMbb8YNNj8h8q41I1/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbi9GKZ%2FdJMcadtZtAs%2F2NGKBlMbb8YNNj8h8q41I1%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"주름, 기미, 솜털이 자연스럽게 드러나는 노인의 사실적인 인물 사진. 나이 든 피부의 질감과 따뜻한 자연광이 실제와 같은 느낌을 강조한다.\" loading=\"lazy\" width=\"1408\" height=\"768\" data-filename=\"download.jpg\" data-origin-width=\"1408\" data-origin-height=\"768\"/></span></figure>\n\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>  PURE GLOW 앱으로 만드는 '진짜' 같은 AI 인물 이미지</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">제가 제미나이의 인사이트를 바탕으로 개발한 PURE GLOW 앱은 위에 언급된 모든 원칙과 고급 테크닉들을 손쉽게 적용할 수 있도록 설계되었습니다. 2026년 현재, 이 앱은 나노 바나나 프로(Nano Banana Pro) API를 활용하여 구체적인 설정값만 입력하면 누구든지 클릭 몇 번으로 극사실적인 AI 인물 이미지를 만들 수 있도록 돕습니다.</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"1670\" data-origin-height=\"989\"><span data-url=\"https://blog.kakaocdn.net/dn/bgIKSk/dJMcab33Lbi/3jkzYaqiDwMYAJBvwCuBV1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bgIKSk/dJMcab33Lbi/3jkzYaqiDwMYAJBvwCuBV1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bgIKSk/dJMcab33Lbi/3jkzYaqiDwMYAJBvwCuBV1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbgIKSk%2FdJMcab33Lbi%2F3jkzYaqiDwMYAJBvwCuBV1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"'PURE GLOW' 앱의 사용자 인터페이스 화면으로, 조명, 카메라, 피부 질감 등 다양한 설정 옵션과 함께 자연스러운 AI 인물 사진 미리보기가 보인다.\" loading=\"lazy\" width=\"1670\" height=\"989\" data-origin-width=\"1670\" data-origin-height=\"989\"/></span></figure>\n\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; padding-left: 10px; border-left: 3px solid #1a73e8;\" data-ke-size=\"size23\"><b>PURE GLOW의 기본 기능</b></h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">PURE GLOW의 기본 설정은 다음과 같이 구성되어 있습니다:</p>\n<table style=\"width: 100%; border-collapse: collapse; margin-bottom: 20px; text-align: left; color: #3c4043;\" data-ke-align=\"alignLeft\">\n<thead>\n<tr style=\"background-color: #e8eaed;\">\n<th style=\"border: 1px solid #dadce0; padding: 10px; font-weight: bold; color: #3c4043;\">카테고리</th>\n<th style=\"border: 1px solid #dadce0; padding: 10px; font-weight: bold; color: #3c4043;\">주요 설정 옵션</th>\n</tr>\n</thead>\n<tbody>\n<tr style=\"background-color: white;\">\n<td style=\"border: 1px solid #dadce0; padding: 10px; color: #3c4043;\">외형 설정</td>\n<td style=\"border: 1px solid #dadce0; padding: 10px; color: #3c4043;\">성별, 국적, 나이, 헤어스타일</td>\n</tr>\n<tr style=\"background-color: #f1f3f4;\">\n<td style=\"border: 1px solid #dadce0; padding: 10px; color: #3c4043;\">인물 디테일</td>\n<td style=\"border: 1px solid #dadce0; padding: 10px; color: #3c4043;\">표정, 포즈, 의상, 장소</td>\n</tr>\n<tr style=\"background-color: white;\">\n<td style=\"border: 1px solid #dadce0; padding: 10px; color: #3c4043;\">사실감 강화</td>\n<td style=\"border: 1px solid #dadce0; padding: 10px; color: #3c4043;\">조명 및 카메라 설정, 피부 질감 (모공, 잡티, 솜털)</td>\n</tr>\n<tr style=\"background-color: #f1f3f4;\">\n<td style=\"border: 1px solid #dadce0; padding: 10px; color: #3c4043;\">필터 및 기타</td>\n<td style=\"border: 1px solid #dadce0; padding: 10px; color: #3c4043;\">필름 필터(코닥 포트라 400 등), 랜덤 생성 기능</td>\n</tr>\n</tbody>\n</table>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">예를 들어, 20대 한국 여성에 오버사이즈 화이트 셔츠, 도서관 장소, 은은한 미소와 벽에 기대는 포즈, 아침 창가광 조명, 폰 카메라 후면 장비, 코닥 포트라 400 필름 필터, 그리고 <b>초정밀 모공, 사실적 잡티, 솜털까지 적용한 피부 질감</b>을 선택하면, 실제로 촬영한 것과 같은 높은 퀄리티의 이미지를 얻을 수 있습니다.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; padding-left: 10px; border-left: 3px solid #1a73e8;\" data-ke-size=\"size23\"><b>PURE GLOW Pro: AI 인플루언서 기능으로 확장된 활용성</b></h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">PURE GLOW 프로 버전은 단순한 이미지 생성을 넘어, 가상 인플루언서의 하루 일과나 특정 이벤트를 AI가 자동으로 생성하고 여기에 광고까지 포함할 수 있는 획기적인 기능을 제공합니다. 이 기능은 특히 소셜 미디어 콘텐츠 제작자나 마케터에게 유용할 것입니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">인물 설정을 마친 후 '랜덤 일과'를 클릭하면 시간대별로 다양한 이벤트 목록이 생성되며, 이를 기반으로 레퍼런스 이미지를 등록하거나 바로 이미지를 생성하여 가상 인플루언서의 특별한 하루를 연출할 수 있습니다. 또한, 사용자가 직접 'AI 플래닝 입력 창'에 간단한 하루 일정을 입력하면, AI가 이를 바탕으로 맞춤형 이벤트 목록을 생성해주기도 합니다.</p>\n<div style=\"background-color: #e8f4fd; border-left: 4px solid #1a73e8; padding: 15px; margin: 20px 0; border-radius: 0 8px 8px 0;\">  <b>주목:</b> 광고 모드에서는 특정 광고 유형을 선택하고 상품 사진과 제품명을 입력하면, 선택한 이벤트와 상품이 자연스럽게 적용된 이미지를 생성할 수 있습니다. 이는 제품 홍보 콘텐츠 제작에 혁신적인 변화를 가져올 거예요.</div>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">생성된 이미지와 함께 인스타그램에 바로 올릴 수 있는 적절한 캡션과 해시태그까지 자동으로 제공되니, 콘텐츠 제작 과정이 훨씬 간편해집니다. 만약 캡션이 마음에 들지 않는다면 '다시 생성' 버튼을 클릭하여 새로운 캡션을 받아볼 수도 있습니다. 제가 직접 해보니 정말 편리하고 아이디어를 얻기에도 좋았습니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">&nbsp;</p>\n<figure id=\"og_1769260181702\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"PureGlow: AI Influencer Studio\" data-og-description=\"\" data-og-host=\"pure-glow-basic.vercel.app\" data-og-source-url=\"https://pure-glow-basic.vercel.app/\" data-og-url=\"https://pure-glow-basic.vercel.app/\" data-og-image=\"\"><a href=\"https://pure-glow-basic.vercel.app/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://pure-glow-basic.vercel.app/\">\n<div class=\"og-image\" style=\"background-image: url();\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">PureGlow: AI Influencer Studio</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">&nbsp;</p>\n<p class=\"og-host\" data-ke-size=\"size16\">pure-glow-basic.vercel.app</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>  핵심 요약: PURE GLOW가 선사하는 AI 이미지의 새 지평</b></h2>\n<div style=\"background-color: #f8f9fa; border: 1px solid #dadce0; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); padding: 25px; margin-bottom: 30px;\">\n<div style=\"font-size: 26px; color: #1a73e8; font-weight: bold; margin-bottom: 15px; padding-bottom: 10px; border-bottom: 2px solid #1a73e8;\">  핵심 요약</div>\n<ul style=\"list-style: none; padding: 0; margin-bottom: 20px;\" data-ke-list-type=\"disc\">\n<li style=\"font-size: 17px; margin-bottom: 10px;\"><b>1. 'AI 티' 제거:</b> PURE GLOW는 매끈한 피부, 완벽 대칭 등 AI 이미지의 흔한 실수를 보완하여 실제 사진 같은 리얼리티를 구현합니다.</li>\n<li style=\"font-size: 17px; margin-bottom: 10px;\"><b>2. 4가지 핵심 원칙:</b> 조명, 카메라, 피부 질감, 움직임의 디테일한 설정으로 극사실적인 인물 표현을 가능하게 합니다.</li>\n<li style=\"font-size: 17px; margin-bottom: 10px;\"><b>3. 고급 테크닉 적용:</b> 필름 룩, 혼합광, 재질감 표현 등 전문적인 사진 기법을 앱 내에서 쉽게 사용할 수 있습니다.</li>\n<li style=\"font-size: 17px; margin-bottom: 10px;\"><b>4. AI 인플루언서 기능:</b> 프로 버전은 가상 인플루언서의 하루를 생성하고 광고 콘텐츠까지 제작할 수 있어 콘텐츠 마케팅에 최적화되어 있습니다.</li>\n</ul>\n<div style=\"font-size: 14px; color: #5f6368; border-top: 1px dashed #dadce0; padding-top: 15px;\">이러한 기능들을 통해 PURE GLOW는 단순한 이미지 생성을 넘어, 새로운 형태의 디지털 콘텐츠를 창조하는 강력한 도구가 될 것입니다.</div>\n</div>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>❓ 자주 묻는 질문 (FAQ)</b></h2>\n<div style=\"margin-bottom: 15px;\">\n<p style=\"margin-bottom: 5px; font-weight: bold; color: #3c4043;\" data-ke-size=\"size16\">Q1: PURE GLOW 앱은 어떤 AI 엔진을 사용하나요?</p>\n<p style=\"margin-bottom: 20px; color: #3c4043;\" data-ke-size=\"size16\">A1: PURE GLOW 앱은 나노 바나나 프로(Nano Banana Pro) API를 활용하여 이미지를 생성합니다. 이를 통해 사용자들은 고품질의 사실적인 이미지를 얻을 수 있습니다.</p>\n</div>\n<div style=\"margin-bottom: 15px;\">\n<p style=\"margin-bottom: 5px; font-weight: bold; color: #3c4043;\" data-ke-size=\"size16\">Q2: PURE GLOW로 만든 이미지는 상업적으로 이용해도 되나요?</p>\n<p style=\"margin-bottom: 20px; color: #3c4043;\" data-ke-size=\"size16\">A2: 네, 상업적 이용이 가능하지만, AI 인플루언서 기능으로 생성된 이미지는 <b>가상의 인물</b>이라는 점을 명확히 고지해야 합니다. SNS 업로드 시 '#AI생성이미지', '#가상인물'과 같은 해시태그를 반드시 포함해 주세요.</p>\n</div>\n<div style=\"margin-bottom: 15px;\">\n<p style=\"margin-bottom: 5px; font-weight: bold; color: #3c4043;\" data-ke-size=\"size16\">Q3: PURE GLOW 베이직 버전과 프로 버전의 차이점은 무엇인가요?</p>\n<p style=\"margin-bottom: 20px; color: #3c4043;\" data-ke-size=\"size16\">A3: 베이직 버전은 사실적인 인물 이미지 생성의 기본 기능을 제공하며, 프로 버전은 여기에 AI 인플루언서 기능(하루 일과 및 광고 콘텐츠 자동 생성)이 추가되어 더욱 폭넓은 활용이 가능합니다.</p>\n</div>\n<div style=\"margin-bottom: 15px;\">\n<p style=\"margin-bottom: 5px; font-weight: bold; color: #3c4043;\" data-ke-size=\"size16\">Q4: 이미지 생성 시 텍스트나 로고가 깨져서 나오는 문제가 발생합니다.</p>\n<p style=\"margin-bottom: 20px; color: #3c4043;\" data-ke-size=\"size16\">A4: AI 이미지 생성 시 텍스트는 아직 완벽하지 않은 경우가 많습니다. 배경에 간판이나 포스터가 포함될 경우, 프롬프트에 '텍스트, 간판, 로고 제외'와 같은 지시어를 추가하여 텍스트 깨짐 현상을 방지하는 것이 좋습니다.</p>\n</div>\n<script type=\"application/ld+json\">\n  {\n    \"@context\": \"https://schema.org\",\n    \"@type\": \"FAQPage\",\n    \"mainEntity\": [\n      {\n        \"@type\": \"Question\",\n        \"name\": \"PURE GLOW 앱은 어떤 AI 엔진을 사용하나요?\",\n        \"acceptedAnswer\": {\n          \"@type\": \"Answer\",\n          \"text\": \"PURE GLOW 앱은 나노 바나나 프로(Nano Banana Pro) API를 활용하여 이미지를 생성합니다. 이를 통해 사용자들은 고품질의 사실적인 이미지를 얻을 수 있습니다.\"\n        }\n      },\n      {\n        \"@type\": \"Question\",\n        \"name\": \"PURE GLOW로 만든 이미지는 상업적으로 이용해도 되나요?\",\n        \"acceptedAnswer\": {\n          \"@type\": \"Answer\",\n          \"text\": \"네, 상업적 이용이 가능하지만, AI 인플루언서 기능으로 생성된 이미지는 가상의 인물이라는 점을 명확히 고지해야 합니다. SNS 업로드 시 '#AI생성이미지', '#가상인물'과 같은 해시태그를 반드시 포함해 주세요.\"\n        }\n      },\n      {\n        \"@type\": \"Question\",\n        \"name\": \"PURE GLOW 베이직 버전과 프로 버전의 차이점은 무엇인가요?\",\n        \"acceptedAnswer\": {\n          \"@type\": \"Answer\",\n          \"text\": \"베이직 버전은 사실적인 인물 이미지 생성의 기본 기능을 제공하며, 프로 버전은 여기에 AI 인플루언서 기능(하루 일과 및 광고 콘텐츠 자동 생성)이 추가되어 더욱 폭넓은 활용이 가능합니다.\"\n        }\n      },\n      {\n        \"@type\": \"Question\",\n        \"name\": \"이미지 생성 시 텍스트나 로고가 깨져서 나오는 문제가 발생합니다.\",\n        \"acceptedAnswer\": {\n          \"@type\": \"Answer\",\n          \"text\": \"AI 이미지 생성 시 텍스트는 아직 완벽하지 않은 경우가 많습니다. 배경에 간판이나 포스터가 포함될 경우, 프롬프트에 '텍스트, 간판, 로고 제외'와 같은 지시어를 추가하여 텍스트 깨짐 현상을 방지하는 것이 좋습니다.\"\n        }\n      }\n    ]\n  }\n  </script>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>마무리하며: AI 이미지의 새로운 기준을 제시하다</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">지금까지 PURE GLOW 앱을 활용하여 모공, 잡티, 솜털까지 표현하는 극사실적인 AI 인물 이미지를 만드는 방법을 자세히 살펴보았습니다. AI 기술이 아직 완벽하지 않더라도, 우리가 디테일한 부분에 신경 쓰고 실제 사진의 특성을 이해한다면 충분히 '진짜보다 더 진짜 같은' 결과물을 만들 수 있다는 것을 알게 되셨으리라 생각합니다. 2026년에 들어서면서 AI 이미지 생성 기술은 더욱 빠르게 발전하고 있으며, PURE GLOW와 같은 도구들은 이러한 발전을 한층 더 가속화할 것이라고 믿어요.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\"><b>중요한 주의사항을 다시 한번 강조합니다:</b> AI 인플루언서 기능으로 생성된 이미지는 <b>가상의 인물</b>입니다. SNS에 올리거나 포트폴리오에 사용할 때는 반드시 <b>\"#AI생성이미지\", \"#가상인물\"</b>과 같은 해시태그를 달아 실제 사람인 것처럼 오인하게 하거나 악의적으로 사용해서는 절대 안 됩니다. 이는 윤리적인 AI 사용에 있어 매우 중요한 부분이에요.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">PURE GLOW는 베이직 버전과 AI 인플루언서 기능까지 포함된 프로 버전으로 나뉘니, 여러분의 프로젝트나 필요에 맞는 버전을 선택하여 활용해보시길 바랍니다. 이 글이 여러분의 AI 이미지 생성 작업에 큰 도움이 되었기를 진심으로 바랍니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<figure data-ke-type=\"video\" data-ke-style=\"alignCenter\" data-video-host=\"youtube\" data-video-url=\"https://www.youtube.com/watch?v=4u1MrSxVYZ0\" data-video-thumbnail=\"https://scrap.kakaocdn.net/dn/PtDCd/dJMb9dHimQ7/hz7yVlea1XkdpJGnUCuXh1/img.jpg?width=1280&amp;height=720&amp;face=106_208_1144_576,https://scrap.kakaocdn.net/dn/blEsrm/dJMb81GRuAd/ciKYhQ8iV3U3903v5MtIy0/img.jpg?width=1280&amp;height=720&amp;face=106_208_1144_576\" data-video-width=\"860\" data-video-height=\"484\" data-video-origin-width=\"860\" data-video-origin-height=\"484\" data-ke-mobilestyle=\"widthContent\" data-video-title=\"이게 진짜 AI라고?&quot; 플라스틱 피부 탈출! 극사실 인물 사진 만드는 비법 (ft. 제미나이)\" data-original-url=\"\"><iframe src=\"https://www.youtube.com/embed/4u1MrSxVYZ0\" width=\"860\" height=\"484\" frameborder=\"\" allowfullscreen=\"true\"></iframe>\n<figcaption style=\"display: none;\"></figcaption>\n</figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n</div>",
        "contentSnippet": "혹시 AI로 만든 인물 이미지가 너무 부자연스러워서 실망한 적 있으신가요? 오늘은 플라스틱 같은 피부, 인형 같은 눈동자 등 이른바 ‘AI 티’ 나는 이미지를 피하고, 모공, 잡티, 솜털까지 섬세하게 표현하는 극사실적인 AI 인물 사진을 만드는 방법을 알려드립니다. 혁신적인 PURE GLOW 앱을 통해 조명, 카메라, 피부 질감, 움직임의 4가지 핵심 원칙을 적용하여 실제 사진처럼 보이는 이미지를 만드는 비법을 최신 정보로 공개합니다.\n\n\n✨ AI 인물 이미지, 왜 '진짜' 같지 않을까?\nAI 기술이 급속도로 발전하면서 이미지 생성 능력도 놀라워졌죠. 그런데 인물 사진만큼은 여전히 어딘가 어색하고 부자연스럽다는 느낌을 지울 수 없습니다. 분명히 “사실적인 인물 사진을 생성해줘”라고 요청했는데, 결과물은 플라스틱 같은 피부나 인형 같은 눈동자를 가진 이미지일 때가 많아요. 이를 우리는 흔히 “AI 티”가 난다고 표현하곤 합니다.\n이런 이미지를 SNS 피드나 블로그 포스팅 섬네일에 사용하면 어떤 일이 벌어질까요? 유저들은 한눈에 “아, 이건 그냥 대충 만든 이미지네”라는 인상을 받게 되고, 이는 곧 서비스나 콘텐츠에 대한 신뢰도 하락으로 이어지기 쉽습니다. 저도 이런 경험이 참 많았어요. 완벽한 것 같으면서도 어딘가 부족한, 이른바 ‘불쾌한 골짜기(Uncanny Valley)’ 현상이 바로 그 원인입니다.\n  제미나이와 함께 찾아낸 극사실 이미지의 비밀\n그렇다면 어떻게 해야 AI 티 없이 실제 사진처럼 보이는 극사실적인 인물 이미지를 만들 수 있을까요? 저는 이 질문을 제미나이(Gemini)에게 던져서 핵심 인사이트를 얻었습니다. 그리고 그 인사이트를 바탕으로 'PURE GLOW'라는 AI 인물 이미지 생성 앱을 직접 만들어봤어요. 제미나이의 답변은 정말 체계적이고 구체적이었습니다.\n제미나이는 AI 이미지가 부자연스러운 주된 이유로 너무 매끈한 피부, 완벽한 대칭, 과한 색감 등을 꼽았으며, 이를 해결하기 위한 네 가지 핵심 원칙을 제시했습니다. 바로 구체적인 시간대의 빛 지정, 실제 렌즈 스펙 활용, 피부의 불완전함 구현, 살아있는 움직임 담기입니다. 또한, 앱 개발 시 필요한 조명 설정, 카메라 설정, 피부 질감 옵션, 포즈 라이브러리, 필름 스톡 같은 기능들도 상세하게 제안했죠. 이제 이 인사이트들을 바탕으로 AI 이미지의 10가지 흔한 실수와 고급 테크닉을 자세히 살펴보겠습니다.\n  AI 티를 내는 10가지 흔한 실수와 개선 방안\n먼저, 왜 AI 이미지가 이상해 보이는지부터 정확히 아는 것이 중요합니다. 제미나이가 알려준 10가지 결정적인 실수를 파악하고 피하면, 여러분의 AI 이미지 퀄리티가 확연히 달라질 거예요. 제가 직접 이미지를 생성하며 겪었던 문제점들이기도 합니다.\n너무 매끈한 피부: AI는 기본적으로 완벽한 피부를 만들려 하지만, 실제 사람은 모공, 미세한 잡티, 솜털이 있어야 자연스럽습니다. 피부의 불완전함을 표현하는 디테일이 핵심이에요.\n완벽한 좌우 대칭: 사람의 얼굴은 완벽하게 대칭적이지 않습니다. 한쪽 눈썹이 살짝 높거나 입꼬리가 약간 다른 등 미묘한 비대칭이 오히려 자연스러움을 더하죠.\n비현실적인 눈동자: 너무 크거나 과하게 반짝이는 눈동자는 인형처럼 보입니다. 실제 눈은 주변 환경의 빛을 자연스럽게 반사하는 그 느낌이 중요해요.\n배경이나 간판의 깨진 글자: AI가 생성한 텍스트는 종종 이상하게 깨져 나옵니다. 배경에 간판이나 로고가 있다면, 아예 텍스트를 제외하는 것이 좋습니다.\n머리카락이 한 덩어리로 뭉쳐있음: 플라스틱 가발처럼 보이는 머리카락은 부자연스럽습니다. '바람에 자연스럽게 흩날리는 머리카락'처럼 디테일을 추가해야 합니다.\n배경이 너무 비현실적: 지나치게 깔끔하거나 과도하게 흐려진 배경은 오히려 어색해요. 실제 사진처럼 자연스러운 디테일과 보케(아웃포커싱)가 필요합니다.\nHDR처럼 색감이 과도함: 너무 선명하고 강렬한 색감은 게임 그래픽처럼 보일 수 있습니다. 좀 더 미묘하고 자연스러운 색감이 실제 사진에 가깝습니다.\n디테일이 지나치게 선명함: 모든 부분이 똑같이 선명하면 부자연스럽습니다. 실제 사진은 초점 맞은 부분만 선명하고 나머지는 살짝 흐릿하게 표현됩니다.\n포즈가 너무 과도함: 억지스러운 모델 화보 같은 포즈보다는 자연스러운 일상적 순간을 포착한 느낌이 훨씬 리얼합니다.\n조명이 너무 완벽함: 스튜디오 조명처럼 그림자가 전혀 없고 모든 것이 균일하게 밝으면 인공적으로 보입니다. 실제 환경은 빛과 그림자의 자연스러운 대비가 있어야 합니다.\n이 10가지 실수만 제대로 피해도 AI 이미지의 '티'는 정말 확연히 줄어들 것입니다. 저도 이 점들을 유의하면서 PURE GLOW 앱을 개발하고 사용하고 있습니다. 물론 처음부터 완벽하게 구현하기는 어렵지만, 꾸준히 시도하면 분명 좋은 결과를 얻을 수 있을 거예요.\n\n\n  극사실 표현을 위한 고급 테크닉: PURE GLOW의 핵심 원리\n앞서 언급된 실수들을 피하는 것을 넘어, 한 차원 높은 리얼리티를 구현하기 위한 제미나이의 고급 조언들이 있었습니다. PURE GLOW 앱은 이러한 기법들을 옵션으로 제공하여 사용자가 쉽게 적용할 수 있도록 했습니다.\n1. 필름 룩: 디지털 느낌을 없애고 아날로그 감성 더하기\n디지털 이미지는 때로 너무 선명하고 색감이 강렬해서 인공적으로 보입니다. 반면 실제 필름 카메라로 찍은 사진은 색이 좀 더 부드럽고 미묘하며, 특유의 감성을 담고 있죠. PURE GLOW는 대표적인 필름 스톡을 모방한 필터를 제공하여 이러한 아날로그적인 느낌을 연출할 수 있습니다.\n코닥 포트라 400: 따뜻하고 부드러운 색감으로, 특히 피부 톤을 아름답게 표현하는 데 탁월합니다.\n후지 400H: 살짝 차갑고 청량한 느낌으로, 현대적인 감성을 더할 때 유용합니다.\n코닥 골드 200: 따뜻하고 빈티지한 느낌을 선사하여 아련한 분위기를 연출할 수 있습니다.\n또한, 필름 사진 특유의 미세한 입자감, 즉 그레인(Grain) 효과는 오히려 이미지에 자연스럽고 따뜻한 감성을 불어넣어줍니다. PURE GLOW에서는 이러한 디테일한 옵션까지 조절할 수 있습니다.\n2. 혼합광: 실제 환경의 빛을 재현하라\n실제 실내 환경을 생각해보면, 단 하나의 광원만 있는 경우는 거의 없습니다. 창문으로 들어오는 차가운 자연광, 천장의 따뜻한 램프 조명 등 여러 광원이 섞여있는 것이 현실이죠. PURE GLOW의 '아침 창가광' 같은 조명 설정은 이처럼 복합적인 혼합광 효과를 재현하여 이미지에 극도의 사실감을 부여합니다.\n  팁: 자연스러운 혼합광은 인물의 입체감을 살리고 그림자를 부드럽게 만들어, AI 이미지가 가진 평면적인 느낌을 효과적으로 줄여줍니다.\n3. 재질감: 고급스러움과 현실감의 결정적 요소\n의상 설명을 할 때 단순히 “예쁜 원피스”라고 하는 대신, “실크 소재의 은은하게 빛나는 원피스”처럼 재질을 구체적으로 지정하는 것이 중요합니다. 재질마다 빛을 받았을 때 반사되는 방식이 다르기 때문이에요. 실크는 은은한 광택을, 린넨은 거친 반사를, 캐시미어는 부드러운 빛 흡수를 보여주죠. PURE GLOW 앱에서는 이러한 재질감 옵션을 세밀하게 조절하여 의상과 배경의 현실감을 높일 수 있습니다.\n\n\n  PURE GLOW 앱으로 만드는 '진짜' 같은 AI 인물 이미지\n제가 제미나이의 인사이트를 바탕으로 개발한 PURE GLOW 앱은 위에 언급된 모든 원칙과 고급 테크닉들을 손쉽게 적용할 수 있도록 설계되었습니다. 2026년 현재, 이 앱은 나노 바나나 프로(Nano Banana Pro) API를 활용하여 구체적인 설정값만 입력하면 누구든지 클릭 몇 번으로 극사실적인 AI 인물 이미지를 만들 수 있도록 돕습니다.\n\n\nPURE GLOW의 기본 기능\nPURE GLOW의 기본 설정은 다음과 같이 구성되어 있습니다:\n카테고리\n주요 설정 옵션\n\n\n\n\n외형 설정\n성별, 국적, 나이, 헤어스타일\n\n\n인물 디테일\n표정, 포즈, 의상, 장소\n\n\n사실감 강화\n조명 및 카메라 설정, 피부 질감 (모공, 잡티, 솜털)\n\n\n필터 및 기타\n필름 필터(코닥 포트라 400 등), 랜덤 생성 기능\n\n\n\n예를 들어, 20대 한국 여성에 오버사이즈 화이트 셔츠, 도서관 장소, 은은한 미소와 벽에 기대는 포즈, 아침 창가광 조명, 폰 카메라 후면 장비, 코닥 포트라 400 필름 필터, 그리고 초정밀 모공, 사실적 잡티, 솜털까지 적용한 피부 질감을 선택하면, 실제로 촬영한 것과 같은 높은 퀄리티의 이미지를 얻을 수 있습니다.\nPURE GLOW Pro: AI 인플루언서 기능으로 확장된 활용성\nPURE GLOW 프로 버전은 단순한 이미지 생성을 넘어, 가상 인플루언서의 하루 일과나 특정 이벤트를 AI가 자동으로 생성하고 여기에 광고까지 포함할 수 있는 획기적인 기능을 제공합니다. 이 기능은 특히 소셜 미디어 콘텐츠 제작자나 마케터에게 유용할 것입니다.\n인물 설정을 마친 후 '랜덤 일과'를 클릭하면 시간대별로 다양한 이벤트 목록이 생성되며, 이를 기반으로 레퍼런스 이미지를 등록하거나 바로 이미지를 생성하여 가상 인플루언서의 특별한 하루를 연출할 수 있습니다. 또한, 사용자가 직접 'AI 플래닝 입력 창'에 간단한 하루 일정을 입력하면, AI가 이를 바탕으로 맞춤형 이벤트 목록을 생성해주기도 합니다.\n  주목: 광고 모드에서는 특정 광고 유형을 선택하고 상품 사진과 제품명을 입력하면, 선택한 이벤트와 상품이 자연스럽게 적용된 이미지를 생성할 수 있습니다. 이는 제품 홍보 콘텐츠 제작에 혁신적인 변화를 가져올 거예요.\n생성된 이미지와 함께 인스타그램에 바로 올릴 수 있는 적절한 캡션과 해시태그까지 자동으로 제공되니, 콘텐츠 제작 과정이 훨씬 간편해집니다. 만약 캡션이 마음에 들지 않는다면 '다시 생성' 버튼을 클릭하여 새로운 캡션을 받아볼 수도 있습니다. 제가 직접 해보니 정말 편리하고 아이디어를 얻기에도 좋았습니다.\n \n\n \nPureGlow: AI Influencer Studio\n \npure-glow-basic.vercel.app\n\n \n  핵심 요약: PURE GLOW가 선사하는 AI 이미지의 새 지평\n  핵심 요약\n1. 'AI 티' 제거: PURE GLOW는 매끈한 피부, 완벽 대칭 등 AI 이미지의 흔한 실수를 보완하여 실제 사진 같은 리얼리티를 구현합니다.\n2. 4가지 핵심 원칙: 조명, 카메라, 피부 질감, 움직임의 디테일한 설정으로 극사실적인 인물 표현을 가능하게 합니다.\n3. 고급 테크닉 적용: 필름 룩, 혼합광, 재질감 표현 등 전문적인 사진 기법을 앱 내에서 쉽게 사용할 수 있습니다.\n4. AI 인플루언서 기능: 프로 버전은 가상 인플루언서의 하루를 생성하고 광고 콘텐츠까지 제작할 수 있어 콘텐츠 마케팅에 최적화되어 있습니다.\n이러한 기능들을 통해 PURE GLOW는 단순한 이미지 생성을 넘어, 새로운 형태의 디지털 콘텐츠를 창조하는 강력한 도구가 될 것입니다.\n❓ 자주 묻는 질문 (FAQ)\nQ1: PURE GLOW 앱은 어떤 AI 엔진을 사용하나요?\nA1: PURE GLOW 앱은 나노 바나나 프로(Nano Banana Pro) API를 활용하여 이미지를 생성합니다. 이를 통해 사용자들은 고품질의 사실적인 이미지를 얻을 수 있습니다.\nQ2: PURE GLOW로 만든 이미지는 상업적으로 이용해도 되나요?\nA2: 네, 상업적 이용이 가능하지만, AI 인플루언서 기능으로 생성된 이미지는 가상의 인물이라는 점을 명확히 고지해야 합니다. SNS 업로드 시 '#AI생성이미지', '#가상인물'과 같은 해시태그를 반드시 포함해 주세요.\nQ3: PURE GLOW 베이직 버전과 프로 버전의 차이점은 무엇인가요?\nA3: 베이직 버전은 사실적인 인물 이미지 생성의 기본 기능을 제공하며, 프로 버전은 여기에 AI 인플루언서 기능(하루 일과 및 광고 콘텐츠 자동 생성)이 추가되어 더욱 폭넓은 활용이 가능합니다.\nQ4: 이미지 생성 시 텍스트나 로고가 깨져서 나오는 문제가 발생합니다.\nA4: AI 이미지 생성 시 텍스트는 아직 완벽하지 않은 경우가 많습니다. 배경에 간판이나 포스터가 포함될 경우, 프롬프트에 '텍스트, 간판, 로고 제외'와 같은 지시어를 추가하여 텍스트 깨짐 현상을 방지하는 것이 좋습니다.\n마무리하며: AI 이미지의 새로운 기준을 제시하다\n지금까지 PURE GLOW 앱을 활용하여 모공, 잡티, 솜털까지 표현하는 극사실적인 AI 인물 이미지를 만드는 방법을 자세히 살펴보았습니다. AI 기술이 아직 완벽하지 않더라도, 우리가 디테일한 부분에 신경 쓰고 실제 사진의 특성을 이해한다면 충분히 '진짜보다 더 진짜 같은' 결과물을 만들 수 있다는 것을 알게 되셨으리라 생각합니다. 2026년에 들어서면서 AI 이미지 생성 기술은 더욱 빠르게 발전하고 있으며, PURE GLOW와 같은 도구들은 이러한 발전을 한층 더 가속화할 것이라고 믿어요.\n중요한 주의사항을 다시 한번 강조합니다: AI 인플루언서 기능으로 생성된 이미지는 가상의 인물입니다. SNS에 올리거나 포트폴리오에 사용할 때는 반드시 \"#AI생성이미지\", \"#가상인물\"과 같은 해시태그를 달아 실제 사람인 것처럼 오인하게 하거나 악의적으로 사용해서는 절대 안 됩니다. 이는 윤리적인 AI 사용에 있어 매우 중요한 부분이에요.\nPURE GLOW는 베이직 버전과 AI 인플루언서 기능까지 포함된 프로 버전으로 나뉘니, 여러분의 프로젝트나 필요에 맞는 버전을 선택하여 활용해보시길 바랍니다. 이 글이 여러분의 AI 이미지 생성 작업에 큰 도움이 되었기를 진심으로 바랍니다.",
        "guid": "https://muzbox.tistory.com/483704",
        "categories": [
          "AI, 미래기술/AI 챗봇 및 지침 무료 배포",
          "AI 인물 사진",
          "ai 인플루언서",
          "AI 피부 표현",
          "DALL-E 프롬프트",
          "PURE GLOW 앱",
          "극사실 이미지",
          "모공 잡티 솜털",
          "바이브코딩",
          "불쾌한 골짜기",
          "제미나이 ai"
        ],
        "isoDate": "2026-01-24T13:04:43.000Z"
      }
    ]
  },
  {
    "name": "동우리의 블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "하테나",
    "category": "개인",
    "posts": []
  },
  {
    "name": "늑돌이네 라지온",
    "category": "개인",
    "posts": []
  },
  {
    "name": "루리웹 - 루리웹 리뷰 게시판",
    "category": "게임",
    "posts": [
      {
        "creator": "｜RULIWEB｜",
        "title": "악역영애 4컷 만화 - 37화, 찾으러가는데스와",
        "link": "https://bbs.ruliweb.com/news/board/11/read/2414",
        "pubDate": "Wed, 28 Jan 2026 23:40:22 +0900",
        "author": "｜RULIWEB｜",
        "content": "<img width=\"236\" height=\"177\" src=\"https://i1.ruliweb.com/thumb/26/01/28/19c050be82b51ad6b.webp\">",
        "contentSnippet": "",
        "categories": [
          "웹툰"
        ],
        "isoDate": "2026-01-28T14:40:22.000Z"
      },
      {
        "creator": "(RULIWEB`Д')/",
        "title": "[MULTI] 순수 재미로 의혹 돌파할까, 레이드 슈터 ‘하이가드’",
        "link": "https://bbs.ruliweb.com/news/board/11/read/2413",
        "pubDate": "Tue, 27 Jan 2026 03:00:00 +0900",
        "author": "(RULIWEB`Д')/",
        "content": "<img width=\"236\" height=\"177\" src=\"https://i2.ruliweb.com/thumb/26/01/27/19bfb2451344c329e.jpg\">",
        "contentSnippet": "",
        "categories": [
          "프리뷰"
        ],
        "isoDate": "2026-01-26T18:00:00.000Z"
      },
      {
        "creator": "［RULIWEB］",
        "title": "[MULTI] 낡음과 익숙함 속에서 폭발적 각별함이 터지다, 드래곤소드",
        "link": "https://bbs.ruliweb.com/news/board/11/read/2412",
        "pubDate": "Mon, 26 Jan 2026 17:34:55 +0900",
        "author": "［RULIWEB］",
        "content": "<img width=\"236\" height=\"177\" src=\"https://i3.ruliweb.com/thumb/26/01/26/19bf96ff0455104c1.webp\">",
        "contentSnippet": "",
        "categories": [
          "리뷰"
        ],
        "isoDate": "2026-01-26T08:34:55.000Z"
      }
    ]
  },
  {
    "name": "Reasontobe",
    "category": "개인",
    "posts": []
  },
  {
    "name": "에스티마의 인터넷이야기 EstimaStory.com",
    "category": "개인",
    "posts": []
  },
  {
    "name": "나긋한 개발 - 데비안 리눅스와 프로그램 언어",
    "category": "개인",
    "posts": []
  },
  {
    "name": "일상을 여행처럼...",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Just hack'em",
    "category": "개인",
    "posts": []
  },
  {
    "name": "C++ Truths",
    "category": "개인",
    "posts": []
  },
  {
    "name": "jacking75",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Joel on Software",
    "category": "개인",
    "posts": []
  },
  {
    "name": "벤자민로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "악보쓰는 프로그래머",
    "category": "개인",
    "posts": []
  },
  {
    "name": "쭌안아빠",
    "category": "개인",
    "posts": []
  },
  {
    "name": "A Gangster World",
    "category": "개인",
    "posts": []
  },
  {
    "name": "요우의 내맘대로 블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "개발자스럽다",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "Against All Odds.",
    "category": "개인",
    "posts": []
  },
  {
    "name": "움직이는 게임서버",
    "category": "개인",
    "posts": []
  },
  {
    "name": "이상욱",
    "category": "개인",
    "posts": []
  },
  {
    "name": "임철재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "어쩐지 오늘은",
    "category": "개인",
    "posts": []
  },
  {
    "name": "oddpoet’s étude",
    "category": "개인",
    "posts": []
  },
  {
    "name": "0x00 - NULL",
    "category": "개인",
    "posts": []
  },
  {
    "name": "퇴근 후 서버다운",
    "category": "개인",
    "posts": [
      {
        "creator": "SIDNFT",
        "title": "JIGCAT 프로젝트 작업중 - 고양이 직소 퍼즐",
        "link": "https://serverdown.tistory.com/1566",
        "pubDate": "Thu, 29 Jan 2026 19:19:10 +0900",
        "author": "SIDNFT",
        "comments": "https://serverdown.tistory.com/1566#entry1566comment",
        "content": "<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"512\" data-origin-height=\"512\"><span data-url=\"https://blog.kakaocdn.net/dn/BjwNb/dJMcaaKTtyu/YCYyysTt4od2YTBQ46pBV1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/BjwNb/dJMcaaKTtyu/YCYyysTt4od2YTBQ46pBV1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/BjwNb/dJMcaaKTtyu/YCYyysTt4od2YTBQ46pBV1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FBjwNb%2FdJMcaaKTtyu%2FYCYyysTt4od2YTBQ46pBV1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"512\" height=\"512\" data-origin-width=\"512\" data-origin-height=\"512\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">직소 퍼즐 소스를 다른팀에다 만들어줬는데</p>\n<p data-ke-size=\"size16\">제 프로젝트에도 하나 만들어봤습니다.</p>\n<p data-ke-size=\"size16\">관리툴로 그림을 올리면&nbsp;<br />앱에 적용되게 만드는게 목표입니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">v1 ~ 6&nbsp;</p>\n<p data-ke-size=\"size16\">서버에 이미지를 올리면 게임 스테이지에 표시되는게 목표였습니다.</p>\n<p data-ke-size=\"size16\">미완성이라 보여줄게 없습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">v7 - 2026-01-30</p>\n<p data-ke-size=\"size16\">- 광고 / 배너 + 전면<br />- 메인화면 배경에 클리어한 그림 표시</p>\n<p data-ke-size=\"size16\">영상: <a href=\"https://www.youtube.com/shorts/z0Zk7W0JVco\" target=\"_blank\" rel=\"noopener&nbsp;noreferrer\">https://www.youtube.com/shorts/z0Zk7W0JVco</a></p>\n<figure data-ke-type=\"video\" data-ke-style=\"alignCenter\" data-video-host=\"youtube\" data-video-url=\"https://www.youtube.com/shorts/z0Zk7W0JVco\" data-video-thumbnail=\"https://scrap.kakaocdn.net/dn/fmXaY/dJMb9lL6nff/CrwryoLrKDIcXDoNr4r84k/img.jpg?width=380&amp;height=720&amp;face=0_319_129_448,https://scrap.kakaocdn.net/dn/brpDJE/dJMb9lk1XhF/S7ksXES3ACvFk6rxyayoH0/img.jpg?width=380&amp;height=720&amp;face=0_319_129_448,https://scrap.kakaocdn.net/dn/dfR3sy/dJMb9bvW5k2/NcgOjZcyoSFnDc4b4ZWV11/img.jpg?width=380&amp;height=720&amp;face=0_319_129_448\" data-video-width=\"380\" data-video-height=\"720\" data-video-origin-width=\"380\" data-video-origin-height=\"720\" data-ke-mobilestyle=\"widthContent\" data-video-title=\"JIGCAT v7 playdemo\" data-original-url=\"\"><iframe src=\"https://www.youtube.com/embed/z0Zk7W0JVco\" width=\"380\" height=\"720\" frameborder=\"\" allowfullscreen=\"true\"></iframe>\n<figcaption style=\"display: none;\"></figcaption>\n</figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "직소 퍼즐 소스를 다른팀에다 만들어줬는데\n제 프로젝트에도 하나 만들어봤습니다.\n관리툴로 그림을 올리면 \n앱에 적용되게 만드는게 목표입니다.\n \nv1 ~ 6 \n서버에 이미지를 올리면 게임 스테이지에 표시되는게 목표였습니다.\n미완성이라 보여줄게 없습니다.\n \nv7 - 2026-01-30\n- 광고 / 배너 + 전면\n- 메인화면 배경에 클리어한 그림 표시\n영상: https://www.youtube.com/shorts/z0Zk7W0JVco",
        "guid": "https://serverdown.tistory.com/1566",
        "categories": [
          "프로그래밍/자작",
          "1인개발",
          "1인게임",
          "JIGCAT",
          "부업",
          "인디게임"
        ],
        "isoDate": "2026-01-29T10:19:10.000Z"
      },
      {
        "creator": "SIDNFT",
        "title": "유니티 애셋 / 2026년 퀵 스타트 번들  $2 짜리 이거는 사야합니다.",
        "link": "https://serverdown.tistory.com/1565",
        "pubDate": "Wed, 28 Jan 2026 22:36:20 +0900",
        "author": "SIDNFT",
        "comments": "https://serverdown.tistory.com/1565#entry1565comment",
        "content": "<p data-ke-size=\"size16\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"194\" data-origin-height=\"191\"><span data-url=\"https://blog.kakaocdn.net/dn/ciPAGg/dJMcahXyMra/OHRMvA5CsjXo4nKNBEkwg0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/ciPAGg/dJMcahXyMra/OHRMvA5CsjXo4nKNBEkwg0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/ciPAGg/dJMcahXyMra/OHRMvA5CsjXo4nKNBEkwg0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FciPAGg%2FdJMcahXyMra%2FOHRMvA5CsjXo4nKNBEkwg0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"194\" height=\"191\" data-origin-width=\"194\" data-origin-height=\"191\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">오건 썸네일 쓸 그림</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"837\" data-origin-height=\"282\"><span data-url=\"https://blog.kakaocdn.net/dn/1EkkE/dJMcaiWp90u/2eCBVWc9mv6OsKveXcT2Ik/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/1EkkE/dJMcaiWp90u/2eCBVWc9mv6OsKveXcT2Ik/img.png\"><img src=\"https://blog.kakaocdn.net/dn/1EkkE/dJMcaiWp90u/2eCBVWc9mv6OsKveXcT2Ik/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F1EkkE%2FdJMcaiWp90u%2F2eCBVWc9mv6OsKveXcT2Ik%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"837\" height=\"282\" data-origin-width=\"837\" data-origin-height=\"282\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">매년 하나보군요 작년엔 이걸로 SPUM 을 샀었는데 이번에도 좋은게 많쿤요</p>\n<p data-ke-size=\"size16\">딱 일주일</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"379\" data-origin-height=\"123\"><span data-url=\"https://blog.kakaocdn.net/dn/bGXwYE/dJMcagxyoSI/GElEUxAfrkBus4kMqmby1k/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bGXwYE/dJMcagxyoSI/GElEUxAfrkBus4kMqmby1k/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bGXwYE/dJMcagxyoSI/GElEUxAfrkBus4kMqmby1k/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbGXwYE%2FdJMcagxyoSI%2FGElEUxAfrkBus4kMqmby1k%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"379\" height=\"123\" data-origin-width=\"379\" data-origin-height=\"123\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">둘러봅시다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"text-align: start;\">첫줄</span></h2>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"865\" data-origin-height=\"272\"><span data-url=\"https://blog.kakaocdn.net/dn/b0haDP/dJMcafrSPOp/h89IPgCfD0jFaUn2akg0Y0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/b0haDP/dJMcafrSPOp/h89IPgCfD0jFaUn2akg0Y0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/b0haDP/dJMcafrSPOp/h89IPgCfD0jFaUn2akg0Y0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb0haDP%2FdJMcafrSPOp%2Fh89IPgCfD0jFaUn2akg0Y0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"865\" height=\"272\" data-origin-width=\"865\" data-origin-height=\"272\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">발자국 소리 필요했습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"text-align: start;\">두번째 줄</span></h2>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"848\" data-origin-height=\"330\"><span data-url=\"https://blog.kakaocdn.net/dn/cNSaBO/dJMcaiB7igc/2pAf1txgsuqhbixYaRwbB1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/cNSaBO/dJMcaiB7igc/2pAf1txgsuqhbixYaRwbB1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/cNSaBO/dJMcaiB7igc/2pAf1txgsuqhbixYaRwbB1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcNSaBO%2FdJMcaiB7igc%2F2pAf1txgsuqhbixYaRwbB1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"848\" height=\"330\" data-origin-width=\"848\" data-origin-height=\"330\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">Low-Ploy Medieval Fantasy Heroes - Basick Pack 은 3D 케릭터이고</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"194\" data-origin-height=\"234\"><span data-url=\"https://blog.kakaocdn.net/dn/bEF46O/dJMcahwt3fK/YaJmoPa9oYkKD66ZzSaynk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bEF46O/dJMcahwt3fK/YaJmoPa9oYkKD66ZzSaynk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bEF46O/dJMcahwt3fK/YaJmoPa9oYkKD66ZzSaynk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbEF46O%2FdJMcahwt3fK%2FYaJmoPa9oYkKD66ZzSaynk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"194\" height=\"234\" data-origin-width=\"194\" data-origin-height=\"234\"/></span></figure>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"193\" data-origin-height=\"230\"><span data-url=\"https://blog.kakaocdn.net/dn/POmhj/dJMcagK7m9m/QpzWLpa3TOyL7WW9AVx3D0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/POmhj/dJMcagK7m9m/QpzWLpa3TOyL7WW9AVx3D0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/POmhj/dJMcagK7m9m/QpzWLpa3TOyL7WW9AVx3D0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FPOmhj%2FdJMcagK7m9m%2FQpzWLpa3TOyL7WW9AVx3D0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"193\" height=\"230\" data-origin-width=\"193\" data-origin-height=\"230\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">눈 수염 머리 같은걸 교체할 수 있더군요<br />단순해서 호불호가 안생길 디자인 입니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">과일 마켓</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"614\" data-origin-height=\"316\"><span data-url=\"https://blog.kakaocdn.net/dn/rTaRZ/dJMcag5pMvE/48E96kmhnd68T7G66kCFC1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/rTaRZ/dJMcag5pMvE/48E96kmhnd68T7G66kCFC1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/rTaRZ/dJMcag5pMvE/48E96kmhnd68T7G66kCFC1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FrTaRZ%2FdJMcag5pMvE%2F48E96kmhnd68T7G66kCFC1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"614\" height=\"316\" data-origin-width=\"614\" data-origin-height=\"316\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">맛있게 생겼네요 수박게임 만들고 싶군요</p>\n<p data-ke-size=\"size16\">KayKit&nbsp; 어드벤처 케릭터</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"656\" data-origin-height=\"361\"><span data-url=\"https://blog.kakaocdn.net/dn/cLwCJX/dJMcai9V4dK/OqZwn9L3ifGTqnmCM0HkRK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/cLwCJX/dJMcai9V4dK/OqZwn9L3ifGTqnmCM0HkRK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/cLwCJX/dJMcai9V4dK/OqZwn9L3ifGTqnmCM0HkRK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcLwCJX%2FdJMcai9V4dK%2FOqZwn9L3ifGTqnmCM0HkRK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"656\" height=\"361\" data-origin-width=\"656\" data-origin-height=\"361\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">귀여운 SD 스타일 입니다. 멋지군요</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><span style=\"text-align: start;\">세번째 줄</span></h3>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"852\" data-origin-height=\"317\"><span data-url=\"https://blog.kakaocdn.net/dn/cLOyqH/dJMcahXyMw3/KWU3Vk2glnN2Adwu5rMcaK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/cLOyqH/dJMcahXyMw3/KWU3Vk2glnN2Adwu5rMcaK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/cLOyqH/dJMcahXyMw3/KWU3Vk2glnN2Adwu5rMcaK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcLOyqH%2FdJMcahXyMw3%2FKWU3Vk2glnN2Adwu5rMcaK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"852\" height=\"317\" data-origin-width=\"852\" data-origin-height=\"317\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">스팰은 발사체 이팩트 입니다.</p>\n<p data-ke-size=\"size16\">날씨는 구름 보여주는군요</p>\n<p data-ke-size=\"size16\">공격 애니메이션은 ... 잘모르겠습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"text-align: start;\">마지막줄</span></h2>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"248\" data-origin-height=\"296\"><span data-url=\"https://blog.kakaocdn.net/dn/bJNx8b/dJMcab35ngb/ltImkvG5QDubHFLzog0vXK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bJNx8b/dJMcab35ngb/ltImkvG5QDubHFLzog0vXK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bJNx8b/dJMcab35ngb/ltImkvG5QDubHFLzog0vXK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbJNx8b%2FdJMcab35ngb%2FltImkvG5QDubHFLzog0vXK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"248\" height=\"296\" data-origin-width=\"248\" data-origin-height=\"296\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">카이킷 !!</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"635\" data-origin-height=\"362\"><span data-url=\"https://blog.kakaocdn.net/dn/bKGcST/dJMcagRSzlk/Cn3k8BGK8iQyrXk9Q4S4J1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bKGcST/dJMcagRSzlk/Cn3k8BGK8iQyrXk9Q4S4J1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bKGcST/dJMcagRSzlk/Cn3k8BGK8iQyrXk9Q4S4J1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbKGcST%2FdJMcagRSzlk%2FCn3k8BGK8iQyrXk9Q4S4J1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"635\" height=\"362\" data-origin-width=\"635\" data-origin-height=\"362\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">바닥 판 같은걸 주는군요 쓸일 있으려나 ...</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">지난번에 무료로 정글 나무 준거 생각나네요</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"1078\" data-origin-height=\"418\"><span data-url=\"https://blog.kakaocdn.net/dn/Y3ZWH/dJMcaihQFQf/CpkIdM3VehcOsRtVNyLa20/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/Y3ZWH/dJMcaihQFQf/CpkIdM3VehcOsRtVNyLa20/img.png\"><img src=\"https://blog.kakaocdn.net/dn/Y3ZWH/dJMcaihQFQf/CpkIdM3VehcOsRtVNyLa20/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FY3ZWH%2FdJMcaihQFQf%2FCpkIdM3VehcOsRtVNyLa20%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1078\" height=\"418\" data-origin-width=\"1078\" data-origin-height=\"418\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">나무 위가 평평했었던 애셋인데 쓸일은 없었습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">$2 에 이정도 주다니 놀랍습니다.</p>\n<p data-ke-size=\"size16\">이런건 꼭사야합니다.</p>",
        "contentSnippet": "오건 썸네일 쓸 그림\n \n\n\n매년 하나보군요 작년엔 이걸로 SPUM 을 샀었는데 이번에도 좋은게 많쿤요\n딱 일주일\n\n\n둘러봅시다.\n \n첫줄\n\n\n발자국 소리 필요했습니다.\n \n \n두번째 줄\n\n\nLow-Ploy Medieval Fantasy Heroes - Basick Pack 은 3D 케릭터이고\n\n\n\n눈 수염 머리 같은걸 교체할 수 있더군요\n단순해서 호불호가 안생길 디자인 입니다.\n \n과일 마켓\n\n\n맛있게 생겼네요 수박게임 만들고 싶군요\nKayKit  어드벤처 케릭터\n\n\n귀여운 SD 스타일 입니다. 멋지군요\n \n \n세번째 줄\n\n\n스팰은 발사체 이팩트 입니다.\n날씨는 구름 보여주는군요\n공격 애니메이션은 ... 잘모르겠습니다.\n \n마지막줄\n\n\n카이킷 !!\n\n\n바닥 판 같은걸 주는군요 쓸일 있으려나 ...\n \n \n지난번에 무료로 정글 나무 준거 생각나네요\n\n\n나무 위가 평평했었던 애셋인데 쓸일은 없었습니다.\n \n$2 에 이정도 주다니 놀랍습니다.\n이런건 꼭사야합니다.",
        "guid": "https://serverdown.tistory.com/1565",
        "categories": [
          "프로그래밍/유니티 에셋 리뷰",
          "스타터팩",
          "애셋",
          "유니티"
        ],
        "isoDate": "2026-01-28T13:36:20.000Z"
      },
      {
        "creator": "SIDNFT",
        "title": "406일차 - NFT 는 안되고 WEB3 로 방향 선회",
        "link": "https://serverdown.tistory.com/1564",
        "pubDate": "Mon, 26 Jan 2026 12:59:54 +0900",
        "author": "SIDNFT",
        "comments": "https://serverdown.tistory.com/1564#entry1564comment",
        "content": "<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"296\" data-origin-height=\"274\"><span data-url=\"https://blog.kakaocdn.net/dn/k8u9A/dJMcafk6Hdk/qgwBWmsKsOtWrZUFeKMKN0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/k8u9A/dJMcafk6Hdk/qgwBWmsKsOtWrZUFeKMKN0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/k8u9A/dJMcafk6Hdk/qgwBWmsKsOtWrZUFeKMKN0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fk8u9A%2FdJMcafk6Hdk%2FqgwBWmsKsOtWrZUFeKMKN0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"296\" height=\"274\" data-origin-width=\"296\" data-origin-height=\"274\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">영상: <a href=\"https://youtu.be/VAqWSlALXGM\" target=\"_blank\" rel=\"noopener&nbsp;noreferrer\">https://youtu.be/VAqWSlALXGM</a></p>\n<figure data-ke-type=\"video\" data-ke-style=\"alignCenter\" data-video-host=\"youtube\" data-video-url=\"https://www.youtube.com/watch?v=VAqWSlALXGM\" data-video-thumbnail=\"https://scrap.kakaocdn.net/dn/ilzSY/dJMb81fM2nD/PvapoWF7DSWT4F2gBzYZPK/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720,https://scrap.kakaocdn.net/dn/2rJEi/dJMb9aKzv7m/sE8HCES8aZjs7mMfUmK051/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720,https://scrap.kakaocdn.net/dn/hwNFz/dJMb895X1np/Dlt9fkk7I07uuRKQaWyWOK/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720\" data-video-width=\"860\" data-video-height=\"484\" data-video-origin-width=\"860\" data-video-origin-height=\"484\" data-ke-mobilestyle=\"widthContent\" data-video-title=\"406일차 - NFT 안된다 WEB3 로 간다.\" data-original-url=\"\"><iframe src=\"https://www.youtube.com/embed/VAqWSlALXGM\" width=\"860\" height=\"484\" frameborder=\"\" allowfullscreen=\"true\"></iframe>\n<figcaption style=\"display: none;\"></figcaption>\n</figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">슬픈 일이지만 NFT 는 24년에 정부 가이드라인이 있었나봅니다.</p>\n<p data-ke-size=\"size16\">그래서 수익창출이 힘들다고 하네요</p>\n<p data-ke-size=\"size16\">기능 구현 위주로 가야하고 한국쪽에 서비스하면 안됡 것 같습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">유이티 웹빌드라도 결과물을 확실히 챙겨야겠습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "영상: https://youtu.be/VAqWSlALXGM\n\n\n\n \n \n슬픈 일이지만 NFT 는 24년에 정부 가이드라인이 있었나봅니다.\n그래서 수익창출이 힘들다고 하네요\n기능 구현 위주로 가야하고 한국쪽에 서비스하면 안됡 것 같습니다.\n \n유이티 웹빌드라도 결과물을 확실히 챙겨야겠습니다.",
        "guid": "https://serverdown.tistory.com/1564",
        "categories": [
          "프로그래밍/개발메모",
          "aptos",
          "nft",
          "web3",
          "앱토스",
          "유니티"
        ],
        "isoDate": "2026-01-26T03:59:54.000Z"
      },
      {
        "creator": "SIDNFT",
        "title": "유니티 광고 레벨플레이 아이언소스 / LevelPlay / IronSource",
        "link": "https://serverdown.tistory.com/1563",
        "pubDate": "Sat, 24 Jan 2026 14:34:30 +0900",
        "author": "SIDNFT",
        "comments": "https://serverdown.tistory.com/1563#entry1563comment",
        "content": "<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"939\" data-origin-height=\"633\"><span data-url=\"https://blog.kakaocdn.net/dn/c4xnA7/dJMcaaqyq2g/dkhv8K8nKJkfK4PZfeGbpk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/c4xnA7/dJMcaaqyq2g/dkhv8K8nKJkfK4PZfeGbpk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/c4xnA7/dJMcaaqyq2g/dkhv8K8nKJkfK4PZfeGbpk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc4xnA7%2FdJMcaaqyq2g%2Fdkhv8K8nKJkfK4PZfeGbpk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"939\" height=\"633\" data-origin-width=\"939\" data-origin-height=\"633\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">애드몹 광고가 부족해서 표시 못하는 상황을 겪다보니</p>\n<p data-ke-size=\"size16\">찾은 방법이 매디에이션 입니다.</p>\n<p data-ke-size=\"size16\">애드몹 자체 광고만으로 부족하기 때문에</p>\n<p data-ke-size=\"size16\">더 많은 입찰 업체를 연결해서 남는 광고를 경매로 넘기는 방식입니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">미디에이션을 하려면 입찰자들의 광고 모듈을 붙여줘야합니다.</p>\n<p data-ke-size=\"size16\">각 회사마다 방식이 있기 때문에 붙이기 어려운데요</p>\n<p data-ke-size=\"size16\">유니티에서는 레벨플레이를 지원합니다.</p>\n<p data-ke-size=\"size16\">그런데 버튼만 누르면 끝이군요&nbsp;</p>\n<p data-ke-size=\"size16\">아주 편합니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">뒤에 더 많은 작업이 잇을꺼 같긴한데</p>\n<p data-ke-size=\"size16\">일단 문서를 보고 더 진행해야겠습니다.</p>\n<p data-ke-size=\"size16\">개발 문서: <a href=\"https://developers.is.com/ironsource-mobile/unity/unity-plugin/\" target=\"_blank\" rel=\"noopener&nbsp;noreferrer\">https://developers.is.com/ironsource-mobile/unity/unity-plugin/</a></p>\n<figure id=\"og_1769233088923\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"Unity Package Integration - IronSource Knowledge Center\" data-og-description=\"Unity Package Integration ⚡ Before you start Unity LevelPlay mediation supports Unity version 2021.3+, and Android operating systems version 4.4 (API level 19)+. iOS version support is defined per network. ironSource Ads and LevelPlay mediation support i\" data-og-host=\"developers.is.com\" data-og-source-url=\"https://developers.is.com/ironsource-mobile/unity/unity-plugin/\" data-og-url=\"https://developers.is.com/ironsource-mobile/unity/unity-plugin/\" data-og-image=\"https://scrap.kakaocdn.net/dn/ctyKIA/dJMb83Sc562/CubWm5029KQ5MSUDHVNIKK/img.jpg?width=500&amp;height=160&amp;face=0_0_500_160,https://scrap.kakaocdn.net/dn/sUtw5/dJMb84p21nv/KzyaMIHe5SzKVzy56OsAK0/img.png?width=774&amp;height=793&amp;face=0_0_774_793\"><a href=\"https://developers.is.com/ironsource-mobile/unity/unity-plugin/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://developers.is.com/ironsource-mobile/unity/unity-plugin/\">\n<div class=\"og-image\" style=\"background-image: url('https://scrap.kakaocdn.net/dn/ctyKIA/dJMb83Sc562/CubWm5029KQ5MSUDHVNIKK/img.jpg?width=500&amp;height=160&amp;face=0_0_500_160,https://scrap.kakaocdn.net/dn/sUtw5/dJMb84p21nv/KzyaMIHe5SzKVzy56OsAK0/img.png?width=774&amp;height=793&amp;face=0_0_774_793');\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">Unity Package Integration - IronSource Knowledge Center</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">Unity Package Integration ⚡ Before you start Unity LevelPlay mediation supports Unity version 2021.3+, and Android operating systems version 4.4 (API level 19)+. iOS version support is defined per network. ironSource Ads and LevelPlay mediation support i</p>\n<p class=\"og-host\" data-ke-size=\"size16\">developers.is.com</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">개발 순서</h2>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"730\" data-origin-height=\"323\"><span data-url=\"https://blog.kakaocdn.net/dn/c1knk7/dJMcafMa7oY/uvwDzjuQIf3sdVfo4Tv5H0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/c1knk7/dJMcafMa7oY/uvwDzjuQIf3sdVfo4Tv5H0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/c1knk7/dJMcafMa7oY/uvwDzjuQIf3sdVfo4Tv5H0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc1knk7%2FdJMcafMa7oY%2FuvwDzjuQIf3sdVfo4Tv5H0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"730\" height=\"323\" data-origin-width=\"730\" data-origin-height=\"323\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">어린이를 대상으로 하나요 ?</p>\n<p data-ke-size=\"size16\">저는 아니요 골랐습니다.</p>\n<p data-ke-size=\"size16\">어린이 게임 만드실분은 예 눌러아겠죠</p>\n<p data-ke-size=\"size16\">다음</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"725\" data-origin-height=\"443\"><span data-url=\"https://blog.kakaocdn.net/dn/cRT7dC/dJMcaaKQ8sM/UDggUev3RaAGonJPpjyZ2k/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/cRT7dC/dJMcaaKQ8sM/UDggUev3RaAGonJPpjyZ2k/img.png\"><img src=\"https://blog.kakaocdn.net/dn/cRT7dC/dJMcaaKQ8sM/UDggUev3RaAGonJPpjyZ2k/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcRT7dC%2FdJMcaaKQ8sM%2FUDggUev3RaAGonJPpjyZ2k%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"725\" height=\"443\" data-origin-width=\"725\" data-origin-height=\"443\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">레벨플레이 대시보드 안내하네요</p>\n<p data-ke-size=\"size16\">유니티 로그인을 거쳐</p>\n<p data-ke-size=\"size16\">아이언소스 쪽으로 로그인 되네요</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"753\" data-origin-height=\"454\"><span data-url=\"https://blog.kakaocdn.net/dn/J5SBv/dJMcagdexSZ/qwDXKKCb9hlSFUbogGCuWK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/J5SBv/dJMcagdexSZ/qwDXKKCb9hlSFUbogGCuWK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/J5SBv/dJMcagdexSZ/qwDXKKCb9hlSFUbogGCuWK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FJ5SBv%2FdJMcagdexSZ%2FqwDXKKCb9hlSFUbogGCuWK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"753\" height=\"454\" data-origin-width=\"753\" data-origin-height=\"454\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">아이언소스 회원가입하나봅니다.</p>\n<p data-ke-size=\"size16\">다음</p>\n<p data-ke-size=\"size16\">이름이랑 도시 넣는데 스샷 안찍었습니다.</p>\n<p data-ke-size=\"size16\">다</p>\n<p data-ke-size=\"size16\">앱이 출시되었는지 묻는군요 알아서 입력하시구요</p>\n<p data-ke-size=\"size16\">저는 새 앱에 적용할 꺼라 아니오 눌렀습니다.</p>\n<p data-ke-size=\"size16\">아래로 가면</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"522\" data-origin-height=\"243\"><span data-url=\"https://blog.kakaocdn.net/dn/s9eTl/dJMcai28CVd/1wDQSqkcTzsBBGHCkI2hR1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/s9eTl/dJMcai28CVd/1wDQSqkcTzsBBGHCkI2hR1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/s9eTl/dJMcai28CVd/1wDQSqkcTzsBBGHCkI2hR1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fs9eTl%2FdJMcai28CVd%2F1wDQSqkcTzsBBGHCkI2hR1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"522\" height=\"243\" data-origin-width=\"522\" data-origin-height=\"243\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">COPPA* 는 정보 보호법이라고 하네요</p>\n<p data-ke-size=\"size16\">따를 것이기 때문에 Directed 디렉티드 고릅니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"776\" data-origin-height=\"64\"><span data-url=\"https://blog.kakaocdn.net/dn/bWIj8S/dJMcaaqysoA/P9nFUSGDcDX8tRbYY0jQn1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bWIj8S/dJMcaaqysoA/P9nFUSGDcDX8tRbYY0jQn1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bWIj8S/dJMcaaqysoA/P9nFUSGDcDX8tRbYY0jQn1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbWIj8S%2FdJMcaaqysoA%2FP9nFUSGDcDX8tRbYY0jQn1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"776\" height=\"64\" data-origin-width=\"776\" data-origin-height=\"64\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">플레이 가능한 광고를 사용하겠느냐인데</p>\n<p data-ke-size=\"size16\">저는 플레이가능한 광고 싫습니다.</p>\n<p data-ke-size=\"size16\">체크를 하면 거부하겠다는 뜻 같군요&nbsp;</p>\n<p data-ke-size=\"size16\">저는 체크 했습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"816\" data-origin-height=\"424\"><span data-url=\"https://blog.kakaocdn.net/dn/zpWVQ/dJMcabJMSkG/JIaRqx3H1fagGT0KlzMdc1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/zpWVQ/dJMcabJMSkG/JIaRqx3H1fagGT0KlzMdc1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/zpWVQ/dJMcabJMSkG/JIaRqx3H1fagGT0KlzMdc1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FzpWVQ%2FdJMcabJMSkG%2FJIaRqx3H1fagGT0KlzMdc1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"816\" height=\"424\" data-origin-width=\"816\" data-origin-height=\"424\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">이 뒤론 메뉴가 너무 많쿤요 Admob 같은 화면이 나왔습니다.</p>\n<p data-ke-size=\"size16\">메뉴는 더 확인해봐야겠습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">투자 관점</h2>\n<p data-ke-size=\"size16\">이부분에서 유명한 회사가 앱러빈 입니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"808\" data-origin-height=\"512\"><span data-url=\"https://blog.kakaocdn.net/dn/qUQCf/dJMcadgulBx/00bKyKlVsAY7Wp9jD6urH1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/qUQCf/dJMcadgulBx/00bKyKlVsAY7Wp9jD6urH1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/qUQCf/dJMcadgulBx/00bKyKlVsAY7Wp9jD6urH1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FqUQCf%2FdJMcadgulBx%2F00bKyKlVsAY7Wp9jD6urH1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"808\" height=\"512\" data-origin-width=\"808\" data-origin-height=\"512\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">5년치 차트인데요 엄청난 주가 향상이 있었죠</p>\n<p data-ke-size=\"size16\">게임은 유니티로 만드는데 광고는 앱러빈이 먹고 있었습니다.</p>\n<p data-ke-size=\"size16\">이번에 유니티가 이걸 하겠다고 만든게 레벨플레이 구요</p>\n<p data-ke-size=\"size16\">모듈 붙이기가 아주 쉬워졌으니 효과가 있을 것 같습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"828\" data-origin-height=\"701\"><span data-url=\"https://blog.kakaocdn.net/dn/dqCLxu/dJMcaiWn88u/cLnG1wxQTuItn9Y7gok0rK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/dqCLxu/dJMcaiWn88u/cLnG1wxQTuItn9Y7gok0rK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/dqCLxu/dJMcaiWn88u/cLnG1wxQTuItn9Y7gok0rK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdqCLxu%2FdJMcaiWn88u%2FcLnG1wxQTuItn9Y7gok0rK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"828\" height=\"701\" data-origin-width=\"828\" data-origin-height=\"701\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">이제 시작 아닌가 싶군요</p>\n<p data-ke-size=\"size16\">저는 왕창 사야겠습니다. 가즈아~</p>\n<p data-ke-size=\"size16\">붙이는법 쓸려고 문서 썼는데</p>\n<p data-ke-size=\"size16\">투자로 갔군요 ㅎㅎ</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "애드몹 광고가 부족해서 표시 못하는 상황을 겪다보니\n찾은 방법이 매디에이션 입니다.\n애드몹 자체 광고만으로 부족하기 때문에\n더 많은 입찰 업체를 연결해서 남는 광고를 경매로 넘기는 방식입니다.\n \n미디에이션을 하려면 입찰자들의 광고 모듈을 붙여줘야합니다.\n각 회사마다 방식이 있기 때문에 붙이기 어려운데요\n유니티에서는 레벨플레이를 지원합니다.\n그런데 버튼만 누르면 끝이군요 \n아주 편합니다.\n \n뒤에 더 많은 작업이 잇을꺼 같긴한데\n일단 문서를 보고 더 진행해야겠습니다.\n개발 문서: https://developers.is.com/ironsource-mobile/unity/unity-plugin/\n\n \nUnity Package Integration - IronSource Knowledge Center\nUnity Package Integration ⚡ Before you start Unity LevelPlay mediation supports Unity version 2021.3+, and Android operating systems version 4.4 (API level 19)+. iOS version support is defined per network. ironSource Ads and LevelPlay mediation support i\ndevelopers.is.com\n\n \n \n개발 순서\n\n\n어린이를 대상으로 하나요 ?\n저는 아니요 골랐습니다.\n어린이 게임 만드실분은 예 눌러아겠죠\n다음\n\n\n레벨플레이 대시보드 안내하네요\n유니티 로그인을 거쳐\n아이언소스 쪽으로 로그인 되네요\n\n\n아이언소스 회원가입하나봅니다.\n다음\n이름이랑 도시 넣는데 스샷 안찍었습니다.\n다\n앱이 출시되었는지 묻는군요 알아서 입력하시구요\n저는 새 앱에 적용할 꺼라 아니오 눌렀습니다.\n아래로 가면\n\n\nCOPPA* 는 정보 보호법이라고 하네요\n따를 것이기 때문에 Directed 디렉티드 고릅니다.\n\n\n플레이 가능한 광고를 사용하겠느냐인데\n저는 플레이가능한 광고 싫습니다.\n체크를 하면 거부하겠다는 뜻 같군요 \n저는 체크 했습니다.\n\n\n이 뒤론 메뉴가 너무 많쿤요 Admob 같은 화면이 나왔습니다.\n메뉴는 더 확인해봐야겠습니다.\n \n \n \n \n \n \n투자 관점\n이부분에서 유명한 회사가 앱러빈 입니다.\n\n\n5년치 차트인데요 엄청난 주가 향상이 있었죠\n게임은 유니티로 만드는데 광고는 앱러빈이 먹고 있었습니다.\n이번에 유니티가 이걸 하겠다고 만든게 레벨플레이 구요\n모듈 붙이기가 아주 쉬워졌으니 효과가 있을 것 같습니다.\n\n\n이제 시작 아닌가 싶군요\n저는 왕창 사야겠습니다. 가즈아~\n붙이는법 쓸려고 문서 썼는데\n투자로 갔군요 ㅎㅎ",
        "guid": "https://serverdown.tistory.com/1563",
        "categories": [
          "프로그래밍/개발메모",
          "LevelPlay",
          "unity",
          "레벨플레이",
          "아이언소스",
          "유니티"
        ],
        "isoDate": "2026-01-24T05:34:30.000Z"
      }
    ]
  },
  {
    "name": "coolspeed",
    "category": "개인",
    "posts": []
  },
  {
    "name": "오늘도 끄적끄적",
    "category": "개인",
    "posts": []
  },
  {
    "name": "dx11 Vanica's Lifelog - 夢が夢で終わらないように",
    "category": "개인",
    "posts": []
  },
  {
    "name": "초코사랑",
    "category": "개인",
    "posts": []
  },
  {
    "name": "ZeroCho Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "imays게임엔진개발자",
    "category": "개인",
    "posts": []
  },
  {
    "name": "RSS feed for hurinmon Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "기억보단 기록을",
    "category": "개인",
    "posts": [
      {
        "creator": "향로 (기억보단 기록을)",
        "title": "잡코리아 더 리부트 (2026) 참석 후기",
        "link": "https://jojoldu.tistory.com/860",
        "pubDate": "Fri, 30 Jan 2026 09:34:09 +0900",
        "author": "향로 (기억보단 기록을)",
        "comments": "https://jojoldu.tistory.com/860#entry860comment",
        "content": "<p data-ke-size=\"size16\">2026.01.29 에 진행된 잡코리아 더 리부트 행사에 다녀왔다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"2026-01-29 15.21.59.jpg\" data-origin-width=\"3024\" data-origin-height=\"4032\"><span data-url=\"https://blog.kakaocdn.net/dn/uvQSP/dJMcaaKTGl5/kB0As0dvmHC51VFKK0c9m0/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/uvQSP/dJMcaaKTGl5/kB0As0dvmHC51VFKK0c9m0/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/uvQSP/dJMcaaKTGl5/kB0As0dvmHC51VFKK0c9m0/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FuvQSP%2FdJMcaaKTGl5%2FkB0As0dvmHC51VFKK0c9m0%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"417\" height=\"556\" data-filename=\"2026-01-29 15.21.59.jpg\" data-origin-width=\"3024\" data-origin-height=\"4032\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">사명을 왜 변경하는지에 대해, AI 시대에 어떻게 채용이 변해야하는지, 잡코리아는 어떻게 갈 것 인지를 여러 리더분들이 나오셔서 이야기를 해주셨다.</p>\n<p data-ke-size=\"size16\">아무래도 교육과 채용은 Value Chain이 강한 관계라서 국내 채용 1위 플랫폼인 잡코리아에서는 AI Agent에 대해 어떤 고민을 하고, 어떤 제품 방향성을 가지실지 궁금해서 참여하게 되었다.<br /><br />구인자의 채용, 구직자의 커리어 2개에 관한 AI Agent 발표와 실제 시연 부스도 운영하고 있어서 어떤 방향성을 가지시는지 쉽게 알 수 있었다.<br />이런 채용 과정의 에이전트는&nbsp;<a href=\"https://www.wanted.co.kr/ai/agent/intro\" target=\"_blank\" rel=\"noopener\">원티드에서도 비슷한 제품</a>을 출시했고, 앞으로도 출시할 것으로 보고 있는데 잡코리아의 에이전트와 원티드의 에이전트, 그 외 다른 채용 플랫폼들의 에이전트들을 비교해보는 것도 앞으로 기대해볼 수 있겠다.</p>\n<h2 data-ke-size=\"size26\">1. AI 시대 채용의 기준, 새로워진 잡코리아 (09:30)</h2>\n<p data-ke-size=\"size16\"><b>발표자: 윤현준 CEO</b></p>\n<h3 data-ke-size=\"size23\">1-1. 혁신의 딜레마와 업의 재정의</h3>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>과거의 성공이 미래를 보장하지 않음 (블록버스터 vs 넷플릭스)</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>블록버스터는 2004년 북미 9천여 개 매장을 가진 1위 기업이었지만, 'DVD 대여업'이라는 수단에만 집중하다 2010년 파산함.</li>\n<li>반면 넷플릭스는 '영상 콘텐츠 전달'이라는 본질에 집중해 살아남음.</li>\n<li>잡코리아도 지난 30년 1등이었지만, \"우리는 채용 공고 상품(DVD)을 파는 회사인가, 일과 사람을 연결(콘텐츠 전달)하는 회사인가?\"라는 질문에 봉착.</li>\n<li>단기적 성과(공고 판매)에 안주하는 환상형 혁신을 경계하고, 업의 본질을 재정의하기로 결정.</li>\n</ul>\n</li>\n<li><b>새로운 사명: Worxphere (웍스피어)</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>Work + Experience + Sphere: 일과 경험, 그리고 그 모든 영역을 아우르는 생태계를 의미함.</li>\n<li>잡코리아(정규직), 알바몬(비정규직), 잡플래닛(기업 정보/평판), 나인하이어(ATS), 클릭(외국인), 놉(명함)을 모두 아우르는 통합 HR 테크 그룹으로 재출범하여 일의 경험 전체를 설계할 예정</li>\n</ul>\n</li>\n</ul>\n<h3 data-ke-size=\"size23\">1-2. 핵심 기술: Context Link (맥락적 연결)</h3>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>기존 데이터의 한계: 정량적 데이터 (Spec)</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>기존 이력서나 공고는 스펙의 나열에 불과함.</li>\n<li><b>옷 비유</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>\"2XL 양가죽 재킷\"이라는 정보(스펙)만으로는 이 옷의 핏(Fit)이나 촉감, 스타일이 나에게 맞는지 알 수 없다.</li>\n</ul>\n</li>\n<li>이로 인해 서류상 조건은 맞는데 막상 만나보면 안 맞는 미스 매칭 발생.</li>\n</ul>\n</li>\n<li><b>새로운 솔루션: 정성적 데이터 (Context)의 결합</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>잡플래닛 인수 시너지:</b> 잡플래닛이 가진 기업 리뷰, 면접 경험, 조직 문화 등 '맥락 데이터'를 AI가 학습함.</li>\n<li><b>Context Link:</b> 정량적 데이터(잡코리아) + 정성적 데이터(잡플래닛)를 결합해 AI가 맥락(Context)을 파악.</li>\n<li><b>사용 예시:</b> 단순히 \"마케터 검색해줘\"가 아니라, \"육아휴직 간 1년 차 김대리 자리에 딱 맞는, 우리 팀 분위기에 적응할 사람 찾아줘\"라고 하면 AI가 숨은 의도까지 파악해 매칭함.</li>\n</ul>\n</li>\n</ul>\n<h3 data-ke-size=\"size23\">1-3. 미래 비전: Career Genome (커리어 게놈)</h3>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>이력서의 종말 (Resume -&gt; Genome)</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>사람이 직접 쓰고 관리하는 정적인 이력서(Resume)는 아날로그의 산물임.</li>\n<li>AI가 실시간 행동 데이터와 성과를 분석해 관리하는 '다이내믹 디지털 프로필(Career Genome)'로 진화.</li>\n</ul>\n</li>\n<li><b>검증에서 예측으로</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>과거 이력 확인(Verification)을 넘어, 이 사람이 우리 조직에 왔을 때 낼 미래 성과(Prediction)를 예측하는 모델로 전환.</li>\n</ul>\n</li>\n</ul>\n<h3 data-ke-size=\"size23\">1-4. 출시 예정 서비스 요약 (Roadmap)</h3>\n<p data-ke-size=\"size16\">컨퍼런스에서 공개된, 올 상반기부터 순차적으로 출시될 핵심 서비스들.</p>\n<ol style=\"list-style-type: decimal;\" data-ke-list-type=\"decimal\">\n<li><b>Talent Agent (기업용):</b> 인사 담당자가 자연어로 질문하면(\"육아휴직 대체자 찾아줘\"), AI가 내부/외부 데이터를 분석해 최적의 후보자를 추천하고 소싱까지 대행.</li>\n<li><b>Career Agent (개인용):</b> 구직자의 행동 패턴을 분석해, 공고를 검색하지 않아도 \"이 회사가 당신에게 딱 맞아요\"라고 선제적으로 제안.</li>\n<li><b>Hiring Center (통합 솔루션):</b> 공고 등록, 지원자 관리, 채용 성과 분석, 평판 관리(잡플래닛 연동)를 한곳에서 처리하는 올인원 대시보드.</li>\n</ol>\n<hr data-ke-style=\"style1\" />\n<h2 data-ke-size=\"size26\">2. 왜 채용은 항상 어려운가: 구조적 진단과 전환 (10:50)</h2>\n<p data-ke-size=\"size16\"><b>발표자: 이창준 CSO</b></p>\n<h3 data-ke-size=\"size23\">2-1. 채용 시장의 난제 (Problem)</h3>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>구직자는 늘었는데 채용은 더 어려움</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>지난 3년간 구직자는 늘었지만, 기업이 느끼는 채용 난이도와 비용은 역대 최고 수준임.</li>\n<li>본인도 인재 영입을 위해 지난 9월 충무로에서 판교까지 한 달간 20번 넘게 직접 찾아가 티타임을 했던 경험을 공유</li>\n</ul>\n</li>\n<li><b>원인 1: 희소성 (Scarcity)</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>과거엔 '마케터'면 충분했지만, 지금은 '그로스 마케터 + 특정 예산 운영 경험 + 대행사 핸들링 경험 + 우리 문화 적합성'을 모두 요구함.</li>\n<li>맥락(Context)에 맞는 인재를 찾다 보니 대상자가 극도로 희소해짐.</li>\n</ul>\n</li>\n<li><b>원인 2: 시급성 (Urgency)</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>구직자들은 더 이상 기다리지 않음. 쇼핑하듯 여러 기업을 동시에 비교하고, 경험이 나쁘면 바로 이탈함.</li>\n<li>기업은 검증하느라 시간이 걸리는데, 그사이 인재는 다른 곳으로 가버림.</li>\n</ul>\n</li>\n</ul>\n<h3 data-ke-size=\"size23\">2-2. 해결책: AI Agent의 4가지 역할 (Solution)</h3>\n<p data-ke-size=\"size16\">AI가 단순 요약봇이 아니라, 채용 프로세스를 대행하는 에이전트(Agent)가 되어야 한다며 4가지 역할을 정의</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>Role 1: 결정 (Decision) - \"맞는지 판단해줘\"</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>면접관들이 \"이 사람 툴은 잘 쓰는데 우리 문화에 맞을까?\"라며 결정을 미루는 시간을 줄여줌.</li>\n<li>스펙뿐만 아니라 '맥락 증거'를 찾아 판단 근거를 제공하여 모호한 의사결정 지원.</li>\n</ul>\n</li>\n<li><b>Role 2: 발굴 (Discovery) - \"찾아가서 데려와\"</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>수동 -&gt; 능동:</b> 공고 올리고 기다리는 방식은 끝남.</li>\n<li>AI가 우리 회사에 맞을법한 인재(심지어 구직 의사가 없는 잠재 후보자)를 먼저 찾아내 리스트업하고 설득함.</li>\n</ul>\n</li>\n<li><b>Role 3: 전환 (Conversion) - \"놓치지 않게 잡아줘\"</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>'2분 vs 1주일의 법칙'</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>밤 11시에 침대에서 스마트폰으로 지원한 후보자에게 2분 만에 \"내일 면접 가능하신가요?\"라고 묻는 기업과, 1주일 뒤에 이메일 보내는 기업의 승패는 명확함.</li>\n</ul>\n</li>\n<li>AI 에이전트가 24시간 대응하며 지원자가 이탈하지 않도록 경험 관리.</li>\n</ul>\n</li>\n<li><b>Role 4: 통합 (Integration) - Hiring Center</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>슬랙, 노션, 엑셀, 채용 사이트 등으로 파편화된 데이터를 하나로 모음.</li>\n<li>하이어링 센터(Hiring Center): 모든 채용 데이터가 흐르도록 통합해, 데이터 기반의 의사결정 지원.</li>\n</ul>\n</li>\n</ul>\n<hr data-ke-style=\"style1\" />\n<h2 data-ke-size=\"size26\">3. AI Agent가 바꾸는 채용의 미래 (14:30)</h2>\n<p data-ke-size=\"size16\"><b>발표자: 김요섭 CTO</b></p>\n<h3 data-ke-size=\"size23\">3-1. 검색의 한계와 에이전트의 필요성</h3>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>검색은 정확한 키워드가 필요함</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>포워드 디플로이드 엔지니어(Forward Deployed Engineers, FDE)를 뽑으려면 JD를 정확히 이해하고 키워드를 넣어야 함.</li>\n<li>하지만 인사담당자가 회사 내 모든 직무의 전문 용어와 맥락을 이해하는 것은 불가능에 가까움.</li>\n<li>그 결과, \"시니어 프론트엔드 개발자\" 1명을 찾기 위해 100명의 프로필을 뒤지고 2시간을 허비하는 비효율이 발생함.</li>\n</ul>\n</li>\n<li><b>상황(Context)을 이해하는 AI</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>음악 추천의 예: 과거엔 \"네가 좋아하는 장르니까 들어봐\"였다면, AI 시대엔 <b>\"야근 중이고 집중이 필요한 너에게 맞는 곡이야\"</b>라고 제안함.</li>\n<li>채용도 마찬가지. 단순 조건 매칭이 아니라 \"지금 우리 팀이 신규 서비스 런칭 직전인데, 실행력 빠른 스타트업 출신 자바 개발자가 필요해\"라는 문장을 이해해야 함.</li>\n</ul>\n</li>\n</ul>\n<h3 data-ke-size=\"size23\">3-2. 웍스피어의 AI 에이전트 라인업</h3>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>Talent Agent (기업용)</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>자연어 기반 소싱:</b> 복잡한 필터링 대신 문장으로 질문하면 AI가 의도를 파악해 인재 추천.</li>\n<li><b>유사 인재 찾기:</b> \"이 링크드인 프로필과 비슷한 사람 찾아줘\"라고 요청하면, 해당 인재의 역량과 맥락을 분석해 유사한 후보자를 웍스피어 데이터베이스(천만 명 규모)에서 발굴함.</li>\n<li>포워드&nbsp;디플로이드&nbsp;엔지니어(Forward&nbsp;Deployed&nbsp;Engineers,&nbsp;FDE)처럼 낯선 직무도 AI가 먼저 JD를 학습하고 적합한 스펙을 역제안함.</li>\n</ul>\n</li>\n<li><b>Career Agent (구직자용)</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>단순 공고 추천을 넘어 <b>'합격 전략'</b>까지 제공.</li>\n<li>구직자의 이력과 행동 데이터를 학습해, \"이 공고는 당신의 A 경험과는 맞지만 B 역량은 보완이 필요해요\"라며 구체적인 가이드와 자소서 작성 전략까지 코칭함.</li>\n</ul>\n</li>\n<li><b>왜 웍스피어인가? (Why Now?)</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>AI 기술력은 기본, 핵심은 데이터의 <b>'양과 깊이'</b>.</li>\n<li>잡코리아, 알바몬, 게임잡 등 전 산업군의 데이터 + <b>잡플래닛의 정성적 리뷰 데이터(맥락)</b>가 결합되어야만 진짜 '성향 파악'이 가능</li>\n</ul>\n</li>\n</ul>\n<hr data-ke-style=\"style1\" />\n<h2 data-ke-size=\"size26\">4. 이제 채용도 퍼포먼스다 (15:00)</h2>\n<p data-ke-size=\"size16\"><b>발표자: 박소리 JK 사업실장</b></p>\n<h3 data-ke-size=\"size23\">4-1. 기존 채용 상품의 문제: '성실함'의 함정</h3>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>누구나 똑같은 광고를 봄</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>기존 배너 광고는 고정된 위치에 고정된 시간 동안 노출되는 '성실한' 상품이었음.</li>\n<li>하지만 마케터 공고를 시설관리자가 보거나, 부산 기업 공고를 서울 구직자가 보는 비효율 발생.</li>\n<li>스타트업 시절, 돈을 써서 광고를 해도 지원자가 없거나 핏이 안 맞는 지원자만 몰려서 이력서 검토하느라 시간만 쓰는 악순환을 경험함.</li>\n</ul>\n</li>\n</ul>\n<h3 data-ke-size=\"size23\">4-2. 해결책: Smart Pick (스마트픽)</h3>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>집요하고 똑똑한 채용 상품</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>타겟팅:</b> 컨텍스트 링크 알고리즘을 통해, 내 공고에 반응할 확률이 높은 구직자에게만 선별적으로 노출함.</li>\n<li><b>성과 기반 과금:</b> 기간제(기간당 얼마)가 아니라, 구직자가 공고를 <b>클릭하거나 조회했을 때만 비용이 발생</b>하는 합리적 모델 도입.</li>\n</ul>\n</li>\n<li><b>실제 성과 사례</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>F&amp;B 기업:</b> 기존 상품 대비 지원자 수 3배 증가, 비용 30% 절감.</li>\n<li><b>K-뷰티 기업:</b> 8개월간 못 뽑던 포지션 채용 성공, 비용 60% 절감. 담당자가 \"나만 알고 싶으니 홍보하지 말라\"고 할 정도.</li>\n</ul>\n</li>\n</ul>\n<h3 data-ke-size=\"size23\">4-3. 통합 관리와 편의성</h3>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>Hiring Center (하이어링 센터)</b>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>잡코리아, 알바몬, 외국인 채용(클릭) 등 모든 채용 공고와 지원자를 한 곳에서 관리.</li>\n<li>데이터 시각화 대시보드를 제공해, 인사담당자가 상사에게 \"이번 채용 광고 효율이 이렇습니다\"라고 캡처 한 방으로 보고할 수 있게 지원.</li>\n</ul>\n</li>\n</ul>",
        "contentSnippet": "2026.01.29 에 진행된 잡코리아 더 리부트 행사에 다녀왔다.\n\n\n사명을 왜 변경하는지에 대해, AI 시대에 어떻게 채용이 변해야하는지, 잡코리아는 어떻게 갈 것 인지를 여러 리더분들이 나오셔서 이야기를 해주셨다.\n아무래도 교육과 채용은 Value Chain이 강한 관계라서 국내 채용 1위 플랫폼인 잡코리아에서는 AI Agent에 대해 어떤 고민을 하고, 어떤 제품 방향성을 가지실지 궁금해서 참여하게 되었다.\n구인자의 채용, 구직자의 커리어 2개에 관한 AI Agent 발표와 실제 시연 부스도 운영하고 있어서 어떤 방향성을 가지시는지 쉽게 알 수 있었다.\n이런 채용 과정의 에이전트는 원티드에서도 비슷한 제품을 출시했고, 앞으로도 출시할 것으로 보고 있는데 잡코리아의 에이전트와 원티드의 에이전트, 그 외 다른 채용 플랫폼들의 에이전트들을 비교해보는 것도 앞으로 기대해볼 수 있겠다.\n1. AI 시대 채용의 기준, 새로워진 잡코리아 (09:30)\n발표자: 윤현준 CEO\n1-1. 혁신의 딜레마와 업의 재정의\n과거의 성공이 미래를 보장하지 않음 (블록버스터 vs 넷플릭스)\n\n블록버스터는 2004년 북미 9천여 개 매장을 가진 1위 기업이었지만, 'DVD 대여업'이라는 수단에만 집중하다 2010년 파산함.\n반면 넷플릭스는 '영상 콘텐츠 전달'이라는 본질에 집중해 살아남음.\n잡코리아도 지난 30년 1등이었지만, \"우리는 채용 공고 상품(DVD)을 파는 회사인가, 일과 사람을 연결(콘텐츠 전달)하는 회사인가?\"라는 질문에 봉착.\n단기적 성과(공고 판매)에 안주하는 환상형 혁신을 경계하고, 업의 본질을 재정의하기로 결정.\n새로운 사명: Worxphere (웍스피어)\n\nWork + Experience + Sphere: 일과 경험, 그리고 그 모든 영역을 아우르는 생태계를 의미함.\n잡코리아(정규직), 알바몬(비정규직), 잡플래닛(기업 정보/평판), 나인하이어(ATS), 클릭(외국인), 놉(명함)을 모두 아우르는 통합 HR 테크 그룹으로 재출범하여 일의 경험 전체를 설계할 예정\n1-2. 핵심 기술: Context Link (맥락적 연결)\n기존 데이터의 한계: 정량적 데이터 (Spec)\n\n기존 이력서나 공고는 스펙의 나열에 불과함.\n옷 비유\n\n\"2XL 양가죽 재킷\"이라는 정보(스펙)만으로는 이 옷의 핏(Fit)이나 촉감, 스타일이 나에게 맞는지 알 수 없다.\n이로 인해 서류상 조건은 맞는데 막상 만나보면 안 맞는 미스 매칭 발생.\n새로운 솔루션: 정성적 데이터 (Context)의 결합\n\n잡플래닛 인수 시너지: 잡플래닛이 가진 기업 리뷰, 면접 경험, 조직 문화 등 '맥락 데이터'를 AI가 학습함.\nContext Link: 정량적 데이터(잡코리아) + 정성적 데이터(잡플래닛)를 결합해 AI가 맥락(Context)을 파악.\n사용 예시: 단순히 \"마케터 검색해줘\"가 아니라, \"육아휴직 간 1년 차 김대리 자리에 딱 맞는, 우리 팀 분위기에 적응할 사람 찾아줘\"라고 하면 AI가 숨은 의도까지 파악해 매칭함.\n1-3. 미래 비전: Career Genome (커리어 게놈)\n이력서의 종말 (Resume -> Genome)\n\n사람이 직접 쓰고 관리하는 정적인 이력서(Resume)는 아날로그의 산물임.\nAI가 실시간 행동 데이터와 성과를 분석해 관리하는 '다이내믹 디지털 프로필(Career Genome)'로 진화.\n검증에서 예측으로\n\n과거 이력 확인(Verification)을 넘어, 이 사람이 우리 조직에 왔을 때 낼 미래 성과(Prediction)를 예측하는 모델로 전환.\n1-4. 출시 예정 서비스 요약 (Roadmap)\n컨퍼런스에서 공개된, 올 상반기부터 순차적으로 출시될 핵심 서비스들.\nTalent Agent (기업용): 인사 담당자가 자연어로 질문하면(\"육아휴직 대체자 찾아줘\"), AI가 내부/외부 데이터를 분석해 최적의 후보자를 추천하고 소싱까지 대행.\nCareer Agent (개인용): 구직자의 행동 패턴을 분석해, 공고를 검색하지 않아도 \"이 회사가 당신에게 딱 맞아요\"라고 선제적으로 제안.\nHiring Center (통합 솔루션): 공고 등록, 지원자 관리, 채용 성과 분석, 평판 관리(잡플래닛 연동)를 한곳에서 처리하는 올인원 대시보드.\n2. 왜 채용은 항상 어려운가: 구조적 진단과 전환 (10:50)\n발표자: 이창준 CSO\n2-1. 채용 시장의 난제 (Problem)\n구직자는 늘었는데 채용은 더 어려움\n\n지난 3년간 구직자는 늘었지만, 기업이 느끼는 채용 난이도와 비용은 역대 최고 수준임.\n본인도 인재 영입을 위해 지난 9월 충무로에서 판교까지 한 달간 20번 넘게 직접 찾아가 티타임을 했던 경험을 공유\n원인 1: 희소성 (Scarcity)\n\n과거엔 '마케터'면 충분했지만, 지금은 '그로스 마케터 + 특정 예산 운영 경험 + 대행사 핸들링 경험 + 우리 문화 적합성'을 모두 요구함.\n맥락(Context)에 맞는 인재를 찾다 보니 대상자가 극도로 희소해짐.\n원인 2: 시급성 (Urgency)\n\n구직자들은 더 이상 기다리지 않음. 쇼핑하듯 여러 기업을 동시에 비교하고, 경험이 나쁘면 바로 이탈함.\n기업은 검증하느라 시간이 걸리는데, 그사이 인재는 다른 곳으로 가버림.\n2-2. 해결책: AI Agent의 4가지 역할 (Solution)\nAI가 단순 요약봇이 아니라, 채용 프로세스를 대행하는 에이전트(Agent)가 되어야 한다며 4가지 역할을 정의\nRole 1: 결정 (Decision) - \"맞는지 판단해줘\"\n\n면접관들이 \"이 사람 툴은 잘 쓰는데 우리 문화에 맞을까?\"라며 결정을 미루는 시간을 줄여줌.\n스펙뿐만 아니라 '맥락 증거'를 찾아 판단 근거를 제공하여 모호한 의사결정 지원.\nRole 2: 발굴 (Discovery) - \"찾아가서 데려와\"\n\n수동 -> 능동: 공고 올리고 기다리는 방식은 끝남.\nAI가 우리 회사에 맞을법한 인재(심지어 구직 의사가 없는 잠재 후보자)를 먼저 찾아내 리스트업하고 설득함.\nRole 3: 전환 (Conversion) - \"놓치지 않게 잡아줘\"\n\n'2분 vs 1주일의 법칙'\n\n밤 11시에 침대에서 스마트폰으로 지원한 후보자에게 2분 만에 \"내일 면접 가능하신가요?\"라고 묻는 기업과, 1주일 뒤에 이메일 보내는 기업의 승패는 명확함.\nAI 에이전트가 24시간 대응하며 지원자가 이탈하지 않도록 경험 관리.\nRole 4: 통합 (Integration) - Hiring Center\n\n슬랙, 노션, 엑셀, 채용 사이트 등으로 파편화된 데이터를 하나로 모음.\n하이어링 센터(Hiring Center): 모든 채용 데이터가 흐르도록 통합해, 데이터 기반의 의사결정 지원.\n3. AI Agent가 바꾸는 채용의 미래 (14:30)\n발표자: 김요섭 CTO\n3-1. 검색의 한계와 에이전트의 필요성\n검색은 정확한 키워드가 필요함\n\n포워드 디플로이드 엔지니어(Forward Deployed Engineers, FDE)를 뽑으려면 JD를 정확히 이해하고 키워드를 넣어야 함.\n하지만 인사담당자가 회사 내 모든 직무의 전문 용어와 맥락을 이해하는 것은 불가능에 가까움.\n그 결과, \"시니어 프론트엔드 개발자\" 1명을 찾기 위해 100명의 프로필을 뒤지고 2시간을 허비하는 비효율이 발생함.\n상황(Context)을 이해하는 AI\n\n음악 추천의 예: 과거엔 \"네가 좋아하는 장르니까 들어봐\"였다면, AI 시대엔 \"야근 중이고 집중이 필요한 너에게 맞는 곡이야\"라고 제안함.\n채용도 마찬가지. 단순 조건 매칭이 아니라 \"지금 우리 팀이 신규 서비스 런칭 직전인데, 실행력 빠른 스타트업 출신 자바 개발자가 필요해\"라는 문장을 이해해야 함.\n3-2. 웍스피어의 AI 에이전트 라인업\nTalent Agent (기업용)\n\n자연어 기반 소싱: 복잡한 필터링 대신 문장으로 질문하면 AI가 의도를 파악해 인재 추천.\n유사 인재 찾기: \"이 링크드인 프로필과 비슷한 사람 찾아줘\"라고 요청하면, 해당 인재의 역량과 맥락을 분석해 유사한 후보자를 웍스피어 데이터베이스(천만 명 규모)에서 발굴함.\n포워드 디플로이드 엔지니어(Forward Deployed Engineers, FDE)처럼 낯선 직무도 AI가 먼저 JD를 학습하고 적합한 스펙을 역제안함.\nCareer Agent (구직자용)\n\n단순 공고 추천을 넘어 '합격 전략'까지 제공.\n구직자의 이력과 행동 데이터를 학습해, \"이 공고는 당신의 A 경험과는 맞지만 B 역량은 보완이 필요해요\"라며 구체적인 가이드와 자소서 작성 전략까지 코칭함.\n왜 웍스피어인가? (Why Now?)\n\nAI 기술력은 기본, 핵심은 데이터의 '양과 깊이'.\n잡코리아, 알바몬, 게임잡 등 전 산업군의 데이터 + 잡플래닛의 정성적 리뷰 데이터(맥락)가 결합되어야만 진짜 '성향 파악'이 가능\n4. 이제 채용도 퍼포먼스다 (15:00)\n발표자: 박소리 JK 사업실장\n4-1. 기존 채용 상품의 문제: '성실함'의 함정\n누구나 똑같은 광고를 봄\n\n기존 배너 광고는 고정된 위치에 고정된 시간 동안 노출되는 '성실한' 상품이었음.\n하지만 마케터 공고를 시설관리자가 보거나, 부산 기업 공고를 서울 구직자가 보는 비효율 발생.\n스타트업 시절, 돈을 써서 광고를 해도 지원자가 없거나 핏이 안 맞는 지원자만 몰려서 이력서 검토하느라 시간만 쓰는 악순환을 경험함.\n4-2. 해결책: Smart Pick (스마트픽)\n집요하고 똑똑한 채용 상품\n\n타겟팅: 컨텍스트 링크 알고리즘을 통해, 내 공고에 반응할 확률이 높은 구직자에게만 선별적으로 노출함.\n성과 기반 과금: 기간제(기간당 얼마)가 아니라, 구직자가 공고를 클릭하거나 조회했을 때만 비용이 발생하는 합리적 모델 도입.\n실제 성과 사례\n\nF&B 기업: 기존 상품 대비 지원자 수 3배 증가, 비용 30% 절감.\nK-뷰티 기업: 8개월간 못 뽑던 포지션 채용 성공, 비용 60% 절감. 담당자가 \"나만 알고 싶으니 홍보하지 말라\"고 할 정도.\n4-3. 통합 관리와 편의성\nHiring Center (하이어링 센터)\n\n잡코리아, 알바몬, 외국인 채용(클릭) 등 모든 채용 공고와 지원자를 한 곳에서 관리.\n데이터 시각화 대시보드를 제공해, 인사담당자가 상사에게 \"이번 채용 광고 효율이 이렇습니다\"라고 캡처 한 방으로 보고할 수 있게 지원.",
        "guid": "https://jojoldu.tistory.com/860",
        "categories": [
          "세미나",
          "Worxphere",
          "더 리부트",
          "웍스피어",
          "잡코리아",
          "잡플래닛"
        ],
        "isoDate": "2026-01-30T00:34:09.000Z"
      },
      {
        "creator": "향로 (기억보단 기록을)",
        "title": "AI로 레거시 환경 개선하기",
        "link": "https://jojoldu.tistory.com/859",
        "pubDate": "Mon, 26 Jan 2026 10:22:35 +0900",
        "author": "향로 (기억보단 기록을)",
        "comments": "https://jojoldu.tistory.com/859#entry859comment",
        "content": "<p data-ke-size=\"size16\">요즘의 개발자 채용은 채용하는 팀과 채용하지 않는 팀으로 완전히 나뉘어 있다.</p>\n<p data-ke-size=\"size16\">비즈니스 속도가 안정권에 들어섰고, 레거시가 충분히 해소된 팀은 개발자 채용을 거의 하지 않는다.<br />누군가 퇴사를 해도 그 자리를 채우지 않는다.</p>\n<p data-ke-size=\"size16\">반면, 여전히 비즈니스 속도를 내는 팀은 공격적으로 개발자를 채용하고 있다.<br />이런 속도를 유지하는 팀은 기술 부채와 레거시를 계속해서 쌓이는 것으로만 보며, 해결해야 할 대상으로 보지 않는다.</p>\n<p data-ke-size=\"size16\">지금의 인원으로 속도가 점점 떨어진다면, 더 많은 개발자를 채용함으로써 계속해서 그 속도를 유지한다.<br />속도를 최우선으로 하면서 쌓이는 부채보다 비즈니스 성장이 떨어지는 것이 더 무서운 일이기 때문이다.</p>\n<p data-ke-size=\"size16\">개발자를 공격적으로 채용하고 있으니 그만큼 개발문화와 개발 환경이 좋을 것이라 예상했으나 첫 온보딩때 그 예상은 산산히 부서진다.</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>해당 프로젝트의 히스토리를 아는 개발자는 모두 퇴사한 상태라 알려줄 사람이 없다.</li>\n<li>개발/정책 문서가 없어서 흩어진 슬랙 메시지, 극 초기의 컨셉만 담은 노션의 몇 안되는 페이지들, 주석이 없는 애플리케이션 코드, comment가 없는 테이블 스키마 등을 보면서 분석해야 한다.</li>\n<li>시스템 구조를 알 수 있는 방법이 없어 지금 호출하고 있는 API에 신규 속성을 추가하려면 누구와 이야기해야 할지, 어느 프로젝트를 봐야 할지도 알 수 없다.</li>\n<li>테스트 코드가 없어 지금의 이 코드를 수정하면 무슨 일이 벌어질지 알 수 없다. 테스트 코드를 넣자니 인풋/아웃풋이 무엇이어야 하는지조차 알 수가 없다.</li>\n<li>히스토리와 기능이 전혀 분석이 되지 않는 상태에서 PO/PM은 새로운 기능에 대해 요구사항 분석과 언제까지 가능한지 일정을 알려달라 한다.</li>\n<li>기술 리더는 기술 부채를 해결하는 것 보다 제품지표/비즈니스 성과에 대해서 더 중요하다고 이야기한다. 레거시를 해결할 시간 따위 여기선 별도로 할당 받을 수 없는 상황이다.</li>\n</ul>\n<p data-ke-size=\"size16\">이렇게 레거시가 가득한 환경에선 도저히 일할 수 없을 것 같다는 생각이 매일 출근마다 머릿속을 가득 채운다.</p>\n<p data-ke-size=\"size16\">하지만, 지금의 개발자를 채용하는 대부분의 회사는 이렇게 레거시가 가득할 확률이 높다.<br />그렇지 않으면 이미 많은 개발자가 있음에도 더 많은 개발자를 채용할 이유가 요즘에 찾기가 어렵기 때문이다.</p>\n<p data-ke-size=\"size16\">\"레거시 코드 활용 전략\", \"리팩토링 데이터베이스\" 등 점진적으로 레거시를 해소할 여러 노하우가 담긴 책들이 많다.<br />하지만, 이 책을 지금 읽어보고 현재의 내 상황에 하나하나 대입하기엔 시간이 너무 부족하다.</p>\n<p data-ke-size=\"size16\">수습 기간의 카운트다운은 이미 시작되었다.<br />조직에서는 새로 합류한 나에 대한 평가를 계속해서 하고 있는 상황에서, \"레거시가 심해서 도저히 일할 수가 없어요\"라고만 말할 수도 없다.<br />그렇게 얘기했다가는 이 사람들이 나와 함께 할 필요가 없어지니.</p>\n<p data-ke-size=\"size16\">막막한 이 상황을 도대체 어디서부터 풀어나가야 할까?</p>\n<hr data-ke-style=\"style1\" />\n<p data-ke-size=\"size16\">다행히 이제는 개발자에겐 AI라는 막강한 도구가 있다.<br />개발자를 적극적으로 채용하는 회사로 합류하는 모든 개발자들에겐 레거시 환경과 기술 부채 환경은 기본값으로 봐야한다.<br />그리고 그런 막막한 상황에서 우린 AI의 도움을 어떻게 받을 것인지를 고민해야 한다.</p>\n<p data-ke-size=\"size16\">기존 코드를 분석해서 기능 명세서를 만드는 것도,<br />로그를 분석해서 테스트 코드를 만드는 것도,<br />암호 같던 PO/PM의 기획서를 분석하는 것도,<br />예전이라면 며칠씩 걸릴 것 같던 레거시 분석 + 기존 기능 수정이 이제는 단 몇시간만에 해결될 수 있는 상황이다.</p>\n<p data-ke-size=\"size16\">근데 레거시에 어떻게 AI의 도움을 받을 것인지도 공부해야 할 대상이 아닌가?</p>\n<hr data-ke-style=\"style1\" />\n<p data-ke-size=\"size16\">이번에 재민님의 레거시와 AI 활용편 강의가 출시되었다.<br />재민님은 17년차 개발자로, 토스페이먼츠 기술이사(Director of Engineering), 우아한형제들 서버 개발자 등을 거치며 레거시가 가득한 수많은 상황을 경험하고 해결해온 시니어이다.</p>\n<p data-ke-size=\"size16\">그리고 이젠 어떻게 하면 AI의 도움을 받으며 레거시 해결과 비즈니스 속도감을 유지하는지에 대해 노하우를 나누어 주신다.</p>\n<p data-ke-size=\"size16\">딱 9시간이면 \"막막한 레거시 환경에서 어떻게 적응하고 성과를 낼 것인가\" 에 대해 재민님의 노하우를 배워볼 수 있다.</p>\n<p data-ke-size=\"size16\">새 회사에, 새 팀에 합류해야 할 개발자분들이나,<br />현재 기술 부채가 있는 환경에서 일하고 있는 개발자분들이라면 이번 재민님의 강의를 적극 추천한다.</p>\n<p data-ke-size=\"size16\"><a href=\"https://inf.run/ZzaQn\">https://inf.run/ZzaQn</a></p>\n<blockquote data-ke-style=\"style2\">\n<p data-ke-size=\"size16\">현재 <b>얼리버드로 30% 할인 중</b>이니 할인 기회를 놓치지 않으시길 추천드린다.</p>\n</blockquote>",
        "contentSnippet": "요즘의 개발자 채용은 채용하는 팀과 채용하지 않는 팀으로 완전히 나뉘어 있다.\n비즈니스 속도가 안정권에 들어섰고, 레거시가 충분히 해소된 팀은 개발자 채용을 거의 하지 않는다.\n누군가 퇴사를 해도 그 자리를 채우지 않는다.\n반면, 여전히 비즈니스 속도를 내는 팀은 공격적으로 개발자를 채용하고 있다.\n이런 속도를 유지하는 팀은 기술 부채와 레거시를 계속해서 쌓이는 것으로만 보며, 해결해야 할 대상으로 보지 않는다.\n지금의 인원으로 속도가 점점 떨어진다면, 더 많은 개발자를 채용함으로써 계속해서 그 속도를 유지한다.\n속도를 최우선으로 하면서 쌓이는 부채보다 비즈니스 성장이 떨어지는 것이 더 무서운 일이기 때문이다.\n개발자를 공격적으로 채용하고 있으니 그만큼 개발문화와 개발 환경이 좋을 것이라 예상했으나 첫 온보딩때 그 예상은 산산히 부서진다.\n해당 프로젝트의 히스토리를 아는 개발자는 모두 퇴사한 상태라 알려줄 사람이 없다.\n개발/정책 문서가 없어서 흩어진 슬랙 메시지, 극 초기의 컨셉만 담은 노션의 몇 안되는 페이지들, 주석이 없는 애플리케이션 코드, comment가 없는 테이블 스키마 등을 보면서 분석해야 한다.\n시스템 구조를 알 수 있는 방법이 없어 지금 호출하고 있는 API에 신규 속성을 추가하려면 누구와 이야기해야 할지, 어느 프로젝트를 봐야 할지도 알 수 없다.\n테스트 코드가 없어 지금의 이 코드를 수정하면 무슨 일이 벌어질지 알 수 없다. 테스트 코드를 넣자니 인풋/아웃풋이 무엇이어야 하는지조차 알 수가 없다.\n히스토리와 기능이 전혀 분석이 되지 않는 상태에서 PO/PM은 새로운 기능에 대해 요구사항 분석과 언제까지 가능한지 일정을 알려달라 한다.\n기술 리더는 기술 부채를 해결하는 것 보다 제품지표/비즈니스 성과에 대해서 더 중요하다고 이야기한다. 레거시를 해결할 시간 따위 여기선 별도로 할당 받을 수 없는 상황이다.\n이렇게 레거시가 가득한 환경에선 도저히 일할 수 없을 것 같다는 생각이 매일 출근마다 머릿속을 가득 채운다.\n하지만, 지금의 개발자를 채용하는 대부분의 회사는 이렇게 레거시가 가득할 확률이 높다.\n그렇지 않으면 이미 많은 개발자가 있음에도 더 많은 개발자를 채용할 이유가 요즘에 찾기가 어렵기 때문이다.\n\"레거시 코드 활용 전략\", \"리팩토링 데이터베이스\" 등 점진적으로 레거시를 해소할 여러 노하우가 담긴 책들이 많다.\n하지만, 이 책을 지금 읽어보고 현재의 내 상황에 하나하나 대입하기엔 시간이 너무 부족하다.\n수습 기간의 카운트다운은 이미 시작되었다.\n조직에서는 새로 합류한 나에 대한 평가를 계속해서 하고 있는 상황에서, \"레거시가 심해서 도저히 일할 수가 없어요\"라고만 말할 수도 없다.\n그렇게 얘기했다가는 이 사람들이 나와 함께 할 필요가 없어지니.\n막막한 이 상황을 도대체 어디서부터 풀어나가야 할까?\n다행히 이제는 개발자에겐 AI라는 막강한 도구가 있다.\n개발자를 적극적으로 채용하는 회사로 합류하는 모든 개발자들에겐 레거시 환경과 기술 부채 환경은 기본값으로 봐야한다.\n그리고 그런 막막한 상황에서 우린 AI의 도움을 어떻게 받을 것인지를 고민해야 한다.\n기존 코드를 분석해서 기능 명세서를 만드는 것도,\n로그를 분석해서 테스트 코드를 만드는 것도,\n암호 같던 PO/PM의 기획서를 분석하는 것도,\n예전이라면 며칠씩 걸릴 것 같던 레거시 분석 + 기존 기능 수정이 이제는 단 몇시간만에 해결될 수 있는 상황이다.\n근데 레거시에 어떻게 AI의 도움을 받을 것인지도 공부해야 할 대상이 아닌가?\n이번에 재민님의 레거시와 AI 활용편 강의가 출시되었다.\n재민님은 17년차 개발자로, 토스페이먼츠 기술이사(Director of Engineering), 우아한형제들 서버 개발자 등을 거치며 레거시가 가득한 수많은 상황을 경험하고 해결해온 시니어이다.\n그리고 이젠 어떻게 하면 AI의 도움을 받으며 레거시 해결과 비즈니스 속도감을 유지하는지에 대해 노하우를 나누어 주신다.\n딱 9시간이면 \"막막한 레거시 환경에서 어떻게 적응하고 성과를 낼 것인가\" 에 대해 재민님의 노하우를 배워볼 수 있다.\n새 회사에, 새 팀에 합류해야 할 개발자분들이나,\n현재 기술 부채가 있는 환경에서 일하고 있는 개발자분들이라면 이번 재민님의 강의를 적극 추천한다.\nhttps://inf.run/ZzaQn\n현재 얼리버드로 30% 할인 중이니 할인 기회를 놓치지 않으시길 추천드린다.",
        "guid": "https://jojoldu.tistory.com/859",
        "categories": [
          "생각정리",
          "기술 부채",
          "김재민",
          "레거시",
          "레거시코드 활용 전략",
          "리팩토링 데이터베이스",
          "토스"
        ],
        "isoDate": "2026-01-26T01:22:35.000Z"
      },
      {
        "creator": "향로 (기억보단 기록을)",
        "title": "기다리기",
        "link": "https://jojoldu.tistory.com/858",
        "pubDate": "Sat, 24 Jan 2026 22:20:49 +0900",
        "author": "향로 (기억보단 기록을)",
        "comments": "https://jojoldu.tistory.com/858#entry858comment",
        "content": "<p data-ke-size=\"size16\">샤워를 할 때 수온 조절이 바로 되지 않을 때가 있다.<br />온수 쪽으로 손잡이를 돌렸는데 따뜻한 물이 나오지 않으면, 조급해져서 곧바로 반대쪽으로 돌리게 된다.<br />그렇게 이리저리 돌리다 보면 정작 어느 방향이 온수인지 알 수 없게 된다.<br />한쪽으로 돌려놓고 조금만 기다리면 될 일인데.</p>\n<p data-ke-size=\"size16\">식사할 때도 비슷한 일이 벌어진다.<br />밥을 먹는 중에는 배부름이 잘 느껴지지 않는다.<br />그러다 잠깐 자리에서 일어나 물을 가져오거나, 다른 일을 하다가 돌아오면 갑자기 배가 부른 것을 느끼게 된다.<br />식사와 포만감 사이에는 시간 지연이 있기 때문이다.<br />이 지연을 모르면 이미 충분히 먹었는데도 계속 먹게 되고, 결국 과식하게 된다.</p>\n<p data-ke-size=\"size16\">우리는 일상에서 이런 '지연'을 자주 경험한다.<br />그리고 대부분의 경우, 그것이 지연이라는 것도 알고 있다.</p>\n<hr data-ke-style=\"style1\" />\n<p data-ke-size=\"size16\">그런데 조직에서는 이 지연을 쉽게 잊는다.</p>\n<p data-ke-size=\"size16\">팀에 리소스가 부족해서 사람을 채용했다고 하자.<br />하지만 그 사람이 지금 당장 팀의 부족함을 채워주진 못한다.<br />조직에 적응하고, 업무 맥락을 익히고, 실제로 역량을 발휘하기까지는 최소 3~6개월의 시간이 필요하다.</p>\n<p data-ke-size=\"size16\">그 사이에도 팀은 여전히 리소스가 부족하다고 느낀다.<br />마치 샤워할 때 온수 손잡이를 돌렸는데 따뜻한 물이 나오지 않는 것처럼.<br />그래서 더 채용한다.<br />또 채용한다.</p>\n<p data-ke-size=\"size16\">그러다 어느 순간, <b>채용한 사람들이 모두 적응을 마치고 본격적으로 역량을 발휘하기 시작</b>한다.<br />그때서야 \"왜 이렇게 사람이 많지?\"라는 생각이 든다.<br />밥 먹을 때는 배부르지 않았는데, 일어나서 돌아오니 배가 터질 것 같은 것처럼.</p>\n<p data-ke-size=\"size16\">새로운 기능을 출시했는데 첫 1~2주간 반응이 좋지 않다고 하자.<br />실패한 걸까?<br />아직 알 수 없다.</p>\n<p data-ke-size=\"size16\"><b>고객이 새로운 기능을 인지하고, 학습하고, 습관처럼 사용하기까지는 생각보다 긴 시간이 필요하다.</b><br />샤워 손잡이를 돌리고 따뜻한 물이 나올 때까지 기다려야 하는 것처럼.</p>\n<p data-ke-size=\"size16\">반대로 출시 직후 반응이 폭발적이었다고 해서 그것이 진짜 성공인지도 모른다.<br />초기의 호기심에 의한 사용인지, 이전에 쌓아둔 기능들이 이제서야 효과를 내기 시작한 것인지, 정말 고객의 문제를 해결해주는 제품인지는 <b>시간이 지나고 사용량이 유지되는지를 봐야만</b> 알 수 있다.</p>\n<p data-ke-size=\"size16\">관계도 마찬가지다.</p>\n<p data-ke-size=\"size16\"><b>동료에게 무례한 말을 했을 때, 그 여파가 바로 나타나지는 않는다.</b><br />한두 번의 실수는 괜찮아 보인다.<br />상대방도 웃어넘기고, 분위기도 별로 나빠지지 않는다.</p>\n<p data-ke-size=\"size16\">하지만 그것이 쌓이면 어느 순간 관계가 무너진다.<br />밥 먹을 때는 배부르지 않았는데, 일어서니 배가 터질 것 같은 것처럼.<br /><b>그때는 이미 늦다</b>.</p>\n<hr data-ke-style=\"style1\" />\n<p data-ke-size=\"size16\"><b>행동과 결과 사이에는 항상 어느 정도의 지연이 있다</b>.</p>\n<p data-ke-size=\"size16\">지금 느끼는 불안이나 조급함을 해소하려고 같은 행동을 계속 반복하면, 나중에 그 행동들이 한꺼번에 쏟아져 새로운 문제가 된다.</p>\n<p data-ke-size=\"size16\">샤워할 때는 손잡이를 돌려놓고 잠시 기다릴 줄 안다.<br />식사할 때는 천천히 먹으면 과식을 피할 수 있다는 것도 안다.</p>\n<p data-ke-size=\"size16\">조직에서도 그렇게 해보자.<br />채용을 했다면, 그 사람이 자리 잡을 시간을 주자.<br />기능을 출시했다면, 고객이 적응할 시간을 주자.<br />관계에서 실수를 했다면, 지금 당장 괜찮아 보여도 되돌아보자.</p>\n<p data-ke-size=\"size16\">행동을 했다면, 조금은 기다려보자.</p>",
        "contentSnippet": "샤워를 할 때 수온 조절이 바로 되지 않을 때가 있다.\n온수 쪽으로 손잡이를 돌렸는데 따뜻한 물이 나오지 않으면, 조급해져서 곧바로 반대쪽으로 돌리게 된다.\n그렇게 이리저리 돌리다 보면 정작 어느 방향이 온수인지 알 수 없게 된다.\n한쪽으로 돌려놓고 조금만 기다리면 될 일인데.\n식사할 때도 비슷한 일이 벌어진다.\n밥을 먹는 중에는 배부름이 잘 느껴지지 않는다.\n그러다 잠깐 자리에서 일어나 물을 가져오거나, 다른 일을 하다가 돌아오면 갑자기 배가 부른 것을 느끼게 된다.\n식사와 포만감 사이에는 시간 지연이 있기 때문이다.\n이 지연을 모르면 이미 충분히 먹었는데도 계속 먹게 되고, 결국 과식하게 된다.\n우리는 일상에서 이런 '지연'을 자주 경험한다.\n그리고 대부분의 경우, 그것이 지연이라는 것도 알고 있다.\n그런데 조직에서는 이 지연을 쉽게 잊는다.\n팀에 리소스가 부족해서 사람을 채용했다고 하자.\n하지만 그 사람이 지금 당장 팀의 부족함을 채워주진 못한다.\n조직에 적응하고, 업무 맥락을 익히고, 실제로 역량을 발휘하기까지는 최소 3~6개월의 시간이 필요하다.\n그 사이에도 팀은 여전히 리소스가 부족하다고 느낀다.\n마치 샤워할 때 온수 손잡이를 돌렸는데 따뜻한 물이 나오지 않는 것처럼.\n그래서 더 채용한다.\n또 채용한다.\n그러다 어느 순간, 채용한 사람들이 모두 적응을 마치고 본격적으로 역량을 발휘하기 시작한다.\n그때서야 \"왜 이렇게 사람이 많지?\"라는 생각이 든다.\n밥 먹을 때는 배부르지 않았는데, 일어나서 돌아오니 배가 터질 것 같은 것처럼.\n새로운 기능을 출시했는데 첫 1~2주간 반응이 좋지 않다고 하자.\n실패한 걸까?\n아직 알 수 없다.\n고객이 새로운 기능을 인지하고, 학습하고, 습관처럼 사용하기까지는 생각보다 긴 시간이 필요하다.\n샤워 손잡이를 돌리고 따뜻한 물이 나올 때까지 기다려야 하는 것처럼.\n반대로 출시 직후 반응이 폭발적이었다고 해서 그것이 진짜 성공인지도 모른다.\n초기의 호기심에 의한 사용인지, 이전에 쌓아둔 기능들이 이제서야 효과를 내기 시작한 것인지, 정말 고객의 문제를 해결해주는 제품인지는 시간이 지나고 사용량이 유지되는지를 봐야만 알 수 있다.\n관계도 마찬가지다.\n동료에게 무례한 말을 했을 때, 그 여파가 바로 나타나지는 않는다.\n한두 번의 실수는 괜찮아 보인다.\n상대방도 웃어넘기고, 분위기도 별로 나빠지지 않는다.\n하지만 그것이 쌓이면 어느 순간 관계가 무너진다.\n밥 먹을 때는 배부르지 않았는데, 일어서니 배가 터질 것 같은 것처럼.\n그때는 이미 늦다.\n행동과 결과 사이에는 항상 어느 정도의 지연이 있다.\n지금 느끼는 불안이나 조급함을 해소하려고 같은 행동을 계속 반복하면, 나중에 그 행동들이 한꺼번에 쏟아져 새로운 문제가 된다.\n샤워할 때는 손잡이를 돌려놓고 잠시 기다릴 줄 안다.\n식사할 때는 천천히 먹으면 과식을 피할 수 있다는 것도 안다.\n조직에서도 그렇게 해보자.\n채용을 했다면, 그 사람이 자리 잡을 시간을 주자.\n기능을 출시했다면, 고객이 적응할 시간을 주자.\n관계에서 실수를 했다면, 지금 당장 괜찮아 보여도 되돌아보자.\n행동을 했다면, 조금은 기다려보자.",
        "guid": "https://jojoldu.tistory.com/858",
        "categories": [
          "생각정리",
          "리더십",
          "스타트업",
          "조직 생활",
          "채용",
          "학습하는 조직"
        ],
        "isoDate": "2026-01-24T13:20:49.000Z"
      }
    ]
  },
  {
    "name": "WestwoodForever's Dev Log",
    "category": "개인",
    "posts": []
  },
  {
    "name": "허니몬(Honeymon)의 자바guru",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Wolf Loves Fox :: 일상",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Game Programmer Life",
    "category": "개인",
    "posts": []
  },
  {
    "name": "yuchi's dev",
    "category": "개인",
    "posts": []
  },
  {
    "name": "만화로 나누는 자유/오픈소스 소프트웨어 이야기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "신현석(Hyeonseok Shin)",
    "category": "개인",
    "posts": []
  },
  {
    "name": "즐거운 개발자 :: 네이버  블로그",
    "category": "개인",
    "posts": [
      {
        "creator": "dlehddus84",
        "title": "아르낙 빅박스 모험의 궤의 검은 석판 도색",
        "link": "https://blog.naver.com/dlehddus84/224160512478?fromRss=true&trackingCode=rss",
        "pubDate": "Mon, 26 Jan 2026 19:14:30 +0900",
        "author": "dlehddus84",
        "content": "기존 아르낙의 석판을 도색했는데 원래는 아이보리 색인데 석판 느낌 낸다고 회색으로 도색했는데 이번에 검은색 석판이 나왔다... 드라이브러싱을 회색으로 해버리면 서로 컴포가 비슷한 색으로 수렴하기에 이번 검은석판은 글자 부분에만 붉은 색을 넣어주는식으로 마무리 해주었다. 프라이밍도 건너뛰고 음각이 있는 부분에 색을 칠해줄거라 마감제도 하지 않았다. ak사의 Deep Red로 글자부분을 2회 칠해주었다. 어짜피 벗어난 부분은 다 지워줄것이기에 삐져나오는것에 신경쓰지 말고 잘 칠해주었다. 그리고 삐저나온 부분을 지워주었다. 화장솜에 아크릴 물감을 녹일 수 있는 용액으로 지우면 된다. 나는 이소프로필알코올을 대량사놓은것....... <img src=\"https://blogthumb.pstatic.net/MjAyNjAxMjZfMTc0/MDAxNzY5NDIyMDkzNjQz.q4V3ZPeEAcRpHo0H5OOi0H7MyaOpeuzQ_XE7pKzCni0g.VLH1VJKXgthz5_6pwaWggITptuFfUhix8N-GfyoEPuUg.JPEG/KakaoTalk_20260126_190220817.jpg?type=s3\" />",
        "contentSnippet": "기존 아르낙의 석판을 도색했는데 원래는 아이보리 색인데 석판 느낌 낸다고 회색으로 도색했는데 이번에 검은색 석판이 나왔다... 드라이브러싱을 회색으로 해버리면 서로 컴포가 비슷한 색으로 수렴하기에 이번 검은석판은 글자 부분에만 붉은 색을 넣어주는식으로 마무리 해주었다. 프라이밍도 건너뛰고 음각이 있는 부분에 색을 칠해줄거라 마감제도 하지 않았다. ak사의 Deep Red로 글자부분을 2회 칠해주었다. 어짜피 벗어난 부분은 다 지워줄것이기에 삐져나오는것에 신경쓰지 말고 잘 칠해주었다. 그리고 삐저나온 부분을 지워주었다. 화장솜에 아크릴 물감을 녹일 수 있는 용액으로 지우면 된다. 나는 이소프로필알코올을 대량사놓은것.......",
        "guid": "https://blog.naver.com/dlehddus84/224160512478",
        "categories": [
          "도색과 제작"
        ],
        "isoDate": "2026-01-26T10:14:30.000Z"
      },
      {
        "creator": "dlehddus84",
        "title": "군제 프로콘 보이 직구",
        "link": "https://blog.naver.com/dlehddus84/224158032060?fromRss=true&trackingCode=rss",
        "pubDate": "Sat, 24 Jan 2026 12:11:50 +0900",
        "author": "dlehddus84",
        "content": "10년 전부터 입문용으로 써오던 에어브러쉬가 아직도 잘 되는데 스팩을 좀 높이고 싶어서 가성비 제품으로 일마존에서 직구했다. 국내 가격이랑 차이가 많이 나서 직구를 하면 가격을 많이 저렴하게 살 수 있다. 일단 네이버에 군제 프로콘 보이를 검색했을때 나오는 가격이다. 내가 구매할것은 3호로 범용적으로 쓰기 좋은 모델이다. 정확히는 3호 WA 플래티넘 Ver.2 더블액션 모델을 구입할 예정인데 네이버 쇼핑에서는 15만원 후반대에 가격이 형성되어 있다. 엔화도 그리 비싸지 않아서 일마존에서 해당 제품을 찾아봤다. 세부 모델명인 PS289로 검색해도 나온다. 배송비를 제외하면 11520엔으로 원화로는 10만7천원 정도 나오는데 많이 저렴....... <img src=\"https://blogthumb.pstatic.net/MjAyNjAxMjRfNjgg/MDAxNzY5MjIzMDQ3MzE2.wMSxwpy1IYwgFvjKLn4I2pEH2YEx-XM7Ifuu0ahn1ZAg.Kh7XJshlfCbZ0fbEE0MYNIC6POJWC7cWpsQMdnczZ3Ug.JPEG/%C8%AD%B8%E9_%C4%B8%C3%B3_2026-01-24_114703.jpg?type=s3\" />",
        "contentSnippet": "10년 전부터 입문용으로 써오던 에어브러쉬가 아직도 잘 되는데 스팩을 좀 높이고 싶어서 가성비 제품으로 일마존에서 직구했다. 국내 가격이랑 차이가 많이 나서 직구를 하면 가격을 많이 저렴하게 살 수 있다. 일단 네이버에 군제 프로콘 보이를 검색했을때 나오는 가격이다. 내가 구매할것은 3호로 범용적으로 쓰기 좋은 모델이다. 정확히는 3호 WA 플래티넘 Ver.2 더블액션 모델을 구입할 예정인데 네이버 쇼핑에서는 15만원 후반대에 가격이 형성되어 있다. 엔화도 그리 비싸지 않아서 일마존에서 해당 제품을 찾아봤다. 세부 모델명인 PS289로 검색해도 나온다. 배송비를 제외하면 11520엔으로 원화로는 10만7천원 정도 나오는데 많이 저렴.......",
        "guid": "https://blog.naver.com/dlehddus84/224158032060",
        "categories": [
          "도색과 제작"
        ],
        "isoDate": "2026-01-24T03:11:50.000Z"
      }
    ]
  },
  {
    "name": "황제펭귄의 게임개발이야기 [여기는 한국]",
    "category": "개인",
    "posts": []
  },
  {
    "name": "LINE ENGINEERING",
    "category": "기업",
    "posts": [
      {
        "title": "코드 품질 개선 기법 30편: (투명한) 운명의 붉은 실",
        "link": "https://techblog.lycorp.co.jp/ko/techniques-for-improving-code-quality-30",
        "pubDate": "Fri, 23 Jan 2026 08:00:00 GMT",
        "content": "이 글은 2024년 6월 20일에 일본어로 먼저 발행된 기사를 번역한 글입니다.LY Corporation은 높은 개발 생산성을 유지하기 위해 코드 품질 및 개발 문화 개선에 힘쓰고...",
        "contentSnippet": "이 글은 2024년 6월 20일에 일본어로 먼저 발행된 기사를 번역한 글입니다.LY Corporation은 높은 개발 생산성을 유지하기 위해 코드 품질 및 개발 문화 개선에 힘쓰고...",
        "guid": "https://techblog.lycorp.co.jp/ko/techniques-for-improving-code-quality-30",
        "isoDate": "2026-01-23T08:00:00.000Z"
      }
    ]
  },
  {
    "name": "뱅크샐러드 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "우아한형제들 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "TOAST Meetup",
    "category": "기업",
    "posts": []
  },
  {
    "name": "ZUM 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "SK Planet",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Spoqa tech blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "팀 왈도 번역팀",
    "category": "게임",
    "posts": []
  },
  {
    "name": "근원님",
    "category": "개인",
    "posts": []
  },
  {
    "name": "황의윤",
    "category": "개인",
    "posts": []
  },
  {
    "name": "호돌맨",
    "category": "개인",
    "posts": []
  },
  {
    "name": "박우빈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "문다영",
    "category": "개인",
    "posts": []
  },
  {
    "name": "유수민",
    "category": "개인",
    "posts": []
  },
  {
    "name": "안건주",
    "category": "개인",
    "posts": []
  },
  {
    "name": "손현호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "STARTUP BIBLE",
    "category": "개인",
    "posts": [
      {
        "creator": "Kihong Bae",
        "title": "휴먼지능 할루시네이션",
        "link": "https://www.thestartupbible.com/2026/01/the-human-intelligence-hallucination.html",
        "pubDate": "Wed, 28 Jan 2026 19:41:17 +0000",
        "content:encodedSnippet": "VC가 수많은 창업가 중 성공할 만한 사람을 찾아내서 투자하고, 수많은 아이디어 중 성공할 만한 사업을 찾아내서 투자하는 과정을 흔히 pattern recognition이라고 한다. 본인이 지금까지 쌓았던 경험, 그리고 이 경험을 바탕으로 했던 무수히 많은 옳은 결정과 틀린 결정을 참고해서 성공하는 사업과 창업가의 패턴을 찾아서 성공과 가장 공통점이 많은 곳에 투자하고, 실패와 가장 공통점이 많은 곳은 피하는 방법이다. 이는 마치 인공지능이 수많은 데이터 포인트를 학습하고 체크하는 과정에서 다양한 패턴을 발견하고 연관 짓는 방법과 큰 개념에서는 유사한 점이 많다고 생각하는데, VC들에겐 과거의 개인적인 경험, 판단, 결정, 이 모든 것들의 총체가 이들만의 거대언어모델이라고 해도 될 것 같다.\n나도 VC로서의 14년 동안의 경험에서 얻은 엄청나게 많은 data point가 머리와 가슴속에 나만의 거대 모델로 존재한다. 그리고 새로운 창업가나 비즈니스를 만나게 되면, 나만의 거대 모델에서 그 어떤 패턴을 찾으려고 휴먼지능을 열심히 돌려본다. 과거에 비슷한 비즈니스 모델을 봤는데, 그 사업이 잘 안됐다면, 내 휴먼지능은 그냥 패스하라는 내면의 목소리를 내고, 과거에 비슷한 성향의 창업가에게 투자했는데 크게 성공했다면, 내 휴먼지능은 투자하라는 내면의 목소리를 강하게 낸다.\n실은, 모든 게 이렇게 간단했으면 좋겠지만, 내 안의 데이터 포인트들이 여간 많은 게 아니다. 비슷한 비즈니스 모델을 여러 개 봤고, 몇 군데 이미 투자까지 했는데, 어떤 건 되고 어떤 건 안 됐다면, 과연 여기서 내 휴먼지능은 무슨 패턴을 발견하고 어떤 조언을 나에게 해줄까?\n좋은 학교 나오고 어떤 스타트업의 초기 멤버로서 이 회사가 유니콘으로 성장하는 데 막대한 기여를 한 창업가에겐 투자해야 하나 말아야 하나? 이 분과 비슷한 경험을 한 창업가들을 무수히 만났는데, 대부분 잘 안됐다면 내 휴먼지능은 어떤 판단을 내릴까? 그런데 잘 안된 창업가가 그다음 사업을 했을 땐 대박 났다면, 이런 경우라면 우리는 투자해야 하나 말아야 하나?\n한국에서 규제 때문에 절대로 할 수 없는 사업은? 물론, 이런 사업에는 투자를 안 하는 게 자연스러운 결정이다. 하지만, 내 거대 모델 안에는 규제받는 산업에서 틈새를 잘 찾고, 이 틈새를 크게 만들었던 데이터포인트도 있는데, 그러면 투자해야 하는 것일까? 아, 그런데 휴먼지능을 조금만 더 돌려보면, 전에 이런 논리로 크게 투자했는데 대박 망한 회사들이 여러 개 있다는 데이터포인트도 있는데, 그러면 여기서 내가 얻을 수 있는 패턴은 도대체 어떻게 생겼을까?\n우린 매일 5개 정도의 회사를 만나는데, 대부분의 미팅을 하면서 내 머릿속에서는 이런 수많은 데이터포인트를 기반으로 패턴을 찾으려는 휴먼지능이 과부하 되면서 열심히 돌아가고 있다. 참 혼란스러운 게, 인공지능이라면 데이터포인트가 더 많이 쌓일수록 결과의 정확도가 더 높아져야 하는데, 휴먼지능의 경우 – 내 휴먼 지능 – 데이터포인트가 더 많아질수록 할루시네이션의 확률 또한 높아져서 투자 결정이 점점 더 어려워지고 있다. \n그래서 나는 요새 이런 식으로 투자 결정을 한다. 비슷한 사업이고, 비슷한 창업가이고, 비슷한 산업이고, 비슷한 시기에 시작했는데, 과거에 어떤 사업은 잘됐고 어떤 사업은 잘 안됐다면, 잠시 휴먼지능은 완전히 꺼버리고 그냥 내 직감에 의존한다. 내 직감은 맞을 수도 있고 틀릴 수도 있지만, 최소한 환각을 일으키진 않을 것이니까.",
        "dc:creator": "Kihong Bae",
        "comments": "https://www.thestartupbible.com/2026/01/the-human-intelligence-hallucination.html#respond",
        "content": "VC가 수많은 창업가 중 성공할 만한 사람을 찾아내서 투자하고, 수많은 아이디어 중 성공할 만한 사업을 찾아내서 투자하는 과정을 흔히 pattern recognition이라고 한다. 본인이 지금까지 쌓았던 경험, 그리고 이 경험을 바탕으로 했던 무수히 많은 옳은 결정과 틀린 결정을 참고해서 성공하는 사업과 창업가의 패턴을 찾아서 성공과 가장 공통점이 많은 곳에 투자하고, 실패와 가장 공통점이 많은 곳은 피하는(...)",
        "contentSnippet": "VC가 수많은 창업가 중 성공할 만한 사람을 찾아내서 투자하고, 수많은 아이디어 중 성공할 만한 사업을 찾아내서 투자하는 과정을 흔히 pattern recognition이라고 한다. 본인이 지금까지 쌓았던 경험, 그리고 이 경험을 바탕으로 했던 무수히 많은 옳은 결정과 틀린 결정을 참고해서 성공하는 사업과 창업가의 패턴을 찾아서 성공과 가장 공통점이 많은 곳에 투자하고, 실패와 가장 공통점이 많은 곳은 피하는(...)",
        "guid": "https://www.thestartupbible.com/?p=9675",
        "categories": [
          "Uncategorized",
          "ai",
          "people",
          "Strong",
          "vc"
        ],
        "isoDate": "2026-01-28T19:41:17.000Z"
      },
      {
        "creator": "Kihong Bae",
        "title": "기세",
        "link": "https://www.thestartupbible.com/2026/01/confidence-and-swag.html",
        "pubDate": "Sun, 25 Jan 2026 21:28:00 +0000",
        "content:encodedSnippet": "창업가나 투자자라면 ‘피칭’이라는 말이 너무나 익숙할 것이다. 창업가라면 투자받기 위해서 VC들을 대상으로 셀 수 없을 정도로 사업에 관해서 설명하는 피칭을 했을 것이다. 우리도 작년에 수백 개의 피칭을 듣고 봤다. 그런데 우리 같은 VC도 투자하기 위해서는 남의 투자를 받아야 해서 우리도 펀드레이징을 하고, 꽤 많은 피칭을 한다. 우리는 작년 9월에 새로운 펀드를 만들었는데, 나중에 통계를 내보니까 이 펀드를 만들기 위해서 180명이 넘는 투자자를(=LP) 대상으로 피칭했다. 어떤 분들은 그냥 줌으로 한 번 온라인 미팅만 했지만, 어떤 해외 투자자는 10번 이상 대면 미팅한 경우도 있었다.\n물론, 이들이 스트롱에게 모두 다 돈을 준 건 아니다. 이 중 일부만 우리에게 출자했고, 대부분의 미팅을 전쟁에 임하는 태도로 열심히, 에너지 넘치게, 그리고 기세 넘치게 진행했기 때문에, 특히나 우리에게 자금을 출자한 분들과의 미팅은 아직도 머릿속에 생생하게 기억한다. 이 중 기억에 남는 미팅이 하나 있는데, 당시의 상황을 간략하게 설명하면, 펀드를 마무리해야 하는 데드라인이 몇 주 안 남았었고, 이 투자자의 돈은 꼭 받아야만 하는 상황이었다. 그런데 실은, 이미 이 투자자에게 과거에 두 번 피칭했다 두 번 모두 거절당했기 때문에, 세 번째 시도는(=삼수) 마지막 기회였다. 절박함에 대해서 우리는 자주 이야기하는데, 당시 내 심정은 절박함 그 자체였고, 정말로 비장한 각오로 줌 피칭 미팅을 시작했다.\n피칭하기 바로 전날 밤으로 상황을 리와인드 해보자. 실은 발표해야 하는 내용은 내가 14년 동안 직접 해왔던 거라서 아주 익숙했지만, 그래도 나는 전날 밤에 10번 연달아서 리허설을 했다. 약 20분 정도의 발표이니 거의 세 시간을 같은 내용을 미친 사람처럼 달달 다시 연습했던 것이다. 줌으로 파워포인트 발표를 해본 분들은 잘 아시겠지만, 화면 공유할 때 파워포인트의 포인터/펜 기능이 가끔 충돌을 일으킬 때가 있고, 강조해야 할 부분을 빨간펜으로 체크하면서 발표하다 보면 그다음 슬라이드로 넘어가지 않는 알려진 문제점도 있다. 나는 발표의 흐름을 끊을 수 있는 이런 문제점들을 사전에 모두 방지하기 위해서, 그 전날 10번 리허설을 상대편에는 아무도 없지만, 실제로 줌을 켜고 라이브로 연습했다. 그리고 연습하면서 파워포인트의 포인터 -> 펜 -> 줌의 화면 공유 상에서의 페이지 넘기기, 이 전환이 매끄럽게 될 수 있게 충분히 연습했다.\n다시 실제 피칭하는 순간으로 패스트포워드 해보자. 전날 연습을 충분히 했기 때문에 나는 자신감이 넘쳤고, 이번에도 돈을 못 받는 건 고려 대상이 아니라는 벼랑 끝에 선 각오로 엄청나게 기세 있게 발표했다. 온라인이고 작은 노트북 화면에서 발표하는 거지만, 반대편에 보이는 약 10명의 청중의 눈을 하나씩 맞추려고 노력했고, 엄청난 기세를 이들에게 100% 전달하겠다는 각오로 거의 샤우팅 하듯이 피칭했다. 그리고 이분들에게 우리는 돈을 받았다는 해피엔딩으로 이 피칭 이야기는 끝난다.\n나중에, 이 LP들에게 들었다. 화면으로만 나를 보고 들었지만, 정말로 스트롱이 돈을 꼭 받아야겠다는 독기를 품었다는 각오가 강하게 느껴졌고, 정말 그 엄청난 기세가 줌 화면을 통해서도 생생하게 온몸으로 전달됐다고.\n“인생은 기세다.”라는 말을 우리는 자주 한다. 밑바닥에서 시작해서 성공한 연예인들이 이 말을 하는 것을 나는 자주 들었다. 그런데 나도 좀 살아보고, 일도 좀 해보고, 투자도 좀 해보니 정말로 이 기세가 중요하다는 걸 느끼고 있다. 창업도 기세, 펀드레이징도 기세, 글로벌 진출도 기세, 심지어 우리 같은 VC가 하는 벤처 투자도 기세다. 이 모든 게 안 될 이유가 백만 가지가 있는, 했다 하면 실패할 게 거의 뻔한 일들이다. 그 와중에 되게 만들어야 하고, 되게 할 이유를 찾아야 하는데, 이건 자신감과 기세가 없으면 매우 힘든 일이다. 우리 주변에 불가능을 가능케 하는 사람들을 보면, 가끔 나는 이들이 짐승 같다고 생각한다. 그만큼 본인들이 목표하는 걸 무조건 해야겠다는 의지와 이를 막는 사람들은 모두 다 짐승같이 씹어 먹어버리겠다는 기세가 보이기 때문이다.\n기세는 실제로 그 일을 하는 사람에게 발동을 걸고, 누가 봐도 안 될 일을 계속 시도하게 만드는 희망과 에너지의 원동력이다. 하지만, 기세는 그 일을 하는 사람의 힘든 여정을 같이 하는 주변 사람들에게도 전류와 같이 흐르면서 안 될 일을 되게 만드는 방향으로 기운을 흐르게 만들 수도 있다고 생각한다. 내가 피칭할 때 내 기세가 분명히 발표를 듣는 분들에게도 전달됐고, 이들의 결정에 긍정적인 영향을 미쳤다고 나는 믿는다.\n그럼, 이 기세는 어디서 나오는 건가? 내 경험에 의하면 기세는 누구나 다 후천적으로 습득할 수 있는 기술이자 자산이다. 기세는 자신감에서 나온다. 그럼, 자신감은 어디서 나오나? 결국엔 수년, 수십 년 동안 반복하는 좋은 습관과 연습에서 나온다.",
        "dc:creator": "Kihong Bae",
        "comments": "https://www.thestartupbible.com/2026/01/confidence-and-swag.html#comments",
        "content": "창업가나 투자자라면 ‘피칭’이라는 말이 너무나 익숙할 것이다. 창업가라면 투자받기 위해서 VC들을 대상으로 셀 수 없을 정도로 사업에 관해서 설명하는 피칭을 했을 것이다. 우리도 작년에 수백 개의 피칭을 듣고 봤다. 그런데 우리 같은 VC도 투자하기 위해서는 남의 투자를 받아야 해서 우리도 펀드레이징을 하고, 꽤 많은 피칭을 한다. 우리는 작년 9월에 새로운 펀드를 만들었는데, 나중에 통계를 내보니까(...)",
        "contentSnippet": "창업가나 투자자라면 ‘피칭’이라는 말이 너무나 익숙할 것이다. 창업가라면 투자받기 위해서 VC들을 대상으로 셀 수 없을 정도로 사업에 관해서 설명하는 피칭을 했을 것이다. 우리도 작년에 수백 개의 피칭을 듣고 봤다. 그런데 우리 같은 VC도 투자하기 위해서는 남의 투자를 받아야 해서 우리도 펀드레이징을 하고, 꽤 많은 피칭을 한다. 우리는 작년 9월에 새로운 펀드를 만들었는데, 나중에 통계를 내보니까(...)",
        "guid": "https://www.thestartupbible.com/?p=9672",
        "categories": [
          "Uncategorized",
          "compounding",
          "failure",
          "FoundersAtWork",
          "fundraising",
          "hustle",
          "inspiring",
          "Strong",
          "vc"
        ],
        "isoDate": "2026-01-25T21:28:00.000Z"
      }
    ]
  },
  {
    "name": "매거진 입맛",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "요즘 IT",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "쿠팡 엔지니어링",
    "category": "기업",
    "posts": []
  },
  {
    "name": "지마켓 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "리멤버 (드라마 앤 컴퍼니)",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Kimchi hill",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "Hudi.blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "토스",
    "category": "기업",
    "posts": [
      {
        "title": "Why should sending money be so hard?",
        "link": "https://toss.im/tossfeed/article/sendingmoney",
        "pubDate": "Thu, 29 Jan 2026 07:00:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-of5acw{margin:24px 0 8px;text-align:center;}.css-1pgssrp{max-width:100%;border-radius:16px;}.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}.css-q3ktjb{white-space:pre-wrap;font-weight:bold;}“Make transfers and payments frictionless.”\n.css-1kxrhf3{white-space:pre-wrap;}That single sentence, written on October 21, 2013, became the very first step in bringing Toss’ simple money transfer into the world. While iconic American companies were born in college dorm rooms or in garages, Toss took root in the ordinary streets of Seoul, where people’s everyday lives take place.\nAround that time, four team members were scattered across Seoul, searching for the right business idea. They observed how people behaved, what inconveniences they faced, and where they spent most of their time. From these field observations came more than a hundred potential ideas like services for rating restaurant menus, a platform for amateur singers to upload performance videos, an app that takes photos of and stores receipts, and among them was making “money transfers and payments frictionless.”\nToss, in other words, didn’t set out from the start to be a “fintech startup.” The goal was simply to build something people genuinely needed, whatever that might be. This determination wasn’t built overnight. Before launching Toss, its founder Seunggun Lee had spent five years pursuing projects he wanted to do and building products he wanted to create. He failed eight times. Those repeated disappointments came to a costly and painful realization : it’s not about what I want to do, it’s about what people want.\n.css-2sk6rv{font-size:19px;letter-spacing:0em;line-height:1.6;font-weight:bold;color:var(--adaptiveGrey900);white-space:pre-wrap;margin:24px 0;padding-left:20px;position:relative;}.css-2sk6rv::before{content:'';display:block;position:absolute;top:4px;left:0;width:2px;height:calc(100% - 4px * 2);padding:4px 0;background-color:var(--adaptiveGrey800);}\nThe reasons for his repeated failures finally became clear. At the same time, Seunggun Lee found himself in turmoil. He had started a company to pursue what he loved, only to realize that chasing what he wanted would never lead to success. (...) “It was a wake-up call. I told myself to stop messing around. Forget what you want , forget your ego. From now on, chase what will succeed. Let go of the pride. You’re just someone who makes what people want.”\n.css-18442ym{font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}.css-1ebvaan{white-space:pre-wrap;font-weight:bold;font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}- .css-jfs1hr{white-space:pre-wrap;font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}Kyounghwa Jung, No Ordinary Challenge, Bookstone, 2022, p.29–30.css-7mseny>*{margin-left:0;margin-right:0;}.css-7mseny>:last-child{margin-bottom:0;}blockquote>.css-7mseny:first-child>:first-child{margin-top:0;}\nIt was clear that people wanted ‘convenient’ finance. In the early 2010s, sending money online in Korea was likened to performing a small ritual. You had to install specific software, import your accredited certificate, dig out your security card to enter the code, and then type in a one-time password. Even the simplest everyday moments of splitting a dinner bill with a friend or paying for something online were unnecessarily cumbersome. The world was rapidly moving online, but money transfers and payments remained stuck in the past. Overseas, things were different. In the United States, PayPal and Cash App were already well established, and in the UK, TransferWise (now Wise) had gained traction. Toss began with a simple question : Why should sending and receiving money be so difficult?\nA few years earlier, Seunggun Lee had been making donations to the Purme Foundation, a relationship that began while he was working at a dental hospital for people with disabilities. One day, a question crossed his mind: “What authority does the Purme Foundation have to withdraw money directly from my bank account?” That single thought became the starting point. (...) The answer was in recurring payments. He discovered that recurring payments weren’t just used for nonprofit donations, but also for things like paying for milk deliveries or newspaper subscriptions. When a business used such recurring payments, it could automatically withdraw funds from a customer’s account on a set date and deposit them into the designated account. (…) Finally, it seemed as though he had found a solution.\n- Kyounghwa Jung, No Ordinary Challenge, Bookstone, 2022, p.35–36\nOn February 23, 2015, Toss officially introduced its simple money transfer service to the world. Open the app, enter the amount, type in an account number or contact, and finish with a password made of just four digits and one letter. That was it. The transfer was complete. To reassure users who thought it was almost too easy, Toss displayed a message on the screen: .css-1odxvuk{white-space:pre-wrap;font-style:italic;}“The transfer is complete. Really.”\nPeople responded quickly. Three out of ten users came back the following week, and seven out of ten returned a month later.* The daily transaction volume and number of active users grew 1.4 times every week. That meant four to five times more activity in just a month, a pace rarely seen in financial apps. After a long, arduous journey, Toss finally found itself at the real starting line. Along the way, it embedded two lessons into its DNA: create services people truly need, and never be afraid of failure.\n* In the weeks right after launch, Toss’ weekly retention exceeded 30%, and its monthly retention reached as high as 70%.\nI realized that victory comes with humility, and with the courage to admit your weaknesses. To me, success begins when you no longer fear the sting of failure. Two teams can pursue the same idea, but it’s the mindset that determines which team succeeds.\n- Seunggun Lee, Toss Team Leader, ”The Beginning of Toss: PO SESSION”, Toss YouTube channel\nToss started to raise more questions. “Why should people have to pay to check their credit?” “Why does getting a loan take so long?” “Why is investment so complicated?” These were the kind of questions that were so plain and ordinary that most people overlooked. But Toss didn’t let them pass. It started searching for answers, and those answers turned into services that changed the market. From industry-first offerings like free credit checks and a loan comparison service to Toss Securities, which made investing accessible directly from your phone, and Toss Bank, which made banking as easy and casual as daily life, each question became a spark for everyday change.\nBy the summer of 2025, Toss had surpassed 30 million cumulative users. What began as a simple money transfer app had grown to offer more than 100 services, present at nearly every moment when money moves. The number of services has expanded, but the North Star remains unchanged: to question the obvious, find better answers, and redefine the standards of the world. The questions drawn from people’s everyday lives remain at the heart of Toss, fueling its extraordinary challenge even today.\nFollow us on .css-114ityv{white-space:pre-wrap;cursor:pointer;-webkit-text-decoration:underline!important;text-decoration:underline!important;font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}LinkedIn and explore more detailed updates about Toss.",
        "guid": "https://toss.im/tossfeed/article/sendingmoney",
        "isoDate": "2026-01-29T07:00:00.000Z"
      }
    ]
  },
  {
    "name": "모나미",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김진홍",
    "category": "개인",
    "posts": []
  }
]