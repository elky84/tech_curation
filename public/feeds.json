[
  {
    "name": "ㅍㅍㅅㅅ",
    "category": "큐레이팅",
    "posts": []
  },
  {
    "name": "C++ Team Blog",
    "category": "기업",
    "posts": [
      {
        "creator": "Mryam Girmay",
        "title": "Prevent Critical Bugs with MSVC Code Analysis",
        "link": "https://devblogs.microsoft.com/cppblog/prevent-critical-bugs-with-msvc-code-analysis/",
        "pubDate": "Thu, 22 Aug 2024 18:12:24 +0000",
        "content:encodedSnippet": "Imagine this: You’re deep into a complex C++ project, and everything seems to be running smoothly. But then, out of nowhere, a critical bug surfaces— one that requires a bit more foresight. We’ve all been there, right? This is where code analysis steps in as your silent guardian. \nCode analysis is a great tool for catching those elusive bugs and ensuring your code adheres to the best programming practices. It identifies defects that are difficult to discover through testing by searching for specific code patterns known to cause problems.  \n The analysis results are displayed in the Visual Studio Error List window and as squiggles in the editor. This feature checks for problematic code patterns, such as buffer overruns caused by converting an element count into a byte count and null pointer dereferences, even if the code  looks correct. In this blog, we will focus on MSVC Code Analysis, which is one of the different types of code analysis available in Visual Studio for C++. \nWhere MSVC Code Analysis Shines \nIn 2014, the tech world was shaken by the discovery of the Heartbleed bug in OpenSSL. This critical vulnerability, caused by a missing bounds check, allowed attackers to exploit the TLS heartbeat extension and read sensitive data from server memory, including private keys, usernames, and passwords. The fallout was massive, affecting millions of users and causing widespread panic. \nNow, picture yourself as a C++ developer working on a high-stakes project. You know that even a small mistake can lead to significant security vulnerabilities, just like Heartbleed. This is where MSVC Code Analysis becomes your best ally. \nMSVC Code Analysis is a static analysis tool that checks your code for errors, potential improvements, and adherence to coding best practices when using the Microsoft Visual C++ (MSVC) compiler. For example, failing to initialize a pointer (e.g., int* uninitializedPtr;) in your project can result in unpredictable behavior, crashes, and security vulnerabilities. Consider the following scenario: You declare a pointer and initialize it to nullptr (int* imageData = nullptr;). Later, you attempt to allocate memory for the pointer based on uninitialized width and height variables (imageData = new int[width * height];). This can lead to undefined behavior because width and height are not initialized before use. If the pointer is used before being properly assigned, it can lead to accessing uninitialized memory, which Rule C6001 identifies, helping you catch these issues before they become critical problems. The following sample generates ‘Using uninitialized memory’ warning: \n#include <iostream> \r\n\r\n#include <stdexcept> \r\n\r\n class ImageProcessor { \r\n\r\npublic: \r\n\r\n    void processImage() { \r\n\r\n        int width, height; \r\n\r\n        int* imageData = nullptr; \r\n\r\n        try { \r\n\r\n            // Attempt to allocate memory based on width and height \r\n\r\n            imageData = new int[width * height]; // Uninitialized width and height \r\n\r\n            // Process the image data (this will cause undefined behavior) \r\n\r\n            for (int i = 0; i < width * height; ++i) { \r\n\r\n                imageData[i] = i; // Potentially accessing uninitialized memory \r\n\r\n            } \r\n\r\n            // Simulate further processing \r\n\r\n            std::cout << \"Image processed successfully.\" << std::endl; \r\n\r\n        } \r\n\r\n        catch (const std::bad_alloc& e) { \r\n\r\n            std::cerr << \"Memory allocation failed: \" << e.what() << std::endl; \r\n\r\n        } \r\n\r\n        // Clean up allocated memory \r\n\r\n        delete[] imageData; \r\n\r\n    } \r\n\r\n}; \nNow, let’s use this example to understand the different ways to invoke code analysis in Visual Studio.\nBackground Code Analysis  \nThis integral feature of Visual Studio functions as a real-time code analysis tool. This tool is particularly beneficial for you because it: \nProvides immediate feedback on potential issues, aiding in early problem resolution. \nFocuses on the files that are currently open in the editor, streamlining analysis during active development.  \nBackground code analysis will automatically run after you open or save the file. The warning for uninitialized memory will be enabled by default, displaying a green squiggle in the editor and appearing in the error list, as demonstrated in the example below. \n\nThis feature is enabled by default; however, you can double check the setting by navigating to Tools > Options > Text Editor > C/C++ > Advanced > Code Analysis > Disable Background Code Analysis: False.\nManually Running Code Analysis \nIn addition to background code analysis, you can also manually run code analysis as needed. You can start by clearing all warnings in the current file you’re working on, then invoke Code Analysis for the current project. Periodically, run Code Analysis for the entire solution to maintain overall code quality. You can also manually run code analysis for individual files. There are several ways to manually run a code analysis. Following any of these steps will display the ‘Using uninitialized memory’ warning in the error list window for our example. \n1. Menu-bar: \nOpen the project.\nFrom the menu-bar, select Build -> Run Code Analysis on [Solution | Project Name | File] \nFrom the menu-bar, select Analyze -> Run Code Analysis -> [On Solution | Run Code Analysis on ‘project name’ | Run Code Analysis on File]\n\n2. Keyboard Shortcut: \nFor a single file: Ctrl + Shift + Alt + F7 \nLearn more about the different ways to run code analysis manually in Run code analysis. \nEnable Code Analysis on Build \nThis setup ensures that code analysis runs automatically every time you build your project or compile a single file. Think of code analysis as an inspector who checks your project. While this inspector might take more time to examine everything, catching potential issues early provides peace of mind. This additional time you invest   is necessary to ensure your code is secure. To enable it: \n Open Project Properties in Visual Studio.\nNavigate to Configuration Properties > Code Analysis > General. \nSelect the Yes option for Enable Code Analysis on Build. Note that this option is disabled by default, whereas Microsoft Code Analysis is enabled by default.\n\nTo view all default selected rules, including the rule for detecting uninitialized memory, navigate to Configuration Properties > Code Analysis > Microsoft, and then click Configure.   \n\nAfter enabling code analysis on build, building the example code will cause Visual Studio to generate a warning for the lines imageData = new int[width * height];, indicating that width and height are uninitialized. Background code analysis focuses on the files you’re actively working on, while build-time analysis ensures all project files are checked, catching any missed issues that aren’t in the current file. This warning will appear in the Error List window. \n\nKey Events in Microsoft C++ Code Analysis help you quickly identify and fix defects by providing detailed information in selected warnings from the Error List. They trace code flow to pinpoint root causes, making it easier to understand issues like variable initialization or branching. For example, double-clicking the C6001 ‘Using uninitialized memory: width’ warning in the Error List opens a new window showing the Key Events. For further insights, please refer to the Microsoft C++ Code Analysis Warnings with Key Events blog. \n\nCode Analysis Rules and Rulesets  \nRulesets in Visual Studio for C++ are collections of code analysis rules that ensure code quality and adherence to standards. For example, enabling the MSVC rule C26440, ‘Function can be declared ‘noexcept’,’ suggests marking functions with ‘noexcept’ if they do not throw exceptions. This can improve both performance and reliability. \nTo create a new custom rule set with the “Function can be declared ‘noexcept’” rule added, follow these steps: \nOpen Project Properties in Visual Studio \nNavigate to Configuration Properties > Code Analysis > Microsoft. \nIn the Active rules section, click “Configure”  \nSelect the check box for the rule that you want to include in the ruleset. The Action will automatically change from ‘None’ to ‘Warning’. You can change the severity of this rule based on your needs, with options like Error, Info, Hidden, None, or <Inherit>.  \nSave the rule set with a new file name. The custom rule set is now automatically assigned to the project. \n\nTo learn more about using rule sets in depth, refer to the Use Rule Sets to Specify the C++ Rules to Run article.  \nAdditional Tools and Techniques\nVisual Studio offers several features that can enhance code quality and prevent issues like the Heartbleed bug. Here are some tools you might find useful: \nClang-Tidy Code Analysis \nClang-Tidy, a tool used with the LLVM/clang-cl compiler, is designed to modernize your code, ensure adherence to standards, perform static analysis, and automatically format your code. When using an MSVC toolset, you can configure Clang-Tidy to complement or replace the conventional Code Analysis process. This helps catch different types of issues and improves overall code quality. You can find more details in Using Clang-Tidy in Visual Studio article. \nSuppress Specific Warnings \nSuppressing specific warnings in C++ involves configuring your project settings to ignore certain compiler warnings for a single line, section of code, file, or entire project. This can be done at the project level or for individual files using Visual Studio’s project properties or ‘#pragma warning’ directives. By suppressing less relevant warnings, you can focus on the most critical issues, making the build output cleaner and easier to read, which simplifies identifying and addressing significant problems. To dive deeper into this topic, check out the Suppress compiler warnings article. \nIt’s important to note that Code Analysis tools may occasionally generate false positives. If you encounter a false positive, please report it through the Visual Studio Developer Community channel with detailed repro code and information. This helps us to improve the accuracy of Code Analysis tools and ensures a smoother development experience. \nLearn More \nTo learn more about securing your C++ programs, visit the Build Reliable and Secure C++ programs blog. For the latest updates to the MSVC backend, check out the MSVC Backend Updates in Visual Studio 2022 version 17.10 blog.  \nYour feedback is invaluable in helping us enhance the MSVC Code Analysis experience. Please share your suggestions in the comments below or through the Developer Community. You can also reach us via email at visualcpp@microsoft.com or via X at @VisualC. \nThe post Prevent Critical Bugs with MSVC Code Analysis appeared first on C++ Team Blog.",
        "dc:creator": "Mryam Girmay",
        "comments": "https://devblogs.microsoft.com/cppblog/prevent-critical-bugs-with-msvc-code-analysis/#comments",
        "content": "<p>Imagine this: You’re deep into a complex C++ project, and everything seems to be running smoothly. But then, out of nowhere, a critical bug surfaces— one that requires a bit more foresight. We’ve all been there, right? This is where code analysis steps in as your silent guardian. </p>\n<p>The post <a href=\"https://devblogs.microsoft.com/cppblog/prevent-critical-bugs-with-msvc-code-analysis/\">Prevent Critical Bugs with MSVC Code Analysis</a> appeared first on <a href=\"https://devblogs.microsoft.com/cppblog\">C++ Team Blog</a>.</p>\n",
        "contentSnippet": "Imagine this: You’re deep into a complex C++ project, and everything seems to be running smoothly. But then, out of nowhere, a critical bug surfaces— one that requires a bit more foresight. We’ve all been there, right? This is where code analysis steps in as your silent guardian. \nThe post Prevent Critical Bugs with MSVC Code Analysis appeared first on C++ Team Blog.",
        "guid": "https://devblogs.microsoft.com/cppblog/?p=34525",
        "categories": [
          "C++"
        ],
        "isoDate": "2024-08-22T18:12:24.000Z"
      }
    ]
  },
  {
    "name": "Facebook Engineering",
    "category": "기업",
    "posts": [
      {
        "creator": "",
        "title": "How Meta enforces purpose limitation via Privacy Aware Infrastructure at scale",
        "link": "https://engineering.fb.com/2024/08/27/security/privacy-aware-infrastructure-purpose-limitation-meta/",
        "pubDate": "Tue, 27 Aug 2024 16:00:50 +0000",
        "content:encodedSnippet": "At Meta, we’ve been diligently working to incorporate privacy into different systems of our software stack over the past few years. Today, we’re excited to share some cutting-edge technologies that are part of our Privacy Aware Infrastructure (PAI) initiative. These innovations mark a major milestone in our ongoing commitment to honoring user privacy. \nPAI offers efficient and reliable first-class privacy constructs embedded in Meta infrastructure to address complex privacy issues. For example, we built Policy Zones that apply across our infrastructure to address restrictions on data, such as using it only for allowed purposes, providing strong guarantees for limiting the purposes of its processing.\nAs we expanded PAI across Meta, increasing its maturity, we gained valuable insights. Our understanding of the technology evolved, revealing the need for a larger investment than initially planned to create a cohesive ecosystem of libraries, tool suites, integrations, and more. These investments have been crucial in enforcing complex purpose limitation scenarios while ensuring scalability, reliability, and a streamlined developer experience.\nPurpose limitation, a core data protection principle, is about ensuring data is only processed for explicitly stated purposes. A crucial aspect of purpose limitation is managing data as it flows across systems and services. Commonly, purpose limitation can rely on “point checking” controls at the point of data processing. This approach involves using simple if statements in code (“code assets”) or access control mechanisms for datasets (“data assets”) in data systems. However, this approach can be fragile as it requires frequent and exhaustive code audits to ensure the continuous validity of these controls, especially as the codebase evolves. Additionally, access control mechanisms manage permissions for different datasets to reflect various purposes using mechanisms like access control lists (ACLs), which requires the physical separation of data into distinct assets to ensure each maintains a single purpose. When Meta started to address more and larger-scope purpose limitation requirements that crossed dozens of our systems, these point checking controls did not scale.\nAt Meta, millions of data assets are crucial for powering our product ecosystem, optimizing machine learning models for personalized experiences, and ensuring our products are high quality and meet user expectations. Identifying which code branches and data assets require protection is challenging due to complex propagation requirements and permissions models that need constant revision. For example, when a data consumer reads from one data asset (“source”) and stores the output in another (“sink”), point checking controls would require complex orchestration to ensure propagation from sources to sinks, which can become operationally unviable.\n\nTo address this problem, point checking controls can be enhanced by leveraging data flow signals. Data flows can be tracked from the same origin, where relevant data is collected, using various techniques such as static code analysis, logging, and post-query processing. This creates a graph, known as “data lineage,” that tracks the relationships between source and sink data assets. By utilizing data lineage, permissions can be applied to relevant data assets based on these source-to-sink relationships. The combination of point checking and data lineage, while viable at a small scale, leads to significant operational overhead as point checking still requires auditing many individual assets. \nBuilding on these insights, in our latest iteration, we found that the information flow control (IFC) model offers a more durable and sustainable approach by controlling not only data access but also how data is processed and transferred in real-time, rather than relying on point checking or out-of-band audits. Thus, we developed Policy Zones as our IFC-based technology and integrated it across major Meta systems to enhance our purpose limitation capabilities at scale. This effort was later expanded into the Privacy Aware Infrastructure (PAI) initiative, a transformative investment that integrates first-class privacy support into Meta’s infrastructure systems.\nWe believe PAI is the right investment to protect people’s privacy at scale and can effectively enforce purpose limitation requirements.\nWhy invest in Policy Zones?\nThrough our experience deploying purpose limitation solutions over the years, we identified several key themes:\nNeeds\nProblem\nSolution\n\n\nProgrammatic Control: We needed to rely more on programmatic controls instead of point checking human audits to control data flows, and do so in real-time\nTraditional point checking controls, combined with data lineage checks, can detect data transfers within a specific time frame but not in real-time. Addressing these risks requires implementing resource-intensive human audits at access points.\nIn contrast, PAI is designed to check data flows in real-time during code execution, blocking problematic data flows from occurring, facilitated by UX tooling, thus making it more scalable.\n\n\nGranular Flow Control: We needed to maximize the reuse of existing data and business logic on complex infra\nAccess control is easy to roll out when data is separated physically, but poses significant costs, complexity, and limitations when dealing with Meta’s complex infrastructure, where data for different purposes is often processed by shared code.\nPAI solves this by providing precise decision making at the granular level of individual requests, function calls, or data elements, achieving logical data separation at a relatively low compute cost even on complex infrastructures where it’s needed.\n\n\nAdaptable and Extensible Control: We needed to handle ever-evolving requirements, even multiple for the same data assets\nWe are facing a rapidly changing world for privacy. Data use restrictions can vary over time depending on evolving privacy and product requirements. A single data asset or different parts of it might be subject to multiple privacy requirements. While “point checking” can address this to some extent, it struggles to control downstream data flows, even combined with data lineage.\nPAI is designed to check multiple requirements involved in data flows and is highly flexible to adapt to changing requirements.\n\n\n\n \nHow Policy Zones works\nLet’s dive into what Policy Zones is and how we can leverage it to meet purpose limitation requirements. Policy Zones provides a comprehensive mechanism for encapsulating, evaluating, and propagating privacy constraints for data both “in transit” and “at rest,” including transitions between different systems. It conducts runtime evaluation of constraints, context propagation, and is deeply integrated with numerous data and code frameworks (e.g., HHVM, Presto, and Spark), representing a step change in how we approach information flow control.\nTo make the explanation more relatable and bring some levity to a serious topic, we’ll use a simple example: Let’s say a new requirement comes up, where banana data can only be used for the purposes of making smoothies and fruit baskets, but not for making banana bread. For simplicity, this example and the illustration below only demonstrate the first row of the above table. \n\nHow would developers leverage Policy Zones to implement such a requirement?\nFirst, to demarcate relevant data assets, they assign a metadata label (“data annotation,” e.g., BANANA_DATA) to data assets at different granularities. This annotation is associated with the purpose limitation requirement as a set of data flow rules that enable systems to understand the allowed purposes for the data.\n\nWhen annotated data is processed, Policy Zones kicks in and checks whether the data processing is allowed and data can flow downstream. Policy Zones has been built into different Meta systems, including:\nFunction-based systems that load, process, and propagate data through stacks of function calls in different programming languages. Examples include web frontend, middle-tier, and backend services.\nBatch-processing systems that process data rows in batch (mainly via SQL). Examples include real-time and data warehouse systems that power Meta’s AI and analytics workloads.\nLet’s dive deeper into how Policy Zones works for the function-based systems, while the same logic applies to the batch-processing systems as well.\nIn function-based systems, data is passed through parameters, variables, or return values in a stack of function calls. \nLet’s walk through an example: \nA web request, “BananaRequest,” loads annotated data from BananaDB, causing a data flow violation because the intent of the caller is unknown.\nTo remediate the data flow violation, we annotate BananaRequest with the BANANA_DATA label, creating a zone (“Banana Zone”) for the request. \nBehind the scenes at runtime, Policy Zones programmatically checks all data flows against the flow rules based on the context, flagging new data flow violations from BananaRequest to logB and logC. \nWe annotate logB as banana and remove the logging of banana data into logC to cut off the disallowed data flow. \nWith all data flow violations remediated, the zone can be moved from logging mode to enforcement. If a developer adds a write to a sink outside of the zone, it will be blocked automatically.\n\nIn a more complex scenario, a function, “makeBananaSmoothie()” from a web request, “BreakfastRequest” calls another function, “makeBanana().” Besides the previous data flow violations, we need to remediate another data flow violation: makeBanana() returns banana data to makeBananaSmoothie(). This means we can create a “Banana Zone” from the function makeBananaSmoothie() that includes all functions that it calls directly or indirectly.\n\nIn batch-processing systems, data is processed in batches for rows from tables that are annotated as containing relevant data. When a job runs a query (usually SQL-based) to process the data, a zone is created and Policy Zones flags any data flow violations. Remediation options are provided, similar to those for function-based systems. Once all violations have been remediated, the zone can be moved from logging mode to enforcement mode to prevent future data flow violations. Data annotation can be done at various levels of granularity, including table, column, row, or potentially even cell.\nWhen data flows across different systems (e.g., from frontend, to data warehouse, then to AI), Policy Zones ensures that relevant data is annotated correctly and thus continues to be protected according to the requirements. For some systems that don’t have Policy Zones integrated yet, the point checking control is still used to protect the data.\nHow we applied PAI to existing systems at scale\nThe above gives you a glimpse into how the technology is used to roll out a simple use case. However, adopting Policy Zones is a non-trivial task for complex requirements across tens or hundreds of systems. The requirement owner usually collaborates with other engineers who are code and data asset owners across Meta to implement different aspects of that requirement. In some cases, this may involve hundreds or thousands of engineers to complete the implementation and audits. To address this challenge, PAI offers Policy Zone Manager (PZM), a suite of UX tools that helps requirement owners to efficiently enforce privacy requirements using PAI.\nLet’s take a look at how PZM makes it easy for people to satisfy their purpose limitation needs in existing systems, using the above banana requirement as an example. At a high level, the requirement owner carries out the following workflow, facilitated by PZM:\nIdentify relevant assets: This is to identify which source assets need to be purpose limited for the given requirement.\nDiscover relevant data flows: This is to discover the downstream data flows from the source assets in order to integrate Policy Zones at scale.\nRemediate data flow violations: This is to allow people to choose which option to take to remediate data flow violations.\nContinuously enforce and monitor data flows: This is to turn on Policy Zones enforcement and monitor it to prevent new data flow violations. \n\nTo hear more about this process, check out our presentation at the PEPR conference in June 2024.\n\nStep 1 – Identify relevant assets\nFor a given requirement, we check the relevant product entry points (e.g., mobile apps, web requests, and databases) to pinpoint data assets that are collected. These assets may take the form of request parameters, database entries, or event log entries. We use data structures to represent (“schematize”) these data assets and fields, capturing relevant data at various granularities. In the running example, a table in the banana database might contain entirely banana data, a single banana column, or a mix of banana and other fruit data.\nIn addition to manual code inspection, we heavily rely on various techniques such as our scalable ML-based classifier to automatically identify data assets.\nStep 2 – Discover relevant data flows\nFrom a given annotated source, the requirement owner can identify its downstream data flows and sinks (see diagram below). The owner can then decide how to handle these data flows. However, this process can be time consuming when there are many data flows that are one or multiple hops away from the same origin. This often occurs when implementing a new requirement over existing data flows. \n\nAlthough data lineage presents significant operational overhead for point checking mechanisms, it can efficiently identify where to integrate Policy Zones into the codebase. Therefore, we have integrated data lineage into PZM, allowing requirement owners to discover multiple downstream assets from a given source simultaneously. Once the requirement has been fully implemented, we can rely solely on Policy Zones to enforce the requirements.\nStep 3 – Remediate data flow violations\nBy default, the data flow from a source asset to a sink must meet all of the requirements of the source. If not, it’s considered a data flow violation and needs remediation, enforced by Policy Zones programmatically at runtime. There are three main cases to remediate data flow violations (using the running example to help concretize the general cases):\nCase 1: Safe flow – relevant data is used for allowed purpose(s): Assign the banana annotation to the sink asset.\nCase 2: Unsafe flow – relevant data is used for disallowed purpose(s): Block data access and code execution to prevent further processing of banana data.\nCase 3: Reclassified flow – relevant data is not used or propagated: Annotate the data flow as reclassified as being permitted. Banana data from the source is not used or propagated to the sink.\n\nStep 4 – Continuously enforce and monitor data flows\nPAI is integrated into our major data systems to check data flows and catch violations at runtime. During the initial rollout of a new requirement, Policy Zones can be configured to allow remediations of flow violations in “logging mode.” Once Policy Zones enforcement is enabled, any data flow with unremediated violations is denied. This also prevents new data flow violations, even if code changes or new code is added.\nPAI continuously monitors the enforcement of requirements to ensure that it operates correctly. PZM provides a set of verifiers to check the accuracy of asset annotations and control configurations.\nLessons learned from adoption at scale across Meta\nAs PAI has been adopted by a multitude of purpose limitation requirements across Meta, we’ve learned several key lessons over the past few years:\nFocus on solving one specific end-to-end use case first\nInitially, we developed Policy Zones for batch-processing systems with some basic use cases. However, we realized that our designs for function-based systems were quite abstract and the adoption for a large-scale use case resulted in significant challenges, consequently, requiring considerable effort to map patterns to customer needs. Furthermore, refining the APIs and building missing operational support made it work effectively end-to-end across multiple systems. Only after addressing these challenges were we able to make it more generic and proceed with integrating Policy Zones across extensive platforms.\nStreamline integration complexity\nIntegrating PAI into major Meta systems coherently was a complex, lengthy, and challenging process. We encountered significant difficulties in integrating PAI with Meta’s diverse systems broadly. It took us years to overcome these challenges. For example, initially, product teams expended considerable effort to schematize data assets across different data systems. Then we developed reliable, computationally efficient, and widely applicable PAI libraries in various programming languages (Hack, C++, Python, etc.) that enabled a smoother integration with a broad range of Meta’s systems.\nInvest in computational and developer efficiency early on \nWe also undertook multiple iterations to simplify PAI and improve its computational efficiency. Our initial annotation APIs were overly complex, resulting in high cognitive overhead for engineers. Furthermore, the computational overhead of data flow checking was prohibitively high in Meta’s high-throughput systems. Through several rounds of refinement, we simplified policy lattice representation and evaluation, built language-level features to natively propagate Policy Zones context, and canonicalized policy annotation structures, achieving 10x improvements in computational efficiency.\nSimplified and independent annotations are a must to scale to a wide range of requirements\nInitially, we employed a monolithic annotation API to model intricate data flow rules and annotate relevant code and data. However, as data from multiple requirements were combined, propagating these annotations from sources to sinks became increasingly complex, resulting in data annotation conflicts that were difficult to resolve. To address this challenge, we implemented simplified data annotations to decouple data from requirements and separate data flow rules for different requirements. This significantly streamlined the annotation process, ultimately improving developer experiences.\nBuild tools; they are required \nWe have made significant efforts to ensure the use of PAI is easy and efficient, ultimately improving the developer experience. Initially, we focused on the correctness of the technology first before investing in tooling. Adopting Policy Zones required a lot of manual effort, and it was challenging for engineers to understand how to properly annotate their assets, which led to additional cleanup work later. To address this issue, we developed the PZM tool family, which includes built-in automated rules and classifiers. These tools guide teams through standard workflows, ensuring safe and efficient rollout of purpose limitation requirements and reducing engineering efforts by orders of magnitude.\nDurable privacy protection for everyone\nMeta is committed to protecting user privacy. The PAI initiative is a crucial step in safeguarding data and preserving privacy efficiently and reliably. It provides a robust foundation for Meta to sustainably tackle privacy challenges, meet high reliability standards, and address future privacy issues more efficiently than traditional solutions. While we’ve laid a strong groundwork, our journey is just beginning. We aim to build upon this foundation by expanding our capabilities and controls to accommodate a wider range of privacy requirements, enhancing the developer experience, and exploring new frontiers.\nWe hope our work sparks innovation and fosters collaboration across the industry in the field of privacy.\nAcknowledgements\nThe authors would like to acknowledge the contributions of many current and former Meta employees who have played a crucial role in productionizing and adopting PAI over the years. In particular, we would like to extend special thanks to (in alphabetical order) Adrian Zgorzalek, Alex Gorelik, Amritha Raghunath, Anuja Jaiswal, Brian Sniffen, Brian Romanko, Brian Spanton, David Detlefs, David Mortenson, David Taieb, Gabriela Jacques da Silva, Ian Carmichael, Iuliu Rus, Jafar Husain, Jerry Pan, Jiang Wu, Joel Krebs, Jun Fang, Komal Mangtani, Marc Celani, Mark Konetchy, Michael Levin, Perry Stoll, Peter Prelich, Pieter Viljoen, Prashant Dhamdhere, Rajesh Nishtala, Rajkishan Gunasekaran, Rishab Mangla, Sergey Doroshenko, Seth Silverman, Sriguru Chakravarthi, Tarek Sheasha, Thomas Georgiou, Uday Ramesh Savagaonkar, Vitalii Tsybulnyk, Vlad Fedorov, Wolfram Schulte, and Yi Huang. We would also like to express our gratitude to all reviewers of this post, including (in alphabetical order) Aleksandar Ilic, Benjamin Renard, Emil Vazquez, Emile Litvak, Harrison Fisk, Jason Hendrickson, Jessica Retka, Nimish Shah, Sabrina B Ross, and Sam Blatchford. We would like to especially thank Emily DiPietro for championing the idea, leading the editorial effort, and pulling all required support together to make this blog post happen.\nThe post How Meta enforces purpose limitation via Privacy Aware Infrastructure at scale appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>At Meta, we&#8217;ve been diligently working to incorporate privacy into different systems of our software stack over the past few years. Today, we&#8217;re excited to share some cutting-edge technologies that are part of our Privacy Aware Infrastructure (PAI) initiative. These innovations mark a major milestone in our ongoing commitment to honoring user privacy.  PAI offers [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2024/08/27/security/privacy-aware-infrastructure-purpose-limitation-meta/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2024/08/27/security/privacy-aware-infrastructure-purpose-limitation-meta/\">How Meta enforces purpose limitation via Privacy Aware Infrastructure at scale</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "At Meta, we’ve been diligently working to incorporate privacy into different systems of our software stack over the past few years. Today, we’re excited to share some cutting-edge technologies that are part of our Privacy Aware Infrastructure (PAI) initiative. These innovations mark a major milestone in our ongoing commitment to honoring user privacy.  PAI offers [...]\nRead More...\nThe post How Meta enforces purpose limitation via Privacy Aware Infrastructure at scale appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=21618",
        "categories": [
          "Security"
        ],
        "isoDate": "2024-08-27T16:00:50.000Z"
      },
      {
        "creator": "",
        "title": "RETINAS: Real-Time Infrastructure Accounting for Sustainability",
        "link": "https://engineering.fb.com/2024/08/26/data-infrastructure/retinas-real-time-infrastructure-accounting-for-sustainability/",
        "pubDate": "Mon, 26 Aug 2024 16:00:40 +0000",
        "content:encodedSnippet": "We are introducing a new metric— real-time server fleet utilization effectiveness —as part of the RETINAS initiative to help reduce emissions and achieve net zero emissions across our value chain in 2030.\nThis new metric allows us to measure server resource usage (e.g., compute, storage) and efficiency in our large-scale data center server fleet in near real-time.\nWe are sharing our learnings in adopting depreciation methods for accumulated carbon assets for internal fleet measurements, and encourage further industry improvement and development on these concepts. This is not intended to replace global emissions accounting standards for purposes of external reporting.\nSince 2020, Meta has maintained net zero emissions in our operations and matched 100% of our electricity use with renewable energy. However, we know our work doesn’t stop there, and we recognize our responsibility to decarbonize our footprint beyond our data centers and offices, including emissions from the server components our suppliers manufacture to our employees’ commutes. To align with the Paris Agreement, we have set a goal to reach net zero emissions across our value chain in 2030.\nMeta’s Net Zero Program has three foundational pillars: understanding our emissions, reducing our emissions, and removing remaining emissions. To understand our emissions, improving the granularity, accuracy, and near real-time measurement of our greenhouse gas data goes beyond carbon accounting. The right data will help us apply actionable metrics to advance decarbonization across our business operations and with our suppliers.\nWith this in mind, we have created the Real Time Infrastructure Accounting for Sustainability (RETINAS) initiative, which seeks to study and understand the impact of server reliability, performance, and operational optimization on Meta’s Scope 3 emissions.\nThis initiative has led to the development of a new internal metric— real-time server fleet utilization effectiveness —that enables us to take action to reduce the emissions associated with the embodied carbon of our data center servers and components.  Embodied carbon contributes to Meta’s upstream Scope 3 emissions, and includes the emissions associated with the full lifecycle of the manufacturing, assembly, and transportation of servers and materials in our physical infrastructure.\nOptimizing the utilization of our server fleet is important to reducing these emissions. Real-time server fleet utilization effectiveness provides a framework toward effective measurement and integration of embodied carbon into ubiquitous infrastructure metrics to drive informed decisions to manage our server fleet resource usage (e.g., compute and storage) and their impacts on Meta’s Scope 3 emissions. \nHow we measure greenhouse gas emissions at Meta\nSince 2011, Meta has reported our Scope 1 and 2 emissions. In 2017, we began reporting select Scope 3 emissions categories. Since 2019 we have reported annually on all relevant emissions defined by the Greenhouse Gas Protocol. We obtain limited assurance conducted by a third party for select environmental metrics. In our accounting, data center servers and their components are a significant driver of our Scope 3 emissions footprint, and we have taken numerous steps to deepen our understanding of those emissions in order to surface reduction opportunities. \nAn important reduction strategy we are focused on is the circularity of our servers and components. The more effectively and efficiently servers are utilized, the more sustainable the server fleet. We can extend the lifespans of servers, components, and network infrastructure with improvements to server reliability, efforts to reuse components based on their reliability expectations, and various performance optimizations and operational improvements (e.g., firmware/server upgrades and repairs).\nWhile implementing these circularity strategies, we observed limitations in current carbon accounting practices to understand and weigh Scope 3 emissions trade-offs in our server fleet against traditional power, performance, and total cost of ownership (TCO) metrics, such as performance per dollar, performance per watt, and performance per dollar per watt, in real-time. \nCurrent carbon accounting and reporting practices for Scope 3 emissions are static. For data center servers and components, in particular, this means that the entirety of the embodied emissions from the upstream supply chain, manufacturing, and logistics is attributed in the year of purchase. Benefits from circularity are not realized in our Scope 3 footprint until future purchases of new servers or components are deferred. This does not provide actionable information to our operational teams in real-time on how varying the usage or the expected life of the acquired servers can impact Meta’s Scope 3 emissions. \nWe see a need to develop internal metrics to monitor and incentivize greater efficiency, utilization, and extension of the expected life of servers, which will influence current and future server fleet management. \nIntroducing real-time server fleet utilization effectiveness\nThe RETINAS initiative, launched by Meta’s Infrastructure Engineering team, seeks to study and understand how server reliability, performance, and operational optimization impact Meta’s Scope 3 emissions. To understand this holistically, we introduced a standardized, fleet-wide metric for any given resource (e.g., a server or rack) that measures the utilization of embodied carbon:\n\n \nWhere: \n\n \n\n \nThis metric borrows depreciation concepts from finance and accounting practices and applies them to aspects of server reliability, efficiency, and useful life. The concept of depreciation is used to showcase the expected useful life of acquired assets. This concept also allows for tracking of acquisition and disposition of server resources at fleet scale and is reported on an ongoing basis. \nUtilization metrics like power usage effectiveness (PUE) and hardware usage effectiveness (HUE) measure the effective IT usage from a power perspective at the data center and server level, respectively.  Combining depreciated Scope 3 emissions with these utilization metrics allows us to standardize these measurements along with other fleet health measurements for a defined period of time. \nWe illustrate the usage of this metric with a set of servers and various circularity strategies. \nExample (current static state) \nLet’s consider an example set of servers purchased in 2023 which have associated embodied emissions attributed to the buyer with 1000 tons of CO2e. Here is how this would be represented using current, static carbon accounting methods: \n\nThere is no representation for the useful life of the example set of servers. If we change the server set’s useful life (UL) from four years to five years, the metric doesn’t move. \nExample (with proposed dynamic accounting)\nFor the same example of servers purchased in 2023 with 1000 units of CO2e Scope 3 emissions, we use the concept of depreciation over a period of useful life of four years (example time horizon):\nDepreciation in action:\n\nIf the server set’s useful life is modified from four years to five years, this would be visible as part of the depreciation metric and showcase the longevity of resource usage. \nEffective change in depreciation with extension (from 4y UL to 5y UL): \n\nWithin large-scale infrastructure, there are different layers of availability within the hardware and software stack, such as hardware, firmware, the kernel, the operating system, and the application.  At each layer, there are metrics associated with efficiency based on available capacity, resources, and their effective use. To represent the use of a depreciation-based metric, we examine the efficiency of a service at the application stack. The representative graph below showcases an example set of variations in the utilization effectiveness stemming from application improvements over a larger time scale.\n\nUtilization effectiveness is defined as:  Total resource available / Resource utilized \n\nCombining the depreciation of embodied emissions resources per unit time to the utilization effectiveness for a given unit of time (say, every year), we can arrive at a more real-time measurement of server fleet utilization effectiveness of embodied carbon. (Note: The measurements for utilization effectiveness in the chart are representative values).\nOur goal is to consistently minimize the real-time server fleet utilization effectiveness. Utilization effectiveness ideally is decreasing asymptotically towards 1, when resources available are 100% utilized.  Depreciation of Scope 3 emissions over a longer period of time due to a longer useful life will also minimize this metric. Combined, this metric allows for ranking of different efforts one must pursue within the server fleet and compare and contrast efficiency improvements, reliability efforts like extensions or initial component selection, and associated embodied carbon impacts. \nBelow is the comparison of real-time server fleet utilization effectiveness and the way the metric behaves under server life extension and efficiency improvements with the above considered scenarios.\n\nCharacteristics of the metric:\nThe metric above can enable relative comparison of circularity strategies on the server fleet. It can be sliced horizontally into any given timescale (from seconds to years) for understanding a resource’s (e.g., servers or racks) embodied emissions attribution giving fine-grained real-time insights for the server fleet. The metric can also be vertically sliced to obtain utilization effectiveness at different layers of the stack, from entire servers, to containers, to production workloads, to app residencies for short durations – combining that with the associated resource available for the chosen abstraction. \nTo illustrate how this metric can be used, using the same set example as above: \nIncreasing server useful life from five years to seven years lowers the ​metric by 28% due to slower depreciation. \nEnabling reuse of a component, pursuing an application efficiency improvement, or choosing server parts that have lower emissions will contribute towards the metric and enable cross-stack tradeoff.\nTo close, we can observe in this example that this single metric ties together different fleet operations towards a single goal of reducing embodied emissions, delivering  insights for decision making at any given time horizon. By integrating depreciation and utilization effectiveness to embodied carbon, our operational and server fleet management teams can leverage this metric to make data-driven decisions that address an important portion of Meta’s Scope 3 footprint.  \nThe post RETINAS: Real-Time Infrastructure Accounting for Sustainability appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>We are introducing a new metric— real-time server fleet utilization effectiveness —as part of the RETINAS initiative to help reduce emissions and achieve net zero emissions across our value chain in 2030. This new metric allows us to measure server resource usage (e.g., compute, storage) and efficiency in our large-scale data center server fleet in [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2024/08/26/data-infrastructure/retinas-real-time-infrastructure-accounting-for-sustainability/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2024/08/26/data-infrastructure/retinas-real-time-infrastructure-accounting-for-sustainability/\">RETINAS: Real-Time Infrastructure Accounting for Sustainability</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "We are introducing a new metric— real-time server fleet utilization effectiveness —as part of the RETINAS initiative to help reduce emissions and achieve net zero emissions across our value chain in 2030. This new metric allows us to measure server resource usage (e.g., compute, storage) and efficiency in our large-scale data center server fleet in [...]\nRead More...\nThe post RETINAS: Real-Time Infrastructure Accounting for Sustainability appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=21592",
        "categories": [
          "Data Center Engineering",
          "Data Infrastructure"
        ],
        "isoDate": "2024-08-26T16:00:40.000Z"
      },
      {
        "creator": "",
        "title": "How PyTorch powers AI training and inference",
        "link": "https://engineering.fb.com/2024/08/23/ml-applications/pytorch-ai-training-inference/",
        "pubDate": "Fri, 23 Aug 2024 16:00:54 +0000",
        "content:encodedSnippet": "Learn about new PyTorch advancements for LLMs and how PyTorch is enhancing every aspect of the LLM lifecycle.\nIn this talk from AI Infra @ Scale 2024, software engineers Wanchao Liang and Evan Smothers are joined by Meta research scientist Kimish Patel to discuss our newest features and tools that enable large-scale training, memory efficient fine-tuning, and on-device LLM capabilities.\nFirst, they cover the importance of memory-efficient fine-tuning and a few common architectural and algorithmic techniques to enable fine-tuning on consumer-grade hardware. Then they discuss the challenges of deploying large models for on-device deployment and how techniques such as quantization make these deployments possible.\n\nThe post How PyTorch powers AI training and inference appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>Learn about new PyTorch advancements for LLMs and how PyTorch is enhancing every aspect of the LLM lifecycle. In this talk from AI Infra @ Scale 2024, software engineers Wanchao Liang and Evan Smothers are joined by Meta research scientist Kimish Patel to discuss our newest features and tools that enable large-scale training, memory efficient [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2024/08/23/ml-applications/pytorch-ai-training-inference/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2024/08/23/ml-applications/pytorch-ai-training-inference/\">How PyTorch powers AI training and inference</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "Learn about new PyTorch advancements for LLMs and how PyTorch is enhancing every aspect of the LLM lifecycle. In this talk from AI Infra @ Scale 2024, software engineers Wanchao Liang and Evan Smothers are joined by Meta research scientist Kimish Patel to discuss our newest features and tools that enable large-scale training, memory efficient [...]\nRead More...\nThe post How PyTorch powers AI training and inference appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=21568",
        "categories": [
          "AI Research",
          "ML Applications",
          "Open Source",
          "AI Infra @ Scale"
        ],
        "isoDate": "2024-08-23T16:00:54.000Z"
      },
      {
        "creator": "",
        "title": "Inside the hardware and co-design of MTIA",
        "link": "https://engineering.fb.com/2024/08/22/ml-applications/meta-mtia-hardware-co-design/",
        "pubDate": "Thu, 22 Aug 2024 16:00:34 +0000",
        "content:encodedSnippet": "In this talk from AI Infra @ Scale 2024, Joel Colburn, a software engineer at Meta, technical lead Junqiang Lan, and software engineer Jack Montgomery discuss the second generation of MTIA, Meta’s in-house training and inference accelerator.\nThey cover the co-design process behind building the second generation of Meta’s first-ever custom silicon for AI workloads, including the PyTorch software ecosystem, and the model architectures for Meta’s key applications. They demonstrate how MTIA achieves the performance, efficiency, and developer experience to successfully launch models into production. They also highlight several co-design examples where special silicon features are utilized to accelerate Meta’s models.\n\nThe post Inside the hardware and co-design of MTIA appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>In this talk from AI Infra @ Scale 2024, Joel Colburn, a software engineer at Meta, technical lead Junqiang Lan, and software engineer Jack Montgomery discuss the second generation of MTIA, Meta’s in-house training and inference accelerator. They cover the co-design process behind building the second generation of Meta’s first-ever custom silicon for AI workloads, [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2024/08/22/ml-applications/meta-mtia-hardware-co-design/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2024/08/22/ml-applications/meta-mtia-hardware-co-design/\">Inside the hardware and co-design of MTIA</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "In this talk from AI Infra @ Scale 2024, Joel Colburn, a software engineer at Meta, technical lead Junqiang Lan, and software engineer Jack Montgomery discuss the second generation of MTIA, Meta’s in-house training and inference accelerator. They cover the co-design process behind building the second generation of Meta’s first-ever custom silicon for AI workloads, [...]\nRead More...\nThe post Inside the hardware and co-design of MTIA appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=21571",
        "categories": [
          "AI Research",
          "ML Applications",
          "AI Infra @ Scale"
        ],
        "isoDate": "2024-08-22T16:00:34.000Z"
      },
      {
        "creator": "",
        "title": "Bringing Llama 3 to life",
        "link": "https://engineering.fb.com/2024/08/21/production-engineering/bringing-llama-3-to-life/",
        "pubDate": "Wed, 21 Aug 2024 16:00:49 +0000",
        "content:encodedSnippet": "Llama 3 is Meta’s most capable openly-available LLM to date and the recently-released Llama 3.1 will enable new workflows, such as synthetic data generation and model distillation with unmatched flexibility, control, and state-of-the-art capabilities that rival the best closed source models. \nAt AI Infra @ Scale 2024, Meta engineers discussed every step of how we built and brought Llama 3 to life, from data and training to inference. \nJoe Spisak, Product Director and Head of Generative AI Open Source at Meta, talks about the history of Llama and Meta’s overarching vision for open source AI.\nHe’s joined by Delia David, a software engineer at Meta, to discuss all things data-related for GenAI. David covers the diversity, volume, and freshness of data needed for GenAI and how different data types should be extracted and prepared.\nKaushik Veeraraghavan, a software engineer at Meta, discusses how Meta trains Llama at scale and delves into the data center, networking, and software investments that have enabled the development of Meta’s Llama 3 models.\nFinally, Ye (Charlotte) Qi, a production engineer at Meta, discusses how Meta handles inference for Llama. Optimizing and scaling LLM inference is important for enabling large-scale product applications. Qi introduces key parallelism techniques that help scale model sizes and context windows, which in turn influence inference system designs. She also discusses practical challenges associated with deploying these complex serving paradigms throughout Meta’s internal cloud to our data center of heterogeneous hardware.\n\nThe post Bringing Llama 3 to life appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>Llama 3 is Meta’s most capable openly-available LLM to date and the recently-released Llama 3.1 will enable new workflows, such as synthetic data generation and model distillation with unmatched flexibility, control, and state-of-the-art capabilities that rival the best closed source models.  At AI Infra @ Scale 2024, Meta engineers discussed every step of how we [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2024/08/21/production-engineering/bringing-llama-3-to-life/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2024/08/21/production-engineering/bringing-llama-3-to-life/\">Bringing Llama 3 to life</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "Llama 3 is Meta’s most capable openly-available LLM to date and the recently-released Llama 3.1 will enable new workflows, such as synthetic data generation and model distillation with unmatched flexibility, control, and state-of-the-art capabilities that rival the best closed source models.  At AI Infra @ Scale 2024, Meta engineers discussed every step of how we [...]\nRead More...\nThe post Bringing Llama 3 to life appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=21564",
        "categories": [
          "AI Research",
          "Culture",
          "ML Applications",
          "Open Source",
          "Production Engineering",
          "AI Infra @ Scale"
        ],
        "isoDate": "2024-08-21T16:00:49.000Z"
      }
    ]
  },
  {
    "name": "eBay Tech Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Twitter Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Netflix TechBlog",
    "category": "기업",
    "posts": [
      {
        "creator": "Netflix Technology Blog",
        "title": "Improve Your Next Experiment by Learning Better Proxy Metrics From Past Experiments",
        "link": "https://netflixtechblog.com/improve-your-next-experiment-by-learning-better-proxy-metrics-from-past-experiments-64c786c2a3ac?source=rss----2615bd06b42e---4",
        "pubDate": "Mon, 26 Aug 2024 15:46:24 GMT",
        "content:encodedSnippet": "By Aurélien Bibaut, Winston Chou, Simon Ejdemyr, and Nathan Kallus\n\nWe are excited to share our work on how to learn good proxy metrics from historical experiments at KDD 2024. This work addresses a fundamental question for technology companies and academic researchers alike: how do we establish that a treatment that improves short-term (statistically sensitive) outcomes also improves long-term (statistically insensitive) outcomes? Or, faced with multiple short-term outcomes, how do we optimally trade them off for long-term benefit?\nFor example, in an A/B test, you may observe that a product change improves the click-through rate. However, the test does not provide enough signal to measure a change in long-term retention, leaving you in the dark as to whether this treatment makes users more satisfied with your service. The click-through rate is a proxy metric (S, for surrogate, in our paper) while retention is a downstream business outcome or north star metric (Y). We may even have several proxy metrics, such as other types of clicks or the length of engagement after click. Taken together, these form a vector of proxy metrics.\nThe goal of our work is to understand the true relationship between the proxy metric(s) and the north star metric — so that we can assess a proxy’s ability to stand in for the north star metric, learn how to combine multiple metrics into a single best one, and better explore and compare different proxies.\nSeveral intuitive approaches to understanding this relationship have surprising pitfalls:\n\nLooking only at user-level correlations between the proxy S and north star Y. Continuing the example from above, you may find that users with a higher click-through rate also tend to have a higher retention. But this does not mean that a product change that improves the click-through rate will also improve retention (in fact, promoting clickbait may have the opposite effect). This is because, as any introductory causal inference class will tell you, there are many confounders between S and Y — many of which you can never reliably observe and control for.\nLooking naively at treatment effect correlations between S and Y. Suppose you are lucky enough to have many historical A/B tests. Further imagine the ordinary least squares (OLS) regression line through a scatter plot of Y on S in which each point represents the (S,Y)-treatment effect from a previous test. Even if you find that this line has a positive slope, you unfortunately cannot conclude that product changes that improve S will also improve Y. The reason for this is correlated measurement error — if S and Y are positively correlated in the population, then treatment arms that happen to have more users with high S will also have more users with high Y.\n\nBetween these naive approaches, we find that the second one is the easier trap to fall into. This is because the dangers of the first approach are well-known, whereas covariances between estimated treatment effects can appear misleadingly causal. In reality, these covariances can be severely biased compared to what we actually care about: covariances between true treatment effects. In the extreme — such as when the negative effects of clickbait are substantial but clickiness and retention are highly correlated at the user level — the true relationship between S and Y can be negative even if the OLS slope is positive. Only more data per experiment could diminish this bias — using more experiments as data points will only yield more precise estimates of the badly biased slope. At first glance, this would appear to imperil any hope of using existing experiments to detect the relationship.\nThis figure shows a hypothetical treatment effect covariance matrix between S and Y (white line; negative correlation), a unit-level sampling covariance matrix creating correlated measurement errors between these metrics (black line; positive correlation), and the covariance matrix of estimated treatment effects which is a weighted combination of the first two (orange line; no correlation).\nTo overcome this bias, we propose better ways to leverage historical experiments, inspired by techniques from the literature on weak instrumental variables. More specifically, we show that three estimators are consistent for the true proxy/north-star relationship under different constraints (the paper provides more details and should be helpful for practitioners interested in choosing the best estimator for their setting):\n\nA Total Covariance (TC) estimator allows us to estimate the OLS slope from a scatter plot of true treatment effects by subtracting the scaled measurement error covariance from the covariance of estimated treatment effects. Under the assumption that the correlated measurement error is the same across experiments (homogeneous covariances), the bias of this estimator is inversely proportional to the total number of units across all experiments, as opposed to the number of members per experiment.\nJackknife Instrumental Variables Estimation (JIVE) converges to the same OLS slope as the TC estimator but does not require the assumption of homogeneous covariances. JIVE eliminates correlated measurement error by removing each observation’s data from the computation of its instrumented surrogate values.\nA Limited Information Maximum Likelihood (LIML) estimator is statistically efficient as long as there are no direct effects between the treatment and Y (that is, S fully mediates all treatment effects on Y). We find that LIML is highly sensitive to this assumption and recommend TC or JIVE for most applications.\n\nOur methods yield linear structural models of treatment effects that are easy to interpret. As such, they are well-suited to the decentralized and rapidly-evolving practice of experimentation at Netflix, which runs thousands of experiments per year on many diverse parts of the business. Each area of experimentation is staffed by independent Data Science and Engineering teams. While every team ultimately cares about the same north star metrics (e.g., long-term revenue), it is highly impractical for most teams to measure these in short-term A/B tests. Therefore, each has also developed proxies that are more sensitive and directly relevant to their work (e.g., user engagement or latency). To complicate matters more, teams are constantly innovating on these secondary metrics to find the right balance of sensitivity and long-term impact.\nIn this decentralized environment, linear models of treatment effects are a highly useful tool for coordinating efforts around proxy metrics and aligning them towards the north star:\n\nManaging metric tradeoffs. Because experiments in one area can affect metrics in another area, there is a need to measure all secondary metrics in all tests, but also to understand the relative impact of these metrics on the north star. This is so we can inform decision-making when one metric trades off against another metric.\nInforming metrics innovation. To minimize wasted effort on metric development, it is also important to understand how metrics correlate with the north star “net of” existing metrics.\nEnabling teams to work independently. Lastly, teams need simple tools in order to iterate on their own metrics. Teams may come up with dozens of variations of secondary metrics, and slow, complicated tools for evaluating these variations are unlikely to be adopted. Conversely, our models are easy and fast to fit, and are actively used to develop proxy metrics at Netflix.\n\nWe are thrilled about the research and implementation of these methods at Netflix — while also continuing to strive for great and always better, per our culture. For example, we still have some way to go to develop a more flexible data architecture to streamline the application of these methods within Netflix. Interested in helping us? See our open job postings!\nFor feedback on this blog post and for supporting and making this work better, we thank Apoorva Lal, Martin Tingley, Patric Glynn, Richard McDowell, Travis Brooks, and Ayal Chen-Zion.\n\nImprove Your Next Experiment by Learning Better Proxy Metrics From Past Experiments was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Netflix Technology Blog",
        "guid": "https://medium.com/p/64c786c2a3ac",
        "categories": [
          "data-science",
          "experimentation",
          "machine-learning",
          "a-b-testing",
          "statistics"
        ],
        "isoDate": "2024-08-26T15:46:24.000Z"
      }
    ]
  },
  {
    "name": "JetBrains: Developer Tools for Professionals and Teams – Company Blog | JetBrains",
    "category": "기업",
    "posts": [
      {
        "creator": "Olga Bedrina",
        "title": "TeamCity Pipelines Pulse, Issue #6: Introducing Self- Hosted Agents",
        "link": "https://blog.jetbrains.com/teamcity/2024/08/teamcity-pipelines-pulse-issue6-self-hosted-agents/",
        "pubDate": "Tue, 27 Aug 2024 14:22:34 +0000",
        "content:encodedSnippet": "Want total control over your CI/CD pipeline?\nWe’re introducing self-hosted agents in TeamCity Pipelines! Define your own build environments, manage your resources, and scale effortlessly. No more waiting in line for shared agents.\n\n\n\n\nYou can now choose whether to run jobs on JetBrains-hosted agents or your own. You can also add various requirements for the agents, such as a specific OS, CPU count, CPU arch, or RAM amount, or even go with custom requirements.\n\n\n\n\nJust like with JetBrains-hosted agents, you can open the terminal and connect it directly to the agent during a job to view logs, check installed software, or debug issues – all from the UI.\n\n\n\n\nLearn more about installing self-hosted agents in our docs.\nNew edit mode toggle for pipelines and jobs\nPreviously, it wasn’t obvious how to navigate from the pipeline editor to currently running jobs.\nWe decided to rethink the whole process and introduced the edit mode toggle. Now, you can easily switch between edit mode and run mode. Accessing pipeline and job settings has also become more convenient.\n\n\n\n\nBug fixes and improvements\nWe’ve added a bunch of bug fixes and improvements to TeamCity Pipelines. Here are the highlights.\nIn TeamCity Pipelines, you can run tests in parallel to speed up execution. The Open Terminal button is once again available during parallel tests.\n\n\n\n\nThe Create pipeline button is now completely hidden from users without proper permissions.\n\n\n\n\nWhen TeamCity Pipelines flags errors because settings are wrong, it will now allow you to save your changes after fixing the problems.\n\n\n\n\nTeamCity Pipelines once again correctly auto-detects runners like Maven, Gradle, or npm and suggests optimizations.\n\n\n\n\nAdding a repository to a pipeline no longer triggers a Repository should not be empty error for valid URLs.\n\n\n\n\nWhen duplicating an unsaved job, its dependencies are no longer ignored.\nDid you know?\nIn TeamCity Pipelines, you can define job dependencies conveniently and visually using the drag-and-drop editor. YAML is also available for those who prefer it.\n\n\n\n\nThat’s it for today! As always, feel free to share your feedback in the comments. \nYours truly,\nThe TeamCity Pipelines team",
        "dc:creator": "Olga Bedrina",
        "content": "Want total control over your CI/CD pipeline? We&#8217;re introducing self-hosted agents in TeamCity Pipelines! Define your own build environments, manage your resources, and scale effortlessly. No more waiting in line for shared agents. You can now choose whether to run jobs on JetBrains-hosted agents or your own. You can also add various requirements for the [&#8230;]",
        "contentSnippet": "Want total control over your CI/CD pipeline? We’re introducing self-hosted agents in TeamCity Pipelines! Define your own build environments, manage your resources, and scale effortlessly. No more waiting in line for shared agents. You can now choose whether to run jobs on JetBrains-hosted agents or your own. You can also add various requirements for the […]",
        "guid": "https://blog.jetbrains.com/?post_type=teamcity&p=505821",
        "categories": [
          "news",
          "releases",
          "release",
          "teamcity-pipelines",
          "teamcity-pipelines-pulse"
        ],
        "isoDate": "2024-08-27T14:22:34.000Z"
      },
      {
        "creator": "Anna Rovinskaia",
        "title": "New Livestream: Introduction to Workspaces in IntelliJ IDEA",
        "link": "https://blog.jetbrains.com/idea/2024/08/new-livestream-java-22-and-intellij-ideanew-livestream/",
        "pubDate": "Tue, 27 Aug 2024 06:59:09 +0000",
        "content:encodedSnippet": "Join us for a new IntelliJ IDEA Livestream episode, where we will explore how to take advantage of the new workspace functionality in your IntelliJ IDEA projects with Andrey Belyaev.\nDate: September 12, 2024\nTime: 3:00 pm – 4:00 pm UTC\nREGISTER FOR THE LIVESTREAM\n\n\n\n\nSession abstract\nThis session will provide a comprehensive overview of the new feature in IntelliJ IDEA: workspaces. Andrey will explain what workspaces are, how they function, and how they can be integrated into your current projects. We will demonstrate some practical examples to get you started: creating a workspace, adding projects, and running them within it.\nAsking questions\nAndrey will try to answer all of your questions during the session. If we run out of time, we’ll publish the answers to any remaining questions in a follow-up blog post.\nYour speaker and host\nSpeaker\nAndrey Belyaev\n\nAndrey Belyaev is a software developer on the IntelliJ IDEA Ultimate team. He works on various plugins and is now primarily focused on cloud and deployment support in IntelliJ IDEA.\n\nHost\nMala Gupta\n\nA Java Champion and JUG leader, Mala has authored multiple books with Manning, Packt, and O’Reilly Publications. She has more than two decades of experience in the software industry and is a regular speaker at industry conferences around the world. She is an active supporter of Java certification as a path to career advancement.\n\nHappy developing!",
        "dc:creator": "Anna Rovinskaia",
        "content": "Join us for a new IntelliJ IDEA Livestream episode, where we will explore how to take advantage of the new workspace functionality in your IntelliJ IDEA projects with Andrey Belyaev. Date: September 12, 2024 Time: 3:00 pm – 4:00 pm UTC REGISTER FOR THE LIVESTREAM Session abstract This session will provide a comprehensive overview of [&#8230;]",
        "contentSnippet": "Join us for a new IntelliJ IDEA Livestream episode, where we will explore how to take advantage of the new workspace functionality in your IntelliJ IDEA projects with Andrey Belyaev. Date: September 12, 2024 Time: 3:00 pm – 4:00 pm UTC REGISTER FOR THE LIVESTREAM Session abstract This session will provide a comprehensive overview of […]",
        "guid": "https://blog.jetbrains.com/?post_type=idea&p=505033",
        "categories": [
          "livestreams",
          "intellij-idea",
          "intellijidealivestream",
          "livestream",
          "webinars"
        ],
        "isoDate": "2024-08-27T06:59:09.000Z"
      },
      {
        "creator": "Aleksandra Aganezova",
        "title": "JetBrains JavaScript Day 2024 Registration Is Now Open",
        "link": "https://blog.jetbrains.com/webstorm/2024/08/jetbrains-javascript-day-2024-registration-is-now-open/",
        "pubDate": "Mon, 26 Aug 2024 15:47:01 +0000",
        "content:encodedSnippet": "Hey everyone! JetBrains JavaScript Day is back for year four, and this edition promises to be our best one yet!\n\n\n\n\n\nLet’s face it, keeping up with the rapid changes in the JavaScript ecosystem is no easy task – it can be hard for us, too. That’s exactly why we created JetBrains JavaScript Day! This year, as always, we brought together top experts to share their insights and discuss the latest trends of modern JavaScript and TypeScript development. Make sure to join us live to ask your questions and be a part of the conversation as it happens.\nWhen: October 24 at 9:00 am EDT (check your timezone)\nWhere: Online\nCost: Free\nRegister!\n                                                    \nHere is this year’s lineup of inspiring speakers and talks: \nVite and the Future of JavaScript Tooling, by Evan You\nTypeScript Generics: Practical Fun!, by Josh Goldberg\nThe Silent Open Source Crisis: When Maintainers Walk Away, by Bekah Hawrot Weigel\nYou Don’t Need JavaScript for That, by Kevin Powell \nEverything You Need to Know About React 19, by Shruti Kapoor\nDriving Your JavaScript Library Ecosystem With Nx, by Chau Tran\nEvolving Angular for the Long Run, by Jeremy Elbourn\nTypeScript and Your Codebase: They Deserve Each Other!, by Danny Thompson\nWe’ve also added an Ask Me Anything (AMA) session with the WebStorm team to our agenda for all of you who want to meet the people behind the JavaScript support for JetBrains IDEs.\nGrab your tickets today, and we’ll see you there!\nYour JetBrains team",
        "dc:creator": "Aleksandra Aganezova",
        "content": "Hey everyone! JetBrains JavaScript Day is back for year four, and this edition promises to be our best one yet! Let’s face it, keeping up with the rapid changes in the JavaScript ecosystem is no easy task – it can be hard for us, too. That’s exactly why we created JetBrains JavaScript Day! This year, [&#8230;]",
        "contentSnippet": "Hey everyone! JetBrains JavaScript Day is back for year four, and this edition promises to be our best one yet! Let’s face it, keeping up with the rapid changes in the JavaScript ecosystem is no easy task – it can be hard for us, too. That’s exactly why we created JetBrains JavaScript Day! This year, […]",
        "guid": "https://blog.jetbrains.com/?post_type=webstorm&p=505594",
        "categories": [
          "news",
          "javascript-day"
        ],
        "isoDate": "2024-08-26T15:47:01.000Z"
      },
      {
        "creator": "Andrei Kislitsyn",
        "title": "Track and Analyze GitHub Star Growth With  Kandy and Kotlin DataFrame",
        "link": "https://blog.jetbrains.com/kotlin/2024/08/track-and-analyze-github-star-growth-with-kandy-and-kotlin-dataframe/",
        "pubDate": "Mon, 26 Aug 2024 14:37:54 +0000",
        "content:encodedSnippet": "Kotlin DataFrame and Kandy are two powerful tools for data analysis in Kotlin. Kotlin DataFrame simplifies data manipulation and processing, while Kandy allows you to create visualizations directly within your Kotlin projects.\nIn this post, we’ll show you how these tools can be used together within Kotlin Notebook to analyze the star history of GitHub repositories. This isn’t just a simple exercise for demonstration purposes – it’s a tutorial that can help you learn how to analyze your own repositories, understand their popularity trends, and visualize your data effectively. All examples from this post are available as a Kotlin Notebook on GitHub or a Notebook on Datalore, a data science platform by JetBrains.\nAnalyze your GitHub star history\nUnderstanding the star history of a GitHub repository can provide insights into its popularity and growth over time. By analyzing this data, you can see how different events and activities impact the interest in your project. Our goal is to equip you with the knowledge and tools to perform this analysis on your own repositories.\nObtain repository stargazers data from GitHub\nFirst, we need to gather data about the users who have starred a given repository. To achieve this, we’ll use the GitHub GraphQL API, which requires a GitHub access token. Here’s a simple function to request data about repo stars, including the starring time and user login:\nimport io.ktor.client.request.*\nimport io.ktor.http.*\n\n/**\n*  We need to specify the repository owner and name, as well as the access token.\n* There can be up to 100 results on one response page.\n* For this example, we'll take only the first 3 results.\n* `endCursor` points to the end of the previous page (`null` for the first one). \n*/\nfun fetchStarHistoryPage(owner: String, name: String, token: String, first: Int = 100, endCursor: String? = null): NotebookHttpResponse {\n   // GraphQL query\n   val query = \"\"\"\n       query {\n         repository(owner: \"$owner\", name: \"$name\") {\n           stargazers(first: $first, after: $endCursor) {\n             edges {\n               starredAt\n               node {\n                 login\n               }\n             }\n             pageInfo {\n               endCursor\n               hasNextPage\n             }\n           }\n         }\n       }\n   \"\"\".trimIndent()\n   // `http` is the default Ktor `HttpClient` for Notebook;\n   // it has the same methods but without `suspend` modifiers, \n   // allowing you to make HTTP requests quickly and easily. \n   // Make a \"post\" request to the API with this query\n   return http.post(\"https://api.github.com/graphql\") {\n       // Set authorization header with token\n       bearerAuth(token)\n       // Set content type header\n       contentType(ContentType.Application.Json)\n       // Set query as body\n       setBody(buildJsonObject { put(\"query\", query) })\n   }\n}\nA convenient and easy way to set an environment variable is through the Kotlin Notebook settings:\n\n\n\n\nNext, specify the repository owner and name, and ensure your GitHub token is securely stored:\nval ownerKotlin = \"Kotlin\"\nval repoKandy = \"kandy\"\n// Keep your token safe as an environment variable or a system property!\n// For example, you can place it in environment variables in Kotlin Notebook settings.\nval token = System.getenv(\"GITHUB_TOKEN\")\nTo start, let’s query a single page with a few users to examine the data. \nval rawResponse = fetchStarHistoryPage(ownerKotlin, repoKandy, token, first = 3)\nrawResponse\n\n\n\n\nThe response from the API looks like this:\nHttpResponse[https://api.github.com/graphql, 200 OK]\nNext, we’ll deserialize the JSON response to a Kotlin data class using the .deserializeJson() extension provided by our Kotlin Notebook Ktor integration. This makes it easier to work with the response body data in Kotlin.\nval starHistorySimplePage = rawResponse.deserializeJson()\n// Take the JSON string for further work with DataFrame\nval responseAsJson = starHistorySimplePage.jsonString\nstarHistorySimplePage\nThe result is a structured object representing the data, which looks like this:\n{\n \"data\": {\n   \"repository\": {\n     \"stargazers\": {\n       \"edges\": [\n         {\n           \"starredAt\": \"2022-07-13T22:46:16Z\",\n           \"node\": {\n             \"login\": \"manojselvam\"\n           }\n  ...\n}\nAfter executing the cell above, starHistorySimplePage is converted to a data class, allowing us to easily access those of its properties that correspond to JSON fields. This seamless integration with IntelliJ IDEA autocompletion makes working with the response straightforward.\n\n\n\n\nFor example, we can extract all the starring times from the page:\nstarHistorySimplePage.data.repository.stargazers.edges.map { it.starredAt }\nOutput:\n[2022-07-13T22:46:16Z, 2022-11-05T14:21:10Z, 2022-11-05T18:42:37Z]\nNext, let’s parse the page data into a DataFrame. \nval starHistoryPageDF = DataFrame.readJsonStr(responseAsJson)\nstarHistoryPageDF\n\n\n\n\nWe need two columns: one showing the user logins and the other their starring times. We can retrieve these columns as follows:\nstarHistoryPageDF.data.repository.stargazers.edges\n   .single() // the `edges` column contains a single DataFrame with current page stargazers\n   .flatten() // `login` is a subcolumn of `node`, after `flatten()` it is a simple column\n\n\n\n\nAdditionally, we need page meta-information, including whether there is a next page and the current page end cursor.\nwith(starHistoryPageDF.data.repository.stargazers.pageInfo) {\n   // Both are columns with a single value\n   println(\"end cursor: ${endCursor.single()}\")\n   println(\"has next page: ${hasNextPage.single()}\")\n}\nThis code outputs the following:\nend cursor: Y3Vyc29yOnYyOpIAzhXiSlk=\nhas next page: true\nNow, let’s create a function that iteratively processes all pages with stargazers and returns a DataFrame with complete information:\n// Casts DataFrame to the type of a given DataFrame so we can use\n// extension columns that have already been generated.\n// Temporary workaround, will be available in future DataFrame releases\n// (https://github.com/Kotlin/dataframe/pull/747)\ninline fun <reified T> AnyFrame.castTo(df: DataFrame<T>): DataFrame<T> {\n   return cast<T>(verify = true)\n}\nimport io.ktor.client.statement.*\n\n// Provide repo owner, name, and access token\nfun fetchStarHistory(owner: String, name: String, token: String): AnyFrame {\n   var hasNextPage: Boolean = true\n   var endCursor: String? = null\n   var buffer: DataFrame<*> = DataFrame.Empty\n   while (hasNextPage) {\n       val response = fetchStarHistoryPage(owner, name, token, 100, endCursor)\n       // Cast type of DataFrame to the type of `starHistoryPageDF`,\n       // so we can use its already-generated extensions\n       val responseDF = DataFrame.readJsonStr(response.bodyAsText()).castTo(starHistoryPageDF)\n       val stargazers = responseDF.data.repository.stargazers\n       buffer = buffer.concat(stargazers.edges.first().flatten())\n       val pageInfo = stargazers.pageInfo\n       endCursor = \"\\\"${pageInfo.endCursor.single()}\\\"\"\n       hasNextPage = pageInfo.hasNextPage.single()\n   }\n   return buffer\n}\nUsing this function, we can now retrieve all the Kandy stargazers:\nval kandyStargazers = fetchStarHistory(ownerKotlin, repoKandy, token)\nkandyStargazers\n\n\n\n\nLook at the DataFrame summary using the .describe() method, which shows meta-information and accumulated statistics about DataFrame columns:\nkandyStargazers.describe()\n\n\n\n\nAll login values are unique, indicating that the dataset is correct. Additionally, there are no null values, so no further processing is needed.\nCreate a DataFrame for cumulative star count analysis\nWe now have two key pieces of information: user logins and the times they award stars. Our next step is to perform an initial analysis.\nWe’ll create a visualization showing the cumulative number of stars received over time, illustrating how user interest in our library grows and changes.\nThis approach will help us understand the dynamics of user engagement and the popularity of our library.\nHere’s how to transform this data:\nConvert the starredAt column to LocalDateTime.\nSort the DataFrame by  starredAt, in ascending order.\nAdd a starsCount column to track the total number of stars over time.\nPut the processing code into a function so that it can be reused later on.\nfun AnyFrame.processStargazers(): AnyFrame {\n   return castTo(kandyStargazers)\n       // Convert `starredAt` column to `LocalDateTime`\n       .convert { starredAt }.toLocalDateTime()\n       // Sort rows by `starredAt`\n       .sortBy { starredAt }\n       // Add `starsCount` column with total stars count at each row.\n       // The star count is simply the row index increased by 1\n       .add(\"starsCount\") { index() + 1 }\n}\nval kandyStarHistory = kandyStargazers.processStargazers()\nkandyStarHistory\n\n\n\n\nVisualize star history: plot with Kandy\nWith the data processed, we can now visualize the star history using Kandy. Here’s a simple line plot to show how the number of stars has changed over time. \nkandyStarHistory.plot {\n   line {\n       // The starring time corresponds to the `x` axis\n       x(starredAt) {\n           axis {\n               // Set the name for the `x` axis\n               name = \"date\"\n               // Set the format for axis breaks\n               breaks(format = \"%b, %Y\")\n           }\n       }\n       // The stars count corresponds to the `y` axis\n       y(starsCount) {\n           // Set the name for the `y` axis\n           axis.name = \"GitHub stars\"\n       }\n   }\n    layout {\n       title = \"Kandy GitHub star history\"\n       size = 800 to 500\n   }\n}\n\n\n\n\nThe plot displays the cumulative growth of stars, reflecting how interest in the Kandy library has evolved. Key points of significant increase can often be associated with major announcements or events related to the library.\nTo better understand how user interest in our library evolves over time, we’ll animate this chart using the Kotlin Jupyter API. This dynamic visualization will help us see how engagement patterns shift and grow, providing deeper insights than a static chart could offer.\nWe’ll start by creating a function that builds a star history chart for the first n star(s). \nfun kandyStarHistoryPlot(n: Int) = kandyStarHistory.plot {\n   line {\n       x(starredAt.take(n)) {\n           axis {\n               name = \"date\"\n               breaks(format = \"%b, %Y\")\n           }\n       }\n       y(starsCount.take(n)) {\n           axis.name = \"GitHub stars\"\n       }\n   }\n   layout {\n       title = \"Kandy GitHub star history\"\n       size = 800 to 500\n   }\n}\nThen, we’ll use the ANIMATE() function to update the cell output for a given set of frames. Each frame will be a star history plot, starting with one star and incrementing by one star each frame until we reach the maximum number of stars.\nANIMATE(50.milliseconds, kandyStarHistory.rowsCount()) { frameID ->\n   // frame with `frameID` contsins plot with `frameID + 1` stars\n   kandyStarHistoryPlot(frameID + 1)\n}\n\n\n\n\nAnalyze key events\nWe’ll look at how different events influenced the growth of stars. We’ll add mark lines with the most important events related to Kandy, such as the Kotlin Notebook video, the Kandy introductory post, the Plotting Financial Data in Kotlin with Kandy post, and KotlinConf 2024. Such analysis helps to identify what drives interest and engagement with the project.\nWe’ll look at events starting from October 2023, which was when we initiated our marketing activities:\nval starHistoryFiltered = kandyStarHistory.filter { starredAt >= LocalDateTime(2023, 10, 1, 0, 0, 0, 0) }\nThen we’ll add mark lines with the events:\nval ktnbYTVideodate = LocalDate(2023, 10, 25)\nval kandyIntroductoryPostDate = LocalDate(2023, 12, 14)\nval kandyFinancialPostDate = LocalDate(2024, 4, 9)\nval kotlinConf24Date = LocalDate(2024, 5, 22)\n\nval kandyEvents = listOf(\n   \"Kotlin Notebook\\nYouTube video\",\n   \"Kandy Introduction\\nKotlin Blog post\",\n   \"Financial Plotting\\nMedium post\",\n   \"KotlinConf 2024\"\n)\nval kandyEventsDates = listOf(ktnbYTVideodate, kandyIntroductoryPostDate, kandyFinancialPostDate, kotlinConf24Date)\n\n\n\n\nTo make the plot more visually engaging, we’ll create a custom color palette for these event markers.\nval eventColors = listOf(\n   Color.hex(\"#1f77b4\"),\n   Color.hex(\"#ff7f0e\"),\n   Color.hex(\"#d62728\"),\n   Color.hex(\"#2ca02c\"),\n)\nFinally, we’ll generate the plot with vertical lines representing these events, allowing us to see how each significant event influenced the star history.\nstarHistoryFiltered.plot {\n   // add vertical marklines with event dates\n   vLine {\n       color(kandyEvents, \"event\") { scale = categorical(eventColors, kandyEvents) }\n       xIntercept(kandyEventsDates)\n       width = 1.5\n       alpha = 0.9\n   }\n   line {\n       x(starredAt) {axis.name = \"date\" }\n       y(starsCount) { axis.name = \"GitHub stars\" }\n   }\n   layout {\n       title = \"Kandy GitHub star history & key events\"\n       size = 800 to 500\n       style {\n           legend.position = LegendPosition.Bottom\n       }\n   }\n}\n\n\n\n\nThis plot shows the number of stars Kandy received each month, with different colors representing key events that influenced these numbers. For example, the introductory post and other significant updates coincide with noticeable increases in stars, highlighting the influence of these activities on community engagement.\nAnalyze monthly star growth\nTo analyze the monthly growth of stars, we will create a bar chart to visually display the changes in the number of stars received each month. This visualization will help us identify key growth periods and evaluate the effectiveness of our marketing strategies.\nFirst, let’s define simple extension functions to convert the LocalDate/LocalDateTime to a month and four-figure year format.\nfun LocalDate.toMonthOfYear(): String = \"$month, $year\"\nfun LocalDateTime.toMonthOfYear(): String = \"$month, $year\"\nNow, we’ll add the “month” column to our DataFrame:\nval starHistoryWithMonth = starHistoryFiltered.add(\"month\") {\n   starredAt.toMonthOfYear()\n}\nstarHistoryWithMonth\n\n\n\n\nNext, we’ll group the DataFrame by the “month” column and count the number of stars in each group.\nval starsCountMonthly = starHistoryWithMonth.groupBy { month }.count()\nstarsCountMonthly\n\n\n\n\nNext, we’ll add information about key events to the DataFrame. We’ll include the events in the corresponding months and set the value to null if there were no events.\nFirst, create a DataFrame with events and their corresponding months:\nval eventsDF = dataFrameOf(\"event\" to kandyEvents, \"month\" to kandyEventsDates.map {\n   it.toMonthOfYear()\n})\nThen, perform a left join with our main DataFrame at the month column:\nval starsMonthlyWithEvent = starsCountMonthly.leftJoin(eventsDF) { month }\nstarsMonthlyWithEvent\n\n\n\n\nNow, we can create a bar plot to visualize the distribution of new stars by month, along with the key events.\nstarsMonthlyWithEvent.plot {\n   bars {\n       x(month)\n       y(count)\n       alpha = 0.8\n       fillColor(event) { scale = categorical(eventColors, kandyEvents) }\n   }\n   // add horizontal markline with median of monthly count\n   hLine {\n       val medianMonthly = count.median()\n       yIntercept.constant(medianMonthly)\n       type = LineType.DASHED\n       color = Color.hex(\"#4b0082\")\n       width = 2.0\n   }\n   layout {\n       title = \"Kandy GitHub star history (monthly count)\"\n       size = 800 to 500\n       style {\n           legend.position = LegendPosition.Bottom\n           xAxis.text { angle = 30.0 }\n       }\n   }\n}\n\n\n\n\nThis plot shows the monthly distribution of stars, with bars representing the number of stars each month. The colors of the bars indicate key events, providing a clear visualization of how these events impacted the star counts. The dashed horizontal line represents the median star count per month.\nUnlike the overall star history chart, which shows cumulative growth, the monthly statistics plot helps you pinpoint the exact timing and impact of key events. By creating similar plots for your own projects, you can better understand the effectiveness of your promotional efforts, identify seasonal patterns, and plan future activities more effectively.\nUnderstand your audience\nUnderstanding the top programming languages of your stargazers can provide insights into your audience. With this in mind, we’ll use the GitHub REST API to find out the most popular languages among Kandy stargazers and visualize this data as a pie chart.\nLet’s write a function that requests user repositories:\nimport io.ktor.http.*\n\nfun getUserRepos(login: String): AnyFrame {\n   return DataFrame.readJsonStr(http.get(\"https://api.github.com/users/$login/repos\") {\n       // Set authorization header with token\n       bearerAuth(token)\n       // Add GitHub API custom \"accept\" header\n       header(HttpHeaders.Accept, \"application/vnd.github.v3+json\")\n   }.deserializeJson().jsonString)\n}\nNext, we’ll test this function on our sample repositories:\nval myRepos = getUserRepos(\"Kotlin\")\nmyRepos\n\n\n\n\nEach column in this DataFrame corresponds to a repository and contains different information about that repository. We are interested in the language column. We can count the most frequent language using the .valueCounts() method, where the first entry represents the most popular language:\nval myLanguagesCounts = myRepos.language.valueCounts(dropNA = false) // Don't drop nulls\nmyLanguagesCounts\n\n\n\n\nBecause the rows are sorted by count by default, identifying the most popular language is straightforward – it’s the first one.\nmyLanguagesCounts.language.first()\nKotlin\nTo generalize this process, we’ll write an extension function for a DataFrame obtained from the user’s repositories. This extension function will retrieve the most popular language (returning null if the account is private, has no repositories, or lacks sufficient information).\nfun AnyFrame.getTopLanguage(): String? {\n   //  Handle non-default response bodies (private account, no repositories, etc.)\n   if (!containsColumn(\"language\")) return null\n   return castTo(myRepos).language\n       .valueCounts(dropNA = false)\n       .castTo(myLanguagesCounts)\n       .language.let { languages ->\n           val first = languages.firstOrNull()\n           //  Try to pick the second value if the first one is null\n           if (first == null && languages.size() >= 2) {\n               languages[1]\n           } else first\n       }\n}\nNow, let’s retrieve the most popular languages for all stargazers. Note that this process might take some time to execute:\nval stargazersLanguages = kandyStarHistory.select {\n   login and login.map { login -> getUserRepos(login).getTopLanguage() }.named(\"language\")\n}\nstargazersLanguages\n\n\n\n\nNext, we’ll count the occurrences of each language:\nval languageCounts = stargazersLanguages.language.valueCounts() // Drops null by default\nlanguageCounts\n\n\n\n\nFinally, let’s plot these counts as a pie chart. We’ll take the seven most popular languages and group the remaining ones into an “other” category:\nlanguageCounts.let {\n   val takeFirst = 7\n   it.take(takeFirst).concat(\n       dataFrameOf(\"language\" to listOf(\"other\"), \"count\" to listOf(it.drop(takeFirst).sum {count}))\n   )\n}.plot {\n   pie {\n       slice(\"count\")\n       fillColor(\"language\")\n       size = 25.0\n       hole = 0.3\n   }\n   layout {\n       title = \"Kandy stargazers' most popular languages\"\n       style(Style.Void)\n   }\n}\n\n\n\n\nThe pie chart shows that Kotlin is the most popular language among Kandy stargazers, confirming our primary audience as Kotlin developers. The presence of Java suggests potential for further engagement with related ecosystems. The inclusion of less-common languages highlights the diversity of our user base, which is important for understanding different use cases and potential feature requests.\nThese insights can help tailor your project’s documentation, tutorials, and marketing efforts to better serve and expand your audience.\nCompare star growth: Kandy vs. Kotlin DataFrame\nComparing star data across different projects can provide valuable insights into their popularity and user engagement. Here, we’ll look at the growth of stars for Kandy alongside Kotlin DataFrame. These two projects, launched within a year of each other, target the same audience of Kotlin developers.\nTo ensure a fair comparison, we’ll use the introduction post date as the starting point for both libraries and examine the six months that followed. This way, we can see how each project grew over the same timeframe, giving us a clearer picture of their growth patterns.\nval repoDataframe = \"dataframe\"\n// Use the already written methods to get star history for DataFrame\nval dataFrameStarHistory = fetchStarHistory(ownerKotlin, repoDataframe, token).processStargazers()\n\n\n\n\nDefining the introductory post date for DataFrame:\nval dataFrameIntroductoryPostDate = LocalDate(2022, 6, 30)\nNext, we’ll define a function to process the star history for the six months following the introduction post:\n// Function that will slightly transform the dataframe with star history for a given library: \n// 1) Take a period of six months after the introduction post date; \n// 2) Add a column \"daysAfterPost\" with the number of days after the post date; \n// 3) Take the maximum number of stars for the day; \n// 4) Add a column \"library\" corresponding to the name of the library.\nfun AnyFrame.proccessAfterPostPeriod(introductionPostDate: LocalDate, library: String): AnyFrame {\n   // Six-month period after `introductionPostDate`\n   val period = (introductionPostDate - DatePeriod(days = 1))..(introductionPostDate + DatePeriod(months = 6))\n   return castTo(kandyStarHistory)\n       // Only take stars placed during that period\n       .filter { starredAt.date in period }\n       // Add daysAfterPost column with number of days after post\n       .add(\"daysAfterPost\") {\n           introductionPostDate.daysUntil(starredAt.date)\n       }\n       // Group by number of days and take the max value of `starsCount` for each group\n       .groupBy(\"daysAfterPost\").max { starsCount }\n       // Add a column with library name\n       .add(\"library\") { library }\n}\nFinally, we’ll combine the star histories for Kandy and DataFrame into a single DataFrame for comparison:\n// Count six-month history for both libraries and concatenate them into one DataFrame\nval kandyAndDataFrameStarHistory = kandyStarHistory\n   .proccessAfterPostPeriod(kandyIntroductoryPostDate, \"Kandy\")\n   .concat(\n       dataFrameStarHistory.proccessAfterPostPeriod(dataFrameIntroductoryPostDate, \"DataFrame\")\n   )\nkandyAndDataFrameStarHistory\n\n\n\n\nNext, we’ll visualize the comparison:\nkandyAndDataFrameStarHistory.plot {\n   line {\n       x(daysAfterPost) {\n           axis {\n               name = \"days after post\"\n           }\n       }\n       y(starsCount) {\n           axis.name = \"GitHub stars\"\n       }\n       color(library)\n   }\n   layout {\n       title = \"Kandy vs. DataFrame GitHub stars history\\nwithin 6 months after the introductory post\"\n       size = 800 to 500\n   }\n}\n\n\n\n\nFrom the initial observation, we can see that before the introduction post, both Kandy and Kotlin DataFrame had similar star counts. However, immediately after the post, Kandy showed a significantly higher growth rate, achieving nearly twice as many stars as DataFrame within the first six months.\nThis difference suggests several things. Firstly, it shows the growing interest in Kotlin for data projects. The period of time that elapsed from the initial DataFrame post and the Kandy post was about a year and a half. While DataFrame helped establish a community of Kotlin data enthusiasts, Kandy attracted a new audience interested in visualization.\nAdditionally, Kandy had more intense promotional activities within the six months following its first post, which likely contributed to its rapid growth.\nShared stargazers\nIt’s also interesting to see how many users have starred both Kandy and DataFrame. We hypothesize that there will be a significant overlap, since both libraries serve the same community of Kotlin developers. Here’s how we can analyze this and get the relevant data:\n// inner join star history dataframes of repositories by login,\n// getting a dataframe with all common stargazers, taking its size to get a number of them\nval commonStargazers = kandyStarHistory.innerJoin(dataFrameStarHistory) { login }.rowsCount()\nval kandyTotalStargazers = kandyStarHistory.rowsCount()\nval kandyOnlyStargazers = kandyTotalStargazers - commonStargazers\nval dataFrameTotalStargazers = dataFrameStarHistory.rowsCount()\nval dataFrameOnlyStargazers = dataFrameTotalStargazers - commonStargazers\nPlot this data as a pie chart:\nplot {\n   pie {\n       slice(listOf(commonStargazers, kandyOnlyStargazers, dataFrameOnlyStargazers))\n       fillColor(listOf(\"Common\", \"Kandy only\", \"DataFrame only\")) {\n           scale = categorical(\n               \"Common\" to Color.hex(\"#4A90E2\"),\n               \"Kandy only\" to Color.hex(\"#F5A623\"),\n               \"DataFrame only\" to Color.hex(\"#7ED321\"),\n           )\n           legend.name = \"\"\n       }\n       size = 25.0\n   }\n   layout {\n       title = \"Kandy & DataFrame stargazers ratio\"\n       style(Style.Void)\n   }\n}\n\n\n\n\nThe analysis shows that the majority of stargazers are unique to DataFrame, with fewer users starring both DataFrame and Kandy. Specifically, the share of DataFrame stargazers who also starred Kandy is quite small. This is probably because many users use DataFrame for data tasks that don’t involve visualization, making Kandy less relevant to them.\nInterestingly, only about a quarter of Kandy stargazers have also starred DataFrame. This suggests that Kandy has attracted a new audience mainly interested in plotting, rather than data processing. This reveals a great opportunity to promote how both libraries can work together.\nUsing Kandy for visualization and DataFrame for data processing allows users to benefit from the strengths of both libraries. This combination, as we’ve shown in this post, can help create powerful and comprehensive data analysis solutions. By highlighting this synergy, we can encourage more users to explore how these tools can complement each other and enhance their data projects.\nConclusion\nIn this post, we explored how to use Kotlin DataFrame and Kandy to dive into the star history of GitHub repositories. But it wasn’t just about looking at the numbers – it was about uncovering the stories those numbers tell.\nOne big takeaway is how quickly Kandy gained traction after its launch, highlighting a growing interest in visualization tools within the Kotlin community. Yet, we also found that many Kandy users haven’t tried DataFrame, and vice versa. This shows there’s an opportunity to help developers see how these tools can complement each other.\nWe also noticed that certain events, like blog posts and conferences, had a noticeable impact on star counts. This kind of insight can help you time your own announcements to get the most attention.\nWhat’s next?\nNow it’s your turn! Apply these techniques to your own repositories, analyze their star history, and create your own visualizations within Kotlin Notebook. All examples from this post are available as a Kotlin Notebook on GitHub or a Notebook on Datalore.\nWe’d love to see your results and hear your feedback. Join us in the #datascience channel on Kotlin Slack, or reach out via GitHub issues for Kandy or Kotlin DataFrame.\nIf you find our repositories useful, we’d really appreciate it if you starred them. Your support helps us improve and develop these tools further.\nWhat else to read and watch\nFor more information, check out the following resources:\nKotlin for Data Analysis Overview\nGet started with Kotlin Notebook\nA Step-by-Step Guide to Performing Data Analysis With Kotlin DataFrame\nData Analytics With Kotlin Notebooks, DataFrame, and Kandy",
        "dc:creator": "Andrei Kislitsyn",
        "content": "Kotlin DataFrame and Kandy are two powerful tools for data analysis in Kotlin. Kotlin DataFrame simplifies data manipulation and processing, while Kandy allows you to create visualizations directly within your Kotlin projects. In this post, we’ll show you how these tools can be used together within Kotlin Notebook to analyze the star history of GitHub [&#8230;]",
        "contentSnippet": "Kotlin DataFrame and Kandy are two powerful tools for data analysis in Kotlin. Kotlin DataFrame simplifies data manipulation and processing, while Kandy allows you to create visualizations directly within your Kotlin projects. In this post, we’ll show you how these tools can be used together within Kotlin Notebook to analyze the star history of GitHub […]",
        "guid": "https://blog.jetbrains.com/?post_type=kotlin&p=504121",
        "categories": [
          "ecosystem",
          "data-analysis",
          "dataframe",
          "kandy",
          "notebooks"
        ],
        "isoDate": "2024-08-26T14:37:54.000Z"
      },
      {
        "creator": "Alena Guzharina",
        "title": "A Complete Guide to Credit Risk Analysis With Python and Datalore AI",
        "link": "https://blog.jetbrains.com/datalore/2024/08/26/a-complete-guide-to-credit-risk-analysis-with-python-and-datalore-ai/",
        "pubDate": "Mon, 26 Aug 2024 14:34:49 +0000",
        "content:encodedSnippet": "This is a guest blog post by Ryan O’Connell, CFA, FRM.\nUnderstanding the relationships between various economic indicators is crucial for navigating the financial landscape. These relationships significantly impact the overall state of the economy, affecting businesses, investors, and individuals alike. \n“In this tutorial, we’ll focus on the complex interplay between federal funds rates, 10-year Treasury yields, and corporate bond yields – key indicators that shape investment strategies, economic forecasts, and policy decisions.”\n\n            \nRyan O’Connell\n                                                                CFA, FRM\n                                    \nWe’ll analyze data spanning over two decades of financial history, leveraging the power of Python, the Federal Reserve Economic Data (FRED) API, and Datalore’s AI-assisted coding capabilities. Python is particularly well-suited for this analysis due to its ability to efficiently pull and process large amounts of financial data through APIs. This approach allows for easy automation and replication of the analysis, offering a significant advantage over traditional spreadsheet methods.\nOur goal is to uncover patterns, anomalies, and insights that illuminate the predictive power of yield curve changes on credit spreads. By using Python, we can create a workflow that’s not only powerful but also easily updatable for future analyses in just a few clicks.\n\n\n\n\nIn this tutorial, you’ll learn how to:\nSet up your Python environment in Datalore for financial data analysis.\nUse Datalore’s AI Assistant to generate code for data retrieval and preprocessing.\nCreate visualizations to analyze yield curve dynamics and credit spreads.\nImplement statistical analyses to explore the relationship between yield curves and credit risk.\nLeverage AI to help interpret results and generate insights.\nWhether you’re a credit risk analyst looking to leverage more of Python within your stack or a curious newcomer to the world of credit risk, this tutorial will provide you with the tools and techniques to conduct your own in-depth analysis using Python and Datalore AI assistance.\nLet’s dive in and start coding!\n      \n      Open Datalore Code\n\n\n\n\n\nDisclaimer: This article is for informational and educational purposes only and is not intended to serve as personal financial advice.\nThe Yield Curve: A Financial Crystal Ball?\nBefore we dive into the data, let’s establish some context. The yield curve, particularly the spread between long-term and short-term interest rates, has long been considered a powerful predictive tool in finance. But why?\nEconomic expectations: The yield curve reflects market expectations about future economic conditions. A normal, upward-sloping curve suggests optimism, while an inverted curve often signals pessimism.\nMonetary policy: It provides insights into the market’s view of future monetary policy decisions by central banks.\nCredit risk: The shape of the yield curve can influence lending behaviors and, consequently, credit risk in the economy.\n\n\n\n\nOur analysis focuses on three key components:\nThe federal funds rate: the short-term interest rate at which banks lend to each other overnight.\nThe 10-year Treasury yield: a benchmark for long-term interest rates in the US.\nCorporate bond yields: representing the cost of borrowing for corporations.\n\n\n\n\nBy examining the relationships between these factors, we aim to uncover insights that could help predict changes in credit spreads and, by extension, credit risk in the market.\nSetting up your environment and retrieving data\nBefore we dive into the analysis, you have two options to get started:\nStart a new Datalore notebook to follow along with this tutorial from scratch.\nIf you have a Datalore account, you can click Edit copy on this Datalore report. This option allows you to see the full analysis and modify it as you go.\nWhichever option you choose, we’ll need to set up our Python environment in Datalore and retrieve the necessary data. We’ll be using the Federal Reserve Economic Data (FRED) API to fetch historical data on federal funds rates, 10-year Treasury yields, and corporate bond yields.\nSetting up your FRED API key\nTo use the FRED API, you’ll need to obtain an API key. Here’s how to set it up in Datalore:\nGo to the FRED API Key documentation and click on Request API Key.\nRegister for an account or log in if you already have one.\nOnce you have your API key, go to Environment in your Datalore workspace.\nClick on Environment Variables.\nSelect + New Variable.\nSet up the variable as follows:\n\nVariable title: FRED_API_KEY\nKey: FRED_API_KEY\nValue: Your actual FRED API key\n\n\n\n\nInstalling required packages\nIn Datalore, you can easily install required packages. For this tutorial, we’ll need the fredapi package. To install it:\nGo to the Environment tab in Datalore.\nClick on Package Management.\nSearch for ”fredapi” and click Install.\n\n\n\n\nDatalore’s Environment Manager\n\n\n\nRetrieving fed funds rate data with AI assistance\nNow, let’s use Datalore’s AI assistance to help us write code to retrieve and plot 20 years of federal funds rate data. \nDatalore AI\n\n\n\nHere’s how you can prompt the AI:\n\nAI Prompt\nWrite Python code to retrieve the last 20 years of federal funds rate data using the FRED API. Please include the following steps:\n1. Import necessary libraries (fredapi and pandas)\n2. Set up the FRED API connection using the FRED_API_KEY environment variable\n3. Retrieve the federal funds rate data (series ID: FEDFUNDS) for the last 20 years\n4. Convert the data to a pandas DataFrame\n5. Display the first few rows of the DataFrame\n6. Plot the federal funds rate over time using matplotlib\nNote: Include comments explaining each step of the process.\n\n\n\n\n\nHere’s the code the AI might generate:\n# Import necessary libraries\nimport pandas as pd\nfrom fredapi import Fred\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nimport os\n\n# Set up the FRED API connection using the environment variable\nfred = Fred(api_key=os.environ['FRED_API_KEY'])\n\n# Calculate the date 20 years ago from today\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=20*365)\n\n# Retrieve the federal funds rate data (series ID: FEDFUNDS) for the last 20 years\nfed_funds_data = fred.get_series('FEDFUNDS', start_date, end_date)\n\n# Convert the data to a pandas DataFrame\nfed_funds_df = pd.DataFrame(fed_funds_data, columns=['Rate'])\nfed_funds_df.index.name = 'Date'\n\n# Display the first few rows of the DataFrame\nprint(fed_funds_df.head())\n\n# Plot the federal funds rate over time\nplt.figure(figsize=(12, 6))\nplt.plot(fed_funds_df.index, fed_funds_df['Rate'])\nplt.title('Federal Funds Rate (Last 20 Years)')\nplt.xlabel('Date')\nplt.ylabel('Rate (%)')\nplt.grid(True)\nplt.show()\n      \n      Open Datalore Code\n\n\n\n\n\nThe graph that this code plots can be seen below.\nFederal funds rate: navigating two decades of monetary policy\nNow that we’ve retrieved and plotted the data, let’s analyze the key trends and events visible in our graph:\n\n\n\n\nKey observations from our initial data exploration of the federal funds rate:\nCyclical nature: The federal funds rate exhibits clear cyclical patterns over the 20-year period, reflecting the economic cycles and the Federal Reserve’s monetary policy responses.\nPre-2008 financial crisis: From 2004 to 2006, we observe a steady increase in the rate, peaking at around 5.25% in 2006–2007. This period of rising rates was likely in response to a growing economy and inflationary pressures.\n2008 financial crisis: The rate drops dramatically starting in 2007 and accelerates in 2008, coinciding with the Global Financial Crisis. The Fed rapidly cut rates to near zero in an attempt to stimulate the economy.\nExtended low-rate environment: From 2009 to 2015, we see an unprecedented period of near-zero interest rates, often referred to as the “zero lower bound” era. This reflects the Fed’s aggressive monetary policy to support economic recovery post-crisis.\nGradual normalization: Starting in late 2015, there’s a gradual increase in rates as the Fed began to normalize monetary policy in response to improving economic conditions.\nCOVID-19 pandemic impact: In early 2020, we observe another sharp drop to near-zero rates, mirroring the Fed’s emergency response to the economic shock of the COVID-19 pandemic.\nRecent rapid increase: From 2022 to 2023, we see a steep and rapid increase in rates, reaching levels not seen since before the 2008 crisis. This reflects the Fed’s aggressive tightening to combat high inflation.\n\n\n\n\nThese observations set the stage for our deeper analysis of how federal funds rate changes interact with other key indicators and influence credit risk. Next, we’ll retrieve Treasury yields and corporate bond rates.\nRetrieving 10-year Treasury rate data with AI assistance\nIn this section, we’ll focus on retrieving historical data for 10-year US Treasury yields, a key indicator in financial markets. The 10-year Treasury yield is often used as a benchmark for other interest rates and is crucial for understanding broader economic trends.\nWe’ll use the FRED API to fetch 20 years of daily data, giving us a comprehensive view of long-term interest rate movements.\nLet’s start by prompting the AI to write our data retrieval and visualization code. Here’s the prompt we’ll use:\n\nAI Prompt\nWrite Python code to retrieve the last 20 years of 10-year Treasury yields data using the FRED API. Please include the following steps:\n1. Import necessary libraries (if not already imported)\n2. Use the existing FRED API connection (assuming it’s already set up)\n3. Retrieve the 10-year Treasury yields data (series ID: DGS10) for the last 20 years\n4. Convert the data to a pandas DataFrame\n5. Display the first few rows of the DataFrame\n6. Plot the 10-year Treasury yields over time using matplotlib\nNote: Include comments explaining each step of the process.\n\n\n\n\n\nHere’s the code the AI might generate:\n# Import necessary libraries (if not already imported)\nimport pandas as pd\nfrom fredapi import Fred\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nimport os\n\n# Use the existing FRED API connection\nfred = Fred(api_key=os.environ['FRED_API_KEY'])\n\n# Calculate the date 20 years ago from today\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=20*365)\n\n# Retrieve the 10-year Treasury yields data (series ID: DGS10) for the last 20 years\ntreasury_10y_data = fred.get_series('DGS10', start_date, end_date)\n\n# Convert the data to a pandas DataFrame\ntreasury_10y_df = pd.DataFrame(treasury_10y_data, columns=['Yield'])\ntreasury_10y_df.index.name = 'Date'\n\n# Display the first few rows of the DataFrame\nprint(treasury_10y_df.head())\n\n# Plot the 10-year Treasury yields over time\nplt.figure(figsize=(12, 6))\nplt.plot(treasury_10y_df.index, treasury_10y_df['Yield'])\nplt.title('10-Year Treasury Yields (Last 20 Years)')\nplt.xlabel('Date')\nplt.ylabel('Yield (%)')\nplt.grid(True)\nplt.show()\n      \n      Open Datalore Code\n\n\n\n\n\nNow, let’s take a look at the plot of 20 years of long-term Treasury data.\n10-year Treasury yields: two decades of fluctuations\n\n\n\n\nNow that we’ve retrieved and plotted the data, let’s analyze the key trends and events visible in our graph:\n2008 financial crisis impact: There’s a sharp decline in yields starting in late 2007 and accelerating through 2008, mirroring the flight to safety during the financial crisis, though less dramatic than the federal funds rate drop.\nPost-crisis volatility and recovery: After the initial drop, yields showed significant volatility between 2009 and 2012, followed by a gradual increase until 2018, reflecting the uncertain economic recovery and the effects of changing monetary policies.\nCOVID-19 pandemic shock: In early 2020, we observe a dramatic plunge in yields to historic lows below 1%, coinciding with the economic shock of the COVID-19 pandemic and aggressive Fed intervention.\nRecent rapid increase: From 2021 onwards, there’s a sharp upward trend in yields, reaching levels not seen since before the 2008 crisis, aligning with rising inflation concerns and the Fed’s tightening monetary policy.\n\n\n\n\nThese observations provide crucial context for understanding how both short-term and longer-term interest rates have behaved in response to economic conditions and policy changes over the past two decades. As we proceed, we’ll explore how these rate movements interact to shape the overall yield curve and influence credit risk in the market.\nRetrieving corporate bond yields with AI assistance\nHaving examined the 10-year Treasury yields, we now turn to corporate bond yields, which provide insights into borrowing costs for corporations and serve as a key component in our credit risk analysis. We’ll use the ICE BofA US Corporate Index Effective Yield as our benchmark, offering a broad measure of US dollar-denominated investment-grade corporate debt performance.\nLet’s use Datalore’s AI assistance to generate code that retrieves and visualizes 20 years of daily data from the FRED API. Here’s the prompt:\n\nAI Prompt\nWrite Python code to retrieve the last 20 years of corporate bond yield data using the FRED API. Please include the following steps:\n1. Use the existing FRED API connection\n2. Retrieve the ICE BofA US Corporate Index Effective Yield data (series ID: BAMLC0A0CMEY) for the last 20 years\n3. Convert the data to a pandas DataFrame\n4. Handle any missing values appropriately\n5. Display the first few rows of the DataFrame\n6. Plot the corporate bond yields over time using matplotlib\nNote: Include comments explaining each step of the process.\n\n\n\n\n\nMy Datalore AI Assistant produced this code:\n# Use the existing FRED API connection\n# Assuming fred is already defined and the API key is set up as an environment variable\n\n# Calculate the date 20 years ago from today\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=20*365)\n\n# Retrieve the ICE BofA US Corporate Index Effective Yield data (series ID: BAMLC0A0CMEY) for the last 20 years\ncorporate_yield_data = fred.get_series('BAMLC0A0CMEY', start_date, end_date)\n\n# Convert the data to a pandas DataFrame\ncorporate_yield_df = pd.DataFrame(corporate_yield_data, columns=['Yield'])\ncorporate_yield_df.index.name = 'Date'\n\n# Handle missing values by forward filling (using the last known value)\ncorporate_yield_df = corporate_yield_df.fillna(method='ffill')\n\n# Display the first few rows of the DataFrame\nprint(corporate_yield_df.head())\n\n# Plot the corporate bond yields over time\nplt.figure(figsize=(12, 6))\nplt.plot(corporate_yield_df.index, corporate_yield_df['Yield'])\nplt.title('ICE BofA US Corporate Index Effective Yield (Last 20 Years)')\nplt.xlabel('Date')\nplt.ylabel('Yield (%)')\nplt.grid(True)\nplt.show()\n      \n      Open Datalore Code\n\n\n\n\n\nIn the next section, we will analyze the results of this code block.\nCorporate bond yields: two decades of risk and reward\n\n\n\n\nNow that we’ve retrieved and plotted the data, let’s analyze the key trends and events in corporate bond yields over the past 20 years:\n2008 financial crisis spike: The most dramatic feature of the graph is the sharp spike in yields during the 2008 financial crisis, reaching a peak of over 9%. This reflects the extreme risk aversion and liquidity concerns in the corporate bond market during this period.\nPre-crisis build-up: Leading up to the 2008 crisis, we observe a gradual increase in yields from 2004 to 2007, possibly indicating growing concerns about credit risk in the years preceding the financial crisis.\nPost-crisis decline and stabilization: Following the 2008 peak, yields declined rapidly and then stabilized, fluctuating between 3% and 4% for much of the period from 2010 to 2019. This reflects the impact of the Fed’s accommodative monetary policy and a general improvement in economic conditions.\nCOVID-19 pandemic impact: In early 2020, we see another significant, though less extreme, spike in yields coinciding with the onset of the COVID-19 pandemic. This spike was quickly reversed, likely due to swift central bank intervention.\nRecent upward trend: From 2021 onwards, we observe an upward trend in yields, reaching levels not seen since before the 2008 crisis. This aligns with rising inflation concerns and tightening monetary policy, reflecting increased borrowing costs for corporations.\n\n\n\n\nThese observations provide valuable insights into how corporate borrowing costs and perceived credit risk have evolved over the past two decades. In the next section, we’ll explore how these corporate bond yields interact with the federal funds rate and Treasury yields to shape the overall credit risk landscape.\nFed Funds Rate vs. 10-year Treasury yield and spread analysis\nAfter examining the federal funds rate and 10-year Treasury yields individually, we now turn our attention to the relationship between these two crucial indicators. The spread between these rates, often referred to as the yield curve slope, provides valuable insights into economic expectations and potential credit risk.\nThe yield curve spread is calculated as follows:\nYield Curve Spread = 10-Year Treasury Yield - Federal Funds Rate\nA positive spread indicates a “normal” yield curve, generally associated with economic growth expectations. Conversely, a negative spread signals an inverted yield curve, which is often seen as a predictor of economic downturns.\nLet’s use AI assistance to create a comprehensive line chart visualization that tracks these key elements. Here’s the prompt for the AI:\n\nAI Prompt\nPlot monthly Fed Funds Rate, Corporate Bond Yields, and their spread:\n1. Use ‘fed_funds_data’ for Fed Funds Rate\n2. Retrieve Corporate Bond Yield data (BAMLC0A0CMEY) from FRED\n3. Resample both to monthly frequency and align to common date range\n4. Calculate spread (Corporate Bond Yield minus Fed Funds Rate)\n5. Remove NaN values\n6. Plot all three lines with different colors\n7. Include legend, title, axis labels, and red dashed line at y=0\n8. Show summary statistics\n\n\n\n\n\nYour AI assistant should produce code that looks something like this:\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Resample both series to monthly frequency and align them\nfed_funds_monthly = fed_funds_data.resample('M').last()\ntreasury_10y_monthly = treasury_10y_data.resample('M').last()\n\n# Create a DataFrame with both series, using the common date range\nstart_date = max(fed_funds_monthly.index.min(), treasury_10y_monthly.index.min())\nend_date = min(fed_funds_monthly.index.max(), treasury_10y_monthly.index.max())\n\ndf = pd.DataFrame({\n    'Fed Funds Rate': fed_funds_monthly[start_date:end_date],\n    '10-Year Treasury Yield': treasury_10y_monthly[start_date:end_date]\n})\n\n# Calculate the spread\ndf['Spread'] = df['10-Year Treasury Yield'] - df['Fed Funds Rate']\n\n# Remove any rows with NaN values\ndf = df.dropna()\n\n# Plot the data\nplt.figure(figsize=(12, 6))\nplt.plot(df.index, df['Fed Funds Rate'], label='Fed Funds Rate', color='blue')\nplt.plot(df.index, df['10-Year Treasury Yield'], label='10-Year Treasury Yield', color='orange')\nplt.plot(df.index, df['Spread'], label='Spread', color='green')\n\nplt.title('Fed Funds Rate vs 10-Year Treasury Yield and Spread')\nplt.xlabel('Date')\nplt.ylabel('Rate (%)')\nplt.legend()\nplt.grid(True)\n\n# Add horizontal line at y=0\nplt.axhline(y=0, color='r', linestyle='--')\n\nplt.show()\n\n# Print some summary statistics\nprint(\"\\nSummary Statistics:\")\nprint(df.describe())\n      \n      Open Datalore Code\n\n\n\n\n\nIn the next section, we will dive into the plotted results of this code to draw some conclusions about the economy over time.\nInterpreting yield curve dynamics\n\n\n\n\nNow that we’ve visualized the data, let’s analyze the key trends and events in yield curve dynamics:\nPre-2008 inversion: Leading up to the 2008 financial crisis, we observe a period of yield curve inversion (negative spread), with the fed funds rate exceeding the 10-year Treasury yield. This inversion is often seen as a predictor of economic recessions. In this particular case, the yield curve inversion was an accurate leading indicator.\nPost-crisis widening: Following the 2008 crisis, we see a significant widening of the spread as the Fed Funds Rate dropped dramatically while the 10-year Treasury Yield declined more gradually. This wide spread reflects the Fed’s aggressive monetary easing and attempts to stimulate long-term borrowing and investment.\nExtended period of positive spread: From 2009 to 2019, the spread remained consistently positive, with long-term rates higher than short-term rates. This “normal” yield curve is generally associated with expectations of economic growth.\nRecent inversions: We observe two notable periods of inversion in recent years – a brief inversion in 2019 and a more pronounced one starting in 2022. These inversions coincide with economic uncertainties, including trade tensions and inflation concerns.\nPandemic impact and recovery: The spread widened sharply at the onset of the COVID-19 pandemic in 2020 as the Fed cut rates to near zero. Subsequently, we see a rapid narrowing and eventual inversion as the Fed aggressively raised rates to combat inflation. As of our analysis in 2024, we are currently experiencing an inverted yield curve, with short-term rates higher than long-term rates. Historically, such inversions have often preceded economic recessions, suggesting the possibility of an economic downturn in the coming years. However, it’s important to note that while yield curve inversions have been reliable predictors in the past, economic conditions can vary, and past performance doesn’t guarantee future outcomes.\n\n\n\n\nThese yield curve dynamics provide crucial context for understanding market expectations and potential economic turning points. The current inversion underscores the importance of monitoring these indicators closely in the coming months and years.\n10-year Treasury yield vs. corporate bond yields analysis\nIn this section, we’ll explore a crucial relationship in fixed-income markets: the comparison between 10-year Treasury yields and corporate bond yields. This analysis is fundamental to understanding credit risk in the economy.\nThe spread between these yields, known as the credit spread, is calculated as:\nCredit Spread = Corporate Bond Yield - 10-Year Treasury Yield\nThis spread represents the additional return investors demand for taking on the risk of lending to corporations rather than the US government. A widening spread typically indicates increased perceived risk, while a narrowing spread suggests improving conditions or increased risk appetite.\nLet’s use AI assistance to create a visualization that includes:\nThe 10-year Treasury yield.\nThe corporate bond yield (using the ICE BofA US Corporate Index Effective Yield as a proxy).\nThe spread between these rates.\n\n\n\n\nHere’s the prompt for the AI:\n\nAI Prompt\nPlot monthly 10-Year Treasury Yield, Corporate Bond Yields, and their spread:\n1. Use ‘treasury_10y_data’ for 10-Year Treasury Yield\n2. Retrieve Corporate Bond Yield data (BAMLC0A0CMEY) from FRED\n3. Resample both to monthly frequency and align to common date range\n4. Calculate spread (Corporate Bond Yield minus 10-Year Treasury Yield)\n5. Remove NaN values\n6. Plot all three lines with different colors\n7. Include legend, title, axis labels, and red dashed line at y=0\n8. Show summary statistics\n\n\n\n\n\nThe code output should look like this:\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom fredapi import Fred\nimport os\nfrom datetime import datetime, timedelta\n\n# Use the existing FRED API connection\nfred = Fred(api_key=os.environ['FRED_API_KEY'])\n\n# Calculate the date range\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=20*365)\n\n# Retrieve the corporate bond yield data\ncorporate_yield_data = fred.get_series('BAMLC0A0CMEY', start_date, end_date)\n\n# Resample both series to monthly frequency\ntreasury_10y_monthly = treasury_10y_data.resample('M').last()\ncorporate_yield_monthly = corporate_yield_data.resample('M').last()\n\n# Create a DataFrame with both series, using the common date range\nstart_date = max(treasury_10y_monthly.index.min(), corporate_yield_monthly.index.min())\nend_date = min(treasury_10y_monthly.index.max(), corporate_yield_monthly.index.max())\n\ndf = pd.DataFrame({\n    '10-Year Treasury Yield': treasury_10y_monthly[start_date:end_date],\n    'Corporate Bond Yield': corporate_yield_monthly[start_date:end_date]\n})\n\n# Calculate the spread\ndf['Spread'] = df['Corporate Bond Yield'] - df['10-Year Treasury Yield']\n\n# Remove any rows with NaN values\ndf = df.dropna()\n\n# Plot the data\nplt.figure(figsize=(12, 6))\nplt.plot(df.index, df['10-Year Treasury Yield'], label='10-Year Treasury Yield', color='blue')\nplt.plot(df.index, df['Corporate Bond Yield'], label='Corporate Bond Yield', color='orange')\nplt.plot(df.index, df['Spread'], label='Spread', color='green')\n\nplt.title('10-Year Treasury Yield vs Corporate Bond Yield and Spread')\nplt.xlabel('Date')\nplt.ylabel('Rate (%)')\nplt.legend()\nplt.grid(True)\n\n# Add horizontal line at y=0\nplt.axhline(y=0, color='r', linestyle='--')\n\nplt.show()\n\n# Print some summary statistics\nprint(\"\\nSummary Statistics:\")\nprint(df.describe())\n      \n      Open Datalore Code\n\n\n\n\n\nNow, let’s dive into the results of this analysis in the next section.\nInterpreting credit spread dynamics\n\n\n\n\nNow that we’ve visualized the data, let’s analyze the key trends in credit spread dynamics:\n2008 financial crisis impact: The most striking feature of the graph is the dramatic widening of the spread during the 2008 financial crisis. This spike reflects a severe increase in perceived credit risk as investors fled to the safety of government bonds, demanding much higher yields for corporate debt.\nPre-crisis build-up: In the years leading up to the 2008 crisis, we observe a gradual narrowing of the spread, suggesting increasing investor comfort with corporate credit risk. This trend reversed sharply with the onset of the crisis.\nPost-crisis normalization: Following the crisis, the spread gradually narrowed but largely remained elevated compared to pre-crisis levels for several years, indicating a more cautious approach to credit risk in the aftermath of the financial turmoil.\nCOVID-19 pandemic spike: In early 2020, we see another significant, though less extreme, widening of the spread coinciding with the onset of the COVID-19 pandemic. This reflects the sudden increase in perceived corporate credit risk due to economic uncertainties.\nRecent trends: Since the pandemic spike, the spread has narrowed but remains above pre-pandemic levels, suggesting that while credit risk perceptions have improved, investors still maintain a degree of caution in the current economic environment.\n\n\n\n\nThese credit spread dynamics provide crucial insights for investors and policymakers. A widening spread often signals increasing economic stress and may precede broader market downturns. Conversely, a narrowing spread typically indicates improving conditions and increased confidence in corporate debt.\nHowever, it’s important to consider credit spreads alongside other economic indicators for a comprehensive view of market conditions and credit risk. In the next section, we’ll explore how these insights can be applied to predictive analysis and risk management strategies.\nCongratulations! You’ve made it this far into our tutorial. Want more? Read on to explore the relationship between yield curve changes and credit spreads.\n[Advanced] Yield curve changes and credit spreads\nIn this section, we’ll explore the predictive relationship between yield curve changes and credit spreads. This analysis is crucial for understanding how changes in interest rates might signal future shifts in credit risk.\nWe’ll focus on two key questions:\nDo changes in the yield curve precede changes in credit spreads?\nHave yield curve inversions historically led to widening credit spreads?\n\n\n\n\nLet’s use AI assistance to create a comprehensive Python script that analyzes these relationships:\n\nAI Prompt\nCreate a Python script to analyze yield curve slopes and credit spread changes:\n1. Use FRED API for 40 years of:\n– Federal funds rate\n– 10-year Treasury yield\n– Corporate bond yield\n2. Calculate:\n– Yield curve slope\n– Credit spread\n– Identify inversions\n3. Analyze periods: 1, 7, 14, 30, 60, 90, 180, 270, 365, 547, 730, 1095 days\nFor each:\n– Compute forward credit spread changes\n– Calculate correlations and mean changes (normal/inverted periods)\n– Perform t-tests (normal vs inverted)\n4. Store results in DataFrame\n5. Plot:\n– Correlation vs forward period\n– Mean changes vs forward period\n6. Print results\n\n\n\n\n\nTo check the resulting code, go to Datalore:\n      \n      Open Datalore Code\n\n\n\n\n\nLet’s delve deeper into the relationship between yield curve slopes and future changes in credit spreads. Our goal is to understand how yield curve inversions might predict future spikes in credit risk and, by extension, potential economic downturns.\nMethodology:\nWe analyzed the correlation between the yield curve slope (10-year Treasury yield minus fed funds rate) and forward changes in credit spreads (corporate bond yield minus 10-year Treasury yield) for various future time periods, ranging from 1 day to 3 years (1,095 days).\nAnalysis of correlation between yield curve slope and forward credit spread change\nKey observations:\nShort-term correlations (1–90 days): We observe weak negative correlations, suggesting limited predictive power in the immediate future.\nMedium-term correlations (180–547 days): There’s a stronger negative relationship, with the peak negative correlation occurring around the 547-day mark. This indicates that yield curve inversions tend to precede credit spread widening most reliably in this time frame.\nLong-term correlation (1,095 days): Interestingly, the correlation turns positive, suggesting a potential reversal or cyclical nature of the relationship over very long periods.\n\n\n\n\nImplications:\nThe strongest predictive power of yield curve inversions for credit spread widening appears to be in the 1–2 year range.\nThis aligns with the common observation that recessions often follow yield curve inversions by 12–24 months.\nThe positive correlation at 1,095 days warrants further investigation and could relate to long-term economic cycles.\n\n\n\n\nAnalysis of mean forward credit spread change\n\n\n\n\nThis graph illustrates the average change in credit spreads over various future time periods, comparing normal yield curve periods (blue line) to inverted yield curve periods (orange line).\nKey observations:\nNormal yield curve periods (blue line):\n\nThe line remains relatively flat and close to zero across all time periods.\nThere’s a slight dip into negative territory for medium-term periods (180–547 days), suggesting a tendency for mild credit improvement during normal economic conditions.\nLong-term periods (730–1,095 days) show a small positive change, indicating a slight widening of credit spreads over extended time frames.\nInverted yield curve periods (orange line):\n\nShort-term periods (1–90 days) show small positive changes, indicating slight immediate credit deterioration following inversions.\nMedium-term periods (180–547 days) exhibit large positive changes, peaking around the 547-day mark. This suggests significant credit spread widening in the 1–2 year period following yield curve inversions.\nLong-term periods (730–1,095 days) show a dramatic reversal, with the 1,095-day point dropping into negative territory. This indicates a potential improvement in credit conditions or a cyclical effect over very long time frames.\nComparison between normal and inverted periods:\n\nThe divergence between the two lines is most pronounced in the medium-term range (180–547 days), highlighting the predictive power of yield curve inversions for credit spread changes in this timeframe.\nThe convergence and eventual crossover of the lines in the long term (1,095 days) is particularly interesting, suggesting a potential “mean reversion” effect over extended periods.\n\n\n\n\nImplications:\nTiming of credit risk: The peak in the inverted line around 547 days suggests that the greatest credit risk following a yield curve inversion typically manifests between 1.5 to 2 years after the inversion occurs.\nMagnitude of impact: The large gap between normal and inverted lines in the medium term indicates that yield curve inversions precede significantly larger credit spread widenings compared to normal periods.\nRisk management strategies: Financial institutions and investors might consider implementing more conservative credit policies or increasing hedges against credit risk in the 1–2 year period following yield curve inversions.\nEconomic cycle insights: The reversal seen at 1,095 days could indicate the typical length of a credit cycle or the delayed effects of policy interventions following economic stress periods.\nPredictive power: This graph reinforces the idea that yield curve inversions can serve as early warning indicators for increased credit risk, with a lead time of approximately 1–2 years.\n\n\n\n\nWhile these patterns offer guidance, they should not be treated as infallible predictors. Economic conditions can vary, and past patterns may not always repeat in the same way. Nonetheless, this analysis offers a framework for understanding the relationship between yield curve dynamics and future credit risk, providing a valuable tool for economic forecasting and risk management.\n[Advanced] Credit spread analysis with economic events: a 90-day forward look\nIn this final section, we’ll explore the dynamic relationship between credit spreads and major economic events, focusing on a 90-day forward-looking perspective. This analysis provides crucial insights into how credit risk perceptions evolve over relatively short periods, offering a window into market sentiment and economic conditions.\nWe’ll use AI assistance to create a comprehensive Python script that combines data manipulation, statistical analysis, and advanced visualization techniques. Our analysis will:\nCalculate 90-day forward changes in credit spreads.\nIdentify and visualize outliers in credit spread changes.\nMap these changes to significant economic events from 2001 to 2020.\nCompare credit spread behavior during normal and inverted yield curve periods.\nConduct statistical tests to validate our observations.\n\n\n\n\nLet’s prompt the AI to help us create this analysis:\n\nAI Prompt\nCreate a Python script to analyze and visualize 90-day forward credit spread changes in relation to major economic events:\n1. Use existing DataFrame ‘df’ with credit spread data\n2. Calculate 90-day forward change in credit spreads: diff(periods=90).shift(-90)\n3. Identify outliers using IQR method\n4. Create a time series plot:\n– X-axis: Date (show every 5 years)\n– Y-axis: 90-day forward change in credit spread\n– Plot main line for spread changes\n– Highlight outliers as red scatter points\n5. Add vertical lines for economic events (2001-2020):\n– Enron Bankruptcy, WorldCom Bankruptcy, Lehman Brothers Bankruptcy\n– European Debt Crisis, 2019 Repo Spike, COVID-19 Pandemic Declaration\n6. Customize plot: large fonts, clear labels, grid, legend\n7. Print summary statistics:\n– Number and percentage of outliers\n– Top 10 positive and negative outliers\n– Mean spread change for normal and inverted yield curve periods\n8. Perform t-test comparing normal and inverted periods\nUse pandas for data manipulation, matplotlib for plotting, seaborn for styling, and scipy.stats for the t-test. Ensure the script is well-commented and follows Python best practices.\n\n\n\n\n\nTo check the generated code, go to Datalore:\n      \n      Open Datalore Code\n\n\n\n\n\nLet’s dive into the results of this code and get a unique look at the history of the United States through the lens of credit spread changes.\n\n\n\n\nNow that we’ve visualized the data and generated our statistics, let’s analyze the key trends and events:\nVolatility clusters: The graph reveals distinct periods of high volatility in credit spread changes, particularly around major economic events. This clustering of volatility suggests that credit risk perceptions can change rapidly and dramatically during times of economic uncertainty.\nAsymmetric outliers: We observe that the most extreme outliers (red dots) tend to be on the positive side, indicating sudden, large increases in credit spreads. This asymmetry suggests that credit risk can spike more sharply than it tends to decrease, reflecting the market’s sensitivity to negative news.\nMajor economic events:\n\nDot-com bubble (2000–2002): We see increased volatility and some outliers, reflecting the tech market crash and subsequent economic uncertainty.\n2008 financial crisis: The most dramatic spike in our dataset occurs around the Lehman Brothers bankruptcy, with extreme positive outliers indicating a severe credit crunch.\nEuropean debt crisis (2011): Another cluster of outliers is visible, showing how international events can impact US credit markets.\nCOVID-19 pandemic (2020): A sharp spike in credit spread changes, rivaling the 2008 crisis in magnitude, demonstrates the immediate and severe impact of the pandemic on credit risk perceptions.\nRecovery patterns: Following each major spike, we observe periods of negative changes in credit spreads, indicating a gradual return to lower credit risk perceptions. However, the recovery patterns vary in duration and stability.\nRecent trends: In the years leading up to 2020, we see relatively stable credit spread changes with fewer outliers, possibly reflecting the prolonged period of low interest rates and economic growth. The COVID-19 pandemic disrupts this stability dramatically.\nFrequency of outliers: Outliers appear to be more frequent and extreme during recognized periods of economic stress, serving as a visual indicator of market turmoil.\n\n\n\n\nLimitations:\nWhile this 90-day forward view provides valuable insights, it doesn’t capture longer-term trends or very short-term fluctuations. The causes behind these changes can be complex and multifaceted, often requiring additional context to fully interpret.\nThis analysis offers a nuanced view of credit risk dynamics over the past two decades. It highlights the market’s sensitivity to major economic events and provides a valuable tool for anticipating potential shifts in credit risk perceptions.\nConclusion: applying your new credit risk analysis skills\nCongratulations on completing this comprehensive tutorial on credit risk analysis using Python and AI-assisted coding in Datalore! Let’s recap what you’ve learned and how you can apply these skills in your own financial analyses:\nKey skills acquired\nData retrieval and preprocessing: You’ve learned how to use the FRED API to fetch economic data and prepare it for analysis using pandas.\nVisualization techniques: You’ve created various plots to visualize federal funds rates, Treasury yields, and corporate bond yields, enhancing your data presentation skills.\nYield curve analysis: You’ve explored how to calculate and interpret yield curve slopes and inversions, crucial indicators for economic forecasting.\nCredit spread calculation: You’ve computed and analyzed credit spreads, gaining insights into market risk perception.\nPredictive analysis: You’ve conducted forward-looking analyses to understand the relationship between yield curve changes and future credit spread movements.\nEvent impact assessment: You’ve examined how major economic events affect credit spreads, developing skills in contextual data interpretation.\nAI-assisted coding: Throughout the tutorial, you’ve leveraged AI to generate code, demonstrating how to use this powerful tool to streamline your workflow.\n\n\n\n\nApplying your skills\nCustom analyses: Use the code templates provided to analyze different time periods or other economic indicators that interest you.\nRisk management: Apply the yield curve inversion and credit spread analysis techniques to assess potential risks in your investment portfolio.\nEconomic forecasting: Utilize the predictive analysis methods to create your own forecasts for credit risk and economic conditions.\nData-driven decision making: Incorporate these data analysis techniques into your financial decision-making processes, whether for personal investments or professional applications.\nContinuous learning: As you’ve seen, economic conditions evolve. Use the skills you’ve learned to stay updated on market trends and continually refine your analysis techniques.\n\n\n\n\nNext steps\nExpand your dataset: Try incorporating additional economic indicators or international data to broaden your analysis.\nEnhance your models: Experiment with more advanced statistical methods or machine learning techniques to improve predictive power.\nAutomate your analyses: Create scripts that automatically update your analyses as new data becomes available with Datalore’s Scheduled runs feature.\nCollaborate and share: Use Datalore’s collaboration features to work with others and share your insights.\n\n\n\n\nWhile these tools and techniques are powerful, they should be used in conjunction with a nuanced understanding of economic principles and current market conditions. Stay curious, keep practicing, and continue to refine your credit risk analysis skills!\nIf you’d like to edit the full code of this tutorial in Datalore, including interactive elements and additional resources, click the button below:\n      \n      Open Datalore Code",
        "dc:creator": "Alena Guzharina",
        "content": "This is a guest blog post by Ryan O’Connell, CFA, FRM. Understanding the relationships between various economic indicators is crucial for navigating the financial landscape. These relationships significantly impact the overall state of the economy, affecting businesses, investors, and individuals alike. We&#8217;ll analyze data spanning over two decades of financial history, leveraging the power of [&#8230;]",
        "contentSnippet": "This is a guest blog post by Ryan O’Connell, CFA, FRM. Understanding the relationships between various economic indicators is crucial for navigating the financial landscape. These relationships significantly impact the overall state of the economy, affecting businesses, investors, and individuals alike. We’ll analyze data spanning over two decades of financial history, leveraging the power of […]",
        "guid": "https://blog.jetbrains.com/?post_type=datalore&p=505317",
        "categories": [
          "datalore-ai",
          "python-for-finance"
        ],
        "isoDate": "2024-08-26T14:34:49.000Z"
      },
      {
        "creator": "Andrey Belyaev",
        "title": "Workspaces in IntelliJ IDEA",
        "link": "https://blog.jetbrains.com/idea/2024/08/workspaces-in-intellij-idea/",
        "pubDate": "Mon, 26 Aug 2024 09:01:26 +0000",
        "content:encodedSnippet": "Better late than never! This idiom is probably the best way to describe the functionality we’ll introduce in this article, specifically, support for workspaces in IntelliJ IDEA.\nIntro\nIn a nutshell, a workspace is a meta-project that allows you to manage multiple projects simultaneously. This feature is useful for various reasons, from coordinating complex development environments to having a couple of unrelated projects on your hands\nNote for the impatient: There will be some theory and history included below, so if you’re eager to get workspaces into your IDE, just go to the How to use workspaces chapter and crack on! Also, join the upcoming webinar for more details. Please note that this feature is currently in preview, so you may encounter some bugs and limitations. \nThere’s been a ticket for workspace support on our issue tracker since 2011. However, the rise of microservices and monorepositories in the mid-2010s seemed to make workspaces obsolete. The monorepo model was totally aligned with IntelliJ IDEA’s focus on the “one project – multiple modules” approach. Yet, demand for workspaces continued to grow, something confirmed by the increasing number of likes and supportive comments from users on YouTrack. In the next section, we’ll explore why workspaces are still relevant in 2024.\nWhy monorepos didn’t put an end to the workspace approach \nMonorepos indeed have plenty of advantages over other approaches to code arrangement, namely multi-repos, namely:\nBetter collaboration. All developers are able to see what’s going on in other parts of the project they’re working on. Every change becomes visible as soon as the code is committed. While implementing a service that communicates with another service written by another team, developers can see the other services’ code to get an understanding of how it works.\nEasier dependency management. In most cases, all projects in a monorepo use the same package management and build tool, making it easier to track dependencies and versions. Also, all application components in a monorepo are usually released together, meaning there is no risk of problems arising with inter-project dependencies. \nCross-project refactoring. Having access to the entire codebase allows the IDE to index code references, meaning it can detect changes and easily perform refactorings correctly.\nImproved code discoverability. New team members can easily navigate the entire codebase and understand the dependencies between different projects and components.\n\n\n\n\n\nIf you check out a monorepo and open it with IntelliJ IDEA, you’ll see that the monorepo structure fits into the “project-modules” paradigm perfectly. The root folder becomes a “project”, and all subfolders become “modules”. So, on this basis, it looked like the model worked fine, and the implementation of the workspaces feature could be postponed.\nHowever, monorepos have some disadvantages:\nMonorepos tend to grow quickly. Loading a massive codebase and gathering metadata about the code can be time-consuming. The vast number of commits to all projects within the monorepo can lead to branching, merging, and rebasing operations taking longer than expected. \nRestricting access to subfolders might become a challenge. For example, with Git, you can do this by using submodules or provider-specific features like GitHub’s Codeowners.\nCI/CD integration can be challenging if different projects within a monorepo have different release cycles.\nIn addition to the purely technical challenges of using monorepos, we can add a behavioral dimension. Usually, developers don’t work on the entire monorepo code. Instead, they tend to focus on an isolated service or a few of them. \nSo, despite offering many advantages, usage of the monorepo approach started to wane due to the following:\nPerformance problems: Cloning, pulling, and merging code became challenging as soon as the codebase began to grow.\nLimited autonomy: In a monorepo, it’s harder to develop and release a selected project independently from other parts of the whole system compared to the multi-repository approach. In addition to this, access control remained a challenge.\nUnknown boundaries: Projects are treated as a single monolith, so code and abstractions can “leak” into other projects, creating tight coupling. Another consequence is a “broken master”. If just one project within the monorepo is broken, the whole monorepo is treated as broken.\nDistracted developer’s focus: While developers usually work on a single small project, with monorepos, they have to deal with a codebase of dozens of projects.\nThe above has contributed to increasing votes on issue IDEA-65293, making it evident that we had to include workspaces as a new feature in our IDEs. \nAs a confirmation, surveys show that microservice developers use multiple repositories for development, and this is a growing trend.\n\n\n\n\nWorkspaces improve developers’ experience when they work with multiple projects and repositories. Please note that workspaces can also be useful for developers who prefer monorepos. Having the ability to check out particular projects from a big monorepo and work on those only is a great benefit.\nIf you’d like to dive deeper into this topic, there are some handy articles like “The issue with monorepos” and “Monorepo is a bad idea” that explain things in greater detail.\nWhat is a workspace?\nIn one of the discussions on the JetBrains support forum, someone posted the following:\n\nProject: This is essentially a description of your final software product.\nWorkspace doesn’t have anything to do with actual projects and/or software products! It is (ideally) your IntelliJ IDEA’s desktop, merely a container for projects you’re currently working with. This is your current view of your developer’s environment. It does not produce results, but controls how results are produced.\n\n\n\n\nThis is the best explanation for the “workspace” concept introduced in this plugin. You can treat a workspace as a collection of the projects that you work on. Every project can use its own technology stack and a build tool and can be run independently.\nUser scenarios for workspaces\nWhen we made the plugin, we thought about a “typical” setup for an application, comprising:\nA set of backend services.\nAn API gateway.\nClient applications, such as a React web app or a mobile app.\nA collection of custom libraries (for example, Spring Boot starters) used across services.\nThe code for these components might be organized in a single monorepo or across multiple repositories. Developers might also need to create small “satellite” applications for testing and debugging components like libraries, without the need to commit these auxiliary apps to the main codebase.\nIt’s important to note that this architectural setup can vary. For instance, in a monolithic application, the backend might be consolidated into a single service, and the API gateway could become optional. In another scenario, if an application lacks a client interface, the focus would be solely on the backend services and the API gateway.\nBased on this, we’ve come up with the following major use cases and corresponding workspace configurations for the plugin:\n1. Fullstack developer\nIn many cases, we need to update both front-end and back-end components, which could be stored in different repositories. The workspaces plugin allows us to check out projects from repositories independently, add them to the same workspace, and work with their code as if they were in one project.\n\n\n\n\n2. Microservices developer\nLet’s assume that we have dozens of microservices in a monorepo, and we need to update only some of them. There’s no longer any need to import the entire repository into IntelliJ IDEA. We can create a workspace and add only the required services to it. Moreover, we can also use different build tools for different projects. If you build one service with Maven and another with Gradle – no problem!\n\n\n\n\n3. Library developer\nThis scenario can be used when we need a simple “Hello, World!” application to test a solution. The perfect example of this is the development of a Spring Boot starter. The test application won’t be committed to the code repository, but we may need it to simplify development. In this case, we can create a simple project right in the workspace in addition to the starter project, run the app, debug the starter, and then just discard the app or reuse it in another workspace.\n\n\n\n\nCurrent implementation\nA workspace is a separate folder that stores references to projects from other folders. Additionally, we can create a project inside a workspace folder if needed. A workspace should not be invasive in terms of project settings. IntelliJ IDEA stores project settings, such as code style, SDK, etc., in a separate file in a project-specific folder (.idea). When we add a project to a workspace, IntelliJ IDEA copies its settings to the workspace folder, and then we can change them locally. This means that one project can be included in several workspaces with different settings. The picture below shows how this works. \n\n\n\n\nSince the functionality of workspaces is still a work in progress, we’ve implemented it as a separate non-bundled plugin to be able to release it faster as we continue to improve it based on the feedback we get from users. We hope that, sooner or later, this feature will become a part of the core IDE functionality.\nHow to use workspaces\nInstall the plugin from JetBrains Marketplace.\nCreate a workspace as a new project type.\n\n\n\n\n\nGive your workspaces folder a name and add all required projects to it.\n\n\n\n\nYou can run projects separately using well-known run configurations in IDEA or Maven/Gradle commands. To run several projects at once, for example, microservices, you’ll probably need a special shell script or Docker compose file. We tried to make the user experience as familiar as possible.\nCurrent version limitations\nLet us share some limitations we are aware of. This isn’t a comprehensive list, but it includes key issues that might seem crucial to you and could prevent you from trying the workspaces today. \nWe do not copy run configurations from the included projects to the workspace. For the time being, you will need to create run configurations manually after project import.\nThere is no constant settings synchronization. For example, if you used JDK 17 in the project and then included it in the workspace – the workspace will use JDK 17. If we upgrade JDK to a newer version in the “main” project settings, the workspace won’t “see” this change.\nProject renaming is not supported.\nResolving dependencies across independent projects is a work in progress.\nYou can find the full list of issues linked to the YouTrack ticket IDEA-65293. \nFurther plans\nWe are currently working on the implementation of a number of tasks to make workspaces support even better, notably:\nProject settings synchronization.\nAbility to store and share workspace in VCS.\nMore build tools to support.\nMaking workspace a first-class citizen in all IDEs.\nSimplify run configuration for running multiple projects at once.\nSmooth debugging across projects.\nFeedback welcome! \n\n\n\n\nThe workspaces feature is far from finished. We’ve only just begun our work as we edge another step toward our goal of creating the ideal IDE. We welcome all feedback you might have, it will help us in our efforts! Try out the new workspaces feature, and let us know your thoughts. Your feedback will help us make IntelliJ IDEA even better. You can rate our efforts on the plugin page or add your thoughts and report issues in YouTrack.\n5-star us if you like the initiative!",
        "dc:creator": "Andrey Belyaev",
        "content": "Better late than never! This idiom is probably the best way to describe the functionality we’ll introduce in this article, specifically, support for workspaces in IntelliJ IDEA. Intro In a nutshell, a workspace is a meta-project that allows you to manage multiple projects simultaneously. This feature is useful for various reasons, from coordinating complex development [&#8230;]",
        "contentSnippet": "Better late than never! This idiom is probably the best way to describe the functionality we’ll introduce in this article, specifically, support for workspaces in IntelliJ IDEA. Intro In a nutshell, a workspace is a meta-project that allows you to manage multiple projects simultaneously. This feature is useful for various reasons, from coordinating complex development […]",
        "guid": "https://blog.jetbrains.com/?post_type=idea&p=505111",
        "categories": [
          "features",
          "tutorials",
          "intellijidea-2",
          "monorepos",
          "monorepositories",
          "workspaces"
        ],
        "isoDate": "2024-08-26T09:01:26.000Z"
      },
      {
        "creator": "Olga Bedrina",
        "title": "The Fail-Fast Strategy: What Is It and How Does TeamCity Support It?",
        "link": "https://blog.jetbrains.com/teamcity/2024/08/fail-fast-strategy-teamcity/",
        "pubDate": "Fri, 23 Aug 2024 09:57:01 +0000",
        "content:encodedSnippet": "This is a guest post from Cameron Pavey, draft.dev.\nSoftware development teams are always looking for ways to move faster and deliver more value in less time. However, one common pitfall for many teams is spending far too much time and effort building something, only to encounter major issues late in the project’s lifecycle.\nThe fail-fast strategy addresses this problem by “shifting left” on potential points of failure and embracing them as part of a virtuous feedback loop. Whether in project management or software development, a failure is often just a signal that something needs to change. The earlier you can detect this signal, the sooner you can adjust for it, thus mitigating the risk of wasted work.\nTo properly implement a fail-fast strategy, you need a tool that supports this way of working. TeamCity is a CI/CD solution that complements the fail-fast strategy and has several features to help you implement it.\nIn this guide, you’ll learn more about the fail fast strategy and how you can leverage TeamCity to adopt this strategy for yourself.\nWhat is the fail-fast strategy?\nIn software development, the fail-fast strategy emphasizes iterative discovery over strict planning. Key principles include:\nRapid feedback loops: Short cycles allow for frequent releases and early feedback, helping teams move quickly and efficiently.\n\n\n\n\nProactive risk management: Early identification of risks allows for timely mitigation or pivoting, reducing wasted effort.\n\n\n\n\nIterative experimentation: Frequent experimentation helps quickly identify dead ends and promising solutions, optimizing the use of resources.\n\n\n\n\nTransparency: Promotes a culture of continuous improvement, where teams share and learn openly.\nThis strategy operates on multiple levels:\nProject management: Focuses on direction, ideation, and finding product-market fit.\nSoftware development: Applies the same principles to concrete practices like continuous integration (CI).\nPairing the fail-fast mentality with a CI service like TeamCity helps shorten feedback cycles and improve outcomes by rapidly identifying and addressing issues.\nMajor advantages of the fail-fast strategy\nCompared to traditional methods like the waterfall strategy, the fail-fast strategy offers significant benefits:\nShorter feedback loop: Submit small, incremental changes for automated checks, catching faults early and fixing them quickly.\nIdentifying pitfalls early on: Incremental builds help identify technical issues early, allowing for timely adjustments and de-risking your approach upfront.\nPotential drawbacks of the fail-fast strategy\nWhile the fail-fast strategy offers many benefits, there are potential drawbacks:\nMindset shift: Teams used to traditional methods may resist the concept of “failure”, impacting morale and causing frustration.\nEmbracing failure: Understanding that failure, discovery, and pivoting are key elements is crucial for the strategy’s success.\nTooling requirements: Effective implementation requires robust tools, particularly a CI/CD tool like TeamCity.\nHow does TeamCity support the fail-fast strategy?\nAs the central goal of the fail-fast strategy is shorter feedback cycles, you need a tool that provides this feedback for you in a timely manner. Continuous integration tools, such as TeamCity, are designed for this purpose. They can run builds, tests, and other scripted workflows in response to code changes.\nReal-time reporting\nLarge software projects can have long, slow-running builds. This makes it especially frustrating when you wait a long time for a build to run, only to discover that it failed. Whether it’s a blocking build issue that stops subsequent steps or an early test failure that makes the rest of the run redundant – the sooner you discover these issues, the better.\nTeamCity mitigates this issue for you by providing real-time reporting. You can view in-progress builds, inspect their current state, and build logs to identify problems as they occur rather than waiting until the end of a CI run.\nBy seeing the live status of running builds, developers can identify and fix problems without waiting for the build to finish. When the build fails, you can see what went wrong, fix it, and run another build. Compared to other CI systems where you need to wait for the build to complete, this workflow offers a shorter feedback cycle that works well with the fail-fast strategy.\n\n\n\n\nThis approach is particularly effective if quick-running build steps provide high-value feedback, like static analysis and other code quality checks. You can view the progress of these live to determine if you need to make any alterations to your code, and once these pass, you can leave the rest of the build to run.\nBuild configurations\nYour build process will likely be nontrivial when dealing with complex software systems. You’ll need to do any number of steps, including:\nBuilding Docker images\nCompiling source code\nDownloading dependencies\nRunning various kinds of tests (with varying degrees of cost)\nAs your build grows over time, it could become too large to reasonably manage. At this point, you may want to find a way to split the build into smaller steps.\nTeamCity solves this through build configurations. Build configurations allow you to split your build into discrete steps. When you do this, each step has a clearly defined responsibility, limiting the potential for complexity to leak between steps as your system grows.\nBuild chains\nAnother helpful feature of TeamCity that can be used to implement the fail-fast strategy is build chains. This feature allows you to declare build configurations as dependent on one another. In practice, this means you can run all your quality gates before the deployment step, allowing you to skip the deployment if there are quality issues that prevent it from being a release candidate.\nExample of a build chain in TeamCity\n\n\n\nFailures in the earlier steps in the chain will stop subsequent build configurations from running. This can save time and resources and help shorten the feedback loop even more by avoiding the effort spent on faulty builds.\nTest reports\nFailing tests are a fact of life for software developers. The key factor that separates frustrating failures from helpful failures is how much information you have when trying to fix them. Ideally, you want to know:\n– What failed: Was it a unit test, an integration test, etc.?\n– When it failed: Is it a new failure? Is this the first time it has happened? Is it a recurring flaky test?\n– Why it failed: Is it a legitimate functional failure or a flaky test?\nThis information helps you narrow down the cause and promptly fix the issue. You could run the test on your development machine and see it fail for yourself, but you’ll likely miss out on a lot of context (for example, information about in which commit this failure first arose). \nTeamCity test reports solve this problem. Every time your CI workflow runs, test data is captured for a wide range of testing frameworks. This data is then presented to you in relation to the CI runs that have experienced test failures, as well as a few other views like Current Problems and Flaky Tests.\nExample of a test report in TeamCity\n\n\n\nThese reports provide immediate insights into what problems your build is facing and the nature of the problems, such as flaky tests, newly introduced issues, or long-standing failures. \nHigh-quality test reports are a must for projects that require heavy use of automated testing at any level, but they’re especially helpful if you have broad coverage from unit tests and integration tests.\nThis way, at a glance, you have a comprehensive snapshot of the state of your code base each time CI runs.\nNotifications\nFail fast is only good if you know about the failure, and detailed build information is only helpful if people know it’s there. Developers would typically prefer to work on things themselves rather than sit and watch something being built by CI.\nThanks to highly configurable notifications, there’s no need to babysit builds. In TeamCity, you can configure rules to determine what you would like to be notified about and where you’d like those notifications to go. From email and browser notifications to Slack and even in-IDE notifications, there are several channels to choose from.\nNotification options in TeamCity\n\n\n\nNotifications are a key requirement in a system that’s intended to help you work more proactively, and the more configuration options you have at your disposal, the more use cases you will be able to satisfy.\nFor a fail-fast workflow, you might want to configure notifications for any failed build to which you’ve contributed. Then, relying on the VCS integration, you can get rapid feedback on your changes directly in your IDE as you make small, atomic changes.\nTeamCity notifications can be configured to only notify you of Builds containing my changes or when The first build error occurs. These settings are great for fine-tuning the notifications you see. Rather than seeing every failing build, you might only want to see failures on builds that include your changes or when the first error happens for a build.\nIf you set up continuous development through TeamCity, you can also enable notifications to inform you whenever a deployment or infrastructure change (through infrastructure as code tools like Terraform or Kubernetes deployments) occurs. In this case, you’d likely want to be notified in case of success as well as failure.\nArtifacts\nIssues often arise in CI that you cannot replicate locally. This can lead to a lot of misdirected time as you try to determine which is different between the CI run and your local application. Using artifacts can help address this problem.\nIn TeamCity, artifacts are typically anything produced by your build, such as binaries, logs, recordings, screenshots, etc. You can treat pretty much anything as an artifact, which gives you great flexibility in how you use this feature. Artifacts are then captured by TeamCity and are available for download through the UI after the build.\nThis can greatly streamline the analysis and debugging process that you go through when trying to rectify a failing build. For example, if you have end-to-end (E2E) tests that only seem to fail in your CI runs, there’s a good chance that your E2E tool has the ability to produce screenshots and screen recordings when failures occur. Being able to capture these as artifacts gives you a trove of data to help with debugging.\nArtifacts can also be used for any other use case where you want to capture the output of a build. Perhaps your project produces binary executables. In that case, you could capture the built binary for each CI run, allowing you to test any build for any commit that runs through your CI workflow.\nWrapping up\nThis guide introduced the fail-fast strategy, including its benefits, such as proactive risk management, transparency, and adaptability. TeamCity supports fail-fast through various powerful features, including real-time reporting, flexible notifications, and detailed test reports.\nWhen utilized properly, the fail-fast strategy can be a powerful tool. It can help you move faster and deliver value without the constraints of slower, more traditional ways of working. However, the process needs to be supported by suitably powerful and flexible tools. If you’re looking for a CI/CD server that fits the bill, consider taking TeamCity for a spin today.",
        "dc:creator": "Olga Bedrina",
        "content": "This is a guest post from Cameron Pavey, draft.dev. Software development teams are always looking for ways to move faster and deliver more value in less time. However, one common pitfall for many teams is spending far too much time and effort building something, only to encounter major issues late in the project&#8217;s lifecycle. The [&#8230;]",
        "contentSnippet": "This is a guest post from Cameron Pavey, draft.dev. Software development teams are always looking for ways to move faster and deliver more value in less time. However, one common pitfall for many teams is spending far too much time and effort building something, only to encounter major issues late in the project’s lifecycle. The […]",
        "guid": "https://blog.jetbrains.com/?post_type=teamcity&p=505010",
        "categories": [
          "news"
        ],
        "isoDate": "2024-08-23T09:57:01.000Z"
      },
      {
        "creator": "Sarah Haggarty",
        "title": "Kotlin 2.0.20 Released",
        "link": "https://blog.jetbrains.com/kotlin/2024/08/kotlin-2-0-20-released/",
        "pubDate": "Thu, 22 Aug 2024 14:51:13 +0000",
        "content:encodedSnippet": "The Kotlin 2.0.20 release is out! This version includes performance improvements and bug fixes for Kotlin 2.0.0, where we announced the Kotlin K2 compiler as Stable. Here are some additional highlights from this release:\nThe data class copy function will have the same visibility as the constructor\nStatic accessors for source sets from the default target hierarchy are now available in multiplatform projects\nConcurrent marking for Kotlin/Native has been made possible in the garbage collector\nThe @ExperimentalWasmDsl annotation in Kotlin/Wasm has a new location\nSupport has been added for Gradle versions 8.6–8.8\nA new option allows sharing JVM artifacts between Gradle projects as class files\nThe Compose compiler has been updated\nSupport for UUIDs has been added to the common Kotlin standard library\n\n\n\n\nFor the complete list of changes, refer to What’s new in Kotlin 2.0.20 or the release notes on GitHub.\nHow to install Kotlin 2.0.20\nStarting from IntelliJ IDEA 2023.3 and Android Studio Iguana (2023.2.1) Canary 15, the Kotlin plugin is distributed as a bundled plugin included in your IDE. This means that you can’t install the plugin from JetBrains Marketplace anymore.\nTo update to the new Kotlin version, change the Kotlin version to 2.0.20 in your build scripts.\nIf you need the command-line compiler, download it from the GitHub release page.\nIf you run into any problems\nFind help on Slack (get an invite).\nReport issues to our issue tracker, YouTrack.\n\n\n\n\n\nStay up to date with the latest Kotlin features! Subscribe to receive Kotlin updates by filling out the form at the bottom of this post. ⬇️\nSpecial thanks to our EAP Champions 🥇👏\nZac Sweers\nRick Clephas\nFlorian Schreiber\nSechaba Mofokeng\nYang\nŁukasz Wasylkowski\nDavid Lopez\nBernd Prünster\nAlexander Nozik\nBenoit ‘BoD’ Lubek\nSterling Albury\nKacper Wojciechowski\nMohamed Rejeb\nJake Wharton\nJohannes Svensson\nArtyom Shendrik\n\n\n\n\nWhat else to read and watch\nWhat’s new in Kotlin 2.0.20 documentation\nK2 compiler migration guide\nK2 Compiler Performance Benchmarks and How to Measure Them on Your Projects\nMeet Renovated Kotlin Support – K2 Mode: What You Need to Know\nJetpack Compose compiler moving to the Kotlin repository\nKotlin EAP Champions",
        "dc:creator": "Sarah Haggarty",
        "content": "The Kotlin 2.0.20 release is out! This version includes performance improvements and bug fixes for Kotlin 2.0.0, where we announced the Kotlin K2 compiler as Stable. Here are some additional highlights from this release: For the complete list of changes, refer to What&#8217;s new in Kotlin 2.0.20 or the release notes on GitHub. How to [&#8230;]",
        "contentSnippet": "The Kotlin 2.0.20 release is out! This version includes performance improvements and bug fixes for Kotlin 2.0.0, where we announced the Kotlin K2 compiler as Stable. Here are some additional highlights from this release: For the complete list of changes, refer to What’s new in Kotlin 2.0.20 or the release notes on GitHub. How to […]",
        "guid": "https://blog.jetbrains.com/?post_type=kotlin&p=503938",
        "categories": [
          "releases"
        ],
        "isoDate": "2024-08-22T14:51:13.000Z"
      },
      {
        "creator": "Alena Guzharina",
        "title": "What’s New in Datalore 2024.4: Limit 500 Toggle, Easier Folder Import, Polars DataFrames Support, and More",
        "link": "https://blog.jetbrains.com/datalore/2024/08/22/what-s-new-in-datalore-2024-4-limit-500-toggle-easier-folder-import-polars-dataframes-support-and-more/",
        "pubDate": "Thu, 22 Aug 2024 10:07:56 +0000",
        "content:encodedSnippet": "Datalore 2024.4 introduces quality-of-life improvements for SQL cells, a revamped folder import workflow, support for Polars DataFrames, and more. Datalore Cloud customers have already received the new update automatically, and Datalore On-Premises customers can upgrade by following these instructions.\nLimit 500 toggle for SQL cells\nTo help you prototype SQL queries faster, we’ve introduced a new Limit 500 feature inside SQL cells in Datalore notebooks. There is now a toggle visible in the SQL cell next to the Browse schema action. This setting will persist after a kernel restart, ensuring a seamless workflow.\n\n\n\n\nRevamped import folder workflow\nIt is now easier to import your previous work into Datalore. We’ve enhanced the folder import experience with a new, streamlined dialog that appears when you click Import | Upload folder. You can now easily select which notebooks and files to import, with the option to import notebooks as files. Hidden files and folders are fully supported and can be imported if selected. \n\n\n\n\nUpdates to the notebook UI\nWe’ve made several updates to the notebook UI to enhance your editing experience. Unfocused code cells now have borders, improving cell visibility and making it easier to target the insert cell buttons. Interactive controls and metric cells now have their options merged with the cell toolbar, no longer occupying the full width of the notebook. Additionally, cell type-specific actions, such as selecting a datasource or target variable, are now positioned to the right of the execution time for easier access. Cells now also feature a Run button, which can be gray or blue depending on whether the cell was changed after execution – offering a clearer indication of a cell’s status.\n\n\n\n\nDrop-down cell improvements\nFor large lists, you can now use a search string within the drop-down to quickly find your desired option. Additionally, multiselect drop-downs now include Select all and Clear all options.\n\n\n\n\nQuick notebook access from the editor\nYou can now quickly search for and open notebooks in a separate tab directly from the Notebooks view. Enjoy easier navigation without extra context switching.\n\n\n\n\nSupport for Polars DataFrames\nDatalore now supports rich table outputs for Polars DataFrames. To get an interactive table output, specify the DataFrame name at the end of the cell without a print statement.\n\n\n\n\nOther updates and bug fixes\nDuplicate file names are no longer permitted inside Datalore’s file system, with existing duplicates renamed with (#N) prefixes to make files easier to find and minimize naming confusion. \nPython `input()` commands are now supported for interactive reports. \nGit repositories with submodules are now cloned correctly. \nTables are no longer cropped when printing to PDF. \nWe’ve resolved the issue where pages would not load when Datalore was set up with an HTTP proxy on a non-standard port. \nWe’ve fixed the issue where refreshing the schema in the Edit database connection dialog would not work for users logged in via OAuth.\nIt is now possible to copy data from the cell outputs to the clipboard via the output’s context menut. This feature is supported for most cell output types. \nThe workspace owner is now indicated in the left-hand sidebar of the home page.\nDatalore Cloud customers have already received these updates automatically. \nFor Datalore On-Premises customers, it is highly important to ensure the database that stores all the Datalore content is backed up before upgrading to the 2024.4 version.\n      \n      Upgrade to 2024.4\n    \n\n\n\n\nKind regards,\nThe Datalore team",
        "dc:creator": "Alena Guzharina",
        "content": "Datalore 2024.4 introduces quality-of-life improvements for SQL cells, a revamped folder import workflow, support for Polars DataFrames, and more. Datalore Cloud customers have already received the new update automatically, and Datalore On-Premises customers can upgrade by following these instructions. Limit 500 toggle for SQL cells To help you prototype SQL queries faster, we&#8217;ve introduced a [&#8230;]",
        "contentSnippet": "Datalore 2024.4 introduces quality-of-life improvements for SQL cells, a revamped folder import workflow, support for Polars DataFrames, and more. Datalore Cloud customers have already received the new update automatically, and Datalore On-Premises customers can upgrade by following these instructions. Limit 500 toggle for SQL cells To help you prototype SQL queries faster, we’ve introduced a […]",
        "guid": "https://blog.jetbrains.com/?post_type=datalore&p=504871",
        "categories": [
          "releases"
        ],
        "isoDate": "2024-08-22T10:07:56.000Z"
      },
      {
        "creator": "Khalid Abuhakmeh",
        "title": "ReSharper Tips & Tricks with Matt Ellis – Livestream",
        "link": "https://blog.jetbrains.com/dotnet/2024/08/21/resharper-tips-tricks-with-matt-ellis-livestream/",
        "pubDate": "Wed, 21 Aug 2024 13:58:37 +0000",
        "content:encodedSnippet": "Join us on September 12, 2024, at 2:00 PM UTC for another JetBrains livestream event. You can register for the event using the link below or subscribe to our YouTube channel, JetBrainsTV, to get a notification closer to the event.\nRegister\n                                \nfor the livestream\n\n\n\n\nWith over 20 years of ReSharper setting the standard for the .NET development experience, it’s time for .NET developers, new and accomplished, to see what the #1 productivity extension for Visual Studio has to offer. In this livestream, we’ll be joined by JetBrains Developer Advocate Matt Ellis, who will highlight some of his best tips and tricks to get the most productivity gains in your .NET workflows. Discover what you’ve been missing with ReSharper.\nMatt Ellis – Developer Advocate Game Development, JetBrains\nMatt Ellis is a developer advocate at JetBrains, working with lots of different IDEs and technologies such as the Unity and Unreal Engine game development tools in Rider, Code With Me for collaborative editing and remote development with JetBrains Gateway. He has spent (well) over 20 years shipping software in various industries, and thoroughly enjoys working with IDEs and development tools, having fun with syntax trees and source code analysis. He also helps build the Unity support in Rider, and contributes to the popular IdeaVim plugin.",
        "dc:creator": "Khalid Abuhakmeh",
        "content": "Join us on September 12, 2024, at 2:00 PM UTC for another JetBrains livestream event. You can register for the event using the link below or subscribe to our YouTube channel, JetBrainsTV, to get a notification closer to the event. With over 20 years of ReSharper setting the standard for the .NET development experience, it&#8217;s [&#8230;]",
        "contentSnippet": "Join us on September 12, 2024, at 2:00 PM UTC for another JetBrains livestream event. You can register for the event using the link below or subscribe to our YouTube channel, JetBrainsTV, to get a notification closer to the event. With over 20 years of ReSharper setting the standard for the .NET development experience, it’s […]",
        "guid": "https://blog.jetbrains.com/?post_type=dotnet&p=503092",
        "categories": [
          "net-tools",
          "c",
          "resharper",
          "visual-studio"
        ],
        "isoDate": "2024-08-21T13:58:37.000Z"
      },
      {
        "creator": "Evgenia Verbina",
        "title": "How to Build Chatbots With LangChain",
        "link": "https://blog.jetbrains.com/pycharm/2024/08/how-to-build-chatbots-with-langchain/",
        "pubDate": "Wed, 21 Aug 2024 10:06:08 +0000",
        "content:encodedSnippet": "This is a guest post from Dido Grigorov, a deep learning engineer and Python programmer with 17 years of experience in the field.\n\n\n\n\nChatbots have evolved far beyond simple question-and-answer tools. With the power of large language models (LLMs), they can understand the context of conversations and generate human-like responses, making them invaluable for customer support applications and other types of virtual assistance. \nLangChain, an open-source framework, streamlines the process of building these conversational chatbots by providing tools for seamless model integration, context management, and prompt engineering.\nIn this blog post, we’ll explore how LangChain works and how chatbots interact with LLMs. We’ll also guide you step by step through building a context-aware chatbot that delivers accurate, relevant responses using LangChain and GPT-3.\n\n\n\n\nWhat are the chatbots in the realm of LLMs?\nChatbots in the field of LLMs are cutting-edge software that simulate human-like conversations with users through text or voice interfaces. These chatbots exploit the advanced capabilities of LLMs, which are neural networks trained on huge amounts of text data which allows them to produce human-like responses to a wide range of input prompts.\nOne among all other matters is that LLM-based chatbots can take a conversation’s context into account when generating a response. This means they can keep coherence across several exchanges and can process complex queries to produce outputs that are in line with the users’ intentions. Additionally, these chatbots assess the emotional tone of a user’s input and adjust their responses to match the user’s sentiments.\nChatbots are highly adaptable and personalized. They learn from how users interact with them thus improving on their responses by adjusting them according to individual preferences and needs. \nWhat is LangChain?\nLangChain is a framework that’s open-source developed for creating apps that use large language models (LLMs). It comes with tools and abstractions to better personalize the information produced from these models while maintaining accuracy and relevance. \nOne common term you can see when you read about LLMs is “prompt chains”. A prompt chain refers to a sequence of prompts or instructions used in the context of artificial intelligence and machine learning, with the purpose to guide the AI model through a multi-step process to generate more accurate, detailed, or refined outputs. This method can be employed for various tasks, such as writing, problem-solving, or generating code. \nDevelopers can create new prompt chains using LangChain, which is one of the strongest sides of the framework. They can even modify existing prompt templates without needing to train the model again when using new datasets.\nHow does LangChain work?\nLangChain is a framework designed to simplify the development of applications that utilize language models. It offers a suite of tools that help developers efficiently build and manage applications that involve natural language processing (NLP) and Large Language Models. By defining the steps needed to achieve the desired outcome (this might be a chatbot, task automation, virtual assistant, customer support, and even more), developers can adapt language models flexibly to specific business contexts using LangChain. \nHere’s a high-level overview of how LangChain works.\nModel integration\nLangChain supports various Language models including those from OpenAI, Hugging Face, Cohere, Anyscale, Azure Models, Databricks, Ollama, Llama, GPT4All, Spacy, Pinecone, AWS Bedrock, MistralAI, among others. Developers can easily switch between different models or use multiple models in one application. They can build custom-developed model integration solutions, which allow developers to take advantage of specific capabilities tailored to their specific applications.\nChains\nThe core concept of LangChain is chains, which bring together different AI components for context-aware responses. A chain represents a set of automated actions between a user prompt and the final model output. There are two types of chains provided by LangChain:\nSequential chains: These chains enable the output of a model or function to be used as an input for another one. This is particularly helpful in making multi-step processes that depend on each other.\nParallel chains: It allows for simultaneous running of multiple tasks, with their outputs merged at the end. This makes it perfect for doing tasks that can be divided into subtasks that are completely independent.\nMemory\nLangChain facilitates the storage and retrieval of information across various interactions. This is essential where there is need for persistence of context such as with chat-bots or interactive agents. There are also two types of memory provided:\nShort-term memory – Helps keep track of recent sessions.\nLong-term memory – Allows retention of information from previous sessions enhancing system recall capability on past chats and user preferences.\nTools and utilities\nLangChain provides many tools, but the most used ones are Prompt Engineering, Data Loaders and Evaluators.  When it comes to Prompt Engineering, LangChain contains utilities to develop good prompts, which are very important in getting the best responses from language models.\nIf you want to load up files like csv, pdf or other format, Data Loaders are here to help you to load and pre-process different types of data hence making them usable in model interactions.\nEvaluation is an essential part of working with machine learning models and large language models. That’s why LangChain provides Evaluators – tools used for testing language models and chains so that generated results meet the required criteria, which might include:\nDatasets criteria:\nManually curated examples: Start with high-quality, diverse inputs.\nHistorical logs: Use real user data and feedback.\nSynthetic data: Generate examples based on initial data.\nTypes of evaluations:\nHuman: Manual scoring and feedback.\nHeuristic: Rule-based functions, both reference-free and reference-based.\nLLM-as-judge: LLMs score outputs based on encoded criteria.\nPairwise: Compare two outputs to pick the better one.\nApplication evaluations:\nUnit tests: Quick, heuristic-based checks.\nRegression testing: Measure performance changes over time.\nBack-testing: Re-run production data on new versions.\nOnline evaluation: Evaluate in real-time, often for guardrails and classifications.\nAgents\nLangChain agents are essentially autonomous entities that leverage LLMs to interact with users, perform tasks, and make decisions based on natural language inputs.\nAction-driven agents use language models to decide on optimal actions for predefined tasks. On the other side interactive agents or interactive applications such as chatbots make use of these agents, which also take into account user input and stored memory when responding to queries.\nHow do chatbots work with LLMs?\nLLMs underlying chatbots use Natural Language Understanding (NLU) and Natural Language Generation (NLG), which are made possible through pre-training of models on vast textual data.\nNatural Language Understanding (NLU)\nContext awareness: LLMs can understand the subtlety and allusions in a conversation, and they can keep track of the conversation from one turn to the next. This makes it possible for the chatbots to generate logical and contextually appropriate responses to the clients.\nIntent recognition: These models should be capable of understanding the user’s intent from their queries, whether the language is very specific or quite general. They can discern what the user wants to achieve and determine the best way to help them reach that goal.\nSentiment analysis: Chatbots can determine the emotion of the user through the tone of language used and adapt to the user’s emotional state, which increases the engagement of the user.\nNatural Language Generation (NLG)\nResponse generation: When LLMs are asked questions, the responses they provide are correct both in terms of grammar and the context. This is because the responses that are produced by these models mimic human communication, due to the training of the models on vast amounts of natural language textual data.\nCreativity and flexibility: Apart from simple answers, LLM-based chatbots can tell a story, create a poem, or provide a detailed description of a specific technical issue and, therefore, can be considered to be very flexible in terms of the provided material.\nPersonalization and adaptability\nLearning from interactions: Chatbots make the interaction personalized because they have the ability to learn from the users’ behavior, as well as from their choices. It can be said that it is constantly learning, thereby making the chatbot more effective and precise in answering questions.\nAdaptation to different domains: The LLMs can be tuned to particular areas or specialties that allow the chatbots to perform as subject matter experts in customer relations, technical support, or the healthcare domain.\nLLMs are capable of understanding and generating text in multiple languages, making them suitable for applications in diverse linguistic contexts.\nBuilding your own chatbot with LangChain in five steps\nThis project aims to build a chatbot that leverages GPT-3 to search for answers within documents. First, we scrape content from online articles, split them into small chunks, compute their embeddings, and store them in Deep Lake. Then, we use a user query to retrieve the most relevant chunks from Deep Lake, which are incorporated into a prompt for generating the final answer with the LLM.\nIt’s important to note that using LLMs carries a risk of generating hallucinations or false information. While this may be unacceptable for many customer support scenarios, the chatbot can still be valuable for assisting operators in drafting answers that they can verify before sending to users.\nNext, we’ll explore how to manage conversations with GPT-3 and provide examples to demonstrate the effectiveness of this workflow\nStep 1: Project creation, prerequisites, and required library installation\nFirst create your PyCharm project for the chatbot. Open up Pycharm and click on “new project”. Then give a name of your project.\n\n\n\n\nOnce ready with the project set up, generate your `OPENAI_API_KEY` on the OpenAI API Platform Website, once you are logged in (or sign up on the OpenAI website for that purpose). To do that go to the “API Keys” section on the left navigation menu and then click on the button “+Create new secret key”. Don’t forget to copy your key.\nAfter that get your `ACTIVELOOP_TOKEN` by signing up on the Activeloop website. Once logged in, just click on the button “Create API Token” and you’ll be navigated to the token creation page. Copy this token as well.\nOnce you have both the token and the key, open your configuration settings in PyCharm, by clicking on the 3 dots button next to the run and debug buttons, and choose “Edit”. You should see the following window:\n\n\n\n\nNow locate the field “Environment variables” and find the icon on the right side of the field. Then click there – you’ll see the following window:\n\n\n\n\nAnd now by clicking the + button start adding your environmental variables and be careful with their names. They should be the same as mentioned above: `OPENAI_API_KEY` and `ACTIVELOOP_TOKEN`. When ready just click OK on the first window and then “Apply” and “OK” on the second one.\nThat’s a very big advantage of PyCharm and I very much love it, because it handles the environment variables for us automatically without the requirement for additional calls to them, allowing us to think more about the creative part of the code.\nNote: ActiveLoop is a technology company that focuses on developing data infrastructure and tools for machine learning and artificial intelligence. The company aims to streamline the process of managing, storing, and processing large-scale datasets, particularly for deep learning and other AI applications.\nDeepLake is an ActiveLoop’s flagship product. It provides efficient data storage, management, and access capabilities, optimized for large-scale datasets often used in AI.\nInstall the required libraries\nWe’ll use the `SeleniumURLLoader` class from LangChain, which relies on the `unstructured` and `selenium` Python libraries. Install these using pip.  It is recommended to install the latest version, although the code has been specifically tested with version 0.7.7. \nTo do that use the following command in your PyCharm terminal:\npip install unstructured selenium\n\n\n\n\nNow we need to install langchain, deeplake and openai. To do that just use this command in your terminal (same window you used for Selenium) and wait a bit until everything is successfully installed:\npip install langchain==0.0.208 deeplake openai==0.27.8 psutil tiktoken\nTo make sure all libraries are properly installed, just add the following lines needed for our chatbot app and click on the Run button:\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nfrom langchain.vectorstores import DeepLake\n\nfrom langchain.text_splitter import CharacterTextSplitter\n\nfrom langchain import OpenAI\n\nfrom langchain.document_loaders import SeleniumURLLoader\n\nfrom langchain import PromptTemplate\nAnother way to install your libraries is through the settings of PyCharm. Open them and go to the section Project -> Project Interpreter. Then locate the + button, search for your package and hit the button “Install Package”. Once ready, close it, and on the next window click “Apply” and then “OK”.\n\n\n\n\nStep 2: Splitting content into chunks and computing their embeddings\nAs previously mentioned, our chatbot will “communicate” with content coming out of online articles, that’s why I picked Digitaltrends.com as my source of data and selected 8 articles to start. All of them are organized into a Python list and assigned to a variable called “articles”.\narticles = ['https://www.digitaltrends.com/computing/claude-sonnet-vs-gpt-4o-comparison/',\n           'https://www.digitaltrends.com/computing/apple-intelligence-proves-that-macbooks-need-something-more/',\n           'https://www.digitaltrends.com/computing/how-to-use-openai-chatgpt-text-generation-chatbot/',\n           'https://www.digitaltrends.com/computing/character-ai-how-to-use/',\n           'https://www.digitaltrends.com/computing/how-to-upload-pdf-to-chatgpt/']\nWe load the documents from the provided URLs and split them into chunks using the `CharacterTextSplitter` with a chunk size of 1000 and no overlap:\n# Use the selenium to load the documents\nloader = SeleniumURLLoader(urls=articles)\ndocs_not_splitted = loader.load()\n\n# Split the documents into smaller chunks\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(docs_not_splitted)\nIf you run the code till now you should receive the following output, if everything works well:\n[Document(page_content=\"techcrunch\\n\\ntechcrunch\\n\\nWe, TechCrunch, are part of the Yahoo family of brandsThe sites and apps that we own and operate, including Yahoo and AOL, and our digital advertising service, Yahoo Advertising.Yahoo family of brands.\\n\\n    When you use our sites and apps, we use \\n\\nCookiesCookies (including similar technologies such as web storage) allow the operators of websites and apps to store and read information from your device. Learn more in our cookie policy.cookies to:\\n\\nprovide our sites and apps to you\\n\\nauthenticate users, apply security measures, and prevent spam and abuse, and\\n\\nmeasure your use of our sites and apps\\n\\n    If you click '\", metadata={'source': ……………]\nNext, we generate the embeddings using OpenAIEmbeddings and save them in a DeepLake vector store hosted in the cloud. Ideally, in a production environment, we could upload an entire website or course lesson to a DeepLake dataset, enabling searches across thousands or even millions of documents. \nBy leveraging a serverless Deep Lake dataset in the cloud, applications from various locations can seamlessly access a centralized dataset without the necessity of setting up a vector store on a dedicated machine.\nWhy do we need embeddings and documents in chunks?\nWhen building chatbots with Langchain, embeddings and chunking documents are essential for several reasons that relate to the efficiency, accuracy, and performance of the chatbot.\nEmbeddings are vector representations of text (words, sentences, paragraphs, or documents) that capture semantic meaning. They encapsulate the context and meaning of words in a numerical form. This allows the chatbot to understand and generate responses that are contextually appropriate by capturing nuances, synonyms, and relationships between words.\nThanks to the embeddings, the chatbot can also quickly identify and retrieve the most relevant responses or information from a knowledge base, because they allow matching user queries with the most semantically relevant chunks of information, even if the wording differs.\nChunking, on the other side, involves dividing large documents into smaller, manageable pieces or chunks. Smaller chunks are faster to process and analyze compared to large, monolithic documents. This results in quicker response times from the chatbot.\nDocument chunking helps also with the relevancy of the output, because when a user asks a question, it is often only in a specific part of a document. Chunking allows the system to pinpoint and retrieve just the relevant sections and the chatbot can provide more precise and accurate answers.\nNow let’s get back to our application and let’s update the following code by including your Activeloop organization ID. Keep in mind that, by default, your organization ID is the same as your username.\n# TODO: use your organization id here. (by default, org id is your username)\nmy_activeloop_org_id = \"didogrigorov\"\nmy_activeloop_dataset_name = \"jetbrains_article_dataset\"\ndataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\ndb = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n\n\n# add documents to our Deep Lake dataset\ndb.add_documents(docs)\nAnother great feature of PyCharm I love is the option TODO notes to be added directly in Python comments. Once you type TODO with capital letters, all notes go to a section of PyCharm where you can see them all:\n# TODO: use your organization id here. (by default, org id is your username)\nYou can click on them and PyCharm directly shows you where they are in your code. I find it very convenient for developers and use it all the time:\n\n\n\n\nIf you execute the code till now you should see the following output, if everything works normal:\n\n\n\n\nTo find the most similar chunks to a given query, we can utilize the similarity_search method provided by the Deep Lake vector store:\n# Check the top relevant documents to a specific query\nquery = \"how to check disk usage in linux?\"\ndocs = db.similarity_search(query)\nprint(docs[0].page_content)\nStep 3: Let’s build the prompt for GPT-3\nWe will design a prompt template that integrates role-prompting, pertinent Knowledge Base data, and the user’s inquiry. This template establishes the chatbot’s persona as an outstanding customer support agent. It accepts two input variables: chunks_formatted, containing the pre-formatted excerpts from articles, and query, representing the customer’s question. The goal is to produce a precise response solely based on the given chunks, avoiding any fabricated or incorrect information.\nStep 4: Building the chatbot functionality\nTo generate a response, we begin by retrieving the top-k (e.g., top-3) chunks that are most similar to the user’s query. These chunks are then formatted into a prompt, which is sent to the GPT-3 model with a temperature setting of 0.\n# user question\nquery = \"How to check disk usage in linux?\"\n\n# retrieve relevant chunks\ndocs = db.similarity_search(query)\nretrieved_chunks = [doc.page_content for doc in docs]\n\n# format the prompt\nchunks_formatted = \"\\n\\n\".join(retrieved_chunks)\nprompt_formatted = prompt.format(chunks_formatted=chunks_formatted, query=query)\n\n# generate answer\nllm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\nanswer = llm(prompt_formatted)\nprint(answer)\nIf everything works fine, your output should be:\nTo upload a PDF to ChatGPT, first log into the website and click the paperclip icon next to the text input field. Then, select the PDF from your local hard drive, Google Drive, or Microsoft OneDrive. Once attached, type your query or question into the prompt field and click the upload button. Give the system time to analyze the PDF and provide you with a response.\nStep 5: Build conversational history\n# Create conversational memory\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"input\")\n\n# Define a prompt template that includes memory\ntemplate = \"\"\"You are an exceptional customer support chatbot that gently answers questions.\n\n{chat_history}\n\nYou know the following context information.\n\n{chunks_formatted}\n\nAnswer the following question from a customer. Use only information from the previous context information. Do not invent stuff.\n\nQuestion: {input}\n\nAnswer:\"\"\"\n\nprompt = PromptTemplate(\n    input_variables=[\"chat_history\", \"chunks_formatted\", \"input\"],\n    template=template,\n)\n\n# Initialize the OpenAI model\nllm = OpenAI(openai_api_key=\"YOUR API KEY\", model=\"gpt-3.5-turbo-instruct\", temperature=0)\n\n# Create the LLMChain with memory\nchain = LLMChain(\n    llm=llm,\n    prompt=prompt,\n    memory=memory\n)\n\n# User query\nquery = \"What was the 5th point about on the question how to remove spotify account?\"\n\n# Retrieve relevant chunks\ndocs = db.similarity_search(query)\nretrieved_chunks = [doc.page_content for doc in docs]\n\n# Format the chunks for the prompt\nchunks_formatted = \"\\n\\n\".join(retrieved_chunks)\n\n# Prepare the input for the chain\ninput_data = {\n    \"input\": query,\n    \"chunks_formatted\": chunks_formatted,\n    \"chat_history\": memory.buffer\n}\n\n# Simulate a conversation\nresponse = chain.predict(**input_data)\n\nprint(response)\nLet’s walk through the code in a more conversational manner.\nTo start with, we set up a conversational memory using `ConversationBufferMemory`. This allows our chatbot to remember the ongoing chat history, using `input_key=”input”` to manage the incoming user inputs.\nNext, we design a prompt template. This template is like a script for the chatbot, including sections for chat history, the chunks of information we’ve gathered, and the current user question (input). This structure helps the chatbot know exactly what context it has and what question it needs to answer.\nThen, we move on to initializing our language model chain, or `LLMChain`. Think of this as assembling the components: we take our prompt template, the language model, and the memory we set up earlier, and combine them into a single workflow.\nWhen it’s time to handle a user query, we prepare the input. This involves creating a dictionary that includes the user’s question (`input`) and the relevant information chunks (`chunks_formatted`). This setup ensures that the chatbot has all the details it needs to craft a well-informed response.\nFinally, we generate a response. We call the `chain.predict` method, passing in our prepared input data. The method processes this input through the workflow we’ve built, and out comes the chatbot’s answer, which we then display.\nThis approach allows our chatbot to maintain a smooth, informed conversation, remembering past interactions and providing relevant answers based on the context.\nAnother favorite trick with PyCharm that helped me a lot to build this functionality was the opportunity to put my cursor over a method, to hit the key “CTRL” and click on it.\nIn conclusion\nGPT-3 excels at creating conversational chatbots capable of answering specific questions based on contextual information provided in the prompt. However, ensuring the model generates answers solely based on this context can be challenging, as it often tends to hallucinate (i.e., generate new, potentially false information). The impact of such false information varies depending on the use case.\nIn summary, we developed a context-aware question-answering system using LangChain, following the provided code and strategies. The process included splitting documents into chunks, computing their embeddings, implementing a retriever to find similar chunks, crafting a prompt for GPT-3, and using the GPT-3 model for text generation. This approach showcases the potential of leveraging GPT-3 to create powerful and contextually accurate chatbots while also emphasizing the importance of being vigilant about the risk of generating false information.\nAbout the author\nDido Grigorov\nDido is a seasoned Deep Learning Engineer and Python programmer with an impressive 17 years of experience in the field. He is currently pursuing advanced studies at the prestigious Stanford University, where he is enrolled in a cutting-edge AI program, led by renowned experts such as Andrew Ng, Christopher Manning, Fei-Fei Li and Chelsea Finn, providing Dido with unparalleled insights and mentorship.\nDido’s passion for Artificial Intelligence is evident in his dedication to both work and experimentation. Over the years, he has developed a deep expertise in designing, implementing, and optimizing machine learning models. His proficiency in Python has enabled him to tackle complex problems and contribute to innovative AI solutions across various domains.",
        "dc:creator": "Evgenia Verbina",
        "content": "This is a guest post from Dido Grigorov, a deep learning engineer and Python programmer with 17 years of experience in the field. Chatbots have evolved far beyond simple question-and-answer tools. With the power of large language models (LLMs), they can understand the context of conversations and generate human-like responses, making them invaluable for customer [&#8230;]",
        "contentSnippet": "This is a guest post from Dido Grigorov, a deep learning engineer and Python programmer with 17 years of experience in the field. Chatbots have evolved far beyond simple question-and-answer tools. With the power of large language models (LLMs), they can understand the context of conversations and generate human-like responses, making them invaluable for customer […]",
        "guid": "https://blog.jetbrains.com/?post_type=pycharm&p=503887",
        "categories": [
          "data-science",
          "how-tos",
          "chatbots",
          "langchain",
          "llms"
        ],
        "isoDate": "2024-08-21T10:06:08.000Z"
      }
    ]
  },
  {
    "name": "Airbnb Engineering & Data Science",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Instagram Engineering",
    "category": "기업",
    "posts": []
  },
  {
    "name": "PayPal Engineering",
    "category": "기업",
    "posts": []
  },
  {
    "name": "article New updates to Planner comment notifications and settings in Planner Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Visual Studio Blog",
    "category": "기업",
    "posts": [
      {
        "creator": "Mads Kristensen",
        "title": "New IDE features in Visual Studio v17.11",
        "link": "https://devblogs.microsoft.com/visualstudio/new-ide-features-in-visual-studio-v17-11/",
        "pubDate": "Mon, 26 Aug 2024 15:53:30 +0000",
        "content:encodedSnippet": "In Visual Studio 2022 v17.11, you’ll discover several new features that address specific issues developers have reported. These updates don’t belong to a distinct category, but we’re dedicated to sharing them with you in this blog post. You can download the update and view the comprehensive release notes for full details.\nFind the code you’re looking for\nDo you ever feel like you’re seeing too many results in code search? Narrow down your focus with the newly added scoping options in Code Search.\n\nFor the default code search experience and each filter, you can now set the scope to Entire solution, Current project, or Current document and toggle inclusion of external files.\nYou can set different scopes for different filters. For example, the default experience can be set to look through Entire solution and members can be set to look through only current document. Your selections will be preserved past the current session.\nFamiliar keyboard shortcuts\nWhen moving between different IDEs and editors, it can be frustrating to have to relearn keyboard shortcuts. We’ve made some changes to some default keyboard shortcuts to make them more familiar and to preserve your muscle memory.\nToggle line comments\nYou’ve been able to toggle line comments in Visual Studio for a long time, but the default keyboard shortcut was Ctrl+K, Ctrl+/. We’ve now added Ctrl+/ as an alternative shortcut, which is the default in many other IDEs and editors.\n\nOpen Command Palette\nOr Feature Search as it’s called in Visual Studio. The default keyboard shortcut for this feature is now Ctrl+Shift+P, which should be familiar to VS Code users for opening the Command Palette.\n\nNever miss installing a component\nMany teams use *.vsconfig files to standardize their teams’ Visual Studio installations. The *.vsconfig files can be placed in a repo or a project’s solution directory, and Visual Studio will automatically detect if components specified in the *.vsconfig file are missing.\n\nIf any are missing, then a notification such as the one pictured below will appear.\n\nWe’ve made two improvements to this experience in Visual Studio 2022 version 17.11 Preview 1.\nFirst, Visual Studio can now detect if any local or network hosted extensions are missing from the installation, and if so, it’ll prompt you to install them. Previously, with respect to extensions, Visual Studio was only able to recognize if marketplace extensions were missing.\nSecondly, Visual Studio will now re-prompt the notification in certain situations, such as if the *.vsconfig file has changed because new components or extensions get added to it. Previously, the notification would only pop until you acted upon it, at which point it would be suppressed forever.\nImproved user authentication\nVisual Studio now uses the Web Account Manager (WAM) as its main authentication mechanism. This integration not only streamlines the authentication experience for Visual Studio, but it also enhances the security of your credentials.\nHere’s how the new WAM experience looks like:\n\nHow does this impact your experience?\nUsing WAM as the default authentication experience has many benefits, including:\nWindows integration: In addition to reducing the overall number of authentication prompts, you can now select existing Windows accounts instead of repeatedly entering credentials.\nBetter token protection: Refresh tokens are better secured as they are now device bound.\nSupport for the latest security features:\n\nLeverage rich OS capabilities such as Windows Hello & FIDO keys.\nAccess the latest and greatest Microsoft Entra ID capabilities and conditional access policies.\nNew Teams Toolkit templates\nTeams Toolkit now offers an empty Teams template for you to connect with your existing projects or use it as a starting point for new Teams apps.\nStart with this empty template to create a Teams app.\nIf you want to add Teams capability to your existing project, add Empty Teams App to your project and then connect two projects by making simple edits follow https://aka.ms/Config-Teams-app.\n\nTeams Toolkit supports authentication for Search Results from API Message Extensions app.\n\nWe hope you enjoy this update to Visual Studio, and we look forward to hearing what you think. You can share feedback with us via Developer Community, by reporting issues via report a problem and share your suggestions for new features or improvements to existing ones.\nStay connected with the Visual Studio team by following us on Twitter, YouTube, LinkedIn, and on Microsoft Learn.\nThank you for using Visual Studio and happy coding!\nDownload Visual Studio 17.11\n\n \nThe post New IDE features in Visual Studio v17.11 appeared first on Visual Studio Blog.",
        "dc:creator": "Mads Kristensen",
        "content": "<p>In Visual Studio 2022 v17.11, you&#8217;ll discover several new features that address specific issues developers have reported. These updates don&#8217;t belong to a distinct category, but we&#8217;re dedicated to sharing them with you in this blog post. You can download the update and view the comprehensive release notes for full details.</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/visualstudio/new-ide-features-in-visual-studio-v17-11/\">New IDE features in Visual Studio v17.11</a> appeared first on <a href=\"https://devblogs.microsoft.com/visualstudio\">Visual Studio Blog</a>.</p>\n",
        "contentSnippet": "In Visual Studio 2022 v17.11, you’ll discover several new features that address specific issues developers have reported. These updates don’t belong to a distinct category, but we’re dedicated to sharing them with you in this blog post. You can download the update and view the comprehensive release notes for full details.\nThe post New IDE features in Visual Studio v17.11 appeared first on Visual Studio Blog.",
        "guid": "https://devblogs.microsoft.com/visualstudio/?p=250326",
        "categories": [
          "Productivity",
          "Team and Development",
          "Visual Studio",
          "code search",
          "Keyboard Shortcuts",
          "vsconfig"
        ],
        "isoDate": "2024-08-26T15:53:30.000Z"
      },
      {
        "creator": "Rhea Patel",
        "title": "Enhancing Code Comprehension: GitHub Copilot’s Features in Visual Studio 2022 17.11",
        "link": "https://devblogs.microsoft.com/visualstudio/enhancing-code-comprehension-github-copilots-features-in-visual-studio-2022-17-11/",
        "pubDate": "Thu, 22 Aug 2024 10:00:28 +0000",
        "content:encodedSnippet": "GitHub Copilot in Visual Studio 2022 17.11 now offers an improved experience you to refer to their methods, classes, functions, and entire solution directly within the chat. By using the # symbol followed by the name of a method, class, or function, or by referring to your solution with @workspace, you can provide specific context that helps GitHub Copilot better understand their code and the problem at hand.\nRefer to your solution\nGitHub Copilot Chat now allows you to refer to your entire solution. This feature helps provide an even deeper understanding of your solution to get the best results. To refer to your solution, simply use the @workspace and Copilot will answer high level questions about your solution, files and how they work together.\n\nTry asking GitHub Copilot Chat questions like:\n@workspace What project can I run in my solution?\n@workspace In my workspace, where is #methodname referenced?\n@workspace Where does serialization occur in my solution?\n \nReference your methods, classes, functions\nGitHub Copilot Chat now allows you to refer to your methods, classes, and functions inline. By referring to methods, classes, and functions directly within the chat, you can provide specific context that helps GitHub Copilot better comprehend your code and the problem at hand. This feature empowers you to provide richer context to GitHub Copilot, enabling it to deliver more precise responses by gaining a deeper understanding of your codebase.\n\nTry asking GitHub Copilot Chat questions like:\nI have a test method named #methodName. How can I ensure that it’s being executed correctly?\nWhat are some optimizations that could be applied to #methodName?\nHow does #methodName1 use #methodName2 and what issues should I look out for?\n/explain #methodName\n \nSearch the web in Copilot Chat\nGitHub Copilot now includes context from your entire repository & can search the web.\nGitHub Copilot Enterprise subscribers in Visual Studio can now use GitHub Copilot Chat to get answers enriched with context from their entire repository and Bing search results.\n\nGet answers from across your entire codebase\nGitHub Copilot Chat can now answer questions with understanding of your full repository, not just the tabs you have open. Index your repository on GitHub.com, and then ask a question mentioning @github. You can ask questions like @github where is rate limiting implemented?\nSearch with the context of the web\nGitHub Copilot chat can also search Bing to find information outside of its general knowledge or your codebase. When you mention @github, GitHub Copilot will intelligently decide when to use Bing. You can ask questions like @github what is the latest LTS version of .NET?\nBing search is only available if enabled by an administrator – for more details, see Enabling GitHub Copilot Enterprise features or read the docs.\nTry it out today!\nWe hope you enjoy this new feature in GitHub Copilot for Visual Studio! Your feedback is important to us, so please share your thoughts using the thumbs up or down in the Chat, or by visiting the Developer Community. We look forward to hearing from you!\n\nHappy coding!\nWe appreciate the time you’ve spent reporting issues/suggestions and hope you continue to give us feedback when using GitHub Copilot in Visual Studio on what you like and what we can improve. Your feedback is critical to help us make GitHub Copilot  the best tool it can be! You can share feedback with us via Developer Community: report any bugs or issues via report a problem and share your suggestions for new features or improvements to existing ones.\nStay connected with the Visual Studio team by following us on YouTube, Twitter, LinkedIn, Twitch and on Microsoft Learn.\n\nThe post Enhancing Code Comprehension: GitHub Copilot’s Features in Visual Studio 2022 17.11 appeared first on Visual Studio Blog.",
        "dc:creator": "Rhea Patel",
        "content": "<p>GitHub Copilot in Visual Studio 2022 17.11 now offers an improved experience you to refer to their methods, classes, functions, and entire solution directly within the chat. By using the # symbol followed by the name of a method, class, or function,</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/visualstudio/enhancing-code-comprehension-github-copilots-features-in-visual-studio-2022-17-11/\">Enhancing Code Comprehension: GitHub Copilot’s Features in Visual Studio 2022 17.11</a> appeared first on <a href=\"https://devblogs.microsoft.com/visualstudio\">Visual Studio Blog</a>.</p>\n",
        "contentSnippet": "GitHub Copilot in Visual Studio 2022 17.11 now offers an improved experience you to refer to their methods, classes, functions, and entire solution directly within the chat. By using the # symbol followed by the name of a method, class, or function,\nThe post Enhancing Code Comprehension: GitHub Copilot’s Features in Visual Studio 2022 17.11 appeared first on Visual Studio Blog.",
        "guid": "https://devblogs.microsoft.com/visualstudio/?p=249711",
        "categories": [
          "Copilot",
          "Visual Studio",
          "Context",
          "GitHub Copilot"
        ],
        "isoDate": "2024-08-22T10:00:28.000Z"
      },
      {
        "creator": "Sy Brand",
        "title": "New C++ features in Visual Studio v17.11",
        "link": "https://devblogs.microsoft.com/visualstudio/new-c-features-in-visual-studio-v17-11/",
        "pubDate": "Wed, 21 Aug 2024 10:00:31 +0000",
        "content:encodedSnippet": "Visual Studio 2022 version 17.11 is here and comes with a host of new features for C++ developers. We’ve made improvements across our toolchain, Unreal Engine support, Build Insights, CMake debugger, and more. This post gives a quick overview of what’s available; for all the details, see What’s New for C++ Developers in Visual Studio 2022 17.11 on the C++ blog.\nStandard Library\nThe three main areas of improvement in this release are in formatted output, vectorization, and diagnostics. For formatted output, we’ve implemented parts of Formatting Ranges and all of Printing Blank Lines with println. Our vectorization improvements hit over a dozen of the standard algorithms, and we’ve improved the diagnostics of common misuses of std::ranges::to and std::get(std::tuple). See the STL changelog for all the details.\nCMake Debugger\nWe have added support for the CMake debugger in CMake projects targeting Linux via WSL or SSH. The CMake debugger allows you to debug your CMake scripts and CMakeLists.txt files through the Visual Studio debugger.\nTo start a CMake debugging session, set a breakpoint in your CMakeLists.txt file and then navigate to Project > Configure Cache with CMake Debugging.\n\nUnreal Engine Support\nWe’ve added several new features for Unreal Engine developers:\nAn Unreal Engine toolbar that provides quick access to Unreal Engine actions such as attaching to UE processes and accessing the UE log\nAn Add Class dialog for adding common UE class templates to your project\nAn Add Module dialog for adding UE modules to your project\n\nBuild Insights\nIn this update, we added quality of life changes to C++ Build Insights integration. You can now filter your Build Insights trace results by project. For results in each row, you will now see the relative path and file name instead of the full path. We have also improved the grouping of results in the Included Files view.\n\nBreakpoint Performance\nWe have significantly enhanced the performance of conditional breakpoints in C++ through a reworked implementation.\nBeginning with version 17.11, our initial assessment finds that execution time is almost four times as fast, reducing execution time from 80 seconds to 21 seconds over 80,000 iterations.\nSummary\nWe hope this has given you a taste of what’s new. For all the details, see What’s New for C++ Developers in Visual Studio 2022 17.11 on the C++ blog.\nWe are very much interested in your feedback. The comments below are open. Feedback can also be shared through Visual Studio Developer Community. You can also reach us on Twitter (@VisualC), or via email at visualcpp@microsoft.com.\nThe post New C++ features in Visual Studio v17.11 appeared first on Visual Studio Blog.",
        "dc:creator": "Sy Brand",
        "content": "<p>Visual Studio 2022 version 17.11 is here and comes with a host of new features for C++ developers. We’ve made improvements across our toolchain, Unreal Engine support, Build Insights, CMake debugger, and more. This post gives a quick overview of what’s available;</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/visualstudio/new-c-features-in-visual-studio-v17-11/\">New C++ features in Visual Studio v17.11</a> appeared first on <a href=\"https://devblogs.microsoft.com/visualstudio\">Visual Studio Blog</a>.</p>\n",
        "contentSnippet": "Visual Studio 2022 version 17.11 is here and comes with a host of new features for C++ developers. We’ve made improvements across our toolchain, Unreal Engine support, Build Insights, CMake debugger, and more. This post gives a quick overview of what’s available;\nThe post New C++ features in Visual Studio v17.11 appeared first on Visual Studio Blog.",
        "guid": "https://devblogs.microsoft.com/visualstudio/?p=250237",
        "categories": [
          "Cross-Platform",
          "Gaming",
          "Visual Studio",
          "C++",
          "Visual Studio 2022"
        ],
        "isoDate": "2024-08-21T10:00:31.000Z"
      }
    ]
  },
  {
    "name": "Instagram Engineering",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Dropbox Tech Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Joshua",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권재명",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김석기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김선영",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권진호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강대명",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권정혁",
    "category": "개인",
    "posts": []
  },
  {
    "name": "줌구",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수보",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김시은",
    "category": "개인",
    "posts": []
  },
  {
    "name": "곽민수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김범진",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민서",
    "category": "개인",
    "posts": []
  },
  {
    "name": "I am not Okay",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권창현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권기호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수민",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강태욱",
    "category": "개인",
    "posts": [
      {
        "title": "인공지능 딥러닝 모델 성능 지표",
        "link": "http://daddynkidsmakers.blogspot.com/2024/08/blog-post.html",
        "pubDate": "2024-08-25T09:46:00.000Z",
        "author": "Daddy Maker",
        "content": "<div style=\"text-align: left;\">이 글은&nbsp;인공지능 딥러닝 모델 성능 지표를 간략히 정리한 것이다. 개발 목적에 따라 다양한 딥러닝 모델을 사용하고, 그에 따라 적절한 성능 지표를 모니터링해 튜닝 등 품질을 개선하는 것이 좋다. 필요할 때 찾아보기 귀찮으니 정리해 놓기로 한다. 이와 관련된 다양한 성능지표를 확인해 본다.</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEid4gFMYYST6FBuhnZnTi_81U3sox36neNlc6Zl88lIXUyUXN5H8A9jDHJOYBqf0FIxS5oiOWpHUT4Cemd_IIhROVJhMvsmzVh7Mw6puJ__pXHubj9tmQuZDmDjBKGSAxKrqmkvxxZ3yo7ChuqqRYh3GFcDYWzKn_1yhcQLOl2KBeROGvVvhd_8tNnN9lm1\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"480\" data-original-width=\"720\" height=\"213\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEid4gFMYYST6FBuhnZnTi_81U3sox36neNlc6Zl88lIXUyUXN5H8A9jDHJOYBqf0FIxS5oiOWpHUT4Cemd_IIhROVJhMvsmzVh7Mw6puJ__pXHubj9tmQuZDmDjBKGSAxKrqmkvxxZ3yo7ChuqqRYh3GFcDYWzKn_1yhcQLOl2KBeROGvVvhd_8tNnN9lm1\" width=\"320\" /></a></div></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b>개요</b></div><div style=\"text-align: left;\">많은 딥러닝 모델 개발의 경우, 성능 지표를 미리 고민하지 않고, 개발을 하였다가 뒤늦게 관련 기능을 추가하느라 고생하는 경우가 있다. 이런 경우를 대비해, 미리 어떤 지표가 있는 지 확인해 본다.</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b>본론</b></div><div style=\"text-align: left;\">가장 단순한 성능 지표부터 시작해, LLM에서 많이 사용되는 지표도 확인해 보자.&nbsp;</div><div style=\"text-align: left;\">여기서 수식은 n개의 데이터 셋을 가진 입력 변수 x에 대한 참 값 y가 있다고 가정한다. ŷ 은 입력 x 에 대한 f(x)의 결과로 가정한다. f(x)의 참 값은 y이다.&nbsp;</div><div style=\"text-align: left;\"><ul style=\"text-align: left;\"><li>MSE (Mean Squared Error) : 편차의 제곱을 n으로 나눈 편차 평균</li></ul></div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEh69EAXcrN8uA6Nmo9DcVlTgn4EJJ2f4Aqa9iWQenvcLJ2N9UL8l5_pHkB80SsIa5yno-r18Xi6uNeo7U5gm7FXtVpAKxU7_4-2dUSv1akMEiPhnZGDYS8NMa8d0zuimOwLjr0Lt8yILzR55oPyMWdfS7j8xqDmuLxT1N-RoW8V9tRtOCwYu0qM_JK-KUsd\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"85\" data-original-width=\"262\" height=\"64\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEh69EAXcrN8uA6Nmo9DcVlTgn4EJJ2f4Aqa9iWQenvcLJ2N9UL8l5_pHkB80SsIa5yno-r18Xi6uNeo7U5gm7FXtVpAKxU7_4-2dUSv1akMEiPhnZGDYS8NMa8d0zuimOwLjr0Lt8yILzR55oPyMWdfS7j8xqDmuLxT1N-RoW8V9tRtOCwYu0qM_JK-KUsd=w196-h64\" width=\"196\" /></a></div><ul style=\"text-align: left;\"><li>RMSE (Root Mean Squared Error): 예측값과 실제값의 편차 평균의 제곱근 값</li></ul><div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEgSQHJRltZB-1O4lzoVcSBAiL7NAhs3t6TEtxHNenraOIthWZl2H3e3GArY0nSY5NwAok0zx3p88tTwullR-i-AAuZZNGx2R1U0lDH3V5nArvaIfs6oJnIiKRiAnwh619tTqbOU2ocPS1vwPWdxaNPL2mWs0Y3AMeEd40nEITq2VEcwSdBXbpOCbwgheGkm\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"85\" data-original-width=\"307\" height=\"60\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEgSQHJRltZB-1O4lzoVcSBAiL7NAhs3t6TEtxHNenraOIthWZl2H3e3GArY0nSY5NwAok0zx3p88tTwullR-i-AAuZZNGx2R1U0lDH3V5nArvaIfs6oJnIiKRiAnwh619tTqbOU2ocPS1vwPWdxaNPL2mWs0Y3AMeEd40nEITq2VEcwSdBXbpOCbwgheGkm=w217-h60\" width=\"217\" /></a></div></div><ul style=\"text-align: left;\"><li>CvRMSE (Coefficent of Variation of RMSE): RMSE를 표준화하여 데이터 변동성을 알려주는 지표. RMSE의 백분율임. 데이터 평균에 대한 RMSE의 백분류로 계산</li></ul><div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEj2s3ScOnAhYLIEBw9NSKf3QWd2rGMoya9pPEHMm-VGGA5UV1aNQXo2iVcH9UlnrubLLV486LdFWuvbJezRn-7fF8uWqVhqFmuaWOsG4dwFnfYb6LbwlhdQV95S-8-BsHZEVDl-2CGFAQJMBjCELAsVyauUCJALAGEWFFxdKa3NYE9RxXcQcoQDfKGCKR7I\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"69\" data-original-width=\"303\" height=\"49\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEj2s3ScOnAhYLIEBw9NSKf3QWd2rGMoya9pPEHMm-VGGA5UV1aNQXo2iVcH9UlnrubLLV486LdFWuvbJezRn-7fF8uWqVhqFmuaWOsG4dwFnfYb6LbwlhdQV95S-8-BsHZEVDl-2CGFAQJMBjCELAsVyauUCJALAGEWFFxdKa3NYE9RxXcQcoQDfKGCKR7I=w218-h49\" width=\"218\" /></a></div></div><ul style=\"text-align: left;\"><li>MAE (Mean Absolute Error) : 편차값에 가까운 절대 차이 평균값</li></ul><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEjSaWNJ11wjB8baPUkxN5k1Ib2u0TnJnfsdpSXVkTm-I8CioNRokhfI72BBcV8b7LUCX9o3M7HOkarrql7SL_dTlSgPg4bnSZN_X2-OaT_Ap79OKeeBsbJ82NbZULsArpVaoIXny-A45IwbG1J2LQBUZNIyVZDYdgoLM4aII8ovy3JpVUsxskldFJOgB_UV\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"85\" data-original-width=\"265\" height=\"61\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjSaWNJ11wjB8baPUkxN5k1Ib2u0TnJnfsdpSXVkTm-I8CioNRokhfI72BBcV8b7LUCX9o3M7HOkarrql7SL_dTlSgPg4bnSZN_X2-OaT_Ap79OKeeBsbJ82NbZULsArpVaoIXny-A45IwbG1J2LQBUZNIyVZDYdgoLM4aII8ovy3JpVUsxskldFJOgB_UV=w189-h61\" width=\"189\" /></a></div><ul style=\"text-align: left;\"><li>Accuray : 모델이 정확히 값을 예측했는 지 지표. 분류 모델 등 사용. 여기서 1은 조건 부합 시 참(1), 거짓(0)을 반환하는 indicator function 임</li></ul><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEjO1y7n-M_EW3a3wnrFQKqTNdyvMOYfKVgYxf1yTAgt1wFEhJBVYd0C4ayetZuIgXoz-M2WZ5gvTVlrceOvtJPmaqZurwNOBSmwWKNIOUfe6YnVMAYG8FkPX3AmXOPcL2oTRErhideW_bzB_uFhnB-FbUZ8SjEReolSj-DHuScBruZmzTBZgXodRb5ydHO0\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"89\" data-original-width=\"315\" height=\"62\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjO1y7n-M_EW3a3wnrFQKqTNdyvMOYfKVgYxf1yTAgt1wFEhJBVYd0C4ayetZuIgXoz-M2WZ5gvTVlrceOvtJPmaqZurwNOBSmwWKNIOUfe6YnVMAYG8FkPX3AmXOPcL2oTRErhideW_bzB_uFhnB-FbUZ8SjEReolSj-DHuScBruZmzTBZgXodRb5ydHO0=w219-h62\" width=\"219\" /></a></div><ul style=\"text-align: left;\"><li>MAPE (Mean Abslute Percentage Error): 회귀분석 모델 예측에 주로 사용되는 지표.&nbsp;</li></ul></div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEhVLddPwwpb5fzh78A7OuFqjQcjW74tEgVtZZ5XWi---S8xLB9sFC-89hapWA2252PssVLYN9bNpg4CMdDra7JtHTbRoGiM5atC4m7XHna4odMw1EDDmXNSpOij2dFKtFY6k4jG78wUJVE8dvQqhSrZTxGFc83fWEd5pdaMMUGUq2l2UmEPIWOIK-OXcsKf\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"83\" data-original-width=\"336\" height=\"54\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEhVLddPwwpb5fzh78A7OuFqjQcjW74tEgVtZZ5XWi---S8xLB9sFC-89hapWA2252PssVLYN9bNpg4CMdDra7JtHTbRoGiM5atC4m7XHna4odMw1EDDmXNSpOij2dFKtFY6k4jG78wUJVE8dvQqhSrZTxGFc83fWEd5pdaMMUGUq2l2UmEPIWOIK-OXcsKf=w217-h54\" width=\"217\" /></a></div></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">이외에, LLM에 주로 사용되는 ROGUE, BLEU, 컴퓨터 비전에 주로 사용되는 mAP, IoU, F1, call, recall 등 지표가 있다.&nbsp;</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><br /></div>",
        "contentSnippet": "이 글은 인공지능 딥러닝 모델 성능 지표를 간략히 정리한 것이다. 개발 목적에 따라 다양한 딥러닝 모델을 사용하고, 그에 따라 적절한 성능 지표를 모니터링해 튜닝 등 품질을 개선하는 것이 좋다. 필요할 때 찾아보기 귀찮으니 정리해 놓기로 한다. 이와 관련된 다양한 성능지표를 확인해 본다.\n\n\n\n\n개요\n많은 딥러닝 모델 개발의 경우, 성능 지표를 미리 고민하지 않고, 개발을 하였다가 뒤늦게 관련 기능을 추가하느라 고생하는 경우가 있다. 이런 경우를 대비해, 미리 어떤 지표가 있는 지 확인해 본다.\n\n\n본론\n가장 단순한 성능 지표부터 시작해, LLM에서 많이 사용되는 지표도 확인해 보자. \n여기서 수식은 n개의 데이터 셋을 가진 입력 변수 x에 대한 참 값 y가 있다고 가정한다. ŷ 은 입력 x 에 대한 f(x)의 결과로 가정한다. f(x)의 참 값은 y이다. \n\nMSE (Mean Squared Error) : 편차의 제곱을 n으로 나눈 편차 평균\n\n\n\n\nRMSE (Root Mean Squared Error): 예측값과 실제값의 편차 평균의 제곱근 값\n\n\n\nCvRMSE (Coefficent of Variation of RMSE): RMSE를 표준화하여 데이터 변동성을 알려주는 지표. RMSE의 백분율임. 데이터 평균에 대한 RMSE의 백분류로 계산\n\n\n\nMAE (Mean Absolute Error) : 편차값에 가까운 절대 차이 평균값\n\n\n\nAccuray : 모델이 정확히 값을 예측했는 지 지표. 분류 모델 등 사용. 여기서 1은 조건 부합 시 참(1), 거짓(0)을 반환하는 indicator function 임\n\n\n\nMAPE (Mean Abslute Percentage Error): 회귀분석 모델 예측에 주로 사용되는 지표. \n\n\n\n\n\n이외에, LLM에 주로 사용되는 ROGUE, BLEU, 컴퓨터 비전에 주로 사용되는 mAP, IoU, F1, call, recall 등 지표가 있다.",
        "id": "tag:blogger.com,1999:blog-5201956450461596914.post-5149285106406844516",
        "isoDate": "2024-08-25T09:46:00.000Z"
      }
    ]
  },
  {
    "name": "권용진",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권영재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김승호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김병환",
    "category": "개인",
    "posts": []
  },
  {
    "name": "RSS feed for boyism Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성준의 린스타트업과 디자인씽킹",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권혁우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김준형",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강동혁",
    "category": "개인",
    "posts": []
  },
  {
    "name": "고종범",
    "category": "개인",
    "posts": [
      {
        "creator": "고종범",
        "title": "가을이 되는 느낌? 독서",
        "link": "https://brunch.co.kr/@@24SO/45",
        "pubDate": "Mon, 26 Aug 2024 00:54:06 GMT",
        "author": "고종범",
        "content": "여유가 주는 바쁨 진행하던 프로젝트가 너무 잘 진행되어 여유가 생겼다. 1차 고개를 넘은 터라 한 달 정도 여유 있을 거라 생각했는데 2달은 여유가 있을 듯싶다. 그래서 그 여유를 즐기기 위해 그동안 정리 못하고 있던 것들을 정리하기 시작했다. 잔뜩 쌓였던 나의 주제 단지를 열어보니 생각보다 많은 주제가 있었다. 게다가 끝난 줄 알았던 주제에 꼬리가 붙어서",
        "contentSnippet": "여유가 주는 바쁨 진행하던 프로젝트가 너무 잘 진행되어 여유가 생겼다. 1차 고개를 넘은 터라 한 달 정도 여유 있을 거라 생각했는데 2달은 여유가 있을 듯싶다. 그래서 그 여유를 즐기기 위해 그동안 정리 못하고 있던 것들을 정리하기 시작했다. 잔뜩 쌓였던 나의 주제 단지를 열어보니 생각보다 많은 주제가 있었다. 게다가 끝난 줄 알았던 주제에 꼬리가 붙어서",
        "guid": "https://brunch.co.kr/@@24SO/45",
        "isoDate": "2024-08-26T00:54:06.000Z"
      }
    ]
  },
  {
    "name": "cheese10yun",
    "category": "개인",
    "posts": []
  },
  {
    "name": "구자철",
    "category": "개인",
    "posts": []
  },
  {
    "name": "FSS",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권동준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김용일",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김도균",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김상훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민석",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김동우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권윤학",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강성훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김만수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "엘키",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권태환",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김슬기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김광현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성빈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김영우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강명훈",
    "category": "개인",
    "posts": [
      {
        "title": "비율 계산",
        "link": "https://kangmyounghun.blogspot.com/2024/08/blog-post_22.html",
        "pubDate": "2024-08-22T08:58:00.001Z",
        "author": "강명훈",
        "content": "<div>프로세스 단위별 합산 구하고,</div><div><br /></div>\n<div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOhXs36W9US2RiekjU5y8DAa8GsEORl5sxBBmgaw5HWBInPzfuQetcjYRWg7SV6rpPWzMjOqxAxGT0xFogcUkWYGgc4HD5Oibw80V1Huyo7K3Vc4aGbWBCaV9WJy65dPmsqWoAgaNgTWkkGTcOHHAaFN7amC7fywBDesHcp5u1StPRUtUAP128JSkh0feq/s1240/percent.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"720\" data-original-width=\"1240\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOhXs36W9US2RiekjU5y8DAa8GsEORl5sxBBmgaw5HWBInPzfuQetcjYRWg7SV6rpPWzMjOqxAxGT0xFogcUkWYGgc4HD5Oibw80V1Huyo7K3Vc4aGbWBCaV9WJy65dPmsqWoAgaNgTWkkGTcOHHAaFN7amC7fywBDesHcp5u1StPRUtUAP128JSkh0feq/s520/percent.png\" width=\"520\" /></a></div><div><br /></div><div><span><a name='more'></a></span>총합 <a href=\"https://docs.splunk.com/Documentation/Splunk/9.3.0/SearchReference/Appendcols\" target=\"_blank\">필드 추가</a> 후,</div><br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimln5NpC3Fdsuv5daWi5vziXQDbwY46Nvz3Zh_kgNQyg6o_Q4jLmahA_DvVO6YdsXWjCjOPk3Qf1h3YUuH-3N3Z7iPCNIG8QlNAD0HZhXpJnAzkSCY03YXRMLs1EmZAjfGSqa2XIojFMjuyZ1s9p_Mpdmc6d7zc1Q2iBTWVLKcncpPbhZzJDEKmCNpXXzv/s1240/percent2.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"720\" data-original-width=\"1240\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimln5NpC3Fdsuv5daWi5vziXQDbwY46Nvz3Zh_kgNQyg6o_Q4jLmahA_DvVO6YdsXWjCjOPk3Qf1h3YUuH-3N3Z7iPCNIG8QlNAD0HZhXpJnAzkSCY03YXRMLs1EmZAjfGSqa2XIojFMjuyZ1s9p_Mpdmc6d7zc1Q2iBTWVLKcncpPbhZzJDEKmCNpXXzv/s520/percent2.png\" width=\"520\" /></a></div><br />\n<div>비율 계산.</div><div><br /></div><div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4dpHdY-hZJxGnhlSoMHPWA6c0CK6WBgxavdAbIBWIiUFaA_vaFZuvXT6VDNr8WFILy6PByRHI6wSC1s-YI6Bo7PbulvuXoMwEDS4MqrwBB1jSF-nwmm2jl3JktWxINodvUqtb1fLocQwMjp6bBzpIiueU0QKCnScM7pdNuO5whFicWfCxcS1sHCH4XJsw/s1240/percent3.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"720\" data-original-width=\"1240\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4dpHdY-hZJxGnhlSoMHPWA6c0CK6WBgxavdAbIBWIiUFaA_vaFZuvXT6VDNr8WFILy6PByRHI6wSC1s-YI6Bo7PbulvuXoMwEDS4MqrwBB1jSF-nwmm2jl3JktWxINodvUqtb1fLocQwMjp6bBzpIiueU0QKCnScM7pdNuO5whFicWfCxcS1sHCH4XJsw/s520/percent3.png\" width=\"520\" /></a></div><div><br /></div><div>아니면 이벤트 단위로 총합, 단위별 합산 추가 후 계산.</div></div><div><br /></div><div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkrY1Cl8fh7lL04rj0SgMxiiaV8xXoTDk6VhMoI0lv27mzptIPny21eod4yLhZnrQHzEZfeKKdWzw_dXPoVUmaIJ5WtT_nCYndvk6ecNisXTtRXY9mVM7Yx3eTRden5ejxJVHEw2PERBKQ10LmbVTKfliPgnzNvUlHLaSwiVLkjQnijIscvdmHHxYdkxWJ/s1280/percent4.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"666\" data-original-width=\"1280\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkrY1Cl8fh7lL04rj0SgMxiiaV8xXoTDk6VhMoI0lv27mzptIPny21eod4yLhZnrQHzEZfeKKdWzw_dXPoVUmaIJ5WtT_nCYndvk6ecNisXTtRXY9mVM7Yx3eTRden5ejxJVHEw2PERBKQ10LmbVTKfliPgnzNvUlHLaSwiVLkjQnijIscvdmHHxYdkxWJ/s520/percent4.png\" width=\"520\" /></a></div><div><br /></div><div>그냥 <a href=\"https://docs.splunk.com/Documentation/Splunk/9.3.0/SearchReference/Top\" target=\"_blank\">top</a> 쓰자.</div><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN-n-UihLnfQ5CN9jamjwW55gQsHGen9R34TVjvnRmApkAp-FoKQPybxnUTfCRAph0GwlCC2Wx-Q_uwJ6mYVSDf211B6_FX0_7vt4_oawkkIHsL_ea9iXkKVuNVGOrLSsrr_mowxmwCU4e4nwdCHq4WwssJTWOahGXe0y4Fi8VEZ-q0MNJkzRoo9Q_6_I-/s1280/percent5.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"554\" data-original-width=\"1280\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN-n-UihLnfQ5CN9jamjwW55gQsHGen9R34TVjvnRmApkAp-FoKQPybxnUTfCRAph0GwlCC2Wx-Q_uwJ6mYVSDf211B6_FX0_7vt4_oawkkIHsL_ea9iXkKVuNVGOrLSsrr_mowxmwCU4e4nwdCHq4WwssJTWOahGXe0y4Fi8VEZ-q0MNJkzRoo9Q_6_I-/s520/percent5.png\" width=\"520\" /></a></div><div><br /></div></div><div><div><b>관련 글</b></div><div><ul><li><a href=\"https://kangmyounghun.blogspot.com/2021/08/splunk-eval-rex.html\">Splunk의 eval과 rex</a></li></ul></div></div>",
        "contentSnippet": "프로세스 단위별 합산 구하고,\n\n\n\n\n\n총합 필드 추가 후,\n\n\n\n비율 계산.\n\n\n\n\n\n아니면 이벤트 단위로 총합, 단위별 합산 추가 후 계산.\n\n\n\n\n\n\n그냥 top 쓰자.\n\n\n\n\n\n관련 글\n\nSplunk의 eval과 rex",
        "id": "tag:blogger.com,1999:blog-2597780270996323853.post-2918153940603938022",
        "isoDate": "2024-08-22T08:58:00.001Z"
      }
    ]
  },
  {
    "name": "김민장",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김덕홍",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성중",
    "category": "개인",
    "posts": []
  },
  {
    "name": "구교준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김덕기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "고명환",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강성희",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강성훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강병수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김봉현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강형석",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수로",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강미경",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강진우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권민재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권태관",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김도곤",
    "category": "개인",
    "posts": []
  },
  {
    "name": "칡토스의 게임 개발",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김선철",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김놀부",
    "category": "개인",
    "posts": []
  },
  {
    "name": "프리웨어 이야기",
    "category": "개인",
    "posts": [
      {
        "creator": "어떤오후의 프리웨어 이야기",
        "title": "윈도우용 추천 프리웨어 (2024.8.26) 모니터밝기조절, 파일관리, Linux에뮬레이션, CAD뷰어, 데이터시각화, 프로젝트관리",
        "link": "http://muzbox.tistory.com/483462",
        "pubDate": "Mon, 26 Aug 2024 08:37:38 +0900",
        "author": "어떤오후의 프리웨어 이야기",
        "comments": "http://muzbox.tistory.com/483462#entry483462comment",
        "content": "<p style=\"text-align: left;\" data-ke-size=\"size18\"><span style=\"background-color: #ffffff; color: #0d0d0d; text-align: start;\">&nbsp;네이버 소프트웨어와 같은 프로그램 소개 사이트가 종료된 후, 윈도우 운영체제를 사용하는 이용자들을 위해 공개 프리웨어 및 오픈소스 프로그램을 소개합니다. 유용한 무료 소프트웨어를 찾고자 하는 사용자들에게 정기적으로 알찬 정보를 제공합니다.</span></p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"freeware.png\" data-origin-width=\"500\" data-origin-height=\"500\"><span data-url=\"https://blog.kakaocdn.net/dn/l9JVa/btsJe7uOawY/WatS3qwKv992v68b9uBeE0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/l9JVa/btsJe7uOawY/WatS3qwKv992v68b9uBeE0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/l9JVa/btsJe7uOawY/WatS3qwKv992v68b9uBeE0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fl9JVa%2FbtsJe7uOawY%2FWatS3qwKv992v68b9uBeE0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"윈도우용 추천 프리웨어 (2024.8.26)\" data-filename=\"freeware.png\" data-origin-width=\"500\" data-origin-height=\"500\"/></span></figure>\n</p>\n<p style=\"text-align: left;\" data-ke-size=\"size18\">&nbsp;</p>\n<p style=\"text-align: left;\" data-ke-size=\"size18\"><span style=\"color: #333333; text-align: left;\">&nbsp;윈도우용 응용프로그램 (Application)은 수없이 많은 종류가 많은 개발자들에 의해 하루에도 수백,수천개가 새로 출시되고 그보다 더 많은 수의 프로그램들이 업데이트 됩니다. 이들 응용프로그램 (Application)은 비율을 지불해야하는<span>&nbsp;</span></span><b><span style=\"color: #009a87;\">상용프로그램</span></b><span style=\"color: #333333; text-align: left;\">과 정품 구매를 확대하기 위해 공급하는 일종의 샘플 개념의<span>&nbsp;</span></span><span style=\"color: #ee2323;\"><b>쉐어웨어</b></span><span style=\"color: #333333; text-align: left;\">, 무료로 사용할 수 있는<span>&nbsp;</span></span><b><span style=\"color: #ef6f53;\">프리웨어</span></b>등으로 크게 3가지로 나뉘게 되는데요.</p>\n<p style=\"text-align: left;\" data-ke-size=\"size18\"><br />&nbsp;물론 프리웨어에도 개인만 사용할 있다던가, 기업이나 관공서에서도 사용이 가능하다던가, 소스까지 같이 공개하여 맘대로 수정과 배포가 가능한 완전 무료등의 추가 분류가 필요합니다. 하지만, 개발자가 공개하는 무료배포의 의미가 정확하지 않는 프로그램도 많고, 저작권의 정의도 각양각색이라 본 블로그에서 소개하는 프리웨어도 <span style=\"color: #006dd7;\"><b>최대한 확인이 가능한 범위에서 개인 또는 기업에서 사용가능한지를 구분하여 소개</b></span>하고 있습니다.</p>\n<p style=\"text-align: left;\" data-ke-size=\"size18\">&nbsp;</p>\n<p style=\"text-align: left;\" data-ke-size=\"size18\">&nbsp;</p>\n<p style=\"text-align: center;\" data-ke-size=\"size18\">'어떤오후의 프리웨어 이야기'에서 추천하는<br /><span style=\"color: #409d00;\">&nbsp;<b>2024년 8월 26일자 공개자료실 윈도우용 추천 프리웨어</b></span>입니다.</p>\n<p id=\"no_1\" data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><i><b>1. Bright Master (트레이 아이콘에서 휠 회전으로 모니터 밝기 조정)<br /></b></i></span></h2>\n<p data-ke-size=\"size18\">&nbsp; 설치가&nbsp;필요&nbsp;없는&nbsp;모니터&nbsp;밝기&nbsp;조절&nbsp;도구입니다. <br />DDC/CI를&nbsp;지원하는&nbsp;외부&nbsp;모니터와&nbsp;WMI&nbsp;서비스가&nbsp;비활성화되지&nbsp;않은&nbsp;노트북,&nbsp;올인원&nbsp;PC,&nbsp;윈도우&nbsp;태블릿의&nbsp;내장&nbsp;디스플레이에서&nbsp;작동합니다. <br />주요&nbsp;기능으로는&nbsp;모든&nbsp;기기의&nbsp;동기화된&nbsp;밝기&nbsp;변경,&nbsp;제3자&nbsp;소프트웨어에&nbsp;의한&nbsp;밝기&nbsp;변경&nbsp;추적,&nbsp;디지털&nbsp;밝기&nbsp;표시,&nbsp;자동&nbsp;시작,&nbsp;마우스&nbsp;휠을&nbsp;이용한&nbsp;밝기&nbsp;조절,&nbsp;밝기&nbsp;조절&nbsp;단계&nbsp;선택,&nbsp;한&nbsp;번의&nbsp;클릭으로&nbsp;최소&nbsp;밝기&nbsp;설정&nbsp;등이&nbsp;있습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Bright Master.png\" data-origin-width=\"504\" data-origin-height=\"304\"><span data-url=\"https://blog.kakaocdn.net/dn/CCuKC/btsJfiXcAQ5/hkfwcbBPBrbsN3fro1pF70/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/CCuKC/btsJfiXcAQ5/hkfwcbBPBrbsN3fro1pF70/img.png\"><img src=\"https://blog.kakaocdn.net/dn/CCuKC/btsJfiXcAQ5/hkfwcbBPBrbsN3fro1pF70/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FCCuKC%2FbtsJfiXcAQ5%2FhkfwcbBPBrbsN3fro1pF70%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"Bright Master.png\" data-origin-width=\"504\" data-origin-height=\"304\"/></span></figure>\n</p>\n<p data-ke-size=\"size18\">▶프리웨어 - 개인</p>\n<p data-ke-size=\"size18\">▶ Windows 10/11</p>\n<p data-ke-size=\"size18\">▶무료 다운로드◀</p>\n<figure id=\"og_1724628763665\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"Bright Master / Screen dimmer\" data-og-description=\"Ru Bright Master Simple screen dimmer The program is designed to adjust the brightness of monitors, laptop displays and other equipment. Runs on Windows 7+ and does not require installation. Works with all external monitors that have DDC/CI support, as wel\" data-og-host=\"brightmaster.ru\" data-og-source-url=\"https://brightmaster.ru/en\" data-og-url=\"https://brightmaster.ru/en\" data-og-image=\"\"><a href=\"https://brightmaster.ru/en\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://brightmaster.ru/en\">\n<div class=\"og-image\" style=\"background-image: url();\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">Bright Master / Screen dimmer</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">Ru Bright Master Simple screen dimmer The program is designed to adjust the brightness of monitors, laptop displays and other equipment. Runs on Windows 7+ and does not require installation. Works with all external monitors that have DDC/CI support, as wel</p>\n<p class=\"og-host\" data-ke-size=\"size16\">brightmaster.ru</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p id=\"no_2\" data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><i><b>2. Cygwin (Linux 기능을 사용할 수 있는 유연한 에뮬레이션 도구)</b></i></span></h2>\n<p data-ke-size=\"size18\">&nbsp; Windows&nbsp;사용자가&nbsp;Linux의&nbsp;기능을&nbsp;경험할&nbsp;수&nbsp;있게&nbsp;해주는&nbsp;에뮬레이션&nbsp;레이어입니다. <br />사용자는&nbsp;원하는&nbsp;Linux&nbsp;기능을&nbsp;선택하여&nbsp;설치할&nbsp;수&nbsp;있으며,&nbsp;프로그램은&nbsp;사용자&nbsp;친화적이고&nbsp;잘&nbsp;조직된&nbsp;구조를&nbsp;가지고&nbsp;있습니다.&nbsp;오디오&nbsp;드라이버,&nbsp;그래픽&nbsp;소프트웨어,&nbsp;LXDE,&nbsp;GNOME,&nbsp;KDE&nbsp;등&nbsp;다양한&nbsp;Linux&nbsp;기능을&nbsp;제공합니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Cygwin.png\" data-origin-width=\"1083\" data-origin-height=\"668\"><span data-url=\"https://blog.kakaocdn.net/dn/b2AN3z/btsJfGXIURN/NeV5FdKmPcffTyNmWppCfk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/b2AN3z/btsJfGXIURN/NeV5FdKmPcffTyNmWppCfk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/b2AN3z/btsJfGXIURN/NeV5FdKmPcffTyNmWppCfk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb2AN3z%2FbtsJfGXIURN%2FNeV5FdKmPcffTyNmWppCfk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"Cygwin.png\" data-origin-width=\"1083\" data-origin-height=\"668\"/></span></figure>\n</p>\n<p data-ke-size=\"size18\">▶프리웨어 - 개인/기업(오픈소스)</p>\n<p data-ke-size=\"size18\">▶Windows 10/11</p>\n<p data-ke-size=\"size18\">▶무료 다운로드 ◀</p>\n<figure id=\"og_1724628796923\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"Cygwin\" data-og-description=\"Cygwin Get that Linux feeling - on Windows This is the home of the Cygwin project What... ...is it? Cygwin is: a large collection of GNU and Open Source tools which provide functionality similar to a Linux distribution on Windows. a DLL (cygwin1.dll) which\" data-og-host=\"www.cygwin.com\" data-og-source-url=\"https://www.cygwin.com/\" data-og-url=\"https://www.cygwin.com/\" data-og-image=\"\"><a href=\"https://www.cygwin.com/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://www.cygwin.com/\">\n<div class=\"og-image\" style=\"background-image: url();\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">Cygwin</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">Cygwin Get that Linux feeling - on Windows This is the home of the Cygwin project What... ...is it? Cygwin is: a large collection of GNU and Open Source tools which provide functionality similar to a Linux distribution on Windows. a DLL (cygwin1.dll) which</p>\n<p class=\"og-host\" data-ke-size=\"size16\">www.cygwin.com</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p id=\"no_3\" data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><i><b>3. Rename Master (다기능적인&nbsp;파일&nbsp;이름&nbsp;변경&nbsp;솔루션)</b></i></span></h2>\n<p data-ke-size=\"size18\">&nbsp;사용자가&nbsp;다양한&nbsp;도구를&nbsp;통해&nbsp;파일&nbsp;이름을&nbsp;효율적이고&nbsp;접근하기&nbsp;쉽게&nbsp;변경할&nbsp;수&nbsp;있게&nbsp;해주는&nbsp;프로그램입니다.&nbsp;주요&nbsp;기능은&nbsp;'Renaming&nbsp;Script'&nbsp;탭에서&nbsp;찾을&nbsp;수&nbsp;있으며,&nbsp;단어/숫자&nbsp;제거,&nbsp;표현&nbsp;변경,&nbsp;기존&nbsp;이름&nbsp;교체,&nbsp;카운터&nbsp;추가&nbsp;등&nbsp;다양한&nbsp;옵션을&nbsp;제공합니다. <br />프로그램은&nbsp;일반&nbsp;사용자를&nbsp;위한&nbsp;기본&nbsp;기능뿐만&nbsp;아니라&nbsp;고급&nbsp;사용자를&nbsp;위한&nbsp;'Textfile&nbsp;Wizard'&nbsp;같은&nbsp;기능도&nbsp;제공합니다.&nbsp;전반적으로&nbsp;Rename&nbsp;Master는&nbsp;사용하기&nbsp;쉬우면서도&nbsp;고급&nbsp;사용자를&nbsp;위한&nbsp;충분한&nbsp;구성&nbsp;옵션을&nbsp;제공하는&nbsp;신뢰할&nbsp;수&nbsp;있는&nbsp;종합&nbsp;솔루션입니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Rename Master.png\" data-origin-width=\"1030\" data-origin-height=\"647\"><span data-url=\"https://blog.kakaocdn.net/dn/vUqbY/btsJfElmKdd/sEbM4woMxpjLG6fgE8TpTk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/vUqbY/btsJfElmKdd/sEbM4woMxpjLG6fgE8TpTk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/vUqbY/btsJfElmKdd/sEbM4woMxpjLG6fgE8TpTk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FvUqbY%2FbtsJfElmKdd%2FsEbM4woMxpjLG6fgE8TpTk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"Rename Master.png\" data-origin-width=\"1030\" data-origin-height=\"647\"/></span></figure>\n</p>\n<p data-ke-size=\"size18\">▶프리웨어 - 개인</p>\n<p data-ke-size=\"size18\">▶Windows 10/11&nbsp;</p>\n<p data-ke-size=\"size18\">▶무료 다운로드 ◀</p>\n<figure id=\"og_1724628842166\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"JoeJoe's freeware utilities - [Rename Master]\" data-og-description=\"Rename Master is a freeware utility designed to rename multiple files with a just few clicks. Anyone that has worked with websites, file archives, or collections of music, videos, or pictures has probably spent way too much time renaming hundreds of files.\" data-og-host=\"www.joejoesoft.com\" data-og-source-url=\"https://www.joejoesoft.com/vcms/108/\" data-og-url=\"https://www.joejoesoft.com/vcms/108/\" data-og-image=\"\"><a href=\"https://www.joejoesoft.com/vcms/108/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://www.joejoesoft.com/vcms/108/\">\n<div class=\"og-image\" style=\"background-image: url();\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">JoeJoe's freeware utilities - [Rename Master]</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">Rename Master is a freeware utility designed to rename multiple files with a just few clicks. Anyone that has worked with websites, file archives, or collections of music, videos, or pictures has probably spent way too much time renaming hundreds of files.</p>\n<p class=\"og-host\" data-ke-size=\"size16\">www.joejoesoft.com</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p id=\"no_4\" data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><i><b>4. VariCAD Viewer (다기능&nbsp;CAD&nbsp;파일&nbsp;뷰어)<br /></b></i></span></h2>\n<p data-ke-size=\"size18\">&nbsp;단순히&nbsp;CAD&nbsp;도면을&nbsp;보는&nbsp;것&nbsp;이상의&nbsp;기능을&nbsp;제공합니다.&nbsp;2D&nbsp;DWG,&nbsp;3D&nbsp;STEP,&nbsp;2D/3D&nbsp;VariCAD&nbsp;등&nbsp;다양한&nbsp;형식의&nbsp;파일을&nbsp;지원하며,&nbsp;간단한&nbsp;조정과&nbsp;인쇄&nbsp;기능도&nbsp;갖추고&nbsp;있습니다.&nbsp;이&nbsp;프로그램은&nbsp;3D&nbsp;모델링에&nbsp;대해&nbsp;잘&nbsp;아는&nbsp;전문가들을&nbsp;위해&nbsp;설계되었습니다.</p>\n<p data-ke-size=\"size18\">&nbsp;인터페이스는 현대적이지만 다양한 기능으로 인해 다소 복잡해 보일 수 있습니다. 그러나 CAD 경험이 있는 사용자라면 직관적으로 사용할 수 있을 것입니다. 도구 모음의 배치를 사용자가 원하는 대로 조정할 수 있는 것도 장점입니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"VariCAD Viewer.png\" data-origin-width=\"1360\" data-origin-height=\"757\"><span data-url=\"https://blog.kakaocdn.net/dn/Zxvkp/btsJeQ7R4Ia/5zkpqgQItCCEUbKkidGFhk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/Zxvkp/btsJeQ7R4Ia/5zkpqgQItCCEUbKkidGFhk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/Zxvkp/btsJeQ7R4Ia/5zkpqgQItCCEUbKkidGFhk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FZxvkp%2FbtsJeQ7R4Ia%2F5zkpqgQItCCEUbKkidGFhk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"VariCAD Viewer.png\" data-origin-width=\"1360\" data-origin-height=\"757\"/></span></figure>\n</p>\n<p data-ke-size=\"size18\">▶프리웨어 - 개인</p>\n<p data-ke-size=\"size18\">▶Windows 10/11</p>\n<p data-ke-size=\"size18\">▶무료 다운로드 ◀</p>\n<figure id=\"og_1724628874152\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"Products | VariCAD - 3D/2D CAD system\" data-og-description=\"Products VariCAD - 3D / 2D mechanical CAD software We have a simple strategy - to sell VariCAD, with all its features, for one very affordable price. VariCAD is the best value you will find anywhere for this type of modeling and designing software. VariCAD\" data-og-host=\"www.varicad.com\" data-og-source-url=\"https://www.varicad.com/en/home/products/products/\" data-og-url=\"https://www.varicad.com/en/home/products/products/\" data-og-image=\"\"><a href=\"https://www.varicad.com/en/home/products/products/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://www.varicad.com/en/home/products/products/\">\n<div class=\"og-image\" style=\"background-image: url();\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">Products | VariCAD - 3D/2D CAD system</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">Products VariCAD - 3D / 2D mechanical CAD software We have a simple strategy - to sell VariCAD, with all its features, for one very affordable price. VariCAD is the best value you will find anywhere for this type of modeling and designing software. VariCAD</p>\n<p class=\"og-host\" data-ke-size=\"size16\">www.varicad.com</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p id=\"no_5\" data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><i><b>5. TreeSheets (다목적 데이터 조직화 및 시각화 도구)</b></i></span></h2>\n<p data-ke-size=\"size18\">&nbsp; 스프레드시트,&nbsp;마인드맵,&nbsp;노트&nbsp;애플리케이션의&nbsp;기능을&nbsp;하나의&nbsp;사용하기&nbsp;쉬운&nbsp;인터페이스로&nbsp;결합한&nbsp;프로그램입니다.&nbsp;복잡한&nbsp;데이터&nbsp;구조를&nbsp;쉽게&nbsp;다룰&nbsp;수&nbsp;있는&nbsp;것이&nbsp;주요&nbsp;특징입니다.</p>\n<p data-ke-size=\"size18\"><br />이&nbsp;프로그램은&nbsp;프로젝트&nbsp;관리,&nbsp;일정&nbsp;계획,&nbsp;브레인스토밍,&nbsp;할&nbsp;일&nbsp;목록&nbsp;작성&nbsp;등&nbsp;다양한&nbsp;용도로&nbsp;사용할&nbsp;수&nbsp;있습니다.&nbsp;사용자는&nbsp;데이터를&nbsp;계층적&nbsp;트리,&nbsp;스프레드시트&nbsp;그리드,&nbsp;마인드맵&nbsp;등&nbsp;다양한&nbsp;형태로&nbsp;시각화할&nbsp;수&nbsp;있습니다. <br />또한&nbsp;색상과&nbsp;테두리&nbsp;등을&nbsp;사용자&nbsp;지정하여&nbsp;시트의&nbsp;외관을&nbsp;개선할&nbsp;수&nbsp;있으며,&nbsp;작업을&nbsp;우선순위화하고&nbsp;마감일을&nbsp;설정하는&nbsp;등의&nbsp;기능을&nbsp;통해&nbsp;생산성을&nbsp;높일&nbsp;수&nbsp;있습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"TreeSheets.png\" data-origin-width=\"1149\" data-origin-height=\"761\"><span data-url=\"https://blog.kakaocdn.net/dn/bareS8/btsJelm7E7B/nMJiF5wlkyZskOHMQ57w20/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bareS8/btsJelm7E7B/nMJiF5wlkyZskOHMQ57w20/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bareS8/btsJelm7E7B/nMJiF5wlkyZskOHMQ57w20/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbareS8%2FbtsJelm7E7B%2FnMJiF5wlkyZskOHMQ57w20%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"TreeSheets.png\" data-origin-width=\"1149\" data-origin-height=\"761\"/></span></figure>\n</p>\n<p data-ke-size=\"size18\">▶프리웨어 - 개인/기업 (오픈소스)</p>\n<p data-ke-size=\"size18\">▶Windows 10/11</p>\n<p data-ke-size=\"size18\">▶무료 다운로드 ◀</p>\n<figure id=\"og_1724628900387\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"TreeSheets\" data-og-description=\"Open Source Free Form Data Organizer (Hierarchical Spreadsheet) D O W N L O A D : Download LATEST from github releases: Windows / OS X / Linux Click HERE, pick your platform from the latest release. &nbsp; A &quot;hierarchical spreadsheet&quot; that is a great replaceme\" data-og-host=\"strlen.com\" data-og-source-url=\"https://strlen.com/treesheets/\" data-og-url=\"https://strlen.com/treesheets/\" data-og-image=\"https://scrap.kakaocdn.net/dn/bndYPd/hyWSfzKILl/YLEhMi5yuQXXxgdNMPthdk/img.png?width=607&amp;height=498&amp;face=0_0_607_498\"><a href=\"https://strlen.com/treesheets/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://strlen.com/treesheets/\">\n<div class=\"og-image\" style=\"background-image: url('https://scrap.kakaocdn.net/dn/bndYPd/hyWSfzKILl/YLEhMi5yuQXXxgdNMPthdk/img.png?width=607&amp;height=498&amp;face=0_0_607_498');\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">TreeSheets</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">Open Source Free Form Data Organizer (Hierarchical Spreadsheet) D O W N L O A D : Download LATEST from github releases: Windows / OS X / Linux Click HERE, pick your platform from the latest release. &nbsp; A \"hierarchical spreadsheet\" that is a great replaceme</p>\n<p class=\"og-host\" data-ke-size=\"size16\">strlen.com</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "네이버 소프트웨어와 같은 프로그램 소개 사이트가 종료된 후, 윈도우 운영체제를 사용하는 이용자들을 위해 공개 프리웨어 및 오픈소스 프로그램을 소개합니다. 유용한 무료 소프트웨어를 찾고자 하는 사용자들에게 정기적으로 알찬 정보를 제공합니다.\n\n\n \n 윈도우용 응용프로그램 (Application)은 수없이 많은 종류가 많은 개발자들에 의해 하루에도 수백,수천개가 새로 출시되고 그보다 더 많은 수의 프로그램들이 업데이트 됩니다. 이들 응용프로그램 (Application)은 비율을 지불해야하는 상용프로그램과 정품 구매를 확대하기 위해 공급하는 일종의 샘플 개념의 쉐어웨어, 무료로 사용할 수 있는 프리웨어등으로 크게 3가지로 나뉘게 되는데요.\n 물론 프리웨어에도 개인만 사용할 있다던가, 기업이나 관공서에서도 사용이 가능하다던가, 소스까지 같이 공개하여 맘대로 수정과 배포가 가능한 완전 무료등의 추가 분류가 필요합니다. 하지만, 개발자가 공개하는 무료배포의 의미가 정확하지 않는 프로그램도 많고, 저작권의 정의도 각양각색이라 본 블로그에서 소개하는 프리웨어도 최대한 확인이 가능한 범위에서 개인 또는 기업에서 사용가능한지를 구분하여 소개하고 있습니다.\n \n \n'어떤오후의 프리웨어 이야기'에서 추천하는\n 2024년 8월 26일자 공개자료실 윈도우용 추천 프리웨어입니다.\n \n \n1. Bright Master (트레이 아이콘에서 휠 회전으로 모니터 밝기 조정)\n\n  설치가 필요 없는 모니터 밝기 조절 도구입니다. \nDDC/CI를 지원하는 외부 모니터와 WMI 서비스가 비활성화되지 않은 노트북, 올인원 PC, 윈도우 태블릿의 내장 디스플레이에서 작동합니다. \n주요 기능으로는 모든 기기의 동기화된 밝기 변경, 제3자 소프트웨어에 의한 밝기 변경 추적, 디지털 밝기 표시, 자동 시작, 마우스 휠을 이용한 밝기 조절, 밝기 조절 단계 선택, 한 번의 클릭으로 최소 밝기 설정 등이 있습니다.\n\n\n▶프리웨어 - 개인\n▶ Windows 10/11\n▶무료 다운로드◀\n\n \nBright Master / Screen dimmer\nRu Bright Master Simple screen dimmer The program is designed to adjust the brightness of monitors, laptop displays and other equipment. Runs on Windows 7+ and does not require installation. Works with all external monitors that have DDC/CI support, as wel\nbrightmaster.ru\n\n \n \n \n \n \n \n2. Cygwin (Linux 기능을 사용할 수 있는 유연한 에뮬레이션 도구)\n  Windows 사용자가 Linux의 기능을 경험할 수 있게 해주는 에뮬레이션 레이어입니다. \n사용자는 원하는 Linux 기능을 선택하여 설치할 수 있으며, 프로그램은 사용자 친화적이고 잘 조직된 구조를 가지고 있습니다. 오디오 드라이버, 그래픽 소프트웨어, LXDE, GNOME, KDE 등 다양한 Linux 기능을 제공합니다.\n\n\n▶프리웨어 - 개인/기업(오픈소스)\n▶Windows 10/11\n▶무료 다운로드 ◀\n\n \nCygwin\nCygwin Get that Linux feeling - on Windows This is the home of the Cygwin project What... ...is it? Cygwin is: a large collection of GNU and Open Source tools which provide functionality similar to a Linux distribution on Windows. a DLL (cygwin1.dll) which\nwww.cygwin.com\n\n \n \n \n \n \n \n3. Rename Master (다기능적인 파일 이름 변경 솔루션)\n 사용자가 다양한 도구를 통해 파일 이름을 효율적이고 접근하기 쉽게 변경할 수 있게 해주는 프로그램입니다. 주요 기능은 'Renaming Script' 탭에서 찾을 수 있으며, 단어/숫자 제거, 표현 변경, 기존 이름 교체, 카운터 추가 등 다양한 옵션을 제공합니다. \n프로그램은 일반 사용자를 위한 기본 기능뿐만 아니라 고급 사용자를 위한 'Textfile Wizard' 같은 기능도 제공합니다. 전반적으로 Rename Master는 사용하기 쉬우면서도 고급 사용자를 위한 충분한 구성 옵션을 제공하는 신뢰할 수 있는 종합 솔루션입니다.\n\n\n▶프리웨어 - 개인\n▶Windows 10/11 \n▶무료 다운로드 ◀\n\n \nJoeJoe's freeware utilities - [Rename Master]\nRename Master is a freeware utility designed to rename multiple files with a just few clicks. Anyone that has worked with websites, file archives, or collections of music, videos, or pictures has probably spent way too much time renaming hundreds of files.\nwww.joejoesoft.com\n\n \n \n \n \n \n \n4. VariCAD Viewer (다기능 CAD 파일 뷰어)\n\n 단순히 CAD 도면을 보는 것 이상의 기능을 제공합니다. 2D DWG, 3D STEP, 2D/3D VariCAD 등 다양한 형식의 파일을 지원하며, 간단한 조정과 인쇄 기능도 갖추고 있습니다. 이 프로그램은 3D 모델링에 대해 잘 아는 전문가들을 위해 설계되었습니다.\n 인터페이스는 현대적이지만 다양한 기능으로 인해 다소 복잡해 보일 수 있습니다. 그러나 CAD 경험이 있는 사용자라면 직관적으로 사용할 수 있을 것입니다. 도구 모음의 배치를 사용자가 원하는 대로 조정할 수 있는 것도 장점입니다.\n\n\n▶프리웨어 - 개인\n▶Windows 10/11\n▶무료 다운로드 ◀\n\n \nProducts | VariCAD - 3D/2D CAD system\nProducts VariCAD - 3D / 2D mechanical CAD software We have a simple strategy - to sell VariCAD, with all its features, for one very affordable price. VariCAD is the best value you will find anywhere for this type of modeling and designing software. VariCAD\nwww.varicad.com\n\n \n \n \n \n \n \n5. TreeSheets (다목적 데이터 조직화 및 시각화 도구)\n  스프레드시트, 마인드맵, 노트 애플리케이션의 기능을 하나의 사용하기 쉬운 인터페이스로 결합한 프로그램입니다. 복잡한 데이터 구조를 쉽게 다룰 수 있는 것이 주요 특징입니다.\n이 프로그램은 프로젝트 관리, 일정 계획, 브레인스토밍, 할 일 목록 작성 등 다양한 용도로 사용할 수 있습니다. 사용자는 데이터를 계층적 트리, 스프레드시트 그리드, 마인드맵 등 다양한 형태로 시각화할 수 있습니다. \n또한 색상과 테두리 등을 사용자 지정하여 시트의 외관을 개선할 수 있으며, 작업을 우선순위화하고 마감일을 설정하는 등의 기능을 통해 생산성을 높일 수 있습니다.\n\n\n▶프리웨어 - 개인/기업 (오픈소스)\n▶Windows 10/11\n▶무료 다운로드 ◀\n\n \nTreeSheets\nOpen Source Free Form Data Organizer (Hierarchical Spreadsheet) D O W N L O A D : Download LATEST from github releases: Windows / OS X / Linux Click HERE, pick your platform from the latest release.   A \"hierarchical spreadsheet\" that is a great replaceme\nstrlen.com",
        "guid": "http://muzbox.tistory.com/483462",
        "categories": [
          "NEWS/프리웨어 뉴스",
          "cad뷰어",
          "linux에뮬레이션",
          "기업무료프로그램",
          "데이터시각화",
          "모니터밝기조절",
          "무료프로그램",
          "추천프로그램",
          "파일관리",
          "프로젝트관리",
          "프리웨어"
        ],
        "isoDate": "2024-08-25T23:37:38.000Z"
      },
      {
        "creator": "어떤오후의 프리웨어 이야기",
        "title": "스마트폰으로 DSLR 부럽지 않은 사진 찍는 4가지 방법",
        "link": "http://muzbox.tistory.com/483461",
        "pubDate": "Fri, 23 Aug 2024 17:50:43 +0900",
        "author": "어떤오후의 프리웨어 이야기",
        "comments": "http://muzbox.tistory.com/483461#entry483461comment",
        "content": "<p data-ke-size=\"size16\">스마트폰으로 DSLR 못지않은 사진을 찍고 싶으신가요? 이 글에서 소개하는 간단한 비법들을 통해, DSLR 없이도 스마트폰만으로 멋진 사진을 촬영하는 방법을 알아보세요.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"0000.png\" data-origin-width=\"1234\" data-origin-height=\"694\"><span data-url=\"https://blog.kakaocdn.net/dn/dgYX4o/btsJcJnZHOm/sHaXaT6CAXv3XpQ5iSCWG0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/dgYX4o/btsJcJnZHOm/sHaXaT6CAXv3XpQ5iSCWG0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/dgYX4o/btsJcJnZHOm/sHaXaT6CAXv3XpQ5iSCWG0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdgYX4o%2FbtsJcJnZHOm%2FsHaXaT6CAXv3XpQ5iSCWG0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"스마트폰으로 DSLR 부럽지 않은 사진 찍는 4가지 방법\" data-filename=\"0000.png\" data-origin-width=\"1234\" data-origin-height=\"694\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">스마트폰이 우리 일상에서 없어서는 안 될 필수 아이템이 된 지도 오래입니다. 특히, 최근 몇 년간 스마트폰 카메라 기술이 비약적으로 발전하면서, 이제는 DSLR 없이도 훌륭한 사진을 촬영할 수 있게 되었습니다. 하지만 좋은 장비만으로는 뛰어난 사진을 찍을 수 없다는 사실, 알고 계셨나요? 이번 글에서는 별도의 DSLR 없이도 스마트폰으로 멋진 사진을 찍을 수 있는 몇 가지 비법을 소개하고자 합니다. 이제 언제 어디서나 스마트폰을 꺼내들어 전문가처럼 사진을 찍어보세요.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><b>1. 카메라 설정 최적화하기</b></span></h2>\n<p data-ke-size=\"size16\">스마트폰의 기본 카메라 앱은 많은 설정을 자동으로 처리해줍니다. 하지만 설정을 조금만 손봐도 사진의 퀄리티를 크게 향상시킬 수 있습니다. 예를 들어, HDR(High Dynamic Range) 기능을 활성화하면 밝고 어두운 영역이 고르게 노출되어 더 자연스럽고 선명한 사진을 얻을 수 있습니다. 또한, 촬영 전에 초점을 수동으로 맞추고, 노출을 조정하는 것 역시 중요합니다. 빛이 너무 많거나 적으면 사진이 흐릿해지거나, 디테일이 사라질 수 있습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"a.png\" data-origin-width=\"1234\" data-origin-height=\"694\"><span data-url=\"https://blog.kakaocdn.net/dn/eXksWk/btsJcQHmAMR/qlHktJNK87wm2pUXyqECgk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/eXksWk/btsJcQHmAMR/qlHktJNK87wm2pUXyqECgk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/eXksWk/btsJcQHmAMR/qlHktJNK87wm2pUXyqECgk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FeXksWk%2FbtsJcQHmAMR%2FqlHktJNK87wm2pUXyqECgk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"카메라 설정 최적화하기\" data-filename=\"a.png\" data-origin-width=\"1234\" data-origin-height=\"694\"/></span></figure>\n</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>HDR 기능 사용:</b> 강한 대비가 있는 장면에서 더 디테일한 사진을 촬영할 수 있습니다.</li>\n<li><b>초점 수동 조절:</b> 사진의 주제가 명확하게 보이도록 초점을 맞추세요.</li>\n<li><b>노출 조정:</b> 밝기를 조절하여 디테일을 더 살릴 수 있습니다.</li>\n</ul>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><b>2. 자연광을 활용하라</b></span></h2>\n<p data-ke-size=\"size16\">빛은 사진의 품질을 좌우하는 중요한 요소입니다. 스마트폰 카메라 센서의 크기는 DSLR보다 작기 때문에, 충분한 조명이 없다면 사진이 흐릿하게 나오기 쉽습니다. 자연광을 잘 활용하면 이런 문제를 해결할 수 있습니다. 예를 들어, 창가에서 들어오는 자연광을 활용하거나, 해가 질 무렵의 부드러운 빛을 이용해 촬영해 보세요. 실내 촬영 시에는 인공 조명보다는 자연광을 최대한 활용하는 것이 좋습니다. 단, 역광을 피하고 피사체가 빛을 충분히 받도록 각도를 조절해야 합니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"b.png\" data-origin-width=\"1234\" data-origin-height=\"694\"><span data-url=\"https://blog.kakaocdn.net/dn/ba9KfT/btsJdmThLeR/r6rk4BWLkFmtlBfZVDOu60/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/ba9KfT/btsJdmThLeR/r6rk4BWLkFmtlBfZVDOu60/img.png\"><img src=\"https://blog.kakaocdn.net/dn/ba9KfT/btsJdmThLeR/r6rk4BWLkFmtlBfZVDOu60/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fba9KfT%2FbtsJdmThLeR%2Fr6rk4BWLkFmtlBfZVDOu60%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"자연광을 활용하라\" data-filename=\"b.png\" data-origin-width=\"1234\" data-origin-height=\"694\"/></span></figure>\n</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>황금 시간대 촬영:</b> 일출이나 일몰 시의 부드러운 빛을 활용해보세요.</li>\n<li><b>역광 피하기:</b> 피사체가 어두워지지 않도록 주의하세요.</li>\n<li><b>반사된 빛 활용:</b> 직사광선이 강할 때는 반사된 빛을 이용하세요.</li>\n</ul>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><b>3. 구도와 각도에 신경 쓰기</b></span></h2>\n<p data-ke-size=\"size16\">좋은 사진은 구도에서 시작된다고 해도 과언이 아닙니다. 촬영할 때 단순히 피사체를 중앙에 두기보다는 3분할 구도를 활용하거나, 다양한 각도에서 찍어보는 것이 좋습니다. 예를 들어, 낮은 각도에서 촬영하면 피사체가 더 돋보이게 되며, 높은 각도에서는 넓은 배경을 효과적으로 담을 수 있습니다. 또한, 수평을 맞추는 것 역시 중요합니다. 스마트폰의 그리드 라인을 활용하면 수평을 쉽게 맞출 수 있습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"c.png\" data-origin-width=\"1234\" data-origin-height=\"694\"><span data-url=\"https://blog.kakaocdn.net/dn/QXn8G/btsJeAvZqmg/3DJuAfVCyf3jb2r1qEqH50/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/QXn8G/btsJeAvZqmg/3DJuAfVCyf3jb2r1qEqH50/img.png\"><img src=\"https://blog.kakaocdn.net/dn/QXn8G/btsJeAvZqmg/3DJuAfVCyf3jb2r1qEqH50/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FQXn8G%2FbtsJeAvZqmg%2F3DJuAfVCyf3jb2r1qEqH50%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"구도와 각도에 신경 쓰기\" data-filename=\"c.png\" data-origin-width=\"1234\" data-origin-height=\"694\"/></span></figure>\n</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>3분할 구도:</b> 사진의 중심을 3등분하여 구성하세요.</li>\n<li><b>다양한 각도 실험:</b> 여러 각도에서 촬영하여 가장 인상적인 장면을 찾아보세요.</li>\n<li><b>수평 맞추기:</b> 그리드 라인을 사용하여 수평을 정확히 맞추세요.</li>\n</ul>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><b>4. 간단한 후반 작업으로 완성도 높이기</b></span></h2>\n<p data-ke-size=\"size16\">사진을 찍고 난 후에도 스마트폰에서 바로 간단한 편집 작업을 통해 완성도를 높일 수 있습니다. 예를 들어, 밝기와 대비를 조정하거나, 색온도를 맞추는 것만으로도 사진의 분위기가 완전히 달라질 수 있습니다. 요즘 스마트폰에는 다양한 편집 앱이 있어서 전문가처럼 색상 보정, 노이즈 제거, 크롭 등을 손쉽게 할 수 있습니다. 하지만 과도한 편집은 오히려 사진의 자연스러움을 해칠 수 있으므로, 적절한 선에서 작업을 마무리하는 것이 좋습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"d.png\" data-origin-width=\"1234\" data-origin-height=\"694\"><span data-url=\"https://blog.kakaocdn.net/dn/bHSy0O/btsJekmEDnB/IqyGjmZRrUxZx8mgErOMqK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bHSy0O/btsJekmEDnB/IqyGjmZRrUxZx8mgErOMqK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bHSy0O/btsJekmEDnB/IqyGjmZRrUxZx8mgErOMqK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbHSy0O%2FbtsJekmEDnB%2FIqyGjmZRrUxZx8mgErOMqK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"d.png\" data-origin-width=\"1234\" data-origin-height=\"694\"/></span></figure>\n</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>밝기 및 대비 조절:</b> 간단한 조정만으로도 사진이 선명해질 수 있습니다.</li>\n<li><b>색온도 맞추기:</b> 사진의 분위기를 원하는 대로 조정하세요.</li>\n<li><b>과한 편집 피하기:</b> 자연스러움을 잃지 않도록 주의하세요.</li>\n</ul>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><b>결론</b></span></h2>\n<p data-ke-size=\"size16\">스마트폰 카메라는 이제 우리 삶의 필수품이 되었으며, DSLR에 뒤지지 않는 성능을 자랑합니다. 하지만 그만큼 사용자의 노력도 중요합니다. 이번에 소개한 간단한 팁들을 활용하면, 언제 어디서나 스마트폰으로 전문가 못지않은 사진을 촬영할 수 있을 것입니다. 중요한 것은 장비가 아닌, 그 장비를 어떻게 활용하느냐입니다. 다음번에 사진을 찍을 때는 이 비법들을 꼭 기억하고 적용해 보세요.</p>",
        "contentSnippet": "스마트폰으로 DSLR 못지않은 사진을 찍고 싶으신가요? 이 글에서 소개하는 간단한 비법들을 통해, DSLR 없이도 스마트폰만으로 멋진 사진을 촬영하는 방법을 알아보세요.\n\n\n \n스마트폰이 우리 일상에서 없어서는 안 될 필수 아이템이 된 지도 오래입니다. 특히, 최근 몇 년간 스마트폰 카메라 기술이 비약적으로 발전하면서, 이제는 DSLR 없이도 훌륭한 사진을 촬영할 수 있게 되었습니다. 하지만 좋은 장비만으로는 뛰어난 사진을 찍을 수 없다는 사실, 알고 계셨나요? 이번 글에서는 별도의 DSLR 없이도 스마트폰으로 멋진 사진을 찍을 수 있는 몇 가지 비법을 소개하고자 합니다. 이제 언제 어디서나 스마트폰을 꺼내들어 전문가처럼 사진을 찍어보세요.\n \n \n1. 카메라 설정 최적화하기\n스마트폰의 기본 카메라 앱은 많은 설정을 자동으로 처리해줍니다. 하지만 설정을 조금만 손봐도 사진의 퀄리티를 크게 향상시킬 수 있습니다. 예를 들어, HDR(High Dynamic Range) 기능을 활성화하면 밝고 어두운 영역이 고르게 노출되어 더 자연스럽고 선명한 사진을 얻을 수 있습니다. 또한, 촬영 전에 초점을 수동으로 맞추고, 노출을 조정하는 것 역시 중요합니다. 빛이 너무 많거나 적으면 사진이 흐릿해지거나, 디테일이 사라질 수 있습니다.\n\n\n\nHDR 기능 사용: 강한 대비가 있는 장면에서 더 디테일한 사진을 촬영할 수 있습니다.\n초점 수동 조절: 사진의 주제가 명확하게 보이도록 초점을 맞추세요.\n노출 조정: 밝기를 조절하여 디테일을 더 살릴 수 있습니다.\n \n \n2. 자연광을 활용하라\n빛은 사진의 품질을 좌우하는 중요한 요소입니다. 스마트폰 카메라 센서의 크기는 DSLR보다 작기 때문에, 충분한 조명이 없다면 사진이 흐릿하게 나오기 쉽습니다. 자연광을 잘 활용하면 이런 문제를 해결할 수 있습니다. 예를 들어, 창가에서 들어오는 자연광을 활용하거나, 해가 질 무렵의 부드러운 빛을 이용해 촬영해 보세요. 실내 촬영 시에는 인공 조명보다는 자연광을 최대한 활용하는 것이 좋습니다. 단, 역광을 피하고 피사체가 빛을 충분히 받도록 각도를 조절해야 합니다.\n\n\n\n황금 시간대 촬영: 일출이나 일몰 시의 부드러운 빛을 활용해보세요.\n역광 피하기: 피사체가 어두워지지 않도록 주의하세요.\n반사된 빛 활용: 직사광선이 강할 때는 반사된 빛을 이용하세요.\n \n \n3. 구도와 각도에 신경 쓰기\n좋은 사진은 구도에서 시작된다고 해도 과언이 아닙니다. 촬영할 때 단순히 피사체를 중앙에 두기보다는 3분할 구도를 활용하거나, 다양한 각도에서 찍어보는 것이 좋습니다. 예를 들어, 낮은 각도에서 촬영하면 피사체가 더 돋보이게 되며, 높은 각도에서는 넓은 배경을 효과적으로 담을 수 있습니다. 또한, 수평을 맞추는 것 역시 중요합니다. 스마트폰의 그리드 라인을 활용하면 수평을 쉽게 맞출 수 있습니다.\n\n\n\n3분할 구도: 사진의 중심을 3등분하여 구성하세요.\n다양한 각도 실험: 여러 각도에서 촬영하여 가장 인상적인 장면을 찾아보세요.\n수평 맞추기: 그리드 라인을 사용하여 수평을 정확히 맞추세요.\n \n \n4. 간단한 후반 작업으로 완성도 높이기\n사진을 찍고 난 후에도 스마트폰에서 바로 간단한 편집 작업을 통해 완성도를 높일 수 있습니다. 예를 들어, 밝기와 대비를 조정하거나, 색온도를 맞추는 것만으로도 사진의 분위기가 완전히 달라질 수 있습니다. 요즘 스마트폰에는 다양한 편집 앱이 있어서 전문가처럼 색상 보정, 노이즈 제거, 크롭 등을 손쉽게 할 수 있습니다. 하지만 과도한 편집은 오히려 사진의 자연스러움을 해칠 수 있으므로, 적절한 선에서 작업을 마무리하는 것이 좋습니다.\n\n\n\n밝기 및 대비 조절: 간단한 조정만으로도 사진이 선명해질 수 있습니다.\n색온도 맞추기: 사진의 분위기를 원하는 대로 조정하세요.\n과한 편집 피하기: 자연스러움을 잃지 않도록 주의하세요.\n \n \n결론\n스마트폰 카메라는 이제 우리 삶의 필수품이 되었으며, DSLR에 뒤지지 않는 성능을 자랑합니다. 하지만 그만큼 사용자의 노력도 중요합니다. 이번에 소개한 간단한 팁들을 활용하면, 언제 어디서나 스마트폰으로 전문가 못지않은 사진을 촬영할 수 있을 것입니다. 중요한 것은 장비가 아닌, 그 장비를 어떻게 활용하느냐입니다. 다음번에 사진을 찍을 때는 이 비법들을 꼭 기억하고 적용해 보세요.",
        "guid": "http://muzbox.tistory.com/483461",
        "categories": [
          "ANDROID &amp; 모바일/안드로이드 꿀팁",
          "dslr 없이 사진",
          "hdr 기능",
          "사진 구도",
          "사진 편집",
          "스마트폰 사진 비법",
          "스마트폰 사진 촬영",
          "스마트폰 카메라 설정",
          "자연광 활용"
        ],
        "isoDate": "2024-08-23T08:50:43.000Z"
      },
      {
        "creator": "어떤오후의 프리웨어 이야기",
        "title": "이미지 품질 향상 AIarty, 무료 1년 라이센스 받는 방법",
        "link": "http://muzbox.tistory.com/483460",
        "pubDate": "Thu, 22 Aug 2024 09:24:29 +0900",
        "author": "어떤오후의 프리웨어 이야기",
        "comments": "http://muzbox.tistory.com/483460#entry483460comment",
        "content": "<p data-ke-size=\"size16\">&nbsp;AIarty 이미지 향상 프로그램으로 사진을 놀랍도록 향상시키세요. 무료 1년 라이선스를 통해 Windows와 Mac에서 고해상도 이미지를 손쉽게 제작할 수 있습니다. 지금 한정된 시간 동안 무료로 제공되는 이 기회를 놓치지 마세요!</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"1280\" data-origin-height=\"720\"><span data-url=\"https://blog.kakaocdn.net/dn/XdCYQ/btsJbtj4Hgg/ZKza8SqAkQgpbhyUvGpE10/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/XdCYQ/btsJbtj4Hgg/ZKza8SqAkQgpbhyUvGpE10/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/XdCYQ/btsJbtj4Hgg/ZKza8SqAkQgpbhyUvGpE10/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FXdCYQ%2FbtsJbtj4Hgg%2FZKza8SqAkQgpbhyUvGpE10%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"이미지 품질 향상 AIarty, 무료 1년 라이센스 받는 방법\" data-origin-width=\"1280\" data-origin-height=\"720\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;이미지의 품질은 높을수록 시각적 경험이 향상되며, 이는 개인 프로젝트든 비즈니스 마케팅이든 매우 중요한 요소로 작용합니다. 그러나 때로는 저해상도 이미지나 노이즈가 많은 사진 때문에 원하는 결과를 얻지 못할 때가 있습니다. 이런 문제를 해결하기 위해 AIarty 이미지 향상 소프트웨어가 등장했습니다. AIarty는 최신 딥러닝 기술을 활용하여 이미지를 향상시키고, 저해상도 이미지를 고해상도로 변환하며, 노이즈 제거와 블러 현상을 개선하는 강력한 기능을 제공합니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><span style=\"color: #ee2323;\"><i>&nbsp;특히, 이 소프트웨어의 일반적인 1년 라이선스는 85달러이지만, 현재 무료로 제공되고 있어 누구나 손쉽게 이 강력한 도구를 활용할 수 있습니다.</i></span></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #009a87;\"><b>AIarty 소개</b></span></h2>\n<p data-ke-size=\"size16\">AIarty 이미지 향상 프로그램은 Mac과 Windows에서 모두 사용할 수 있는 친환경 데스크탑 프로그램으로, 혁신적인 딥러닝 기술을 사용하여 이미지의 품질을 향상시키고 업스케일링하며, 세부 정보를 생성하는 데 최적화되어 있습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"1092\" data-origin-height=\"724\"><span data-url=\"https://blog.kakaocdn.net/dn/cL5hCO/btsJaAxsUyE/VygUOy55ShYubfj04aIzqK/img.webp\" data-phocus=\"https://blog.kakaocdn.net/dn/cL5hCO/btsJaAxsUyE/VygUOy55ShYubfj04aIzqK/img.webp\"><img src=\"https://blog.kakaocdn.net/dn/cL5hCO/btsJaAxsUyE/VygUOy55ShYubfj04aIzqK/img.webp\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcL5hCO%2FbtsJaAxsUyE%2FVygUOy55ShYubfj04aIzqK%2Fimg.webp\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"AIarty\" data-origin-width=\"1092\" data-origin-height=\"724\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">이 소프트웨어는 최대 32K(Windows) 또는 16K(Mac) 해상도로 이미지를 업스케일링할 수 있으며, 이를 통해 저해상도 이미지를 200%, 400%, 800%로 확대해도 고품질로 인쇄할 수 있습니다. 뿐만 아니라 2K, 4K, 8K의 고해상도 이미지를 제공하여 노이즈와 블러를 제거하고, 압축된 JPEG를 복원하는 기능도 포함되어 있습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">이 소프트웨어는 이미지 품질을 크게 개선하여 잡음과 결함이 없는 선명한 이미지를 제공합니다. 특히 AIarty의 기술은 저품질, 저해상도 이미지나 AI가 생성한 이미지를 고화질의 예술 작품으로 변환하는 데 탁월한 성능을 발휘합니다. 사용자는 1K, 2K, 4K, 8K 해상도를 선택할 수 있으며, 최대 32K 이미지 품질을 제공하는 업스케일 기능도 포함되어 있어 큰 포맷의 출력물을 손쉽게 제작할 수 있습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"AIarty 01.png\" data-origin-width=\"1392\" data-origin-height=\"745\"><span data-url=\"https://blog.kakaocdn.net/dn/bgYWwC/btsJbKeOf5C/Kc8ni1oTzcLj4yM6PCjda1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bgYWwC/btsJbKeOf5C/Kc8ni1oTzcLj4yM6PCjda1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bgYWwC/btsJbKeOf5C/Kc8ni1oTzcLj4yM6PCjda1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbgYWwC%2FbtsJbKeOf5C%2FKc8ni1oTzcLj4yM6PCjda1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"AIarty\" data-filename=\"AIarty 01.png\" data-origin-width=\"1392\" data-origin-height=\"745\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;또한 이 프로그램은 이미지의 모서리를 선명하게 만들어주는 디블러 기능과, 노이즈를 제거하고 이미지를 재구성하여 더욱 깨끗한 결과물을 제공하는 디노이즈 기능을 제공합니다. AIarty는 세 가지 AI 모델을 사용하여 사진을 개선합니다. 첫 번째 모델은 이미지의 세부 정보를 더욱 뚜렷하게 하고, 두 번째 모델은 사진을 부드럽게 처리하여 결함을 제거하며, 세 번째 모델은 실제와 같은 세부 정보를 추가하여 이미지를 더 생동감 있게 만듭니다.</p>\n<figure data-ke-type=\"video\" data-ke-style=\"alignCenter\" data-video-host=\"youtube\" data-video-url=\"https://www.youtube.com/watch?v=Y3QdLNFWP0I\" data-video-thumbnail=\"https://scrap.kakaocdn.net/dn/eghvGt/hyWSchXMkT/NcWPFazPRFoZB0H0Pw7yBk/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720\" data-video-width=\"860\" data-video-height=\"484\" data-video-origin-width=\"860\" data-video-origin-height=\"484\" data-ke-mobilestyle=\"widthContent\" data-video-title=\"Aiarty Image Enhancer Quick Start Guide | From Pixelated to Perfect\" data-original-url=\"\"><iframe src=\"https://www.youtube.com/embed/Y3QdLNFWP0I\" width=\"860\" height=\"484\" frameborder=\"\" allowfullscreen=\"true\"></iframe>\n<figcaption style=\"display: none;\"></figcaption>\n</figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #009a87;\"><b>AIarty 무료 1년 라이선스 취득 방법</b></span></h2>\n<p data-ke-size=\"size16\">AIarty 이미지 향상 소프트웨어의 1년 무료 라이선스를 얻는 방법은 매우 간단합니다. 아래 단계를 따라 무료 라이선스를 취득해 보세요:</p>\n<ol style=\"list-style-type: decimal;\" data-ke-list-type=\"decimal\">\n<li>공식 웹사이트 방문: 먼저 <b><span style=\"color: #006dd7;\"><u>[</u> <a href=\"https://www.aiarty.com\" target=\"_blank\" rel=\"noopener\">AIarty 공식 웹사이트</a> ]</span></b>를 방문합니다. 웹사이트에서 해당 프로그램의 무료 1년 라이선스를 제공하는 페이지로 이동하세요.</li>\n<li>다운로드 및 설치: 사용 중인 운영 체제에 맞는 설치 파일을 다운로드합니다. Windows 사용자는 <span style=\"color: #006dd7;\"><b>[</b> <a href=\"https://www.aiarty.com/release/aiarty-image-enhancer.exe\" target=\"_blank\" rel=\"noopener\">여기</a> <b>]</b></span>에서, Mac 사용자는<b><u><span style=\"color: #006dd7;\">[ <a href=\"https://www.aiarty.com/release/aiarty-image-enhancer.dmg\" target=\"_blank\" rel=\"noopener\">여기</a> ]</span></u></b>에서 설치 파일을 다운로드할 수 있습니다. 다운로드가 완료되면 파일을 실행하여 프로그램을 설치합니다.</li>\n<li>라이선스 코드 입력: 프로그램 설치 및 실행 후, 이메일 주소와 함께 제공된 라이선스 코드를 입력합니다. Windows와 Mac 사용자에게 각각 다른 라이선스 코드가 제공되며, 이를 통해 프로그램을 활성화할 수 있습니다.\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>Windows 라이선스 코드:\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>FAN3I-ZJE5W-A4QRQ-JWEBH</li>\n<li>FA5OU-OQKNW-QPHTC-4VMGZ</li>\n<li>FAMU5-MJFVU-T3KAY-664VF</li>\n</ul>\n</li>\n<li>Mac 라이선스 코드:\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>FBOIZ-JJRUD-6IEFN-KGZCR</li>\n<li>FBR6N-A75VN-PVIEO-CCSSU</li>\n<li>FB2HF-24TT5-U7YEB-5562Z</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>프로그램 활성화: 라이선스 코드를 입력하고 \"Activate\" 버튼을 클릭하면 프로그램이 활성화됩니다. 이제 1년 동안 AIarty 이미지 향상 소프트웨어의 모든 기능을 무료로 이용할 수 있습니다.<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"847\" data-origin-height=\"850\"><span data-url=\"https://blog.kakaocdn.net/dn/cseRwY/btsJag7hTya/qHw4JpK2MqJAy61SyhmphK/img.webp\" data-phocus=\"https://blog.kakaocdn.net/dn/cseRwY/btsJag7hTya/qHw4JpK2MqJAy61SyhmphK/img.webp\"><img src=\"https://blog.kakaocdn.net/dn/cseRwY/btsJag7hTya/qHw4JpK2MqJAy61SyhmphK/img.webp\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcseRwY%2FbtsJag7hTya%2FqHw4JpK2MqJAy61SyhmphK%2Fimg.webp\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"AIarty\" data-origin-width=\"847\" data-origin-height=\"850\"/></span></figure>\n</li>\n</ol>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #009a87;\"><b>마치며</b></span></h2>\n<p data-ke-size=\"size16\">AIarty 이미지 향상 소프트웨어는 매우 강력하면서도 사용하기 쉬운 사진 향상 도구로, 이미지 품질을 한 단계 끌어올리려는 모든 사용자에게 추천할 만한 프로그램입니다. 이 소프트웨어는 업스케일링, 노이즈 제거, 블러 개선뿐만 아니라 세부 정보를 추가하는 등 다양한 기능을 제공하여 이미지를 더욱 선명하고 생생하게 만듭니다. 지금 바로 무료 1년 라이선스를 통해 AIarty를 경험해 보세요.&nbsp;</p>",
        "contentSnippet": "AIarty 이미지 향상 프로그램으로 사진을 놀랍도록 향상시키세요. 무료 1년 라이선스를 통해 Windows와 Mac에서 고해상도 이미지를 손쉽게 제작할 수 있습니다. 지금 한정된 시간 동안 무료로 제공되는 이 기회를 놓치지 마세요!\n \n\n\n \n 이미지의 품질은 높을수록 시각적 경험이 향상되며, 이는 개인 프로젝트든 비즈니스 마케팅이든 매우 중요한 요소로 작용합니다. 그러나 때로는 저해상도 이미지나 노이즈가 많은 사진 때문에 원하는 결과를 얻지 못할 때가 있습니다. 이런 문제를 해결하기 위해 AIarty 이미지 향상 소프트웨어가 등장했습니다. AIarty는 최신 딥러닝 기술을 활용하여 이미지를 향상시키고, 저해상도 이미지를 고해상도로 변환하며, 노이즈 제거와 블러 현상을 개선하는 강력한 기능을 제공합니다.\n \n 특히, 이 소프트웨어의 일반적인 1년 라이선스는 85달러이지만, 현재 무료로 제공되고 있어 누구나 손쉽게 이 강력한 도구를 활용할 수 있습니다.\n \n \nAIarty 소개\nAIarty 이미지 향상 프로그램은 Mac과 Windows에서 모두 사용할 수 있는 친환경 데스크탑 프로그램으로, 혁신적인 딥러닝 기술을 사용하여 이미지의 품질을 향상시키고 업스케일링하며, 세부 정보를 생성하는 데 최적화되어 있습니다.\n\n\n \n이 소프트웨어는 최대 32K(Windows) 또는 16K(Mac) 해상도로 이미지를 업스케일링할 수 있으며, 이를 통해 저해상도 이미지를 200%, 400%, 800%로 확대해도 고품질로 인쇄할 수 있습니다. 뿐만 아니라 2K, 4K, 8K의 고해상도 이미지를 제공하여 노이즈와 블러를 제거하고, 압축된 JPEG를 복원하는 기능도 포함되어 있습니다.\n \n이 소프트웨어는 이미지 품질을 크게 개선하여 잡음과 결함이 없는 선명한 이미지를 제공합니다. 특히 AIarty의 기술은 저품질, 저해상도 이미지나 AI가 생성한 이미지를 고화질의 예술 작품으로 변환하는 데 탁월한 성능을 발휘합니다. 사용자는 1K, 2K, 4K, 8K 해상도를 선택할 수 있으며, 최대 32K 이미지 품질을 제공하는 업스케일 기능도 포함되어 있어 큰 포맷의 출력물을 손쉽게 제작할 수 있습니다.\n\n\n \n 또한 이 프로그램은 이미지의 모서리를 선명하게 만들어주는 디블러 기능과, 노이즈를 제거하고 이미지를 재구성하여 더욱 깨끗한 결과물을 제공하는 디노이즈 기능을 제공합니다. AIarty는 세 가지 AI 모델을 사용하여 사진을 개선합니다. 첫 번째 모델은 이미지의 세부 정보를 더욱 뚜렷하게 하고, 두 번째 모델은 사진을 부드럽게 처리하여 결함을 제거하며, 세 번째 모델은 실제와 같은 세부 정보를 추가하여 이미지를 더 생동감 있게 만듭니다.\n\n\n\n \n \n \nAIarty 무료 1년 라이선스 취득 방법\nAIarty 이미지 향상 소프트웨어의 1년 무료 라이선스를 얻는 방법은 매우 간단합니다. 아래 단계를 따라 무료 라이선스를 취득해 보세요:\n공식 웹사이트 방문: 먼저 [ AIarty 공식 웹사이트 ]를 방문합니다. 웹사이트에서 해당 프로그램의 무료 1년 라이선스를 제공하는 페이지로 이동하세요.\n다운로드 및 설치: 사용 중인 운영 체제에 맞는 설치 파일을 다운로드합니다. Windows 사용자는 [ 여기 ]에서, Mac 사용자는[ 여기 ]에서 설치 파일을 다운로드할 수 있습니다. 다운로드가 완료되면 파일을 실행하여 프로그램을 설치합니다.\n라이선스 코드 입력: 프로그램 설치 및 실행 후, 이메일 주소와 함께 제공된 라이선스 코드를 입력합니다. Windows와 Mac 사용자에게 각각 다른 라이선스 코드가 제공되며, 이를 통해 프로그램을 활성화할 수 있습니다.\n\nWindows 라이선스 코드:\n\nFAN3I-ZJE5W-A4QRQ-JWEBH\nFA5OU-OQKNW-QPHTC-4VMGZ\nFAMU5-MJFVU-T3KAY-664VF\nMac 라이선스 코드:\n\nFBOIZ-JJRUD-6IEFN-KGZCR\nFBR6N-A75VN-PVIEO-CCSSU\nFB2HF-24TT5-U7YEB-5562Z\n프로그램 활성화: 라이선스 코드를 입력하고 \"Activate\" 버튼을 클릭하면 프로그램이 활성화됩니다. 이제 1년 동안 AIarty 이미지 향상 소프트웨어의 모든 기능을 무료로 이용할 수 있습니다.\n\n\n \n \n마치며\nAIarty 이미지 향상 소프트웨어는 매우 강력하면서도 사용하기 쉬운 사진 향상 도구로, 이미지 품질을 한 단계 끌어올리려는 모든 사용자에게 추천할 만한 프로그램입니다. 이 소프트웨어는 업스케일링, 노이즈 제거, 블러 개선뿐만 아니라 세부 정보를 추가하는 등 다양한 기능을 제공하여 이미지를 더욱 선명하고 생생하게 만듭니다. 지금 바로 무료 1년 라이선스를 통해 AIarty를 경험해 보세요.",
        "guid": "http://muzbox.tistory.com/483460",
        "categories": [
          "NEWS/윈도우10 한시적 무료앱",
          "ai 이미지 향상",
          "고해상도 업스케일링",
          "노이즈 제거",
          "딥러닝 이미지",
          "무료 라이선스",
          "무료 소프트웨어",
          "블러 제거",
          "사진 품질 개선"
        ],
        "isoDate": "2024-08-22T00:24:29.000Z"
      }
    ]
  },
  {
    "name": "동우리의 블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "하테나",
    "category": "개인",
    "posts": []
  },
  {
    "name": "늑돌이네 라지온",
    "category": "개인",
    "posts": [
      {
        "creator": "늑돌이",
        "title": "한국 이북리더 명맥 잇나? 예스24 크레마 페블 예약판매",
        "link": "https://lazion.com/2513722",
        "pubDate": "Tue, 27 Aug 2024 18:45:01 +0900",
        "author": "늑돌이",
        "comments": "https://lazion.com/2513722#entry2513722comment",
        "content": "<h3 data-ke-size=\"size23\">예스24(YES24)에서 새로운 6인치 이북 리더 <b>크레마 페블(Crema&nbsp;Pebble)</b> 예약판매를 시작합니다.</h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><span style=\"color: #006dd7;\"><b>크레마 페블, 달라진 점</b></span><span style=\"color: #006dd7;\"></span></h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"1846260943_20240827112408_4712478486_web.jpg\" data-origin-width=\"3000\" data-origin-height=\"1688\"><span data-url=\"https://blog.kakaocdn.net/dn/cg9Fgs/btsJgIoRoqA/UkFx6aQKEKzdt1rPnjoKxk/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/cg9Fgs/btsJgIoRoqA/UkFx6aQKEKzdt1rPnjoKxk/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/cg9Fgs/btsJgIoRoqA/UkFx6aQKEKzdt1rPnjoKxk/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcg9Fgs%2FbtsJgIoRoqA%2FUkFx6aQKEKzdt1rPnjoKxk%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"1846260943_20240827112408_4712478486_web.jpg\" data-origin-width=\"3000\" data-origin-height=\"1688\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">6인치 화면 크기에 전작인 크레마 모티프에서 51g 경량화한 <b>139g의 무게</b>로 같은 화면 크기의 이북 리더 가운데에는 가장 가벼운 이 제품은 손에 착 감기는 그립감과 함께 조약돌(Pebble)처럼 부드러운 외관을 자랑합니다. </p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">1448x1072 해상도의 카르타(Carta) 6인치 e-ink 패널과 쿼드코어 프로세서, 안드로이드 11 OS를 가지고 있으며 구글 플레이도 지원합니다. RAM은 전작보다 1GB 늘어난 4GB로 더 쾌적한 반응 속도를 제공합니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"upgradeDef01.jpg\" data-origin-width=\"430\" data-origin-height=\"409\"><span data-url=\"https://blog.kakaocdn.net/dn/s3qtj/btsJioPQqLt/VjLIqkNvbj6Gcvb3FkC0C0/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/s3qtj/btsJioPQqLt/VjLIqkNvbj6Gcvb3FkC0C0/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/s3qtj/btsJioPQqLt/VjLIqkNvbj6Gcvb3FkC0C0/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fs3qtj%2FbtsJioPQqLt%2FVjLIqkNvbj6Gcvb3FkC0C0%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"upgradeDef01.jpg\" data-origin-width=\"430\" data-origin-height=\"409\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">색상은 하양과 까망 2종이며 기본 저장소로 32GB 내장, 마이크로SD 슬롯을 통해 최대 512GB까지 확장 가능합니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><span style=\"color: #006dd7;\"><b>크레마 페블 가격과 예약판매 이벤트</b></span></h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><b>크레마 페블 본체의 가격은 21만9천원</b>이며, 액세서리와 함께 파는 패키지도 있습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">예스24는 크레마 페블 출시를 기념하여 <a href=\"http://app.ac/HqpJYK203\" target=\"_blank\" rel=\"noopener\"><b>예약판매 이벤트</b>도 진행</a>합니다. 예약 판매 기간인 9월 2일까지 예스24 홈페이지에서 크레마 페블을 구매할 경우 10명을 추첨해 YES상품권 5만원을 증정합니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"img0701.jpg\" data-origin-width=\"600\" data-origin-height=\"470\"><span data-url=\"https://blog.kakaocdn.net/dn/ZNW8a/btsJieGIpBP/8cY0qWkyyNZWcgvzPBVkvk/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/ZNW8a/btsJieGIpBP/8cY0qWkyyNZWcgvzPBVkvk/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/ZNW8a/btsJieGIpBP/8cY0qWkyyNZWcgvzPBVkvk/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FZNW8a%2FbtsJieGIpBP%2F8cY0qWkyyNZWcgvzPBVkvk%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"img0701.jpg\" data-origin-width=\"600\" data-origin-height=\"470\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">새로운 6인치 e-ink 리더 <b>크레마 페블의 정식 출시는 9월 3일</b>입니다.<br /><br /></p>\n<p data-ke-size=\"size16\">더 자세한 내용은 <b><span style=\"color: #0593d3;\"><a href=\"http://app.ac/HqpJYK203\" target=\"_blank\" rel=\"noopener\">이곳에서 참고하시기 바랍니다.</a></span></b></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: right;\" data-ke-size=\"size16\"><span style=\"color: #1a5490;\">이 포스트에 포함된 링크를 통해 제품을 구매하는 경우 라지온에서 판매 수수료를 받을 수 있습니다.</span></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: right;\" data-ke-size=\"size16\">(출처 : 예스24)</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<div class=\"notranslate\" style=\"all: initial;\">&nbsp;</div>",
        "contentSnippet": "예스24(YES24)에서 새로운 6인치 이북 리더 크레마 페블(Crema Pebble) 예약판매를 시작합니다.\n \n크레마 페블, 달라진 점\n \n\n\n \n6인치 화면 크기에 전작인 크레마 모티프에서 51g 경량화한 139g의 무게로 같은 화면 크기의 이북 리더 가운데에는 가장 가벼운 이 제품은 손에 착 감기는 그립감과 함께 조약돌(Pebble)처럼 부드러운 외관을 자랑합니다. \n \n1448x1072 해상도의 카르타(Carta) 6인치 e-ink 패널과 쿼드코어 프로세서, 안드로이드 11 OS를 가지고 있으며 구글 플레이도 지원합니다. RAM은 전작보다 1GB 늘어난 4GB로 더 쾌적한 반응 속도를 제공합니다.\n\n\n색상은 하양과 까망 2종이며 기본 저장소로 32GB 내장, 마이크로SD 슬롯을 통해 최대 512GB까지 확장 가능합니다.\n \n \n크레마 페블 가격과 예약판매 이벤트\n \n크레마 페블 본체의 가격은 21만9천원이며, 액세서리와 함께 파는 패키지도 있습니다.\n \n예스24는 크레마 페블 출시를 기념하여 예약판매 이벤트도 진행합니다. 예약 판매 기간인 9월 2일까지 예스24 홈페이지에서 크레마 페블을 구매할 경우 10명을 추첨해 YES상품권 5만원을 증정합니다.\n \n\n\n \n새로운 6인치 e-ink 리더 크레마 페블의 정식 출시는 9월 3일입니다.\n\n더 자세한 내용은 이곳에서 참고하시기 바랍니다.\n \n \n이 포스트에 포함된 링크를 통해 제품을 구매하는 경우 라지온에서 판매 수수료를 받을 수 있습니다.\n \n(출처 : 예스24)",
        "guid": "https://lazion.com/2513722",
        "categories": [
          "#작은PC/#태블릿#e북리더",
          "crema",
          "crema pebble",
          "e-ink",
          "ebook",
          "eBook Reader",
          "new",
          "Tablet",
          "yes24"
        ],
        "isoDate": "2024-08-27T09:45:01.000Z"
      }
    ]
  },
  {
    "name": "루리웹 - 루리웹 리뷰 게시판",
    "category": "게임",
    "posts": [
      {
        "creator": "샤말란의눈",
        "title": "[MULTI] ‘성검전설다움’을 아로새기는 여행, 비전스 오브 마나",
        "link": "https://bbs.ruliweb.com/news/board/11/read/2244",
        "pubDate": "Tue, 27 Aug 2024 10:19:16 +0900",
        "author": "샤말란의눈",
        "content": "<img width=\"236\" height=\"177\" src=\"https://i1.ruliweb.com/thumb/24/08/27/19191a184494c329e.jpg\">",
        "contentSnippet": "",
        "categories": [
          "리뷰"
        ],
        "isoDate": "2024-08-27T01:19:16.000Z"
      },
      {
        "creator": "｜RULIWEB｜",
        "title": "[게임툰] 하중 계산은 어려워, 월드 오브 구 2",
        "link": "https://bbs.ruliweb.com/news/board/11/read/2243",
        "pubDate": "Mon, 26 Aug 2024 20:04:42 +0900",
        "author": "｜RULIWEB｜",
        "content": "<img width=\"236\" height=\"177\" src=\"https://i3.ruliweb.com/thumb/24/08/26/1918e4c2d2151ad6b.jpg\">",
        "contentSnippet": "",
        "categories": [
          "공지"
        ],
        "isoDate": "2024-08-26T11:04:42.000Z"
      },
      {
        "creator": "샤말란의눈",
        "title": "[MULTI] 게임스컴 2024, 공식 방송 및 관련 기사 종합",
        "link": "https://bbs.ruliweb.com/news/board/11/read/2242",
        "pubDate": "Mon, 26 Aug 2024 08:12:38 +0900",
        "author": "샤말란의눈",
        "content": "<img width=\"236\" height=\"177\" src=\"https://i3.ruliweb.com/thumb/24/08/25/191884a1ee413b2a1.jpg\">",
        "contentSnippet": "",
        "categories": [
          "특집"
        ],
        "isoDate": "2024-08-25T23:12:38.000Z"
      }
    ]
  },
  {
    "name": "Reasontobe",
    "category": "개인",
    "posts": []
  },
  {
    "name": "자유로운 생활",
    "category": "개인",
    "posts": []
  },
  {
    "name": "에스티마의 인터넷이야기 EstimaStory.com",
    "category": "개인",
    "posts": []
  },
  {
    "name": "나긋한 개발 - 데비안 리눅스와 프로그램 언어",
    "category": "개인",
    "posts": []
  },
  {
    "name": "일상을 여행처럼...",
    "category": "개인",
    "posts": []
  },
  {
    "name": "khris'log",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Just hack'em",
    "category": "개인",
    "posts": []
  },
  {
    "name": "C++ Truths",
    "category": "개인",
    "posts": []
  },
  {
    "name": "jacking75",
    "category": "개인",
    "posts": [
      {
        "title": "GitHub Copilot 통합으로 향상된 \"Visual Studio 2022 버전 17.10\"의 Git 도구 기능",
        "link": "https://jacking75.github.io/tech-ai_20240822/",
        "pubDate": "Thu, 22 Aug 2024 00:00:00 +0900",
        "content": "<iframe width=\"1024\" height=\"1024\" src=\"https://docs.google.com/document/d/e/2PACX-1vQxAH2MgY6UHmBNwjep5Q4AAf82MNhrFR_3yI-ShVy_mtsrpMMXd25YLuobBWV52JNbZopoo_wpGy76/pub?embedded=true\"></iframe>\n\n",
        "contentSnippet": "",
        "guid": "https://jacking75.github.io/tech-ai_20240822/",
        "isoDate": "2024-08-21T15:00:00.000Z"
      }
    ]
  },
  {
    "name": "Joel on Software",
    "category": "개인",
    "posts": []
  },
  {
    "name": "벤자민로그",
    "category": "개인",
    "posts": [
      {
        "title": "매일 커밋에 실패했던 날",
        "link": "https://jeho.page/essay/2024/08/26/daily-commit-failed.html",
        "pubDate": "2024-08-26T05:00:00.000Z",
        "author": "김재호",
        "content": "<p>2020년, 코로나가 시작되고 루틴이라는 게 생기기 시작했습니다.<br />\n약속도 없고 방구석에만 있다 보니 자연스레 루틴이 잡혔습니다.</p>\n\n<p>아침 9시에 일어난다.<br />\n드르륵드르륵 커피를 간다.<br />\n커피잔을 들고 방에 들어가서 컴퓨터 앞에 앉는다.<br />\n코딩을 한다.</p>\n\n<p>매일 코딩을 했습니다.<br />\nGitHub 잔디도 빽빽이 잘 쌓였습니다.</p>\n\n<p>와 이거 멋진걸.<br />\n2021년에는 진짜로 잔디를 꽉 한 번 채워봐야겠다.</p>\n\n<p><img src=\"/assets/img/daily_coding.png\" alt=\"2021년 커밋 기록\" /><br />\n<em>2021년 GitHub</em></p>\n\n<p>하… 잘나가다가 실패해버렸습니다. 6/16일.</p>\n\n<p>6/16일은 가평에 놀러 갔던 날입니다.<br />\n호텔 방에 돌아왔더니 생각이 났습니다.<br />\n노느라 오늘 코딩을 못했네. 지금이라도 코딩해야지.<br />\n밤 10시쯤 집중해서 코딩하다가..<br />\n문득 시계를 보니 자정이 넘어버린 겁니다.</p>\n\n<blockquote>\n  <p>헉… 망했다. 아직 커밋 안 했는데.</p>\n</blockquote>\n\n<p>이렇게 무너진 건가? 끝난거야?<br />\n어찌나 허무하던지. 미리 커밋부터 정리해둘걸..</p>\n\n<p>아쉬워 하다가 이런 생각이 들었습니다.</p>\n<blockquote>\n  <p>잠깐. 난 잘못한 거 없잖아. 억울하다고.<br />\n놀다가 놓친 것도 아니고 진짜로 코딩하고 있었는걸.<br />\n시간을 돌려서 커밋을 집어넣을까?<br />\n이건 반칙이 아닌 것 같은데?</p>\n</blockquote>\n\n<p>1분쯤 고민했습니다.<br />\nGit 명령어를 찾아보기까지 했습니다.<br />\n하지만 그러지 않기로 했습니다.</p>\n\n<p>반칙이 아니긴 뭐가 아니야. 반칙 맞잖아.<br />\n그렇게 하는 순간 의미가 없어져 버려.<br />\n못한 건 못한 거지, 이게 뭐라고.</p>\n\n<p>마음이 편해졌습니다.<br />\n노력은 하되 실패한 날은 실패한 날대로 두자.</p>\n\n<p>이후로는 그다지 신경쓰며 하지 않았는데도 잔디는 여전히 푸릇푸릇합니다.<br />\n너무 신경 쓰며 할 때보다 오히려 지금이 더 좋습니다.</p>\n\n<p><img src=\"/assets/img/commits.png\" alt=\"최근 5년간 커밋\" /></p>\n\n<p><br />\n<em>함께 읽으면 좋은 글:</em></p>\n<ul>\n  <li><a href=\"/essay/2022/01/05/daily-coding.html\">매일매일 코딩하기</a></li>\n  <li><a href=\"/essay/2024/08/01/commit-count.html\">1년에 몇 개나 커밋하세요?</a></li>\n</ul>",
        "contentSnippet": "2020년, 코로나가 시작되고 루틴이라는 게 생기기 시작했습니다.\n아침 9시에 일어난다.\n매일 코딩을 했습니다.\n와 이거 멋진걸.\n\n2021년 GitHub\n하… 잘나가다가 실패해버렸습니다. 6/16일.\n6/16일은 가평에 놀러 갔던 날입니다.\n헉… 망했다. 아직 커밋 안 했는데.\n이렇게 무너진 건가? 끝난거야?\n아쉬워 하다가 이런 생각이 들었습니다.\n잠깐. 난 잘못한 거 없잖아. 억울하다고.\n1분쯤 고민했습니다.\n반칙이 아니긴 뭐가 아니야. 반칙 맞잖아.\n마음이 편해졌습니다.\n이후로는 그다지 신경쓰며 하지 않았는데도 잔디는 여전히 푸릇푸릇합니다.\n\n\n함께 읽으면 좋은 글:\n매일매일 코딩하기\n1년에 몇 개나 커밋하세요?",
        "summary": "2020년, 코로나가 시작되고 루틴이라는 게 생기기 시작했습니다. 약속도 없고 방구석에만 있다 보니 자연스레 루틴이 잡혔습니다.",
        "id": "https://jeho.page/essay/2024/08/26/daily-commit-failed",
        "isoDate": "2024-08-26T05:00:00.000Z"
      }
    ]
  },
  {
    "name": "악보쓰는 프로그래머",
    "category": "개인",
    "posts": []
  },
  {
    "name": "쭌안아빠",
    "category": "개인",
    "posts": []
  },
  {
    "name": "A Gangster World",
    "category": "개인",
    "posts": []
  },
  {
    "name": "요우의 내맘대로 블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "개발자스럽다",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "Against All Odds.",
    "category": "개인",
    "posts": []
  },
  {
    "name": "움직이는 게임서버",
    "category": "개인",
    "posts": []
  },
  {
    "name": "이상욱",
    "category": "개인",
    "posts": []
  },
  {
    "name": "임철재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "어쩐지 오늘은",
    "category": "개인",
    "posts": []
  },
  {
    "name": "oddpoet’s étude",
    "category": "개인",
    "posts": []
  },
  {
    "name": "0x00 - NULL",
    "category": "개인",
    "posts": []
  },
  {
    "name": "퇴근 후 서버다운",
    "category": "개인",
    "posts": []
  },
  {
    "name": "coolspeed",
    "category": "개인",
    "posts": []
  },
  {
    "name": "오늘도 끄적끄적",
    "category": "개인",
    "posts": []
  },
  {
    "name": "dx11 Vanica's Lifelog - 夢が夢で終わらないように",
    "category": "개인",
    "posts": []
  },
  {
    "name": "초코사랑",
    "category": "개인",
    "posts": []
  },
  {
    "name": "ZeroCho Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Hybrid's Notes",
    "category": "개인",
    "posts": []
  },
  {
    "name": "imays게임엔진개발자",
    "category": "개인",
    "posts": []
  },
  {
    "name": "RSS feed for hurinmon Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "기억보단 기록을",
    "category": "개인",
    "posts": [
      {
        "creator": "향로 (기억보단 기록을)",
        "title": "1. TS를 만나고 - 디자인",
        "link": "https://jojoldu.tistory.com/801",
        "pubDate": "Mon, 26 Aug 2024 12:43:13 +0900",
        "author": "향로 (기억보단 기록을)",
        "comments": "https://jojoldu.tistory.com/801#entry801comment",
        "content": "<blockquote data-ke-style=\"style2\">\n<p data-ke-size=\"size16\">JVM 컨퍼런스가 있으면 \"Java 세상에서 살던 사람이 처음 TS 세계를 만나고 느낀 여러가지 차이점\" 을 발표하려고했는데, 시간도 너무 지났고, 발표 준비하기도 쉽지 않은 것 같아 블로그에 시리즈로 시작한다.<br />Java가 구린 언어다를 표현하기 위한 글이기 보다는 <a href=\"https://jojoldu.tistory.com/687\">전작(다른 언어로 성장하기)</a>과 마찬가지로 타 생태계를 통해 성장할 수 있음을 알리는 글이다</p>\n</blockquote>\n<p data-ke-size=\"size16\">자바를 처음 배울때 <code>getter/setter</code> 에 대한 이야기를 많이 들었다.<br />캡슐화 등의 장점을 들으면서 클래스 안에는 항상 무분별하게 <code>getter/setter</code> 를 생성했다.<br />(그때는 Lombok을 배우지 못했던 터라) IDE의 자동 생성 기능을 사용하면서 열심히 <code>getter/setter</code>를 생성했다.</p>\n<p data-ke-size=\"size16\">물론 예전부터 많은 분들은 무분별한 getter/setter를 사용하지말라는 글과 토론을 나누었다.</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><a href=\"https://www.yegor256.com/2014/09/16/getters-and-setters-are-evil.html\">2014.09.16 - getters-and-setters-are-evil</a></li>\n</ul>\n<p data-ke-size=\"size16\">다만, 내가 배울때 법칙?처럼 <code>getter/setter</code> 만든다로 배워서 그렇게 했다.<br />(지금이야 getter/setter를 쓰지 않고, <a href=\"https://martinfowler.com/bliki/TellDontAsk.html\">Tell, Don't Ask</a> 를 당연한 것처럼 다뤄지고 있지만 말이다.)</p>\n<p data-ke-size=\"size16\">여튼, 배우던 당시에는 무분별하게 <code>getter/setter</code>를 만들다보니 \"<b>어차피 단순한 data holders로 사용하는데 그냥 public으로 열면 안되나</b>\" 같은 생각이 들었다.</p>\n<p data-ke-size=\"size16\">이를 테면 다음과 같다.<br />매번 아래와 같이 생성하는데,</p>\n<pre class=\"cpp\"><code>public class Course {\n    private int price;\n\n    public int getPrice() {\n        return price;\n    }\n\n    public void setPrice(int price) {\n        this.price = price;\n    }\n}</code></pre>\n<p data-ke-size=\"size16\">아래와 같이 \"그냥 <code>public</code> 으로 열어두는 것과 무엇이 다르지?\" 라는 생각인 것이다.</p>\n<pre class=\"angelscript\"><code>public class Course {\n    public int price;\n}</code></pre>\n<p data-ke-size=\"size16\">어차피 값을 넣고 빼는 역할 밖에 없는데, 똑같지 않나? 라는 생각을 하곤 했다.</p>\n<p data-ke-size=\"size16\">그러다가 기능이 여러가지로 확장되는 상황을 만나니 접근자를 통해서 사용해야하는 것의 중요성을 알게 되었다.</p>\n<p data-ke-size=\"size16\">예를 들어 단순히 <code>price</code>에 값을 넣기만 하면 되는 로직에서 아래와 같이 <b>부가세 10%가 추가된 가격으로 반환해야한다</b>는 조건이 추가되니 단순히 <code>public</code> 으로 열어두는 것은 기존의 코드를 모두 바꿔야하는 문제가 생겼다.</p>\n<pre class=\"angelscript\"><code>public class Course {\n    ...\n\n    public int getPrice() {\n        return (int) (price * 1.1); // 부가세 10% 적용\n    }\n}\n\ncoursePrice = course.price; // &lt;&lt;&lt; 이렇게 호출되는 코드들이\ncoursePrice = course.getPrice(); // &lt;&lt;&lt; 이 코드로 모두 변경되어야만 했다.</code></pre>\n<p data-ke-size=\"size16\">접근자를 통해서 처음 구현해두지 않으니, 이런 경우가 발생했다.<br />그래서 \"<b>처음부터 접근자를 생성해두고, 이를 호출자들이 사용하도록 해야만 이런 문제를 겪지 않는 다</b>\" 는 것을 알게되었다.</p>\n<p data-ke-size=\"size16\">다만, 그럼 \"매번 이렇게 접근자를 생성해두고 코드를 작성해야하는 불편함과 귀찮음은 어쩔 수 없는 것인가\" 라는 생각은 계속 들었다.</p>\n<p data-ke-size=\"size16\">그러던 중, JavaScript/TypeScript 코드들을 보게 되었는데, 여기서는 <code>get/set</code> 접근자를 언어 레벨에서 지원했다.</p>\n<p data-ke-size=\"size16\">즉, <b>아래 2개 코드는 호출자 입장에서 동일한 방법으로 호출할 수 있다</b>.</p>\n<pre class=\"kotlin\"><code>// 1. 접근자 없이 public 필드\nclass Course {\n    public price: number;\n\n    constructor(price: number) {\n        this.price = price;\n    }\n}\n\n// 2. 접근자를 통한 접근\nclass Course {\n    private _price: number;\n\n    ...\n\n    get price(): number {\n        return this._price;\n    }\n}\n\nval price = course.price; // 1, 2 모두 동일한 코드로 호출</code></pre>\n<p data-ke-size=\"size16\">이건 코틀린도 동일한데, 코틀린 역시 <code>get/set</code> 접근자를 언어 레벨에서 지원한다.</p>\n<pre class=\"kotlin\"><code>// 1. 접근자 없이 public 필드\nclass Course(var price: Int)\n\n// 2. 접근자를 통한 접근\nclass Course(private var _price: Int) {\n    var price: Int\n        get() = (_price * 1.1).toInt() // 부가세 10% 적용\n}\n\nval coursePrice = course.price</code></pre>\n<p data-ke-size=\"size16\">(코틀린과 자바를 굳이 다른 생태계라고 구분하지는 않지만)<br />이런 개념이 요즘의 모던한 언어들에게는 기본적으로 내장되어있다.</p>\n<p data-ke-size=\"size16\">Java를 처음 배우는 입장에서는 \"public 필드를 사용하면 안되고, private 필드를 사용하고 이에 대한 접근자를 무조건 생성해야한다\" 를 배워야만 했다.<br />이걸 그나마 편하게 하기 위해 Lombok 라이브러리 등의 도입도 알아야만 한다.<br />만약 이 모든걸 무시하고 처음 배운 클래스에서 단순히 public 필드를 사용하면 이후에 큰 비용을 지불하게 된다.</p>\n<p data-ke-size=\"size16\">반면, <b>JS, TS, Kotlin 등의 언어를 배우는 입장에서는 이런 것이 고민 거리이자 배워야할 내용이 되지 않는다</b>.<br />public 필드를 사용하는 것과 접근자를 통한 접근이 모두 <b>동일한 인터페이스를 지원하기 때문에 내가 알게되는 지식이 늘어난다고해서 기존의 코드들이 전체 교체 될 일은 거의 없다</b>.<br />즉, 프로그래밍을 배우고 사용할때 중요한 고민거리가 하나 없어지게 되는 셈이다.</p>\n<p data-ke-size=\"size16\">이런 사례는 <b>언어 레벨에서 좋은 디자인 인터페이스를 지원한다</b> 라는 의미로 다가왔다.</p>\n<p data-ke-size=\"size16\">처음 클래스를 배울때 단순하게 public으로 모든 필드를 선언하고 코드를 작성하더라도 이후에 <b>접근자 (accessor) 개념을 배우고 클래스 코드를 변경하더라도 기존 호출자 코드에 변경이 필요하지 않는다</b>는 것을 초보 개발자들에게 알려줄 수 있는 셈이다.</p>\n<p data-ke-size=\"size16\">이걸로 변경에 유연한 디자인을 아주 쉽게 익히게 된다.<br />Lombok 같은 외부의 라이브러리를 도입해야한다거나,<br />접근자 (accessor) 에 대한 개념을 배운다던가 그런 언어 외적인 추가적인 학습없이 말이다.</p>\n<h2 data-ke-size=\"size26\">마무리</h2>\n<p data-ke-size=\"size16\">좋은 언어는 그 자체로 좋은 디자인을 배울 수 있어야 한다는 생각을 한다.<br />그런면에서 문법 설탕에 불과하다고 이야기하는 사람들도 있지만, 특정 프레임워크, 라이브러리의 도움으로 해결하는 문제들을 언어 레벨에서 직접 해결해주고 있다는 점이 참 좋았다.</p>\n<p data-ke-size=\"size16\">그리고 그런 개념들이 TS, Kotlin 등 Java가 아닌 다른 언어에서는 당연하게도 들어가 있다는 점이 섬세하게도 느껴졌다.<br />(물론 Kotlin과 Java 사이에 큰 차이가 있냐는 의견에도 동의하지만 말이다.)</p>\n<h2 data-ke-size=\"size26\">함께 보면 좋은 글</h2>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><a href=\"https://jojoldu.tistory.com/687\">다른 언어로 성장하기</a></li>\n</ul>",
        "contentSnippet": "JVM 컨퍼런스가 있으면 \"Java 세상에서 살던 사람이 처음 TS 세계를 만나고 느낀 여러가지 차이점\" 을 발표하려고했는데, 시간도 너무 지났고, 발표 준비하기도 쉽지 않은 것 같아 블로그에 시리즈로 시작한다.\nJava가 구린 언어다를 표현하기 위한 글이기 보다는 전작(다른 언어로 성장하기)과 마찬가지로 타 생태계를 통해 성장할 수 있음을 알리는 글이다\n자바를 처음 배울때 getter/setter 에 대한 이야기를 많이 들었다.\n캡슐화 등의 장점을 들으면서 클래스 안에는 항상 무분별하게 getter/setter 를 생성했다.\n(그때는 Lombok을 배우지 못했던 터라) IDE의 자동 생성 기능을 사용하면서 열심히 getter/setter를 생성했다.\n물론 예전부터 많은 분들은 무분별한 getter/setter를 사용하지말라는 글과 토론을 나누었다.\n2014.09.16 - getters-and-setters-are-evil\n다만, 내가 배울때 법칙?처럼 getter/setter 만든다로 배워서 그렇게 했다.\n(지금이야 getter/setter를 쓰지 않고, Tell, Don't Ask 를 당연한 것처럼 다뤄지고 있지만 말이다.)\n여튼, 배우던 당시에는 무분별하게 getter/setter를 만들다보니 \"어차피 단순한 data holders로 사용하는데 그냥 public으로 열면 안되나\" 같은 생각이 들었다.\n이를 테면 다음과 같다.\n매번 아래와 같이 생성하는데,\npublic class Course {\n    private int price;\n\n    public int getPrice() {\n        return price;\n    }\n\n    public void setPrice(int price) {\n        this.price = price;\n    }\n}\n아래와 같이 \"그냥 public 으로 열어두는 것과 무엇이 다르지?\" 라는 생각인 것이다.\npublic class Course {\n    public int price;\n}\n어차피 값을 넣고 빼는 역할 밖에 없는데, 똑같지 않나? 라는 생각을 하곤 했다.\n그러다가 기능이 여러가지로 확장되는 상황을 만나니 접근자를 통해서 사용해야하는 것의 중요성을 알게 되었다.\n예를 들어 단순히 price에 값을 넣기만 하면 되는 로직에서 아래와 같이 부가세 10%가 추가된 가격으로 반환해야한다는 조건이 추가되니 단순히 public 으로 열어두는 것은 기존의 코드를 모두 바꿔야하는 문제가 생겼다.\npublic class Course {\n    ...\n\n    public int getPrice() {\n        return (int) (price * 1.1); // 부가세 10% 적용\n    }\n}\n\ncoursePrice = course.price; // <<< 이렇게 호출되는 코드들이\ncoursePrice = course.getPrice(); // <<< 이 코드로 모두 변경되어야만 했다.\n접근자를 통해서 처음 구현해두지 않으니, 이런 경우가 발생했다.\n그래서 \"처음부터 접근자를 생성해두고, 이를 호출자들이 사용하도록 해야만 이런 문제를 겪지 않는 다\" 는 것을 알게되었다.\n다만, 그럼 \"매번 이렇게 접근자를 생성해두고 코드를 작성해야하는 불편함과 귀찮음은 어쩔 수 없는 것인가\" 라는 생각은 계속 들었다.\n그러던 중, JavaScript/TypeScript 코드들을 보게 되었는데, 여기서는 get/set 접근자를 언어 레벨에서 지원했다.\n즉, 아래 2개 코드는 호출자 입장에서 동일한 방법으로 호출할 수 있다.\n// 1. 접근자 없이 public 필드\nclass Course {\n    public price: number;\n\n    constructor(price: number) {\n        this.price = price;\n    }\n}\n\n// 2. 접근자를 통한 접근\nclass Course {\n    private _price: number;\n\n    ...\n\n    get price(): number {\n        return this._price;\n    }\n}\n\nval price = course.price; // 1, 2 모두 동일한 코드로 호출\n이건 코틀린도 동일한데, 코틀린 역시 get/set 접근자를 언어 레벨에서 지원한다.\n// 1. 접근자 없이 public 필드\nclass Course(var price: Int)\n\n// 2. 접근자를 통한 접근\nclass Course(private var _price: Int) {\n    var price: Int\n        get() = (_price * 1.1).toInt() // 부가세 10% 적용\n}\n\nval coursePrice = course.price\n(코틀린과 자바를 굳이 다른 생태계라고 구분하지는 않지만)\n이런 개념이 요즘의 모던한 언어들에게는 기본적으로 내장되어있다.\nJava를 처음 배우는 입장에서는 \"public 필드를 사용하면 안되고, private 필드를 사용하고 이에 대한 접근자를 무조건 생성해야한다\" 를 배워야만 했다.\n이걸 그나마 편하게 하기 위해 Lombok 라이브러리 등의 도입도 알아야만 한다.\n만약 이 모든걸 무시하고 처음 배운 클래스에서 단순히 public 필드를 사용하면 이후에 큰 비용을 지불하게 된다.\n반면, JS, TS, Kotlin 등의 언어를 배우는 입장에서는 이런 것이 고민 거리이자 배워야할 내용이 되지 않는다.\npublic 필드를 사용하는 것과 접근자를 통한 접근이 모두 동일한 인터페이스를 지원하기 때문에 내가 알게되는 지식이 늘어난다고해서 기존의 코드들이 전체 교체 될 일은 거의 없다.\n즉, 프로그래밍을 배우고 사용할때 중요한 고민거리가 하나 없어지게 되는 셈이다.\n이런 사례는 언어 레벨에서 좋은 디자인 인터페이스를 지원한다 라는 의미로 다가왔다.\n처음 클래스를 배울때 단순하게 public으로 모든 필드를 선언하고 코드를 작성하더라도 이후에 접근자 (accessor) 개념을 배우고 클래스 코드를 변경하더라도 기존 호출자 코드에 변경이 필요하지 않는다는 것을 초보 개발자들에게 알려줄 수 있는 셈이다.\n이걸로 변경에 유연한 디자인을 아주 쉽게 익히게 된다.\nLombok 같은 외부의 라이브러리를 도입해야한다거나,\n접근자 (accessor) 에 대한 개념을 배운다던가 그런 언어 외적인 추가적인 학습없이 말이다.\n마무리\n좋은 언어는 그 자체로 좋은 디자인을 배울 수 있어야 한다는 생각을 한다.\n그런면에서 문법 설탕에 불과하다고 이야기하는 사람들도 있지만, 특정 프레임워크, 라이브러리의 도움으로 해결하는 문제들을 언어 레벨에서 직접 해결해주고 있다는 점이 참 좋았다.\n그리고 그런 개념들이 TS, Kotlin 등 Java가 아닌 다른 언어에서는 당연하게도 들어가 있다는 점이 섬세하게도 느껴졌다.\n(물론 Kotlin과 Java 사이에 큰 차이가 있냐는 의견에도 동의하지만 말이다.)\n함께 보면 좋은 글\n다른 언어로 성장하기",
        "guid": "https://jojoldu.tistory.com/801",
        "categories": [
          "Architecture",
          "Java",
          "kotlin",
          "node.js",
          "typescript",
          "인터페이스",
          "자바",
          "코틀린",
          "타입스크립트"
        ],
        "isoDate": "2024-08-26T03:43:13.000Z"
      }
    ]
  },
  {
    "name": "WestwoodForever's Dev Log",
    "category": "개인",
    "posts": []
  },
  {
    "name": "허니몬(Honeymon)의 자바guru",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Wolf Loves Fox :: 일상",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Game Programmer Life",
    "category": "개인",
    "posts": []
  },
  {
    "name": "IT 프리랜서 이야기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "yuchi's dev",
    "category": "개인",
    "posts": [
      {
        "creator": "megayuchi",
        "title": "LightMap 구현전략",
        "link": "https://megayuchi.com/2024/08/22/lightmap-%ea%b5%ac%ed%98%84%ec%a0%84%eb%9e%b5/",
        "pubDate": "Thu, 22 Aug 2024 14:10:10 +0000",
        "content:encodedSnippet": "2024년 8월 22일 방송분입니다.",
        "dc:creator": "megayuchi",
        "comments": "https://megayuchi.com/2024/08/22/lightmap-%ea%b5%ac%ed%98%84%ec%a0%84%eb%9e%b5/#respond",
        "content": "2024년 8월 22일 방송분입니다.",
        "contentSnippet": "2024년 8월 22일 방송분입니다.",
        "guid": "http://megayuchi.com/?p=7047",
        "categories": [
          "Development",
          "Game Engine Development",
          "GPU"
        ],
        "isoDate": "2024-08-22T14:10:10.000Z"
      }
    ]
  },
  {
    "name": "만화로 나누는 자유/오픈소스 소프트웨어 이야기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "신현석(Hyeonseok Shin)",
    "category": "개인",
    "posts": [
      {
        "title": "Puppeteer로 웹페이지 열고 조작하기",
        "link": "https://hyeonseok.com/blog/916",
        "pubDate": "Sun, 25 Aug 2024 17:26:34 GMT",
        "content": "<p>화면 없이 웹 브라우징을 할 수 있는 제품으로 기억나는 최초는 <a href=\"https://phantomjs.org/\">팬텀(PhantomJS)</a>이다. 화면도 없이 브라우저를 열고 스크린 샷도 찍고 탐색하는 것이 엄청 신기했다. 지금은 개발이 중단되었다. 팬텀의 불편함을 극복해보고자 <a href=\"https://github.com/casperjs/casperjs\">캐스퍼(CasperJS)</a>도 나왔었는데 이것 역시 지금은 개발이 중단되었다. 아마도 <a href=\"https://pptr.dev/\">퍼피티어(Puppeteer)</a>와 <a href=\"https://playwright.dev/\">플레이라이트(Playwright)</a>같은 대안 프로젝트들이 구글과 마이크로소프트 같은 큰 회사에서 나와서 그렇게 된 것 같다.</p>\r\n\r\n<p>플레이라이트가 퍼피티어를 포크해서 나왔다고 하는데 두 프로젝트는 지향하는 바가 다르다. 퍼피티어가 헤드리스 크롬이나 <a href=\"https://hacks.mozilla.org/2024/08/puppeteer-support-for-firefox/\">파이어폭스</a>를 편하게 활용할 수 있게 고안되었다면 플레이라이트는 테스트를 위한 스위트에 가깝다. 나는 테스트를 돌리기 보다는 브라우저에서의 작업이 필요해서 퍼피티어를 선택했다.</p>\r\n\r\n<p>API는 상당히 직관적이어서 <a href=\"https://pptr.dev/#example\">예제</a>를 보면 바로 브라우저를 실행해서 테스트 해 볼 수 있다. 브라우저 열고 페이지 열고 등 코드만 봐도 바로 이해가 간다. 나는 처리해야 하는 작업이 많아서 별도 스크립트를 <a href=\"https://pptr.dev/api/puppeteer.page.__eval/\"><code>page.$$eval</code></a>을 사용해서 실행했다.</p>\r\n\r\n<pre><code>const imageCount = await page.$$eval(\"img\", (imgs) => imgs.length)</code></pre>\r\n\r\n<p>서로 다른 컨텍스트를 직관적으로 연결해줘서 아주 사용하기 편하다. 예전과 비교하면 정말 좋아졌다.</p>\r\n\r\n<p>페이지를 열려면 <code>const browser = await puppeteer.launch()</code>로 브라우저를 실행하고 <code>const page = await browser.newPage()</code>로 페이지를 열어서 사용하게 된다. 처음에는 매번 브라우저를 여는 시간도 아깝고 보통 브라우저 사용하듯이 하나 실행해 놓고 페이지만 열고 닫으며 사용해봤다. 이렇게 하니 평소에 브라우저가 메모리 많이 사용하고 있다고 욕하던 상황이 서버에서 벌어져서 서버가 죽었다. 프로세스가 열리는 것을 보니 페이지 하나만 열어도 크롬 관련 프로세스가 열개가 넘게 뜨고 <code>await page.close()</code>로 페이지를 닫아줘도 일부 프로세스들이 계속 남아 있었다. 결국 계속 쓸수록 메모리 점유가 늘어났다. <code>await browser.close()</code>로 실행된 브라우저를 닫아줘야 프로세스가 모두 정리가 된다.</p>\r\n\r\n<p><a href=\"https://accessibility.kr/\">accessibility.kr</a>에서 지금까지는 자바스크립트로 생성되는 페이지를 검사할 수 없었는데 이제 검사할 수 있는 사이트가 많이 늘어났다. 리다이렉션 처리도 많이 개선되었다.</p>",
        "contentSnippet": "화면 없이 웹 브라우징을 할 수 있는 제품으로 기억나는 최초는 팬텀(PhantomJS)이다. 화면도 없이 브라우저를 열고 스크린 샷도 찍고 탐색하는 것이 엄청 신기했다. 지금은 개발이 중단되었다. 팬텀의 불편함을 극복해보고자 캐스퍼(CasperJS)도 나왔었는데 이것 역시 지금은 개발이 중단되었다. 아마도 퍼피티어(Puppeteer)와 플레이라이트(Playwright)같은 대안 프로젝트들이 구글과 마이크로소프트 같은 큰 회사에서 나와서 그렇게 된 것 같다.\n\r\n\r\n플레이라이트가 퍼피티어를 포크해서 나왔다고 하는데 두 프로젝트는 지향하는 바가 다르다. 퍼피티어가 헤드리스 크롬이나 파이어폭스를 편하게 활용할 수 있게 고안되었다면 플레이라이트는 테스트를 위한 스위트에 가깝다. 나는 테스트를 돌리기 보다는 브라우저에서의 작업이 필요해서 퍼피티어를 선택했다.\n\r\n\r\nAPI는 상당히 직관적이어서 예제를 보면 바로 브라우저를 실행해서 테스트 해 볼 수 있다. 브라우저 열고 페이지 열고 등 코드만 봐도 바로 이해가 간다. 나는 처리해야 하는 작업이 많아서 별도 스크립트를 page.$$eval을 사용해서 실행했다.\n\r\n\r\nconst imageCount = await page.$$eval(\"img\", (imgs) => imgs.length)\n\r\n\r\n서로 다른 컨텍스트를 직관적으로 연결해줘서 아주 사용하기 편하다. 예전과 비교하면 정말 좋아졌다.\n\r\n\r\n페이지를 열려면 const browser = await puppeteer.launch()로 브라우저를 실행하고 const page = await browser.newPage()로 페이지를 열어서 사용하게 된다. 처음에는 매번 브라우저를 여는 시간도 아깝고 보통 브라우저 사용하듯이 하나 실행해 놓고 페이지만 열고 닫으며 사용해봤다. 이렇게 하니 평소에 브라우저가 메모리 많이 사용하고 있다고 욕하던 상황이 서버에서 벌어져서 서버가 죽었다. 프로세스가 열리는 것을 보니 페이지 하나만 열어도 크롬 관련 프로세스가 열개가 넘게 뜨고 await page.close()로 페이지를 닫아줘도 일부 프로세스들이 계속 남아 있었다. 결국 계속 쓸수록 메모리 점유가 늘어났다. await browser.close()로 실행된 브라우저를 닫아줘야 프로세스가 모두 정리가 된다.\n\r\n\r\naccessibility.kr에서 지금까지는 자바스크립트로 생성되는 페이지를 검사할 수 없었는데 이제 검사할 수 있는 사이트가 많이 늘어났다. 리다이렉션 처리도 많이 개선되었다.",
        "guid": "https://hyeonseok.com/blog/916",
        "isoDate": "2024-08-25T17:26:34.000Z"
      }
    ]
  },
  {
    "name": "한상곤 - Sigmadream",
    "category": "개인",
    "posts": []
  },
  {
    "name": "개발자 울이 노트",
    "category": "개인",
    "posts": []
  },
  {
    "name": "즐거운 개발자 :: 네이버  블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "황제펭귄의 게임개발이야기 [여기는 한국]",
    "category": "개인",
    "posts": []
  },
  {
    "name": "LINE ENGINEERING",
    "category": "기업",
    "posts": [
      {
        "title": "Kafka와 ETL을 활용해 대용량 데이터 마이그레이션하기",
        "link": "https://techblog.lycorp.co.jp/ko/migrating-large-data-with-kafka-and-etl",
        "pubDate": "Mon, 26 Aug 2024 02:00:00 GMT",
        "content": "들어가며\n안녕하세요. LINE Plus에서 Global E-Commerce Platform 개발을 맡고 있는 장효택입니다. \nLINE Brand Catalog와 통합 커머스 검색 ...",
        "contentSnippet": "들어가며\n안녕하세요. LINE Plus에서 Global E-Commerce Platform 개발을 맡고 있는 장효택입니다. \nLINE Brand Catalog와 통합 커머스 검색 ...",
        "guid": "https://techblog.lycorp.co.jp/ko/migrating-large-data-with-kafka-and-etl",
        "isoDate": "2024-08-26T02:00:00.000Z"
      }
    ]
  },
  {
    "name": "뱅크샐러드 블로그",
    "category": "기업",
    "posts": [
      {
        "title": "이걸 진짜 만든다고요? 세상에 없던 게임, 샐러드게임 | 1편. BX",
        "link": "https://blog.banksalad.com/tech/banksalad-saladgame-1/",
        "pubDate": "Fri, 23 Aug 2024 00:00:00 GMT",
        "content": "“이걸 진짜 만든다고요? 🤯 ” 샐러드게임 탄생 배경 202…",
        "contentSnippet": "“이걸 진짜 만든다고요? 🤯 ” 샐러드게임 탄생 배경 202…",
        "guid": "https://blog.banksalad.com/tech/banksalad-saladgame-1/",
        "isoDate": "2024-08-23T00:00:00.000Z"
      },
      {
        "title": "이걸 진짜 만든다고요? 세상에 없던 게임, 샐러드게임 | 2편. UX",
        "link": "https://blog.banksalad.com/tech/banksalad-saladgame-2/",
        "pubDate": "Fri, 23 Aug 2024 00:00:00 GMT",
        "content": "…",
        "contentSnippet": "…",
        "guid": "https://blog.banksalad.com/tech/banksalad-saladgame-2/",
        "isoDate": "2024-08-23T00:00:00.000Z"
      }
    ]
  },
  {
    "name": "우아한형제들 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "TOAST Meetup",
    "category": "기업",
    "posts": [
      {
        "title": "NHN Cloud의 보안 가이드를 소개합니다",
        "link": "https://meetup.nhncloud.com/posts/384",
        "pubDate": "Sun, 25 Aug 2024 23:38:45 GMT",
        "content": "![NHN Cloud_meetup banner_security guides_202408_900.png](https://image.toast.com/aaaadh/real/2024/techblog/NHN%20Cloudmeetup%20bannersecurity%20guides202408900.png)\r\r\n\r\r\n## 들어가며\r\r\n\r\r\n다양한 산업 분야에서 클라우드 컴퓨팅의 확장성, 유연성, 데이터 분석 및 협업 기능을 활용하여 경쟁력을 강화하고, 서비스 운영 효율성을 높이고 있습니다. 이제 클라우드 전환은 선택이 아닌 필수가 된 것처럼 보입니다. 하지만 가상화 기술, 자원의 공유 및 인터넷을 통한 접속이라는 클라우드의 특성으로 인해 취약점 공격이나 악성 코드, DDoS와 같이 온프레미스 환경에서도 나타났던 보안 위협뿐만 아니라 기존에 없던 유형의 보안 위협도 다양하게 발생할 수 있다는 점에서 클라우드 보안에 대한 우려는 남아 있는 상태입니다.\r\r\n\r\r\n클라우드를 안전하게 이용하기 위해서는 온프레미스 인프라와 대비되는 클라우드의 구조와 보안 관리 방안을 정확히 이해하고, 클라우드 서비스 공급자와 이용자가 협력하여 각자의 책임 영역에서 적극적인 정보보호 활동을 수행할 수 있어야 합니다. 이를 [보안 책임 공유 모델(shared security responsibility, SSRM)](https://www.nhncloud.com/kr/security-center/ssrm)이라고 하는데요. 이 모델에서는 안전한 클라우드 서비스를 위해 클라우드 서비스 공급자와 이용자의 책임과 역할을 분류하고, 각자가 통제하는 요소를 보호하여 클라우드 전반의 안전하고 신뢰성 있는 보안을 확립합니다.\r\r\n\r\r\n보안 책임 공유 모델에서 클라우드 서비스 이용자는 가상 리소스와 데이터 보안을 위해 클라우드 및 보안 서비스를 활용해 보안을 고려한 서비스를 구성하고 관리할 책임을 갖습니다. 이를 위해서는 온프레미스 환경과 대비되는 클라우드 컴퓨팅 환경의 특성과 보안 전반에 대한 이해가 선행되어야 합니다. NHN Cloud는 보안을 위한 다양한 서비스와 기능을 제공할 뿐만 아니라, 다양한 보안 가이드를 제공하여 이용자가 데이터, 애플리케이션, 운영체제 단에서 직접 수행해야 하는 작업이나 반드시 숙지해야 할 사항 등을 안내합니다. 이를 통해 클라우드 서비스 이용자 책임 영역의 보안을 강화하고, 클라우드에 대한 위협과 대응 방안, 클라우드 영역별 보안, 정보보호 등에 대한 이해를 돕기 위한 노력을 지속하고 있습니다.\r\r\n\r\r\n이 글에서는 [NHN Cloud 보안 센터](https://www.nhncloud.com/kr/security-center)에서 제공하는 다양한 보안 가이드의 종류를 알아보고, 상황과 필요에 따라 적합한 문서를 활용하실 수 있도록 각 가이드에 대해 소개하고자 합니다.\r\r\n\r\r\n## NHN Cloud 보안 가이드의 종류와 소개\r\r\n\r\r\n![02_보안가이드메인.png](https://image.toast.com/aaaadh/real/2024/techblog/02uBCF4uC548uAC00uC774uB4DCuBA54uC778.png)\r\r\n\r\r\n### **NHN Cloud 보안 백서**\r\r\n\r\r\n고객이 클라우드 환경의 보안을 쉽게 이해할 수 있도록 돕고, NHN Cloud가 제공하는 보안 체계와 기술을 안내하기 위해 제작된 「NHN Cloud 보안 백서」는 NHN Cloud의 이름으로 발행된 첫 번째 백서입니다. 온프레미스 인프라와 대비되는 클라우드 컴퓨팅 환경의 특성과 그 특성에 기인한 보안 위협, 클라우드 보안 전반에 대해 안내하고, NHN Cloud가 고객의 클라우드 인프라와 서비스, 이용자의 개인정보와 데이터를 보호하기 위해 제공하는 다양한 보안 서비스와 기능을 소개합니다. 클라우드 서비스 이용 시 적절한 기술적 보안 대책을 마련하는 데 참고하거나 취약점 관리, 위협 탐지 및 대응, 인증, 컴플라이언스, 접근 통제, 데이터 보호 및 암호화 서비스를 이용한 안전한 서비스 인프라를 구성하는 데도 활용할 수 있을 것으로 기대합니다.\r\r\n\r\r\n [NHN Cloud 보안 백서 다운로드 신청 페이지 바로 가기](https://info.nhncloud.com/security-whitepaper.html?utm_source=nhncloud&utm_medium=home&utm_campaign=whitepapers&utm_content=security01&utm_term=cta)\r\r\n<br>\r\r\n### **NHN Cloud 개인정보보호 준수 가이드**\r\r\n\r\r\n개인정보는 살아 있는 개인에 대한 정보로 이름, 주민등록번호, 영상 등을 통해 개인을 식별할 수 있는 정보를 의미합니다. 하나의 정보만으로 개인을 식별할 수 없더라도, 다른 정보와 결합하여 개인을 특정할 수 있다면 개인정보로 간주합니다. 인터넷 기술이 발전하고, 소셜 네트워킹, 쇼핑, 금융 거래 등 일상의 거의 모든 행위가 디지털화됨에 따라 우리는 수많은 개인정보를 온라인에 저장하고 공유하는데요. 개인정보가 유출될 경우 신원 도용이나 금융 사기, 프라이버시 침해 등 심각한 문제를 초래할 수 있기 때문에 개인정보보호는 점점 더 중요한 이슈로 부각되고 있습니다.\r\r\n개인정보보호법은 이러한 개인정보의 처리 및 보호에 관한 사항을 정하여 개인의 자유와 권리를 보호하고, 개인의 존엄과 가치를 구현하기 위해 제정된 법입니다. NHN Cloud는 고객이 클라우드 환경에서 개인정보보호법을 준수할 수 있도록 다양한 서비스와 기능을 제공합니다. 「NHN Cloud 개인정보보호 준수 가이드」는 개인정보 및 개인정보보호법이 무엇인지 살펴보고, 개인정보의 안전성 확보조치 기준에 대해 알아본 뒤 NHN Cloud가 제공하는 보안 서비스를 이용해 개인정보를 보호할 수 있는 방안을 안내합니다.\r\r\n\r\r\n [NHN  Cloud 개인정보보호 준수 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN%20Cloud_Personal_Information_Compliance_Guide_2024.pdf)\r\r\n<br>\r\r\n### **NHN Cloud 콘솔 보안 가이드**\r\r\n\r\r\n콘솔은 클라우드 서비스 이용자가 클라우드 서비스를 직접 생성, 사용, 관리, 삭제할 수 있는 접점이며 환경입니다. 따라서 클라우드 콘솔 보안은 매우 중요하며, 이용자가 직접 구성하고 관리한다는 점에서 클라우드 및 네트워크 환경과 보안 요소, 보안 관리 방법, 그리고 클라우드에서 제공하는 보안 기능에 대한 정확한 이해가 선행되어야 합니다.\r\r\n많은 CSP에서 클라우드 보안을 위한 다양한 기능들을 선보이고 있고, NHN Cloud 역시 Network Firewall, Security Groups, Network ACL, DDoS Guard, Security Monitoring, Webshell Threat Detector, Security Advisor, NHN Bastion 등의 서비스를 제공하며 안전한 클라우드 컴퓨팅 환경을 제공하기 위해 노력하고 있습니다. 「NHN Cloud 콘솔 보안 가이드」는 이와 같은 NHN Cloud의 다양한 서비스에서 제공하는 보안 기능을 이용하여 클라우드 환경에서 애플리케이션 및 서비스를 안전하게 구성할 수 있도록 권장되는 보안 설정들에 대해 안내합니다. 계정 관리, 네트워크 보안, 서버 보안, 스토리지 보안, 데이터베이스 보안, 데이터 보호 및 키 관리, 로깅 및 모니터링과 같은 8개 영역에서 점검하고 관리할 수 있는 항목들을 제시하고 설명하며, 클라우드 서비스 이용자는 이를 기준으로 NHN Cloud 콘솔에서 직접 보안 기능을 설정하고 점검할 수 있습니다.\r\r\n\r\r\n [NHN Cloud 콘솔 보안 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN_Cloud_Console_Security_Guide_2023.pdf)\r\r\n<br>\r\r\n### **NHN Cloud 네트워크 아키텍처 보안 가이드**\r\r\n\r\r\n「NHN Cloud 네트워크 아키텍처 보안 가이드」는 NHN Cloud에서 제공하는 다양한 네트워크 기능과 네트워크 보안 서비스를 이용하여 클라우드 환경에서 가상 인프라 및 서비스를 안전하게 구성하고 제공할 수 있는 네트워크 보안 아키텍처를 설명합니다.\r\r\n클라우드 환경의 네트워크 구성 시 따라야 할 네트워크 분리, 접근 제어, 위협 대응 관련 컴플라이언스를 정리하여 안내하며, 이에 기반한 네트워크 아키텍처 보안 고려 사항 및 보안 설계 기준을 제시합니다. 또한 클라우드 환경에서 기업이나 공공 시스템을 설계할 때 참고할 수 있는 네트워크 보안 아키텍처를 사례별로 제시하기 때문에 환경에 따라 다를 수 있는 네트워크 분리 기준과 방법, 분리된 구간의 연결과 접근 통제 방식 등을 좀더 구체적이고 다양한 관점에서 살펴보고 이해할 수 있습니다.\r\r\n\r\r\n [NHN Cloud 네트워크 아키텍처 보안 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN_Cloud_Network_Architecture_Security_Guide_2024.pdf)\r\r\n<br>\r\r\n### **NHN Cloud 랜섬웨어 대응 가이드**\r\r\n\r\r\n랜섬웨어(ransomware)란 사용자의 컴퓨터 시스템을 장악하거나 데이터를 암호화하여 정상적으로 사용하지 못하게 만든 뒤 암호 키 또는 해제 방법을 알려주는 대가로 금전을 요구하는 악성 코드입니다. 최근 몇 년간 랜섬웨어가 사이버 공격의 주요 형태로 떠오르며 기업과 공공 기관, 개인 모두에게 심각한 피해를 입히고 있습니다.\r\r\n「NHN Cloud 랜섬웨어 대응 가이드」는 일반 악성코드와 대비되는 랜섬웨어의 특성과 공격 방식 및 유형을 살펴보고, 랜섬웨어가 사용하는 기술이 무엇인지 설명합니다. 그리고 랜섬웨어에 대비하여 클라우드 서비스 이용자가 취할 수 있는 효과적인 대응 전략과 방안을 제시합니다.\r\r\n\r\r\n [NHN Cloud 랜섬웨어 대응 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN_Cloud_Ransomware_Response_Guide_2024.pdf)\r\r\n<br>\r\r\n### **NHN Cloud 컨테이너 보안 가이드**\r\r\n\r\r\n컨테이너란 데스크톱, 온프레미스, 클라우드 등 어떤 환경에도 구애 받지 않고 실행할 수 있도록 애플리케이션 코드를 해당 라이브러리 및 종속 항목과 함께 패키징한 소프트웨어 실행 유닛입니다. NHN Cloud는 이러한 컨테이너 환경에서 다양한 애플리케이션을 개발하고 운영할 수 있도록 NHN Kubernetes Service(NKS), NHN Container Registry(NCR), NHN Container Service(NCS)를 제공합니다. 「NHN Cloud 컨테이너 보안 가이드」는 NHN Cloud의 컨테이너 서비스에서 제공하는 보안 기능과 Kubernetes 보안 방법을 소개합니다. 본 가이드를 통해 컨테이너의 개념과 주요 구성 요소, 컨테이너 환경의 보안 위협과 보호 대상, 컨테이너 환경과 컨테이너를 기반으로 서비스하는 워크로드를 보호하는 방법 등을 이해할 수 있습니다.\r\r\n\r\r\n [NHN Cloud 컨테이너 보안 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN%20Cloud_Container%20Security%20Guide_2024.pdf)\r\r\n<br>\r\r\n## 마치며\r\r\n\r\r\n여기까지 NHN Cloud의 다양한 보안 가이드의 종류와 활용에 대해 알아보았는데요. 2024년 8월 기준, NHN Cloud는 위와 같이 「NHN Cloud 보안 백서」, 「NHN Cloud 개인정보보호 준수 가이드」, 「NHN Cloud 콘솔 보안 가이드」, 「NHN Cloud 네트워크 아키텍처 가이드」, 「NHN Cloud 랜섬웨어 대응 가이드」, 「NHN Cloud 컨테이너 보안 가이드」를 제공하고 있습니다. 보안 가이드는 NHN Cloud를 활용해 서비스를 개발하고 운영하시는 아키텍트 및 보안 업무 담당자, NHN Cloud를 도입하고 구축하고자 하는 기업의 IT 담당자, 그리고 클라우드와 클라우드 환경 전반의 보안에 대해 알고 싶은 분들 모두가 자유롭게 이용하실 수 있으며, [NHN Cloud 홈페이지 > 보안 센터 > 보안 가이드](https://www.nhncloud.com/kr/security-center/guide) 페이지에서 확인하실 수 있습니다.\r\r\n\r\r\n긴 글을 읽어 주셔서 감사드리며, 앞으로도 NHN Cloud에서 제공하는 다양한 보안 가이드에 대해 많은 기대와 관심을 부탁드립니다. \r\r\n\r\r\n[![NHN Cloud_meetup banner_footer_black_202408_900.png](https://image.toast.com/aaaadh/real/2024/techblog/NHN%20Cloudmeetup%20bannerfooterblack202408900.png)](https://www.nhncloud.com/kr)",
        "contentSnippet": "![NHN Cloud_meetup banner_security guides_202408_900.png](https://image.toast.com/aaaadh/real/2024/techblog/NHN%20Cloudmeetup%20bannersecurity%20guides202408900.png)\r\r\n\r\r\n## 들어가며\r\r\n\r\r\n다양한 산업 분야에서 클라우드 컴퓨팅의 확장성, 유연성, 데이터 분석 및 협업 기능을 활용하여 경쟁력을 강화하고, 서비스 운영 효율성을 높이고 있습니다. 이제 클라우드 전환은 선택이 아닌 필수가 된 것처럼 보입니다. 하지만 가상화 기술, 자원의 공유 및 인터넷을 통한 접속이라는 클라우드의 특성으로 인해 취약점 공격이나 악성 코드, DDoS와 같이 온프레미스 환경에서도 나타났던 보안 위협뿐만 아니라 기존에 없던 유형의 보안 위협도 다양하게 발생할 수 있다는 점에서 클라우드 보안에 대한 우려는 남아 있는 상태입니다.\r\r\n\r\r\n클라우드를 안전하게 이용하기 위해서는 온프레미스 인프라와 대비되는 클라우드의 구조와 보안 관리 방안을 정확히 이해하고, 클라우드 서비스 공급자와 이용자가 협력하여 각자의 책임 영역에서 적극적인 정보보호 활동을 수행할 수 있어야 합니다. 이를 [보안 책임 공유 모델(shared security responsibility, SSRM)](https://www.nhncloud.com/kr/security-center/ssrm)이라고 하는데요. 이 모델에서는 안전한 클라우드 서비스를 위해 클라우드 서비스 공급자와 이용자의 책임과 역할을 분류하고, 각자가 통제하는 요소를 보호하여 클라우드 전반의 안전하고 신뢰성 있는 보안을 확립합니다.\r\r\n\r\r\n보안 책임 공유 모델에서 클라우드 서비스 이용자는 가상 리소스와 데이터 보안을 위해 클라우드 및 보안 서비스를 활용해 보안을 고려한 서비스를 구성하고 관리할 책임을 갖습니다. 이를 위해서는 온프레미스 환경과 대비되는 클라우드 컴퓨팅 환경의 특성과 보안 전반에 대한 이해가 선행되어야 합니다. NHN Cloud는 보안을 위한 다양한 서비스와 기능을 제공할 뿐만 아니라, 다양한 보안 가이드를 제공하여 이용자가 데이터, 애플리케이션, 운영체제 단에서 직접 수행해야 하는 작업이나 반드시 숙지해야 할 사항 등을 안내합니다. 이를 통해 클라우드 서비스 이용자 책임 영역의 보안을 강화하고, 클라우드에 대한 위협과 대응 방안, 클라우드 영역별 보안, 정보보호 등에 대한 이해를 돕기 위한 노력을 지속하고 있습니다.\r\r\n\r\r\n이 글에서는 [NHN Cloud 보안 센터](https://www.nhncloud.com/kr/security-center)에서 제공하는 다양한 보안 가이드의 종류를 알아보고, 상황과 필요에 따라 적합한 문서를 활용하실 수 있도록 각 가이드에 대해 소개하고자 합니다.\r\r\n\r\r\n## NHN Cloud 보안 가이드의 종류와 소개\r\r\n\r\r\n![02_보안가이드메인.png](https://image.toast.com/aaaadh/real/2024/techblog/02uBCF4uC548uAC00uC774uB4DCuBA54uC778.png)\r\r\n\r\r\n### **NHN Cloud 보안 백서**\r\r\n\r\r\n고객이 클라우드 환경의 보안을 쉽게 이해할 수 있도록 돕고, NHN Cloud가 제공하는 보안 체계와 기술을 안내하기 위해 제작된 「NHN Cloud 보안 백서」는 NHN Cloud의 이름으로 발행된 첫 번째 백서입니다. 온프레미스 인프라와 대비되는 클라우드 컴퓨팅 환경의 특성과 그 특성에 기인한 보안 위협, 클라우드 보안 전반에 대해 안내하고, NHN Cloud가 고객의 클라우드 인프라와 서비스, 이용자의 개인정보와 데이터를 보호하기 위해 제공하는 다양한 보안 서비스와 기능을 소개합니다. 클라우드 서비스 이용 시 적절한 기술적 보안 대책을 마련하는 데 참고하거나 취약점 관리, 위협 탐지 및 대응, 인증, 컴플라이언스, 접근 통제, 데이터 보호 및 암호화 서비스를 이용한 안전한 서비스 인프라를 구성하는 데도 활용할 수 있을 것으로 기대합니다.\r\r\n\r\r\n [NHN Cloud 보안 백서 다운로드 신청 페이지 바로 가기](https://info.nhncloud.com/security-whitepaper.html?utm_source=nhncloud&utm_medium=home&utm_campaign=whitepapers&utm_content=security01&utm_term=cta)\r\r\n\r\r\n### **NHN Cloud 개인정보보호 준수 가이드**\r\r\n\r\r\n개인정보는 살아 있는 개인에 대한 정보로 이름, 주민등록번호, 영상 등을 통해 개인을 식별할 수 있는 정보를 의미합니다. 하나의 정보만으로 개인을 식별할 수 없더라도, 다른 정보와 결합하여 개인을 특정할 수 있다면 개인정보로 간주합니다. 인터넷 기술이 발전하고, 소셜 네트워킹, 쇼핑, 금융 거래 등 일상의 거의 모든 행위가 디지털화됨에 따라 우리는 수많은 개인정보를 온라인에 저장하고 공유하는데요. 개인정보가 유출될 경우 신원 도용이나 금융 사기, 프라이버시 침해 등 심각한 문제를 초래할 수 있기 때문에 개인정보보호는 점점 더 중요한 이슈로 부각되고 있습니다.\r\r\n개인정보보호법은 이러한 개인정보의 처리 및 보호에 관한 사항을 정하여 개인의 자유와 권리를 보호하고, 개인의 존엄과 가치를 구현하기 위해 제정된 법입니다. NHN Cloud는 고객이 클라우드 환경에서 개인정보보호법을 준수할 수 있도록 다양한 서비스와 기능을 제공합니다. 「NHN Cloud 개인정보보호 준수 가이드」는 개인정보 및 개인정보보호법이 무엇인지 살펴보고, 개인정보의 안전성 확보조치 기준에 대해 알아본 뒤 NHN Cloud가 제공하는 보안 서비스를 이용해 개인정보를 보호할 수 있는 방안을 안내합니다.\r\r\n\r\r\n [NHN  Cloud 개인정보보호 준수 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN%20Cloud_Personal_Information_Compliance_Guide_2024.pdf)\r\r\n\r\r\n### **NHN Cloud 콘솔 보안 가이드**\r\r\n\r\r\n콘솔은 클라우드 서비스 이용자가 클라우드 서비스를 직접 생성, 사용, 관리, 삭제할 수 있는 접점이며 환경입니다. 따라서 클라우드 콘솔 보안은 매우 중요하며, 이용자가 직접 구성하고 관리한다는 점에서 클라우드 및 네트워크 환경과 보안 요소, 보안 관리 방법, 그리고 클라우드에서 제공하는 보안 기능에 대한 정확한 이해가 선행되어야 합니다.\r\r\n많은 CSP에서 클라우드 보안을 위한 다양한 기능들을 선보이고 있고, NHN Cloud 역시 Network Firewall, Security Groups, Network ACL, DDoS Guard, Security Monitoring, Webshell Threat Detector, Security Advisor, NHN Bastion 등의 서비스를 제공하며 안전한 클라우드 컴퓨팅 환경을 제공하기 위해 노력하고 있습니다. 「NHN Cloud 콘솔 보안 가이드」는 이와 같은 NHN Cloud의 다양한 서비스에서 제공하는 보안 기능을 이용하여 클라우드 환경에서 애플리케이션 및 서비스를 안전하게 구성할 수 있도록 권장되는 보안 설정들에 대해 안내합니다. 계정 관리, 네트워크 보안, 서버 보안, 스토리지 보안, 데이터베이스 보안, 데이터 보호 및 키 관리, 로깅 및 모니터링과 같은 8개 영역에서 점검하고 관리할 수 있는 항목들을 제시하고 설명하며, 클라우드 서비스 이용자는 이를 기준으로 NHN Cloud 콘솔에서 직접 보안 기능을 설정하고 점검할 수 있습니다.\r\r\n\r\r\n [NHN Cloud 콘솔 보안 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN_Cloud_Console_Security_Guide_2023.pdf)\r\r\n\r\r\n### **NHN Cloud 네트워크 아키텍처 보안 가이드**\r\r\n\r\r\n「NHN Cloud 네트워크 아키텍처 보안 가이드」는 NHN Cloud에서 제공하는 다양한 네트워크 기능과 네트워크 보안 서비스를 이용하여 클라우드 환경에서 가상 인프라 및 서비스를 안전하게 구성하고 제공할 수 있는 네트워크 보안 아키텍처를 설명합니다.\r\r\n클라우드 환경의 네트워크 구성 시 따라야 할 네트워크 분리, 접근 제어, 위협 대응 관련 컴플라이언스를 정리하여 안내하며, 이에 기반한 네트워크 아키텍처 보안 고려 사항 및 보안 설계 기준을 제시합니다. 또한 클라우드 환경에서 기업이나 공공 시스템을 설계할 때 참고할 수 있는 네트워크 보안 아키텍처를 사례별로 제시하기 때문에 환경에 따라 다를 수 있는 네트워크 분리 기준과 방법, 분리된 구간의 연결과 접근 통제 방식 등을 좀더 구체적이고 다양한 관점에서 살펴보고 이해할 수 있습니다.\r\r\n\r\r\n [NHN Cloud 네트워크 아키텍처 보안 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN_Cloud_Network_Architecture_Security_Guide_2024.pdf)\r\r\n\r\r\n### **NHN Cloud 랜섬웨어 대응 가이드**\r\r\n\r\r\n랜섬웨어(ransomware)란 사용자의 컴퓨터 시스템을 장악하거나 데이터를 암호화하여 정상적으로 사용하지 못하게 만든 뒤 암호 키 또는 해제 방법을 알려주는 대가로 금전을 요구하는 악성 코드입니다. 최근 몇 년간 랜섬웨어가 사이버 공격의 주요 형태로 떠오르며 기업과 공공 기관, 개인 모두에게 심각한 피해를 입히고 있습니다.\r\r\n「NHN Cloud 랜섬웨어 대응 가이드」는 일반 악성코드와 대비되는 랜섬웨어의 특성과 공격 방식 및 유형을 살펴보고, 랜섬웨어가 사용하는 기술이 무엇인지 설명합니다. 그리고 랜섬웨어에 대비하여 클라우드 서비스 이용자가 취할 수 있는 효과적인 대응 전략과 방안을 제시합니다.\r\r\n\r\r\n [NHN Cloud 랜섬웨어 대응 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN_Cloud_Ransomware_Response_Guide_2024.pdf)\r\r\n\r\r\n### **NHN Cloud 컨테이너 보안 가이드**\r\r\n\r\r\n컨테이너란 데스크톱, 온프레미스, 클라우드 등 어떤 환경에도 구애 받지 않고 실행할 수 있도록 애플리케이션 코드를 해당 라이브러리 및 종속 항목과 함께 패키징한 소프트웨어 실행 유닛입니다. NHN Cloud는 이러한 컨테이너 환경에서 다양한 애플리케이션을 개발하고 운영할 수 있도록 NHN Kubernetes Service(NKS), NHN Container Registry(NCR), NHN Container Service(NCS)를 제공합니다. 「NHN Cloud 컨테이너 보안 가이드」는 NHN Cloud의 컨테이너 서비스에서 제공하는 보안 기능과 Kubernetes 보안 방법을 소개합니다. 본 가이드를 통해 컨테이너의 개념과 주요 구성 요소, 컨테이너 환경의 보안 위협과 보호 대상, 컨테이너 환경과 컨테이너를 기반으로 서비스하는 워크로드를 보호하는 방법 등을 이해할 수 있습니다.\r\r\n\r\r\n [NHN Cloud 컨테이너 보안 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN%20Cloud_Container%20Security%20Guide_2024.pdf)\r\r\n\r\r\n## 마치며\r\r\n\r\r\n여기까지 NHN Cloud의 다양한 보안 가이드의 종류와 활용에 대해 알아보았는데요. 2024년 8월 기준, NHN Cloud는 위와 같이 「NHN Cloud 보안 백서」, 「NHN Cloud 개인정보보호 준수 가이드」, 「NHN Cloud 콘솔 보안 가이드」, 「NHN Cloud 네트워크 아키텍처 가이드」, 「NHN Cloud 랜섬웨어 대응 가이드」, 「NHN Cloud 컨테이너 보안 가이드」를 제공하고 있습니다. 보안 가이드는 NHN Cloud를 활용해 서비스를 개발하고 운영하시는 아키텍트 및 보안 업무 담당자, NHN Cloud를 도입하고 구축하고자 하는 기업의 IT 담당자, 그리고 클라우드와 클라우드 환경 전반의 보안에 대해 알고 싶은 분들 모두가 자유롭게 이용하실 수 있으며, [NHN Cloud 홈페이지 > 보안 센터 > 보안 가이드](https://www.nhncloud.com/kr/security-center/guide) 페이지에서 확인하실 수 있습니다.\r\r\n\r\r\n긴 글을 읽어 주셔서 감사드리며, 앞으로도 NHN Cloud에서 제공하는 다양한 보안 가이드에 대해 많은 기대와 관심을 부탁드립니다. \r\r\n\r\r\n[![NHN Cloud_meetup banner_footer_black_202408_900.png](https://image.toast.com/aaaadh/real/2024/techblog/NHN%20Cloudmeetup%20bannerfooterblack202408900.png)](https://www.nhncloud.com/kr)",
        "isoDate": "2024-08-25T23:38:45.000Z"
      }
    ]
  },
  {
    "name": "ZUM 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "SK Planet",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Spoqa tech blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "팀 왈도 번역팀",
    "category": "게임",
    "posts": []
  },
  {
    "name": "이한",
    "category": "개인",
    "posts": []
  },
  {
    "name": "근원님",
    "category": "개인",
    "posts": []
  },
  {
    "name": "황의윤",
    "category": "개인",
    "posts": []
  },
  {
    "name": "호돌맨",
    "category": "개인",
    "posts": []
  },
  {
    "name": "박우빈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "문다영",
    "category": "개인",
    "posts": []
  },
  {
    "name": "유수민",
    "category": "개인",
    "posts": []
  },
  {
    "name": "안건주",
    "category": "개인",
    "posts": []
  },
  {
    "name": "손현호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "STARTUP BIBLE",
    "category": "개인",
    "posts": [
      {
        "creator": "Kihong Bae",
        "title": "회사는 놀이터가 아니다",
        "link": "https://www.thestartupbible.com/2024/08/your-company-is-not-your-playground.html",
        "pubDate": "Sun, 25 Aug 2024 21:38:00 +0000",
        "content:encodedSnippet": "내가 약 5개월 전에 쓴 글 ‘개발자도 회사의 조직원이다’가 최근에 여기저기서 공유가 많이 된 것 같다. 뭐, 이곳은 내 개인적인 블로그라서 남 눈치 안 보고 그냥 내 생각을 끄적거리는데, 사람들이 세상을 보는 관점도 다르고, 특정 주제에 대한 생각도 달라서 그런지 많은 분들이 여러 가지 의견을 댓글로 남겨줬다.\n댓글, 댓글의 대댓글, 그리고 여기에 대한 주인장의 댓글을 모두 합치면 50개가 넘는 코멘트가 있다. 이 중, 그래도 이성적이고 논리적인 이야기가 가능한 분위기의 댓글에는 내가 최대한 진정성 있는 코멘트를 남겼는데, 그냥 개싸움이 될 것 같은 분위기의 댓글은 건드리지 않았다. 대신, 그런 코멘트에 대해서는 이번 포스팅을 통해서 아주 간략하게 내 생각을 종합적으로 다시 한번 공유하고 싶다.\n일단, 이 글에 이렇게 격한 반응을 해주신 걸 보니, 한국에도 돈을 많이 벌고 싶어 하고, 성공에 목마른 개발자들이 많은 것 같아서 너무 다행이다. 이런 분들이 더 많아져야지 스타트업도 잘 되고, 경쟁력 있는 회사들이 많이 나온다고 생각한다.\n한 가지 사과하고 명확하게 하고 싶은 건, 내가 개발자들을 공격하려는 의도로 이전 글을 쓴 건 아니라는 점이다. 기획자이든 마케터이든 개발자이든, 모든 직원은 회사의 조직원인데 굳이 개발자를 꼭 집어서 글을 썼던 이유는 내가 아는 대부분의 조직에선 제품을 만들고 판매해서 돈을 버는 핵심 업무를 하는 그룹 군에서 돈을 버는 기능에 가장 관심이 적은 조직이 개발 조직이기 때문이다. 물론, 이건 개인적인 관점이다.\n몇 개의 댓글을 읽어보면, 회사가 잘 돼 봤자 사장만 돈 버는데 내가 굳이 열심히 할 필요가 없다는 내용이다. 특히나 회사의 지분도 없는데. 이런 분들은 내 블로그에서 불평하지 말고, 소속된 회사의 사장과 이런 이야기를 하는 걸 권장한다. 회사에 돈을 벌어 주는 일을 열심히 하는 직원에게 스톡옵션 또는, 그 어떤 보상도 하지 않는 사장이라면 굳이 이런 회사에 계속 다닐 필욘 없을 것 같다. 그냥 다른 곳으로 가면 된다. 만약 본인이 열심히 일하지 않거나 실력이 없어서 보상받을 수준이 안되면 그냥 불평하지 말고 그 회사 계속 다니면 된다. 어쨌든 이런 불평을 하면서도 계속 그 회사에 다니고 있다면, 본인 자신의 실력을 의심해 봐야 한다.\n개발자로서 기술적 모험이 제한된다면 굳이 스타트업에 갈 필요가 없다고 한 분도 있다. 이런 의견에 대한 내 생각 두 가지를 공유한다. 일단 본인이 기술적 모험을 하고 싶다면, 그리고 이 모험이 회사의 비즈니스 방향과 크게 상관없다면(=돈을 벌 수 있는 기술이 아니라면) 이걸 허락하는 다른 곳으로 가면 된다. 그런데 돈 버는 거와 상관없는 기술적 모험을 허락하는 내가 아는 곳들은 학교 아니면 연구소다. 회사는 아닐 것이다. 또 다른 생각은, 스타트업을 포함한 모든 회사는 개발자들이 기술적인 모험을 하는 놀이터가 아니다. 남의 돈으로 빨리 돈을 벌어서 압축적인 성장을 해야 하는 조직이다. 회사는 돈 받고 그냥 하루 종일 놀다 퇴근하는 곳이 아니다.\n또한, 회사라는 조직은 분명히 회사라는 집단의 목표가 있고, 이를 달성해야 하지만, 어떤 분들이 주장하는 개인적인 발전도 동시에 균형 있게 가져가야 한다. 나도 이건 동의한다. 하지만, 우선순위를 매기자면 무조건 회사의 목표가 먼저이고, 이게 어느 정도 된 후에 회사의 목표를 같이 만드는 개인의 발전에 신경 써줄 수 있다. 회사의 목표는 무조건 돈 버는 게 돼야 하고, 여기에 먼저 동참할 수 없다면 개발자든 마케터든 회사에겐 부채가 되고, 부채는 가장 먼저 제거해야 하는 짐이 될 수밖에 없다.\n그리고 어떤 분들의 댓글을 보고 나는 정말로 이 사람들이 일하는 회사가 어딘지 궁금해지기도 했다. 그 회사 동료들이 너무 불쌍해서…\n이 글 밑에 분명히 멋진 댓글도 많이 달릴 거지만, 거지 같은 댓글도 많이 올라올 것이다. 그 수준과 정도를 종합적으로 판단해서, 필요하면 또 한 번 내 의견을 공유하는 포스팅을 올릴 계획이다. 그런데 키보드 뒤에서 인신공격적인 코멘트를 달거나, 너무 멍청한 코멘트를 다는 분들은 익명이 아니라 실명을 밝혀주시면 오히려 더 건설적인 이야기가 가능하지 않을까 싶다.",
        "dc:creator": "Kihong Bae",
        "comments": "https://www.thestartupbible.com/2024/08/your-company-is-not-your-playground.html#comments",
        "content": "내가 약 5개월 전에 쓴 글 ‘개발자도 회사의 조직원이다’가 최근에 여기저기서 공유가 많이 된 것 같다. 뭐, 이곳은 내 개인적인 블로그라서 남 눈치 안 보고 그냥 내 생각을 끄적거리는데, 사람들이 세상을 보는 관점도 다르고, 특정 주제에 대한 생각도 달라서 그런지 많은 분들이 여러 가지 의견을 댓글로 남겨줬다. 댓글, 댓글의 대댓글, 그리고 여기에 대한 주인장의 댓글을(...)",
        "contentSnippet": "내가 약 5개월 전에 쓴 글 ‘개발자도 회사의 조직원이다’가 최근에 여기저기서 공유가 많이 된 것 같다. 뭐, 이곳은 내 개인적인 블로그라서 남 눈치 안 보고 그냥 내 생각을 끄적거리는데, 사람들이 세상을 보는 관점도 다르고, 특정 주제에 대한 생각도 달라서 그런지 많은 분들이 여러 가지 의견을 댓글로 남겨줬다. 댓글, 댓글의 대댓글, 그리고 여기에 대한 주인장의 댓글을(...)",
        "guid": "https://www.thestartupbible.com/?p=9188",
        "categories": [
          "Uncategorized",
          "korea",
          "people",
          "technology",
          "vc",
          "스타트업 바이블 QA"
        ],
        "isoDate": "2024-08-25T21:38:00.000Z"
      },
      {
        "creator": "Kihong Bae",
        "title": "제품도 없는데 수익은 어떻게?",
        "link": "https://www.thestartupbible.com/2024/08/how-do-you-make-a-profit-when-you-dont-have-a-product.html",
        "pubDate": "Wed, 21 Aug 2024 21:24:00 +0000",
        "content:encodedSnippet": "얼마 전에 TechCrunch에서 배양육 산업 관련 기사를 읽었다. 우리도 국내 최초의 배양육 스타트업 셀미트에 투자했기 때문에, 관심을 갖고 기사를 정독했다. 기사의 제목은 “Even after $1.6B in VC money, the lab-grown meat industry is facing ‘massive’ issues” 였고, 내용은 암울했다.\n내용을 요약하자면, 너도나도 대체 단백질과 배양육 시장에 투자하기 바쁠 땐, 거의 묻지마 투자 수준으로 많은 돈이 이 시장에 투입됐지만, 연구개발에 생각보다 많은 돈과 시간이 필요하고, 이후에 관계 정부 부서의 승인 받는 것도 어렵다는 걸 이제 많은 사람들이 깨닫고 있다. 하지만, 가장 어려운 현실은, 연구개발을 하고 승인을 받아도, 결국 돈을 벌기 위해선 배양육 제품을 팔아야 하는데, 시장에서 수용할 수 있는 가격대에 대량 생산하기 위해서는 상상 이상의 돈이 설비와 공장에 투입돼야 하므로 투자자들이 이젠 이 분야를 등한시하고 있다는 내용이다.\n실은, 순수 소프트웨어 사업이 아닌, 사람의 개입이 필요한 operation이 필수인 사업도 비슷한 문제에 항상 직면해 있긴 하다. 멀리 볼 필요도 없고 가까운 스트롱 포트폴리오 네트워크에만 보더라도 이런 회사들이 수두룩하다. 대표적인 예가 모바일 세탁소 세탁특공대인데, 앱으로 세탁을 맡길 수 있지만, 결국엔 회사에서 세탁물을 수거해서 본인들이 직접 운영하는 세탁공장으로 운반하고, 여기서 세탁한 후에 다시 고객들에게 배송해야 한다. 분명 소프트웨어 비즈니스이지만, 사업의 절반 이상이 전통적인 물류와 공장 운영이다. 굉장히 돈이 많이 필요하고, 상상 이상의 돈이 설비와 공장에 투입되어야 한다는 점은 위에서 언급한 배양육 회사와 크게 다르지 않다.\n하지만, 한 가지 다른 점이 있다면, 세탁이라는 업은 첫 매출을 만들기 위한 R&D는 필요 없다. 사업을 개선해서 더 많은 매출을 만들기 위한 R&D는 있지만, 이게 없어도 세탁업은 시작할 수 있고, 매출을 만들 수 있다. 이렇게 만든 매출과 다른 의미 있는 수치를 기반으로 계속 적당한 밸류에이션에 투자 받으면서 사업을 키울 수 있다.\n하지만, 배양육 사업은 오랜 기간 동안 아주 무거운 R&D 과정을 거치지 않으면, 제품 자체가 만들어지지 않기 때문에, 실제로 돈을 버는 건 시작도 못 한다. 사업을 시작하지 못하고, 돈을 아예 못 벌면, 투자받는 게 쉽지 않다. 경기가 아주 좋을 땐, 기술력을 평가하고 미래의 수익성을 기반으로 좋은 조건에 큰 투자를 하는 투자자들이 꽤 있다. 실은 우리 투자사 셀미트를 비롯한 이 분야의 많은 회사들이 몇 년 전만 해도 이런 식으로 투자를 잘 받았다. 하지만, 요새 같은 불경기에 투자자들이 회사를 판단하는 가장 중요한 기준은 매출이다. 투자자들은 매출을 선호하고, 더 나아가서 수익을 선호한다. 이 상황에서 팔 제품 자체가 없는 스타트업은 어떻게 수익을 만들고, 어떻게 투자를 받을 수 있을까?\n아마도 이런 상황에 놓인 창업가들이 꽤 있을 것 같고, 최근에 이런 고민을 하는 분과 잠깐 이야기를 했는데, 이분이 나한테 열변을 토했다. “아니, 아직 제품도 없는데 어떻게 매출을 만드나요? 어떻게 우리 같은 회사의 밸류에이션을 매출을 기반으로 산정합니까? 그러면 우린 밸류에이션이 0인 회사인데요.”\n이분은 시드 투자를 받아서 한 2년 동안 열심히 R&D를 해고, 연구 결과도 좋고 방향도 좋아서 실제 제품을 만들고 매출을 발생시키기 위해 추가 투자를 받아야 하는데, 만나는 대부분의 투자자들이 매출이 없어서 거절하거나, 관심 있는 투자자는 매출이 없어서 (본인이 생각하기엔) 터무니없이 낮은 기업 가치를 제시하는 좋지 않은 상황에 부닥쳐있다.\n솔직히 나도 이분에게 특별히 해 줄 말이 없었다. 경기가 좋고 시장에 돈이 넘쳐흐를 땐, 제품도 없고 매출이 없어도 기술 그 자체나 시장의 가능성에 투자하는 VC들이 많았지만, 이젠 대부분의 VC들이 매출이 발생하는 회사를 선호하고, 어떤 VC는 매출로도 부족하고 손익분기를 해서 이익이 발생하는 회사에만 투자하고 있기 때문에 위에서 말한 창업가가 투자받는 건 정말 힘들기 때문이다. 만약에 이런 상황에 처한 창업가가 있다면, 그냥 최대한 많은 투자자를 만나서 제품과 매출이 없는 회사에도 투자하는 곳을 찾는 수밖에 없다. 만약에 운 좋게 이런 곳을 찾더라도, 회사의 밸류에이션과 투자 조건을 결정하는 건 전적으로 투자자의 특권이 될 수밖에 없다. 이 말을 쉽게 해석해 보면, 투자받는 것도 mission impossible이고, 운 좋게 우리 회사에 관심 갖는 투자자를 찾더라도 좋지 않은 조건에 투자받아야 한다는 의미다.\n입장 바꿔서 이야기를 한번 해보고 싶다. 즉, 이런 회사들을 자주 만나는 VC의 입장에서,,,실은 지금 이런 상황에 부닥친 회사에 투자하면, 정말 매력적인 조건에 투자할 수 있다. 이 회사에 살아 남아서 정말로 좋은 기술로 좋은 제품을 만들 수 있다면, 시간은 오래 걸리지만, 매출이 발생하기 시작하면 확실한 해자를 만들면서 성장하기 때문에 멈출 수 없을 정도로 매출이 잘 나올 것이다. 특히나, 이런 기술을 잘 이해하고, 이 시장을 잘 이해하고 있는 대기업의 전략적 투자 부서가 이런 플레이를 스마트하게 하면, 그 대기업의 미래 먹거리를 생각보다 쉽게 확보할 수도 있다.",
        "dc:creator": "Kihong Bae",
        "comments": "https://www.thestartupbible.com/2024/08/how-do-you-make-a-profit-when-you-dont-have-a-product.html#comments",
        "content": "얼마 전에 TechCrunch에서 배양육 산업 관련 기사를 읽었다. 우리도 국내 최초의 배양육 스타트업 셀미트에 투자했기 때문에, 관심을 갖고 기사를 정독했다. 기사의 제목은 “Even after $1.6B in VC money, the lab-grown meat industry is facing ‘massive’ issues” 였고, 내용은 암울했다. 내용을 요약하자면, 너도나도 대체 단백질과 배양육 시장에 투자하기 바쁠 땐, 거의 묻지마 투자 수준으로 많은 돈이(...)",
        "contentSnippet": "얼마 전에 TechCrunch에서 배양육 산업 관련 기사를 읽었다. 우리도 국내 최초의 배양육 스타트업 셀미트에 투자했기 때문에, 관심을 갖고 기사를 정독했다. 기사의 제목은 “Even after $1.6B in VC money, the lab-grown meat industry is facing ‘massive’ issues” 였고, 내용은 암울했다. 내용을 요약하자면, 너도나도 대체 단백질과 배양육 시장에 투자하기 바쁠 땐, 거의 묻지마 투자 수준으로 많은 돈이(...)",
        "guid": "https://www.thestartupbible.com/?p=9185",
        "categories": [
          "Uncategorized",
          "FoundersAtWork",
          "fundraising",
          "technology",
          "vc"
        ],
        "isoDate": "2024-08-21T21:24:00.000Z"
      }
    ]
  },
  {
    "name": "Build a Great Product",
    "category": "개인",
    "posts": []
  },
  {
    "name": "지금 써보러 갑니다",
    "category": "개인",
    "posts": []
  },
  {
    "name": "매거진 입맛",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "요즘 IT",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "쿠팡 엔지니어링",
    "category": "기업",
    "posts": []
  },
  {
    "name": "지마켓 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "리멤버 (드라마 앤 컴퍼니)",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Kimchi hill",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "Hudi.blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "토스",
    "category": "기업",
    "posts": [
      {
        "title": "토스, 개발자 컨퍼런스 ‘슬래시24’ 참가 신청 시작",
        "link": "https://blog.toss.im/article/slash24-conference",
        "pubDate": "Mon, 26 Aug 2024 04:56:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-uswsmm{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;margin:24px 0 8px;padding:0;list-style:none;}.css-uswsmm ul,.css-uswsmm ol{margin:16px 0 0;}.css-uswsmm>li{margin-bottom:16px;padding-left:24px;}.css-uswsmm>li:last-of-type{margin-bottom:0;}.css-uswsmm>li>span{position:relative;}.css-uswsmm>li>span>:first-child::before{content:'•';font-weight:500;color:var(--adaptiveGrey800);position:absolute;left:-24px;}\n.css-1hwiibq{font-size:17px;line-height:1.6;word-break:keep-all;letter-spacing:0em;font-weight:400;color:var(--adaptiveGrey800);}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}.css-q3ktjb{white-space:pre-wrap;font-weight:bold;}9월 12일(목) 코엑스에서 역대 최초 오프라인 개최… 개발자 45명 연사로 나서\n총 1,500명 추첨 통해 선발, 참가 접수 9월 2일까지\n.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n.css-1kxrhf3{white-space:pre-wrap;}토스가 개발자 컨퍼런스 '슬래시24(SLASH24, 이하 슬래시)'의 참가 신청을 시작했다.\n슬래시는 토스 커뮤니티의 개발자들이 기술적인 성취와 러닝을 공유하는 자리다. 4회째를 맞은 올해의 주제는 ‘No Limit: 풀지 못할 문제는 없다'로, 끊임없이 도전하고 시도하며 끝내 문제를 해결해 내는 토스의 개발 문화를 반영했다.\n이번 행사는 오프라인에서 최초로 개최된다는 점에서 특별하다. 9월 12일(목) 오전 10시부터 오후 4시까지 코엑스 그랜드볼룸에서 열린다. 토스, 토스뱅크, 토스증권, 토스페이먼츠, 토스플레이스 등 5개 법인에서 총 45명의 개발자들이 연사로 나선다.\n첫 번째 순서로는 토스의 테크놀로지 총괄 이형석 CTO가 발표를 진행한다. 이형석 CTO는 핀테크 업계의 지형을 바꿔온 토스가 혁신을 이루기 위해 겪은 도전과 실패를 이야기한다. 또한, 그 과정에서 토스 엔지니어들이 견지하는 태도와 이를 가능하게 하는 조직문화를 공유할 예정이다.\n메인 세션은 안드로이드(Android), 데이터(Data), 데브옵스(DevOps), 프론트엔드(Frontend), 인프라(Infra), Node.js, 파이썬(Python), 서버(Server), QA 등 총 9개 직군의 29개 발표로 구성했다. 스페셜 세션으로는 ‘빠르게 성장하고 싶은 주니어 개발자를 위한 소프트 스킬’, ‘팀에 위닝 멘탈리티를 불어넣는 리더십 스킬’, 토스뱅크와 토스증권의 CTO가 전하는 ‘미래의 CTO에게’ 등 3개 발표와 파트너사인 아마존웹서비스(AWS), 노션(Notion), 세일즈포스(Salesforce)의 발표가 준비돼 있다.\n다양한 부대 행사도 함께 마련했다. 슬래시 연사와 세션에 대한 질의나 기술적 고민을 나눌 수 있는 ‘데브챗(DevChat) 존’, 다양한 프로그램에서 4개 이상의 스탬프를 모으면 참여할 수 있는 ‘럭키드로우 존', 직무 상담이나 이력서에 대한 피드백을 받을 수 있는 ‘리크루팅 존'을 운영한다. 더불어 AWS, 베스핀글로벌, 노션, 세일즈포스, 시디즈 등 특별 협업 부스도 별도로 운영된다.\n이형석 CTO는 “이번 슬래시는 ‘No Limit’이라는 주제에 맞게 기술적 도전에 대한 토스 엔지니어들의 열정과 도전 정신을 함께 나눌 수 있는 자리가 될 것”이라며 “처음으로 오프라인에서 행사를 진행하는 만큼, 참가자들이 네트워킹을 통해 서로의 경험을 공유하고 더 큰 임팩트를 얻으실 수 있기를 기대한다”라고 전했다.\n참가 신청은 9월 2일(월) 23시 59분까지 토스 슬래시24 홈페이지에서 할 수 있다. 참가 신청을 완료하면 행사 종료 후 세션 발표 자료를 이메일로 제공한다. 신청한 인원 중 추첨을 통해 총 1,500명을 현장에 초대하며, 선정된 참가자에게는 9월 4일(수)에 개별적으로 안내를 진행할 예정이다.",
        "content": "“No Limit: 풀지 못할 문제는 없다”",
        "contentSnippet": "“No Limit: 풀지 못할 문제는 없다”",
        "guid": "https://blog.toss.im/article/slash24-conference",
        "isoDate": "2024-08-26T04:56:00.000Z"
      },
      {
        "title": "TDF, 우리 모두의 은퇴 준비 필수품",
        "link": "https://blog.toss.im/article/retirement-plans-05",
        "pubDate": "Fri, 23 Aug 2024 06:20:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-nv7vyi{margin:24px 0 8px;padding:16px 40px 32px;border-radius:16px;background-color:var(--adaptiveGrey100);}.css-123co55{font-size:19px;letter-spacing:0em;line-height:1.6;margin:24px 0 0;font-weight:400;color:var(--adaptiveGrey900);background-color:transparent;}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}.css-q3ktjb{white-space:pre-wrap;font-weight:bold;}이 글에서 알 수 있는 것들\n.css-uswsmm{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;margin:24px 0 8px;padding:0;list-style:none;}.css-uswsmm ul,.css-uswsmm ol{margin:16px 0 0;}.css-uswsmm>li{margin-bottom:16px;padding-left:24px;}.css-uswsmm>li:last-of-type{margin-bottom:0;}.css-uswsmm>li>span{position:relative;}.css-uswsmm>li>span>:first-child::before{content:'•';font-weight:500;color:var(--adaptiveGrey800);position:absolute;left:-24px;}\n.css-1hwiibq{font-size:17px;line-height:1.6;word-break:keep-all;letter-spacing:0em;font-weight:400;color:var(--adaptiveGrey800);}\n.css-1kxrhf3{white-space:pre-wrap;}퇴직연금에서 중요한 비중을 차지하는 TDF 개념 이해하기\n내가 계획하는 은퇴시점에 따라 TDF 고르는 기준 알아보기\n\n.css-1feg9au{font-size:24px;letter-spacing:0em;line-height:1.6;font-weight:bold;color:var(--adaptiveGrey900);margin:24px 0 4px;}\n은퇴 준비를 위한 자산 배분 이해하기\n.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n은퇴 준비를 하겠다고 마음 먹었을 때 가장 중요한 첫 번째는 적은 금액이라도 퇴직연금 계좌에 꾸준하게 모으는 것, 두 번째는 자산 배분이다. 만약 이미 은퇴에 가까운 나이라면 힘들게 모은 자산을 안전하게 관리하면서 가져가야 한다. 위험자산보다는 안전자산의 비중을 높여 너무 무리하지 않는 선에서 투자와 관리를 지속하는 것이다. 시장 분위기가 좋을 때 높은 수익을 가져다주는 위험자산은 언제나 유혹적이지만, 최근처럼 한국 주식이 급락해버리면 열심히 모아온 은퇴 자금이 위태로워지고 만다.\n반대로 아직 은퇴까지 20~30년의 시간이 있다면 위험자산을 이용해 적극적인 투자가 가능하다. 물론 자신이 얼마나 위험을 감수할 수 있는 성향인지 이해하고 이를 바탕으로 위험자산의 비중을 조정해야 하지만, 일반적으로 20대와 30대일 때는 전체 자산에서 위험자산의 비중이 중년기보다 많아도 된다고 말한다. 일시적으로 시장이 급락할지라도 은퇴 시점까지 회복할 시간이 충분히 있기 때문이다.\n모두에게 공평하게 적용되는 한 가지는 바로 시간이다. 시간이 흘러 나이를 먹으면서 모두가 점차 은퇴 시점에 가까워진다. .css-16cuouw{white-space:pre-wrap;color:var(--adaptiveGrey800);background-color:#3fd59936;}그렇게 은퇴 시점에 가까워지면 자산 배분도 그에 맞게 변경되어야 한다. 30대의 주식과 채권 배분 비율과 50대의 비율은 당연히 다를 수밖에 없다. 시기에 따라 적절히 주식과 채권 상품을 사고 팔며 비율을 조정해야 한다는 뜻이다. 여기까지 이야기하면 대부분의 사람들이 이런 반응을 보인다.\n“먹고 살기도 바쁜데 어떻게 매번 그걸 체크하고 바꾸나요?”\n“배분 비율을 어떻게 바꿔야 하죠? 기준이 있나요?”\n투자에 관심이 많고 늘 시간을 들일 수 있는 사람이라면 충분히 가능한 일이다. 하지만 대다수의 사람들에게 ‘주기적으로 리밸런싱하고, 위험자산과 안전자산의 비율을 조정하라'는 건 사실 지속 가능하지 않은 방법이다. 그래서 등장한 은퇴용 투자상품이 바로 TDF(Target Date Fund)이다.\n은퇴를 위해 태어났어요, TDF\nTDF는 은퇴 시점을 정하고 투자하면 펀드를 운용하는 회사가 해당 시점에 맞춰 자산 배분을 알아서 관리해주는 상품이다. 따라서 보통 TDF 상품에는 숫자가 들어 있는데, 이 숫자가 바로 은퇴 연도를 뜻한다. 예를 들어 내가 은퇴하고자 하는 시점이 2040년이라면 상품명에 2040이 들어간 TDF를 고르면 된다.(이 숫자를 빈티지(vintage)라고 부른다.) 보통 TDF에 들어 있는 숫자는 2030, 2035, 2040처럼 5년 단위로 설정된다. 따라서 자신의 은퇴 희망 시점과 가장 가까운 숫자를 선택하면 된다.\n문제는 많은 사람들이 TDF에 대한 이해가 부족한 채 무분별하게 투자하고 있다는 데 있다. 실제로 IRP 계좌를 보유한 한 30대를 인터뷰해보니 TDF2035 상품과 TDF2050 상품을 모두 보유하고 있었다. 자신의 은퇴 희망 시점을 고려한 것이 아니라 상품을 권유한 판매자의 말대로 여러 TDF를 구매한 탓이었다. TDF가 만들어진 취지와 맞지 않을 뿐더러, 투자자가 투자 상품에 대해 이해하지 못한 채 돈을 넣어버린 잘못된 투자였다.\n한국 시장에서 TDF 상품이 수면 위로 떠오른 이유는 바로 디폴트옵션(사전지정운용제도) 때문이다. 4화 ‘.css-iynyr0{white-space:pre-wrap;cursor:pointer;color:var(--adaptiveGrey600);-webkit-text-decoration:underline!important;text-decoration:underline!important;}자꾸 디폴트옵션 설정하라고 알림이 와요’에서 말했듯이 오랫동안 저조했던 퇴직연금의 수익률을 개선하기 위해 등장한 디폴트옵션에는 TDF 상품이 매우 많이 포진되어 있다. 심지어 주식 배분이 80% 가까이 올라가도 TDF는 안전자산으로 분류되는 특징이 있다. 시간 흐름에 따라 은퇴 시점에 가까워질수록 안전자산 비중이 높아지면서 변화하기 때문이다. 은퇴 준비를 효과적으로 하기 위해서는 이러한 특징을 가진 TDF 상품을 적절하게 활용할 줄 알아야 한다.\nTDF의 핵심은 바로 글라이드 패스\n시간 흐름에 따라 알아서 자산을 배분해주는 TDF의 특징을 글라이드 패스(glide path)로 설명하곤 한다. 글라이드 패스는 비행기가 착륙할 때 그리는 경로를 뜻하는 말로, 하늘에서 땅으로 부드러운 곡선을 그리며 착륙하는 모습이 은퇴까지 아직 많은 시간이 남은 사회 초년기에는 주식 비중이 크고, 은퇴 시기에 가까워질수록 주식 비중이 줄어드는 흐름과 매우 닮아 있다. 따라서 TDF에서의 글라이드 패스를 쉽게 설명하자면, 가입 시점부터 은퇴 시점까지 시간 흐름에 따라 주식과 채권의 배분을 조정하는 방법이다. 시기에 따른 배분율은 사전에 정해져 있으며, 그 비율에 따라 은퇴 시점이 가까워지면 TDF는 포트폴리오의 주식 보유량을 줄이고 채권 보유량을 늘려준다. 이유는 간단하다. 채권이 주식에 비해 위험이 적다고 판단하기 때문이다.\n다음 그래프는 미국의 자산운용사 블랙록(Black Rock)의 2040 TDF 자산 배분 현황이다. 가로축에서 0은 은퇴 시점이고 5는 은퇴 시점 5년 전, 10은 10년 전을 나타낸다. 해당 포트폴리오의 2023년 상태, 즉 약 8년 정도 남은 상태를 보면 주식 비중은 약 55%이고, 은퇴 시점에는 40%로 줄어든 것을 확인할 수 있다.\n.css-of5acw{margin:24px 0 8px;text-align:center;}.css-1pgssrp{max-width:100%;border-radius:16px;}\n.css-18442ym{font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}.css-jfs1hr{white-space:pre-wrap;font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}은퇴를 기점으로 달라지는 블랙록의 2040 TDF 자산 배분 현황\n그래프처럼 주식 배분이 점점 줄어드는 스케줄이 바로 글라이드 패스다. 글라이드 패스의 높낮이는 TDF를 만드는 회사마다 다르다. 어떤 방식의 글라이드 패스가 정답이라고 말할 수는 없지만, 모든 글라이드 패스를 만들 때 공통적으로 고려하는 몇 가지가 있다.\n첫째, 은퇴 시점이 가까워질수록 주식보다 채권의 비중이 높아진다.\n둘째, 은퇴 시점이 가까워질수록 해외주식보다 자국 주식의 비중이 높아진다. 은퇴에 가까울수록 외환 위험에 노출되지 않도록 자국 주식의 비중을 높이는 방식이 미국에서는 정석으로 통용된다. (그런데 이것은 한국 주식시장의 저조한 실적을 고려하면 한국 TDF에도 맞다고 단언할 수는 없는 부분이다.)\n셋째, 은퇴 시점이 가까워질수록 수익률이 높고 만기가 긴 채권의 비중이 줄고 수익률이 좀 낮더라도 만기가 짧은 채권의 비중이 높아진다.\n넷째, 은퇴 시점부터는 물가연동채권(미국에서는 TIPS), 원자재, 부동산 등의 자산이 많이 포함된다.\n이처럼 글라이드 패스는 주식과 채권의 분배 비율을 담고 있기 때문에 TDF의 수익률을 좌우하는 매우 중요한 요소이다. TDF 상품을 이해하는 데 가장 중요한 정보이지만 아직까지는 어려운 용어로 가득한 투자상품 설명서를 읽어야 파악할 수 있게 되어 있어서 투자자에게 제일 전달되지 않는 정보이기도 하다.\n배분 비율에 따른 수익률 차이는?\nTDF 상품별로 얼마나 주식에 투자되고, 채권에 투자되는지에 따라 매년 수익률이 달라진다. 예를 들어 2024년 1월 1일부터 7월 31일까지의 수익률을 살펴보자. 물론 장기간 투자를 위해 만들어진 상품을 이렇게 단기간 수익률로 분석하는 방법은 상품의 본래 취지에 어긋날 순 있지만, 투자자들이 수익률 차이를 이해하는 데 있어서 중요한 부분이라고 생각한다.\n\n출처=.css-114ityv{white-space:pre-wrap;cursor:pointer;-webkit-text-decoration:underline!important;text-decoration:underline!important;font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}아이랩 글라이드\n2040 빈티지인 TDF 상품들의 2024년 동안의 수익률을 보면 가장 좋은 수익률을 기록한 것은 ‘한국투자TDF알아서ETF포커스’로 7개월 수익률이 13.19%이다. 수익률이 가장 낮은 상품과 비교하면 무려 8% 넘게 차이가 난다. 2040년을 은퇴 시점으로 타깃하는 경우에는 주식 배분율 차이가 최대 77%에서 최소 57%로 다양하기 때문에 상당한 차이가 날 수밖에 없다. 이런 점을 이해한다면 같은 빈티지 안에서도 왜 이렇게 수익률이 다른지 이해할 수 있고, TDF를 고를 때 내가 주식에 더 많이 투자하고 싶다면 주식 배분율을 기준으로 상품을 고르는 방법도 가능하다.(하위 개념인 어떤 종목을 보유했느냐 또한 중요한 요소이지만, 수익률 차이를 가르는 데는 상위 개념인 어떤 자산을 얼마나 보유했느냐가 더 결정적인 역할을 한다.)\nTDF를 고르는 6가지 기준\n만약 내가 은퇴자금을 공격적으로 투자하고 싶다면 상품별로 주식-채권 배분율을 확인해보고, 주식 배분율이 높은 TDF를 고르면 된다. 이 배분율 차이는 곧 수익률, 변동성과 직결된다는 점, 그리고 위에서 살펴본 수익률 순위는 2024년 상반기 한정이며, 투자 기간 전체의 수익률이 아니라는 것도 잊지 않아야 한다.\n\n출처=아이랩 글라이드\n또한 나에게 맞는 TDF를 고르기 위해서는 조금 더 고려해야 할 사항들이 있다. 아래 6가지 기준을 소개한다.\n첫째가 앞서 말한 나의 은퇴 시점, 둘째가 주식 배분의 정도이다. 예를 들어 2045년 은퇴를 목표로 한다면 TDF 2045 빈티지 안에서 주식 배분율을 최소 57%에서 최대 79%까지 고를 수 있다.\n셋째는 미국 주식에 어느 정도 투자하고 싶은가다. 한국 TDF 상품은 미국 주식에 많이 투자하는데 역시 배분율이 조금씩 다르다. 만약 미국 주식에 많이 투자하고 싶다면 최대치를 선택할 수 있다.\n넷째, 수수료 차이를 고려해 패시브 펀드와 액티브 펀드 중 원하는 것을 선택한다. 지수를 따라가는 패시브 펀드는 액티브 펀드보다 수수료가 저렴하다. 또한 패시브 펀드로 구성된 TDF는 펀드 매니저가 종목을 골라 넣는 방식이 아니기 때문에 시장 환경에 따른 수익률 예측이 가능하다.\n다섯째, 위험도 선택이다. 상품설명서를 보면 펀드운용자가 정한 위험등급을 볼 수 있다. 이는 매우 높은 위험의 1부터 매우 낮은 위험의 6까지 구분된다. 자신이 얼마나 위험을 감수할 수 있을지를 고려한다.\n여섯째, 은퇴 시점의 주식 배분율을 고른다. 같은 빈티지 안에서도 은퇴 시점에 주식 배분이 조금 더 높은 상품이 있고 낮은 상품이 있다. 은퇴가 다가왔을 때 예상치 못한 시장 상황에 의한 갑작스러운 손실을 낮추고 싶다면 더 낮은 상품을 고르면 된다.\n위와 같은 질문을 통해 자신의 계획에 맞는 TDF 상품을 찾아갈 수 있다. 단순히 수익률만으로 상품을 고르기보다는 나의 성향에 맞춰 고르는 심층적인 방법이다.\n과거 수익률은 판단하기 위한 최소한의 단서\n상품을 고르는 데 있어서 과거 수익률은 중요한 정보다. 과거의 수익률이 미래 수익률을 보장하지는 않지만 그동안 얼마나 잘해왔는지를 살펴볼 수 있는 중요한 척도이므로, 광고만 보고 고르는 것보다는 낫다. 수익률을 잘 비교하기 위해서는 내 펀드의 수익률만 살펴보는 것이 아니라, 같은 빈티지의 다른 상품이 동일한 기간에 얼마나 수익을 올렸는지 살필 것을 권한다. 앞서 살펴본 것처럼 똑같은 2040 타깃의 TDF여도 최소 5%에서 최대 13%로 차이가 있기 때문이다. 만기를 채우지 않은 채 수시로 상품을 갈아타는 것을 추천할 수는 없지만, 만약 꾸준히 더 좋은 수익률을 내고 나에게 잘 맞는 상품이 있다면 당연히 바꾸어야 하지 않을까?\nTDF는 한국 금융시장에서 은퇴 준비에 있어서 점점 더 큰 부분을 차지하고 있는 상품이므로, 이번 시간을 통해 TDF에 대해 제대로 이해하는 투자자들이 많아지길 기대한다. 그리하여 나에게 맞는 TDF를 골라서 투자하고 다음 단계로 나아가기를 바란다. 퇴직연금 계좌에는 보통 다른 계좌보다 제약이 있기는 하지만 ETF, 자산 배분 펀드 등 다양한 상품이 존재한다. 이를 이용해서 장기투자와 단기투자를 섞거나, 투자에 더 익숙한 투자자라면 ETF를 이용해 자신만의 포트폴리오를 잘 구성해볼 수 있다. 다음 화에서는 퇴직연금으로 투자하는 ETF에 대해 다루어 보도록 하겠다.\n*<노후 준비 액션플랜> 시리즈는 국내 159개 대표 운용펀드의 TDF 글라이드 패스와 수익률을 한눈에 보기 쉽도록 전달하는 .css-1vsqqzg{white-space:pre-wrap;cursor:pointer;-webkit-text-decoration:underline!important;text-decoration:underline!important;font-weight:bold;font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}아이랩 글라이드와 함께 만듭니다.\n.css-1ifza5r{border:0;margin-top:0;margin-bottom:0;height:1px;opacity:1;background:var(--tHairlineBackground);margin:0;}\nEdit 주소은, 김현미(아이랩) Graphic 조수희, 윤여진",
        "content": "퇴직연금을 운용하는 지속 가능한 방법",
        "contentSnippet": "퇴직연금을 운용하는 지속 가능한 방법",
        "guid": "https://blog.toss.im/article/retirement-plans-05",
        "isoDate": "2024-08-23T06:20:00.000Z"
      },
      {
        "title": "건강검진 지원부터 빈 용기 보증금 제도까지, 우리의 건강과 환경을 지키는 돈 되는 정책",
        "link": "https://blog.toss.im/article/money-policies-23",
        "pubDate": "Fri, 23 Aug 2024 01:00:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}.css-1kxrhf3{white-space:pre-wrap;}우리의 건강과 환경, 안전을 지키는 정책과 제도를 살펴봅니다.  \n.css-1feg9au{font-size:24px;letter-spacing:0em;line-height:1.6;font-weight:bold;color:var(--adaptiveGrey900);margin:24px 0 4px;}\n건강이 최우선 \n.css-q3ktjb{white-space:pre-wrap;font-weight:bold;}⓵ 생애주기별 국가건강검진제도\n일반건강검진부터 연령에 따른 암 검진, 영유아와 청소년까지 필수적인 검사를 지원합니다.\n.css-16cuouw{white-space:pre-wrap;color:var(--adaptiveGrey800);background-color:#3fd59936;}일반건강검진은 보통 2년 주기로 받아요. 비만, 시청각 이상, 고혈압, 폐결핵, 신장질환, 빈혈, 당뇨병 여부 등을 공통으로 검사하고요. 연령에 따라 이상지질혈증, B형간염, 골밀도, 인지기능장애, 우울증 등의 검사를 추가로 받을 수 있어요.\n자세한 내용은 국민건강보험공단(www.nhis.or.kr)에서 확인할 수 있습니다.\n⓶ 암 환자 의료비 지원\n암 환자가 있는 가정의 경제적 부담을 덜어드리고 힘든 치료 과정을 잘 견뎌낼 수 있도록 의료비를 지원해요.\n성인 환자 기준, 의료급여 수급자나 차상위 본인 부담 경감대상자인 경우, 최대 3년간 연속으로 연간 최대 300만원을 지원합니다. 주소지 관할 보건소에서 신청할 수 있어요.\n⓷ 찾아가는 금연지원 서비스\n담배를 끊고자 하는 금연 의지가 있지만 프로그램에 참가하기 어려운 상황이라면, 직접 찾아가 관리해드립니다. 위기 청소년, 여성, 장애인, 소규모 사업장 근로자 등이 지원 대상이에요.\n전국 17개 지역금연지원센터를 통해 신청하면 6개월간 9회 이상 지속적인 금연 상담을 받을 수 있고요, 니코틴 보조제나 금연치료제를 지원 받을 수 있어요. 한국건강증진개발원(02-3781-2220)에 더 자세한 내용을 문의해보세요.\n안전을 체감할 수 있는 사회 \n⓸ 여성 긴급전화 1366\n가정폭력, 성폭력, 성매매, 스토킹, 교제폭력 등 폭력 피해자에게 상담 서비스를 지원합니다. 365일 24시간 운영되니 도움이 필요할 때 언제든 전화하면 됩니다. \n초기 상담부터 관련 지원 기관으로의 연계, 피해자와 동반가족이 최대 7일까지 머무를 수 있는 긴급 피난처까지 제공합니다. \n⓹ 안심 상속 원스톱 서비스\n사망자가 남긴 재산과 채무를 한번에 조회하는 서비스입니다. 예금, 대출, 보험, 증권 등 금융거래 내역부터 연금 가입 유무까지 확인할 수 있어요.\n사망일이 속한 달의 말일부터 1년 이내에 상속인(또는 후견인)이 신청할 수 있어요. 가까운 행정복지센터에 방문해도 되고, 정부24 홈페이지에서 온라인 신청도 가능합니다.\n환경을 지키는 돈 되는 정책 \n⓺ 폐가전 무상방문수거\n온라인이나 전화로 폐가전 처리 신청하면, 수거 전담반이 찾아가 무상 수거해 갑니다. 집에서 사용하지 않는 TV나 냉장고, 세탁기, 에어컨 등 부피가 크고 무거운 폐가전 제품을 배출할 때 꼭 확인해보세요. 소형 가전은 5개 이상인 경우 수거 가능합니다.\n폐가전제품 배출예약시스템(www.15990903.or.kr)이나 전화(1599-0903)로 신청할 수 있어요.\n⓻ 빈 용기 보증금 제도\n재사용 표시가 있는 병을 가까운 슈퍼나 대형마트에 반환하면 보증금을 돌려 받을 수 있어요. 돌려받을 수 있는 금액은 병의 용량에 따라 70원~350원으로 달라요.\n예를 들면 소주병은 1병당 100원인데, 제사 지낼 때 많이 쓰는 대형 청주 병은 개당 350원이랍니다. 제품 라벨에 금액이 표시돼 있어요.\n\n.css-1ifza5r{border:0;margin-top:0;margin-bottom:0;height:1px;opacity:1;background:var(--tHairlineBackground);margin:0;}\nEdit 정경화 Graphic 조수희",
        "content": "지키고 살피는 건강, 안전, 환경 서비스",
        "contentSnippet": "지키고 살피는 건강, 안전, 환경 서비스",
        "guid": "https://blog.toss.im/article/money-policies-23",
        "isoDate": "2024-08-23T01:00:00.000Z"
      }
    ]
  },
  {
    "name": "모나미",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김진홍",
    "category": "개인",
    "posts": []
  }
]