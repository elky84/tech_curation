[
  {
    "name": "ㅍㅍㅅㅅ",
    "category": "큐레이팅",
    "posts": []
  },
  {
    "name": "C++ Team Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Facebook Engineering",
    "category": "기업",
    "posts": [
      {
        "creator": "",
        "title": "The key to a happy Rust/C++ relationship",
        "link": "https://engineering.fb.com/2024/06/25/developer-tools/the-key-to-a-happy-rust-c-relationship/",
        "pubDate": "Tue, 25 Jun 2024 16:00:26 +0000",
        "content:encodedSnippet": "The history of Rust at Meta goes all the way back to 2016, when we first started using it for source control. Today, it has been widely embraced at Meta and is one of our primary supported server-side languages (along with C++, Python, and Hack).\nBut that doesn’t mean there weren’t any growing pains.\nAida G., a member of one of Meta’s first Rust teams, joins Pascal Hartig (@passy) on the latest Meta Tech Podcast to dive into the challenges of getting Rust to interact with Meta’s large amount of existing C++ code.\nFortunately, the release of cxx, safe interop between C++, and even async Rust have made things a lot easier.\nDownload or listen to the episode below:\n\nYou can also find the episode wherever you get your podcasts, including:\nApple Podcasts\nSpotify\nPocketCasts\nOvercast\nThe Meta Tech Podcast is a podcast, brought to you by Meta, where we highlight the work Meta’s engineers are doing at every level – from low-level frameworks to end-user features.\nSend us feedback on Instagram, Threads, or X.\nAnd if you’re interested in learning more about career opportunities at Meta visit the Meta Careers page.\nThe post The key to a happy Rust/C++ relationship appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>The history of Rust at Meta goes all the way back to 2016, when we first started using it for source control. Today, it has been widely embraced at Meta and is one of our primary supported server-side languages (along with C++, Python, and Hack). But that doesn’t mean there weren’t any growing pains. Aida [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2024/06/25/developer-tools/the-key-to-a-happy-rust-c-relationship/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2024/06/25/developer-tools/the-key-to-a-happy-rust-c-relationship/\">The key to a happy Rust/C++ relationship</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "The history of Rust at Meta goes all the way back to 2016, when we first started using it for source control. Today, it has been widely embraced at Meta and is one of our primary supported server-side languages (along with C++, Python, and Hack). But that doesn’t mean there weren’t any growing pains. Aida [...]\nRead More...\nThe post The key to a happy Rust/C++ relationship appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=21409",
        "categories": [
          "DevInfra",
          "Open Source",
          "Meta Tech Podcast"
        ],
        "isoDate": "2024-06-25T16:00:26.000Z"
      },
      {
        "creator": "",
        "title": "Leveraging AI for efficient incident response",
        "link": "https://engineering.fb.com/2024/06/24/data-infrastructure/leveraging-ai-for-efficient-incident-response/",
        "pubDate": "Mon, 24 Jun 2024 16:00:57 +0000",
        "content:encodedSnippet": "We’re sharing how we streamline system reliability investigations using a new AI-assisted root cause analysis system.\nThe system uses a combination of heuristic-based retrieval and large language model-based ranking to speed up root cause identification during investigations.\nOur testing has shown this new system achieves 42% accuracy in identifying root causes for investigations at their creation time related to our web monorepo.\nInvestigation is a critical part of ensuring system reliability, and a prerequisite to mitigating issues quickly. This is why Meta is investing in advancing our suite of investigation tooling with tools like Hawkeye, which we use internally for debugging end-to-end machine learning workflows.\nNow, we’re leveraging AI to advance our investigation tools even further. We’ve streamlined our investigations through a combination of heuristic-based retrieval and large language model (LLM)-based ranking to provide AI-assisted root cause analysis. During backtesting, this system has achieved promising results: 42% accuracy in identifying root causes for investigations at their creation time related to our web monorepo.\n\nInvestigations at Meta\nEvery investigation is unique. But identifying the root cause of an issue is necessary to mitigate it properly.  Investigating issues in systems dependent on monolithic repositories can present scalability challenges due to the accumulating number of changes involved across many teams. In addition, responders need to build context on the investigation to start working on it, e.g., what is broken, which systems are involved, and who might be impacted. \nThese challenges can make investigating anomalies a complex and time consuming process. AI offers an opportunity to streamline the process, reducing the time needed and helping responders make better decisions. We focused on building a system capable of identifying potential code changes that might be the root cause for a given investigation.\nFigure 1: A responder’s view of an investigation journey.\nOur approach to root cause isolation\nThe system incorporates a novel heuristics-based retriever that is capable of reducing the search space from thousands of changes to a few hundred without significant reduction in accuracy using, for example., code and directory ownership or exploring the runtime code graph of impacted systems. Once we have reduced the search space to a few hundred changes relevant to the ongoing investigation, we rely on a LLM-based ranker system to identify the root cause across these changes.\nFigure 2: The system flow for our AI-assisted root cause analysis system.\nThe ranker system uses a Llama model to further reduce the search space from hundreds of potential code changes to a list of the top five. We explored different ranking algorithms and prompting scenarios and found that ranking through election was most effective to accommodate context window limitations and enable the model to reason across different changes. To rank the changes, we structure prompts to contain a maximum of 20 changes at a time, asking the LLM to identify the top five changes. The output across the LLM requests are aggregated and the process is repeated until we have only five candidates left. Based on exhaustive backtesting, with historical investigations and the information available at their start, 42% of these investigations had the root cause in the top five suggested code changes.\nFigure 3: Ranking possible code changes through election.\nTraining\nThe biggest lever to achieving 42% accuracy was fine-tuning a Llama 2 (7B) model using historical investigations for which we knew the underlying root cause. We started by running continued pre-training (CPT) using limited and approved internal wikis, Q&As, and code to expose the model to Meta artifacts. Later, we ran a supervised fine-tuning (SFT) phase where we mixed Llama 2’s original SFT data with more internal context and a dedicated investigation root cause analysis (RCA) SFT dataset to teach the model to follow RCA instructions.\nFigure 4: The Llama 2 (7B) root cause analysis training process.\nOur RCA SFT dataset consists of ~5,000 instruction-tuning examples with details of 2-20 changes from our retriever, including the known root cause, and information known about the investigation at its start, e.g., its title and observed impact. Naturally, the available information density is low at this point, however this allows us to perform better in similar real-world scenarios when we have limited information at the beginning of the investigation. \nUsing the same fine-tuning data format for each possible culprit then allows us to gather the model’s llog probabilities(logprobs) and rank our search space based on relevancy to a given investigation. We then curated a set of similar fine-tuning examples where we expect the model to yield a list of potential code changes likely responsible for the issue ordered by their logprobs-ranked relevance, with the expected root cause at the start. Appending this new dataset to the original RCA SFT dataset and re-running SFT gives the model the ability to respond appropriately to prompts asking for ranked lists of changes relevant to the investigation.\nFigure 5: The process for generating fine-tuning prompts to enable the LLM to produce ranked lists.\nThe future of AI-assisted Investigations\nThe application of AI in this context presents both opportunities and risks. For instance, it can reduce effort and time needed to root cause an investigation significantly, but it can potentially suggest wrong root causes and mislead engineers. To mitigate this, we ensure that all employee-facing features prioritize closed feedback loops and explainability of results. This strategy ensures that responders can independently reproduce the results generated by our systems to validate their results. We also rely on confidence measurement methodologies to detect low confidence answers and avoid recommending them to the users – sacrificing reach in favor of precision.\nBy integrating AI-based systems into our internal tools we’ve successfully leveraged them for tasks like onboarding engineers to investigations and root cause isolation. Looking ahead, we envision expanding the capabilities of these systems to autonomously execute full workflows and validate their results. Additionally, we anticipate that we can further streamline the development process by utilizing AI to detect potential incidents prior to code push, thereby proactively mitigating risks before they arise.\nAcknowledgements\nWe wish to thank contributors to this effort across many teams throughout Meta, particularly Alexandra Antiochou, Beliz Gokkaya, Julian Smida, Keito Uchiyama, Shubham Somani; and our leadership: Alexey Subach, Ahmad Mamdouh Abdou, Shahin Sefati, Shah Rahman, Sharon Zeng, and Zach Rait. \nThe post Leveraging AI for efficient incident response appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>We’re sharing how we streamline system reliability investigations using a new AI-assisted root cause analysis system. The system uses a combination of heuristic-based retrieval and large language model-based ranking to speed up root cause identification during investigations. Our testing has shown this new system achieves 42% accuracy in identifying root causes for investigations at their [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2024/06/24/data-infrastructure/leveraging-ai-for-efficient-incident-response/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2024/06/24/data-infrastructure/leveraging-ai-for-efficient-incident-response/\">Leveraging AI for efficient incident response</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "We’re sharing how we streamline system reliability investigations using a new AI-assisted root cause analysis system. The system uses a combination of heuristic-based retrieval and large language model-based ranking to speed up root cause identification during investigations. Our testing has shown this new system achieves 42% accuracy in identifying root causes for investigations at their [...]\nRead More...\nThe post Leveraging AI for efficient incident response appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=21393",
        "categories": [
          "Data Infrastructure",
          "DevInfra",
          "ML Applications"
        ],
        "isoDate": "2024-06-24T16:00:57.000Z"
      },
      {
        "creator": "",
        "title": "PVF: A novel metric for understanding AI systems’ vulnerability against SDCs in model parameters",
        "link": "https://engineering.fb.com/2024/06/19/data-infrastructure/parameter-vulnerability-factor-pvf-ai-silent-data-corruption/",
        "pubDate": "Wed, 19 Jun 2024 16:00:46 +0000",
        "content:encodedSnippet": "We’re introducing parameter vulnerability factor (PVF), a novel metric for understanding and measuring AI systems’ vulnerability against silent data corruptions (SDCs) in model parameters.\nPVF can be tailored to different AI models and tasks, adapted to different hardware faults, and even extended to the training phase of AI models.\nWe’re sharing results of our own case studies using PVF to measure the impact of SDCs in model parameters, as well as potential methods of identifying SDCs in model parameters.\nReliability is an important aspect of any successful AI implementation. But the growing complexity and diversity of AI hardware systems also brings an increased risk of hardware faults such as bit flips. Manufacturing defects, aging components, or environmental factors can lead to data corruptions – errors or alterations in data that can occur during storage, transmission, or processing and result in unintended changes in information.\nSilent data corruptions (SDCs), where an undetected hardware fault results in erroneous application behavior, have become increasingly prevalent and difficult to detect. Within AI systems, an SDC can create what is referred to as parameter corruption, where AI model parameters are corrupted and their original values are altered.\nWhen this occurs during AI inference/servicing it can potentially lead to incorrect or degraded model output for users, ultimately affecting the quality and reliability of AI services.\nFigure 1 shows an example of this, where a single bit flip can drastically alter the output of a ResNet model. \nFigure 1: Flipping a random bit of one parameter in the 1st convolution (conv) layer in ResNet-18 drastically alters the model’s output.\n \nWith this escalating thread in mind, there are two important questions: How vulnerable are AI models to parameter corruptions? And how do different parts (such as modules and layers) of the models exhibit different vulnerability levels to parameter corruptions?\nAnswering these questions is an important part of delivering reliable AI systems and services and offers valuable insights for guiding AI hardware system design, such as when assigning AI model parameters or software variables to hardware blocks with differing fault protection capabilities. Additionally, it can provide important information for formulating strategies to detect and mitigate SDCs in AI systems in an efficient and effective manner.\nParameter vulnerability factor (PVF) is a novel metric we’ve introduced with the aim to standardize the quantification of AI model vulnerability against parameter corruptions. PVF is a versatile metric that can be tailored to different AI models/tasks and is also adaptable to different hardware fault models. Furthermore, PVF can be extended to the training phase to evaluate the effects of parameter corruptions on the model’s convergence capability.\nWhat is PVF?\nPVF is inspired by the architectural vulnerability factor (AVF) metric used within the computer architecture community. We define a model parameter’s PVF as the probability that a corruption in that specific model parameter will lead to an incorrect output. Similar to AVF, this statistical concept can be derived from statistically extensive and meaningful fault injection (FI) experiments. \nPVF has several features:\nParameter-level quantitative assessment\nAs a quantitative metric, PVF concentrates on parameter-level vulnerability, calculating the likelihood that a corruption in a specific model parameter will lead to an incorrect model output. This “parameter” can be defined at different scales and granularities, such as an individual parameter or a group of parameters.\nScalability across AI models/tasks\nPVF is scalable and applicable across a wide range of AI models, tasks, and hardware fault models.\nProvides insights for guiding AI system design\nPVF can provide valuable insights for AI system designers, guiding them in making informed decisions about balancing fault protection with performance and efficiency. For example, engineers might leverage PVF to help map higher vulnerable parameters to better-protected hardware blocks and explore tradeoffs on latency, power, and reliability by enabling a surgical approach to fault tolerance at selective locations instead of a catch-all/none approach. \nCan be used as a standard metric for AI vulnerability/resilience evaluation\nPVF has the potential to unify and standardize such practices, making it easier to compare the reliability of different AI systems/parameters and fostering open collaboration and progress in the industry and research community.\nHow PVF works\nSimilar to AVF as a statistical concept, PVF needs to be derived through a large number of FI  experiments that are statistically meaningful. Figure 2 shows an overall flow to compute PVF through a FI process. We’ve presented a case study on the open-source DLRM inference with more details and example case studies in our paper.\nFigure 2: Computing PVF through FI.\nFigure 3 illustrates the PVF of three DLRM parameter components, embedding table, bot-MLP, and top-MLP, under 1, 2, 4, 8, 16, 32, 64, and 128 bit flips during each inference. We observe different vulnerability levels across different parts of DLRM. For example, under a single bit flip, the embedding table has relatively low PVF; this is attributed to embedding tables being highly sparse, and parameter corruptions are only activated when the particular corrupted parameter is activated by the corresponding sparse feature. However, top-MLP can have 0.4% under even a single bit flip. This is significant – for every 1000 inferences, four inferences will be incorrect. This highlights the importance of protecting specific vulnerable parameters for a given model based on the PVF measurement. \nFigure 3: The PVF of DLRM parameters under random bit flips.\nWe observe that with 128 bit flips during each inference, for MLP components, PVF has increased to 40% and 10% for top-MLP and bot-MLP components respectively, while observing multiple NaN values. Top-MLP component has higher PVF than bot-MLP. This is attributed to the top-MLP being closer to the final model, and hence has less of a chance to be mitigated by inherent error masking probability of neural layers. \nThe applicability of PVF\nPVF is a versatile metric where the definition of an “incorrect output” (which will vary based on the model/task) can be adapted to suit user requirements. To adapt PVF to various hardware fault models the method to calculate PVF remains consistent as depicted in Figure 2. The only modification required is the manner in which the fault is injected, based on the assumed fault models. \nFurthermore, PVF can be extended to the training phase to evaluate the effects of parameter corruptions on the model’s convergence capability. During training, the model’s parameters are iteratively updated to minimize a loss function. A corruption in a parameter could potentially disrupt this learning process, preventing the model from converging to an optimal solution. By applying the PVF concept during training, we could quantify the probability that a corruption in each parameter would result in such a convergence failure.\nDr. DNA and further exploration avenues for PVF\nThe logical progression after understanding AI vulnerability to SDCs is to identify and lessen their impact on AI systems. To initiate this, we’ve introduced Dr. DNA, a method designed to detect and mitigate SDCs that occur during deep learning model inference. Specifically, we formulate and extract a set of unique SDC signatures from the distribution of neuron activations (DNA), based on which we propose early-stage detection and mitigation of SDCs during DNN inference. \nWe perform an extensive evaluation across 10 representative DNN models used in three common tasks (vision, GenAI, and segmentation) including ResNet, Vision Transformer, EfficientNet, YOLO, etc., under four different error models. Results show that Dr. DNA  achieves a 100% SDC detection rate for most cases, a 95% detection rate on average and a >90% detection rate across all cases, representing 20-70% improvement over baselines. Dr. DNA can also mitigate the impact of SDCs by effectively recovering DNN model performance with <1% memory overhead and <2.5% latency overhead. \nRead the research papers\nPVF (Parameter Vulnerability Factor): A Novel Metric for Understanding AI Vulnerability Against SDCs in Model Parameters \nDr. DNA: Combating Silent Data Corruptions in Deep Learning using Distribution of Neuron Activations\nThe post PVF: A novel metric for understanding AI systems’ vulnerability against SDCs in model parameters appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>We’re introducing parameter vulnerability factor (PVF), a novel metric for understanding and measuring AI systems’ vulnerability against silent data corruptions (SDCs) in model parameters. PVF can be tailored to different AI models and tasks, adapted to different hardware faults, and even extended to the training phase of AI models. We’re sharing results of our own [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2024/06/19/data-infrastructure/parameter-vulnerability-factor-pvf-ai-silent-data-corruption/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2024/06/19/data-infrastructure/parameter-vulnerability-factor-pvf-ai-silent-data-corruption/\">PVF: A novel metric for understanding AI systems’ vulnerability against SDCs in model parameters</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "We’re introducing parameter vulnerability factor (PVF), a novel metric for understanding and measuring AI systems’ vulnerability against silent data corruptions (SDCs) in model parameters. PVF can be tailored to different AI models and tasks, adapted to different hardware faults, and even extended to the training phase of AI models. We’re sharing results of our own [...]\nRead More...\nThe post PVF: A novel metric for understanding AI systems’ vulnerability against SDCs in model parameters appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=21376",
        "categories": [
          "Data Infrastructure"
        ],
        "isoDate": "2024-06-19T16:00:46.000Z"
      }
    ]
  },
  {
    "name": "eBay Tech Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Twitter Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Netflix TechBlog",
    "category": "기업",
    "posts": [
      {
        "creator": "Netflix Technology Blog",
        "title": "Enhancing Netflix Reliability with Service-Level Prioritized Load Shedding",
        "link": "https://netflixtechblog.com/enhancing-netflix-reliability-with-service-level-prioritized-load-shedding-e735e6ce8f7d?source=rss----2615bd06b42e---4",
        "pubDate": "Tue, 25 Jun 2024 22:58:09 GMT",
        "content:encodedSnippet": "Applying Quality of Service techniques at the application level\nAnirudh Mendiratta, Kevin Wang, Joey Lynch, Javier Fernandez-Ivern, Benjamin Fedorka\nIntroduction\nIn November 2020, we introduced the concept of prioritized load shedding at the API gateway level in our blog post, Keeping Netflix Reliable Using Prioritized Load Shedding. Today, we’re excited to dive deeper into how we’ve extended this strategy to the individual service level, focusing on the video streaming control plane and data plane, to further enhance user experience and system resilience.\nThe Evolution of Load Shedding at Netflix\nAt Netflix, ensuring a seamless viewing experience for millions of users simultaneously is paramount. Our initial approach for prioritized load shedding was implemented at the Zuul API gateway layer. This system effectively manages different types of network traffic, ensuring that critical playback requests receive priority over less critical telemetry traffic.\nBuilding on this foundation, we recognized the need to apply a similar prioritization logic deeper within our architecture, specifically at the service layer where different types of requests within the same service could be prioritized differently. The advantages of applying these techniques at the service level in addition to our edge API gateway are:\n\nService teams can own their prioritization logic and can apply finer grained prioritization.\nThis can be used for backend to backend communication, i.e. for services not sitting behind our edge API gateway.\nServices can use cloud capacity more efficiently by combining different request types into one cluster and shedding low priority requests when necessary instead of maintaining separate clusters for failure isolation.\n\nIntroducing Service-Level Prioritized Load Shedding\nPlayAPI is a critical backend service on the video streaming control plane, responsible for handling device initiated manifest and license requests necessary to start playback. We categorize these requests into two types based on their criticality:\n\nUser-Initiated Requests (critical): These requests are made when a user hits play and directly impact the user’s ability to start watching a show or a movie.\nPre-fetch Requests (non-critical): These requests are made optimistically when a user browses content without the user hitting play, to reduce latency should the user decide to watch a particular title. A failure in only pre-fetch requests does not result in a playback failure, but slightly increases the latency between pressing play and video appearing on screen.\nNetflix on Chrome making pre-fetch requests to PlayAPI while the user is browsing content\nThe Problem\nIn order to handle large traffic spikes, high backend latency, or an under-scaled backend service, PlayAPI previously used a concurrency limiter to throttle requests that would reduce the availability of both user-initiated and prefetch requests equally. This was not ideal because:\n\nSpikes in pre-fetch traffic reduced availability for user-initiated requests\nIncreased backend latency reduced availability for user-initiated requests and pre-fetch requests equally, when the system had enough capacity to serve all user-initiated requests.\n\nSharding the critical and non-critical requests into separate clusters was an option, which addressed problem 1 and added failure isolation between the two types of requests, however it came with a higher compute cost. Another disadvantage of sharding is that it adds some operational overhead — engineers need to make sure CI/CD, auto-scaling, metrics, and alerts are enabled for the new cluster.\nOption 1 — No isolationOption 2 — Isolation but higher compute cost\nOur Solution\nWe implemented a concurrency limiter within PlayAPI that prioritizes user-initiated requests over prefetch requests without physically sharding the two request handlers. This mechanism uses the partitioning functionality of the open source Netflix/concurrency-limits Java library. We create two partitions in our limiter:\n\nUser-Initiated Partition: Guaranteed 100% throughput.\nPre-fetch Partition: Utilizes only excess capacity.\nOption 3 — Single cluster with prioritized load-shedding offers application-level isolation with lower compute cost. Each instance serves both types of requests and has a partition whose size adjusts dynamically to ensure that pre-fetch requests only get excess capacity. This allows user-initiated requests to “steal” pre-fetch capacity when necessary.\nThe partitioned limiter is configured as a pre-processing Servlet Filter that uses HTTP headers sent by devices to determine a request’s criticality, thus avoiding the need to read and parse the request body for rejected requests. This ensures that the limiter is not itself a bottleneck and can effectively reject requests while using minimal CPU. As an example, the filter can be initialized as\nFilter filter = new ConcurrencyLimitServletFilter(\n        new ServletLimiterBuilder()\n                .named(\"playapi\")\n                .partitionByHeader(\"X-Netflix.Request-Name\")\n                .partition(\"user-initiated\", 1.0)\n                .partition(\"pre-fetch\", 0.0)\n                .build());\nNote that in steady state, there is no throttling and the prioritization has no effect on the handling of pre-fetch requests. The prioritization mechanism only kicks in when a server is at the concurrency limit and needs to reject requests.\nTesting\nIn order to validate that our load-shedding worked as intended, we used Failure Injection Testing to inject 2 second latency in pre-fetch calls, where the typical p99 latency for these calls is < 200 ms. The failure was injected on one baseline instance with regular load shedding and one canary instance with prioritized load shedding. Some internal services that PlayAPI calls use separate clusters for user-initiated and pre-fetch requests and run pre-fetch clusters hotter. This test case simulates a scenario where a pre-fetch cluster for a downstream service is experiencing high latency.\nBaseline — Without prioritized load-shedding. Both pre-fetch and user-initiated see an equal drop in availabilityCanary — With prioritized load-shedding. Only pre-fetch availability drops while user-initiated availability stays at 100%\nWithout prioritized load-shedding, both user-initiated and prefetch availability drop when latency is injected. However, after adding prioritized load-shedding, user-initiated requests maintain a 100% availability and only prefetch requests are throttled.\nWe were ready to roll this out to production and see how it performed in the wild!\nReal-World Application and Results\nNetflix engineers work hard to keep our systems available, and it was a while before we had a production incident that tested the efficacy of our solution. A few months after deploying prioritized load shedding, we had an infrastructure outage at Netflix that impacted streaming for many of our users. Once the outage was fixed, we got a 12x spike in pre-fetch requests per second from Android devices, presumably because there was a backlog of queued requests built up.\nSpike in Android pre-fetch RPS\nThis could have resulted in a second outage as our systems weren’t scaled to handle this traffic spike. Did prioritized load-shedding in PlayAPI help us here?\nYes! While the availability for prefetch requests dropped as low as 20%, the availability for user-initiated requests was > 99.4% due to prioritized load-shedding.\nAvailability of pre-fetch and user-initiated requests\nAt one point we were throttling more than 50% of all requests but the availability of user-initiated requests continued to be > 99.4%.\nGeneric service work prioritization\nBased on the success of this approach, we have created an internal library to enable services to perform prioritized load shedding based on pluggable utilization measures, with multiple priority levels.\nUnlike API gateway, which needs to handle a large volume of requests with varying priorities, most microservices typically receive requests with only a few distinct priorities. To maintain consistency across different services, we have introduced four predefined priority buckets inspired by the Linux tc-prio levels:\n\nCRITICAL: Affect core functionality — These will never be shed if we are not in complete failure.\nDEGRADED: Affect user experience — These will be progressively shed as the load increases.\nBEST_EFFORT: Do not affect the user — These will be responded to in a best effort fashion and may be shed progressively in normal operation.\nBULK: Background work, expect these to be routinely shed.\n\nServices can either choose the upstream client’s priority or map incoming requests to one of these priority buckets by examining various request attributes, such as HTTP headers or the request body, for more precise control. Here is an example of how services can map requests to priority buckets:\nResourceLimiterRequestPriorityProvider requestPriorityProvider() {\n    return contextProvider -> {\n        if (contextProvider.getRequest().isCritical()) {\n              return PriorityBucket.CRITICAL;\n          } else if (contextProvider.getRequest().isHighPriority()) {\n              return PriorityBucket.DEGRADED;\n          } else if (contextProvider.getRequest().isMediumPriority()) {\n              return PriorityBucket.BEST_EFFORT;\n          } else {\n              return PriorityBucket.BULK;\n          }\n        };\n    }\nGeneric CPU based load-shedding\nMost services at Netflix autoscale on CPU utilization, so it is a natural measure of system load to tie into the prioritized load shedding framework. Once a request is mapped to a priority bucket, services can determine when to shed traffic from a particular bucket based on CPU utilization. In order to maintain the signal to autoscaling that scaling is needed, prioritized shedding only starts shedding load after hitting the target CPU utilization, and as system load increases, more critical traffic is progressively shed in an attempt to maintain user experience.\nFor example, if a cluster targets a 60% CPU utilization for auto-scaling, it can be configured to start shedding requests when the CPU utilization exceeds this threshold. When a traffic spike causes the cluster’s CPU utilization to significantly surpass this threshold, it will gradually shed low-priority traffic to conserve resources for high-priority traffic. This approach also allows more time for auto-scaling to add additional instances to the cluster. Once more instances are added, CPU utilization will decrease, and low-priority traffic will resume being served normally.\nPercentage of requests (Y-axis) being load-shed based on CPU utilization (X-axis) for different priority buckets\nExperiments with CPU based load-shedding\nWe ran a series of experiments sending a large request volume at a service which normally targets 45% CPU for auto scaling but which was prevented from scaling up for the purpose of monitoring CPU load shedding under extreme load conditions. The instances were configured to shed noncritical traffic after 60% CPU and critical traffic after 80%.\nAs RPS was dialed up past 6x the autoscale volume, the service was able to shed first noncritical and then critical requests. Latency remained within reasonable limits throughout, and successful RPS throughput remained stable.\nExperimental behavior of CPU based load-shedding using synthetic traffic.P99 latency stayed within a reasonable range throughout the experiment, even as RPS surpassed 6x the autoscale target.\nAnti-patterns with load-shedding\nAnti-pattern 1 — No shedding\nIn the above graphs, the limiter does a good job keeping latency low for the successful requests. If there was no shedding here, we’d see latency increase for all requests, instead of a fast failure in some requests that can be retried. Further, this can result in a death spiral where one instance becomes unhealthy, resulting in more load on other instances, resulting in all instances becoming unhealthy before auto-scaling can kick in.\nNo load-shedding: In the absence of load-shedding, increased latency can degrade all requests instead of rejecting some requests (that can be retried), and can make instances unhealthy\nAnti-pattern 2 — Congestive failure\nAnother anti-pattern to watch out for is congestive failure or shedding too aggressively. If the load-shedding is due to an increase in traffic, the successful RPS should not drop after load-shedding. Here is an example of what congestive failure looks like:\nCongestive failure: After 16:57, the service starts rejecting most requests and is not able to sustain a successful 240 RPS that it was before load-shedding kicked in. This can be seen in fixed concurrency limiters or when load-shedding consumes too much CPU preventing any other work from being done\nWe can see in the Experiments with CPU based load-shedding section above that our load-shedding implementation avoids both these anti-patterns by keeping latency low and sustaining as much successful RPS during load-shedding as before.\nGeneric IO based load-shedding\nSome services are not CPU-bound but instead are IO-bound by backing services or datastores that can apply back pressure via increased latency when they are overloaded either in compute or in storage capacity. For these services we re-use the prioritized load shedding techniques, but we introduce new utilization measures to feed into the shedding logic. Our initial implementation supports two forms of latency based shedding in addition to standard adaptive concurrency limiters (themselves a measure of average latency):\n\nThe service can specify per-endpoint target and maximum latencies, which allow the service to shed when the service is abnormally slow regardless of backend.\nThe Netflix storage services running on the Data Gateway return observed storage target and max latency SLO utilization, allowing services to shed when they overload their allocated storage capacity.\n\nThese utilization measures provide early warning signs that a service is generating too much load to a backend, and allow it to shed low priority work before it overwhelms that backend. The main advantage of these techniques over concurrency limits alone is they require less tuning as our services already must maintain tight latency service-level-objectives (SLOs), for example a p50 < 10ms and p100 < 500ms. So, rephrasing these existing SLOs as utilizations allows us to shed low priority work early to prevent further latency impact to high priority work. At the same time, the system will accept as much work as it can while maintaining SLO’s.\nTo create these utilization measures, we count how many requests are processed slower than our target and maximum latency objectives, and emit the percentage of requests failing to meet those latency goals. For example, our KeyValue storage service offers a 10ms target with 500ms max latency for each namespace, and all clients receive utilization measures per data namespace to feed into their prioritized load shedding. These measures look like:\nutilization(namespace) = {\n  overall = 12\n  latency = {\n    slo_target = 12,\n    slo_max = 0\n  }\n  system = {\n    storage = 17,\n    compute = 10,\n  }\n}\nIn this case, 12% of requests are slower than the 10ms target, 0% are slower than the 500ms max latency (timeout), and 17% of allocated storage is utilized. Different use cases consult different utilizations in their prioritized shedding, for example batches that write data daily may get shed when system storage utilization is approaching capacity as writing more data would create further instability.\nAn example where the latency utilization is useful is for one of our critical file origin services which accepts writes of new files in the AWS cloud and acts as an origin (serves reads) for those files to our Open Connect CDN infrastructure. Writes are the most critical and should never be shed by the service, but when the backing datastore is getting overloaded, it is reasonable to progressively shed reads to files which are less critical to the CDN as it can retry those reads and they do not affect the product experience.\nTo achieve this goal, the origin service configured a KeyValue latency based limiter that starts shedding reads to files which are less critical to the CDN when the datastore reports a target latency utilization exceeding 40%. We then stress tested the system by generating over 50Gbps of read traffic, some of it to high priority files and some of it to low priority files:\n\nIn this test, there are a nominal number of critical writes and a high number of reads to both low and high priority files. In the top-left graph we ramp to 2000 read/second of ~4MiB files until we can trigger overload of the backend store at over 50Gbps in the top-center graph. When that happens, the top-right graph shows that even under significant load, the origin only sheds low priority read work to preserve high-priority writes and reads. Before this change when we hit breaking points, critical writes and reads would fail along with low priority reads. During this test the CPU load of the file serving service was nominal (<10%), so in this case only IO based limiters are able to protect the system. It is also important to note that the origin will serve more traffic as long as the backing datastore continues accepting it with low latency, preventing the problems we had with concurrency limits in the past where they would either shed too early when nothing was actually wrong or too late when we had entered congestive failure.\nConclusion and Future Directions\nThe implementation of service-level prioritized load shedding has proven to be a significant step forward in maintaining high availability and excellent user experience for Netflix customers, even during unexpected system stress.\nStay tuned for more updates as we innovate to keep your favorite shows streaming smoothly, no matter what SLO busters lie in wait.\nAcknowledgements\nWe would like to acknowledge the many members of the Netflix consumer product, platform, and open connect teams who have designed, implemented, and tested these prioritization techniques. In particular: Xiaomei Liu, Raj Ummadisetty, Shyam Gala, Justin Guerra, William Schor, Tony Ghita et al.\n\nEnhancing Netflix Reliability with Service-Level Prioritized Load Shedding was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Netflix Technology Blog",
        "guid": "https://medium.com/p/e735e6ce8f7d",
        "categories": [
          "chaos-engineering",
          "load-shedding",
          "reliability",
          "distributed-systems",
          "netflix"
        ],
        "isoDate": "2024-06-25T22:58:09.000Z"
      },
      {
        "creator": "Netflix Technology Blog",
        "title": "A Recap of the Data Engineering Open Forum at Netflix",
        "link": "https://netflixtechblog.com/a-recap-of-the-data-engineering-open-forum-at-netflix-6b4d4410b88f?source=rss----2615bd06b42e---4",
        "pubDate": "Thu, 20 Jun 2024 15:01:27 GMT",
        "content:encodedSnippet": "A summary of sessions at the first Data Engineering Open Forum at Netflix on April 18th, 2024\nThe Data Engineering Open Forum at Netflix on April 18th, 2024.\nAt Netflix, we aspire to entertain the world, and our data engineering teams play a crucial role in this mission by enabling data-driven decision-making at scale. Netflix is not the only place where data engineers are solving challenging problems with creative solutions. On April 18th, 2024, we hosted the inaugural Data Engineering Open Forum at our Los Gatos office, bringing together data engineers from various industries to share, learn, and connect.\nAt the conference, our speakers share their unique perspectives on modern developments, immediate challenges, and future prospects of data engineering. We are excited to share the recordings of talks from the conference with the rest of the world.\nOpening Remarks\nRecording\nSpeaker: Max Schmeiser (Vice President of Studio and Content Data Science & Engineering)\nSummary: Max Schmeiser extends a warm welcome to all attendees, marking the beginning of our inaugural Data Engineering Open Forum.\nEvolving from Rule-based Classifier: Machine Learning Powered Auto Remediation in Netflix Data Platform\nRecording\nSpeakers:\n\nStephanie Vezich Tamayo (Senior Machine Learning Engineer at Netflix)\nBinbing Hou (Senior Software Engineer at Netflix)\n\nSummary: At Netflix, hundreds of thousands of workflows and millions of jobs are running every day on our big data platform, but diagnosing and remediating job failures can impose considerable operational burdens. To handle errors efficiently, Netflix developed a rule-based classifier for error classification called “Pensive.” However, as the system has increased in scale and complexity, Pensive has been facing challenges due to its limited support for operational automation, especially for handling memory configuration errors and unclassified errors. To address these challenges, we have developed a new feature called “Auto Remediation,” which integrates the rules-based classifier with an ML service.\nAutomating the Data Architect: Generative AI for Enterprise Data Modeling\nRecording\nSpeaker: Jide Ogunjobi (Founder & CTO at Context Data)\nSummary: As organizations accumulate ever-larger stores of data across disparate systems, efficiently querying and gaining insights from enterprise data remain ongoing challenges. To address this, we propose developing an intelligent agent that can automatically discover, map, and query all data within an enterprise. This “Enterprise Data Model/Architect Agent” employs generative AI techniques for autonomous enterprise data modeling and architecture.\nTulika Bhatt, Senior Data Engineer at Netflix, shared how her team manages impression data at scale.\nReal-Time Delivery of Impressions at Scale\nRecording\nSpeaker: Tulika Bhatt (Senior Data Engineer at Netflix)\nSummary: Netflix generates approximately 18 billion impressions daily. These impressions significantly influence a viewer’s browsing experience, as they are essential for powering video ranker algorithms and computing adaptive pages, With the evolution of user interfaces to be more responsive to in-session interactions, coupled with the growing demand for real-time adaptive recommendations, it has become highly imperative that these impressions are provided on a near real-time basis. This talk will delve into the creative solutions Netflix deploys to manage this high-volume, real-time data requirement while balancing scalability and cost.\nReflections on Building a Data Platform From the Ground Up in a Post-GDPR World\nRecording\nSpeaker: Jessica Larson (Data Engineer & Author of “Snowflake Access Control”)\nSummary: The requirements for creating a new data warehouse in the post-GDPR world are significantly different from those of the pre-GDPR world, such as the need to prioritize sensitive data protection and regulatory compliance over performance and cost. In this talk, Jessica Larson shares her takeaways from building a new data platform post-GDPR.\nUnbundling the Data Warehouse: The Case for Independent Storage\nRecording\nSpeaker: Jason Reid (Co-founder & Head of Product at Tabular)\nSummary: Unbundling a data warehouse means splitting it into constituent and modular components that interact via open standard interfaces. In this talk, Jason Reid discusses the pros and cons of both data warehouse bundling and unbundling in terms of performance, governance, and flexibility, and he examines how the trend of data warehouse unbundling will impact the data engineering landscape in the next 5 years.\nClark Wright, Staff Analytics Engineer at Airbnb, talked about the concept of Data Quality Score at Airbnb.\nData Quality Score: How We Evolved the Data Quality Strategy at Airbnb\nRecording\nSpeaker: Clark Wright (Staff Analytics Engineer at Airbnb)\nSummary: Recently, Airbnb published a post to their Tech Blog called Data Quality Score: The next chapter of data quality at Airbnb. In this talk, Clark Wright shares the narrative of how data practitioners at Airbnb recognized the need for higher-quality data and then proposed, conceptualized, and launched Airbnb’s first Data Quality Score.\nData Productivity at Scale\nRecording\nSpeaker: Iaroslav Zeigerman (Co-Founder and Chief Architect at Tobiko Data)\nSummary: The development and evolution of data pipelines are hindered by outdated tooling compared to software development. Creating new development environments is cumbersome: Populating them with data is compute-intensive, and the deployment process is error-prone, leading to higher costs, slower iteration, and unreliable data. SQLMesh, an open-source project born from our collective experience at companies like Airbnb, Apple, Google, and Netflix, is designed to handle the complexities of evolving data pipelines at an internet scale. In this talk, Iaroslav Zeigerman discusses challenges faced by data practitioners today and how core SQLMesh concepts solve them.\nLast but not least, thank you to the organizers of the Data Engineering Open Forum: Chris Colburn, Xinran Waibel, Jai Balani, Rashmi Shamprasad, and Patricia Ho.\nUntil next time!\nIf you are interested in attending a future Data Engineering Open Forum, we highly recommend you join our Google Group to stay tuned to event announcements.\n\nA Recap of the Data Engineering Open Forum at Netflix was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Netflix Technology Blog",
        "guid": "https://medium.com/p/6b4d4410b88f",
        "categories": [
          "data-engineering",
          "data-science",
          "technology",
          "software-engineering",
          "data"
        ],
        "isoDate": "2024-06-20T15:01:27.000Z"
      },
      {
        "creator": "Netflix Technology Blog",
        "title": "Video annotator: building video classifiers using vision-language models and active learning",
        "link": "https://netflixtechblog.com/video-annotator-building-video-classifiers-using-vision-language-models-and-active-learning-8ebdda0b2db4?source=rss----2615bd06b42e---4",
        "pubDate": "Wed, 19 Jun 2024 15:29:29 GMT",
        "content:encodedSnippet": "Video annotator: a framework for efficiently building video classifiers using vision-language models and active learning\nAmir Ziai, Aneesh Vartakavi, Kelli Griggs, Eugene Lok, Yvonne Jukes, Alex Alonso, Vi Iyengar, Anna Pulido\nhttps://medium.com/media/02a5bbf97c619182adba24b45e42edcb/href\nIntroduction\nProblem\nHigh-quality and consistent annotations are fundamental to the successful development of robust machine learning models. Conventional techniques for training machine learning classifiers are resource intensive. They involve a cycle where domain experts annotate a dataset, which is then transferred to data scientists to train models, review outcomes, and make changes. This labeling process tends to be time-consuming and inefficient, sometimes halting after a few annotation cycles.\nImplications\nConsequently, less effort is invested in annotating high-quality datasets compared to iterating on complex models and algorithmic methods to improve performance and fix edge cases. As a result, ML systems grow rapidly in complexity.\nFurthermore, constraints on time and resources often result in leveraging third-party annotators rather than domain experts. These annotators perform the labeling task without a deep understanding of the model’s intended deployment or usage, often making consistent labeling of borderline or hard examples, especially in more subjective tasks, a challenge.\nThis necessitates multiple review rounds with domain experts, leading to unexpected costs and delays. This lengthy cycle can also result in model drift, as it takes longer to fix edge cases and deploy new models, potentially hurting usefulness and stakeholder trust.\nSolution\nWe suggest that more direct involvement of domain experts, using a human-in-the-loop system, can resolve many of these practical challenges. We introduce a novel framework, Video Annotator (VA), which leverages active learning techniques and zero-shot capabilities of large vision-language models to guide users to focus their efforts on progressively harder examples, enhancing the model’s sample efficiency and keeping costs low.\nVA seamlessly integrates model building into the data annotation process, facilitating user validation of the model before deployment, therefore helping with building trust and fostering a sense of ownership. VA also supports a continuous annotation process, allowing users to rapidly deploy models, monitor their quality in production, and swiftly fix any edge cases by annotating a few more examples and deploying a new model version.\nThis self-service architecture empowers users to make improvements without active involvement of data scientists or third-party annotators, allowing for fast iteration.\nVideo understanding\nWe design VA to assist in granular video understanding which requires the identification of visuals, concepts, and events within video segments. Video understanding is fundamental for numerous applications such as search and discovery, personalization, and the creation of promotional assets. Our framework allows users to efficiently train machine learning models for video understanding by developing an extensible set of binary video classifiers, which power scalable scoring and retrieval of a vast catalog of content.\nVideo classification\nVideo classification is the task of assigning a label to an arbitrary-length video clip, often accompanied by a probability or prediction score, as illustrated in Fig 1.\nFig 1- Functional view of a binary video classifier. A few-second clip from ”Operation Varsity Blues: The College Admissions Scandal” is passed to a binary classifier for detecting the ”establishing shots” label. The classifier outputs a very high score (score is between 0 and 1), indicating that the video clip is very likely an establishing shot. In filmmaking, an establishing shot is a wide shot (i.e. video clip between two consecutive cuts) of a building or a landscape that is intended for establishing the time and location of the scene.\nVideo understanding via an extensible set of video classifiers\nBinary classification allows for independence and flexibility, allowing us to add or improve one model independent of the others. It also has the additional benefit of being easier to understand and build for our users. Combining the predictions of multiple models allows us a deeper understanding of the video content at various levels of granularity, illustrated in Fig 2.\nFig 2- Three video clips and the corresponding binary classifier scores for three video understanding labels. Note that these labels are not mutually exclusive. Video clips are from Operation Varsity Blues: The College Admissions Scandal, 6 Underground, and Leave The World Behind, respectively.\nVideo Annotator (VA)\nIn this section, we describe VA’s three-step process for building video classifiers.\nStep 1 — search\nUsers begin by finding an initial set of examples within a large, diverse corpus to bootstrap the annotation process. We leverage text-to-video search to enable this, powered by video and text encoders from a Vision-Language Model to extract embeddings. For example, an annotator working on the establishing shots model may start the process by searching for “wide shots of buildings”, illustrated in Fig 3.\nFig 3- Step 1 — Text-to-video search to bootstrap the annotation process.\nStep 2 — active learning\nThe next stage involves a classic Active Learning loop. VA then builds a lightweight binary classifier over the video embeddings, which is subsequently used to score all clips in the corpus, and presents some examples within feeds for further annotation and refinement, as illustrated in Fig 4.\nFig 4- Step 2 — Active Learning loop. The annotator clicks on build, which initiates classifier training and scoring of all clips in a video corpus. Scored clips are organized in four feeds.\nThe top-scoring positive and negative feeds display examples with the highest and lowest scores respectively. Our users reported that this provided a valuable indication as to whether the classifier has picked up the correct concepts in the early stages of training and spot cases of bias in the training data that they were able to subsequently fix. We also include a feed of “borderline” examples that the model is not confident about. This feed helps with discovering interesting edge cases and inspires the need for labeling additional concepts. Finally, the random feed consists of randomly selected clips and helps to annotate diverse examples which is important for generalization.\nThe annotator can label additional clips in any of the feeds and build a new classifier and repeat as many times as desired.\nStep 3 — review\nThe last step simply presents the user with all annotated clips. It’s a good opportunity to spot annotation mistakes and to identify ideas and concepts for further annotation via search in step 1. From this step, users often go back to step 1 or step 2 to refine their annotations.\nExperiments\nTo evaluate VA, we asked three video experts to annotate a diverse set of 56 labels across a video corpus of 500k shots. We compared VA to the performance of a few baseline methods, and observed that VA leads to the creation of higher quality video classifiers. Fig 5 compares VA’s performance to baselines as a function of the number of annotated clips.\nFig 5- Model quality (i.e. Average Precision) as a function of the number of annotated clips for the “establishing shots” label. We observe that all methods outperform the baseline, and that all methods benefit from additional annotated data, albeit to varying degrees.\nYou can find more details about VA and our experiments in this paper.\nConclusion\nWe presented Video Annotator (VA), an interactive framework that addresses many challenges associated with conventional techniques for training machine learning classifiers. VA leverages the zero-shot capabilities of large vision-language models and active learning techniques to enhance sample efficiency and reduce costs. It offers a unique approach to annotating, managing, and iterating on video classification datasets, emphasizing the direct involvement of domain experts in a human-in-the-loop system. By enabling these users to rapidly make informed decisions on hard samples during the annotation process, VA increases the system’s overall efficiency. Moreover, it allows for a continuous annotation process, allowing users to swiftly deploy models, monitor their quality in production, and rapidly fix any edge cases.\nThis self-service architecture empowers domain experts to make improvements without the active involvement of data scientists or third-party annotators, and fosters a sense of ownership, thereby building trust in the system.\nWe conducted experiments to study the performance of VA, and found that it yields a median 8.3 point improvement in Average Precision relative to the most competitive baseline across a wide-ranging assortment of video understanding tasks. We release a dataset with 153k labels across 56 video understanding tasks annotated by three professional video editors using VA, and also release code to replicate our experiments.\n\nVideo annotator: building video classifiers using vision-language models and active learning was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Netflix Technology Blog",
        "guid": "https://medium.com/p/8ebdda0b2db4",
        "categories": [
          "video-editing",
          "artificial-intelligence",
          "machine-learning"
        ],
        "isoDate": "2024-06-19T15:29:29.000Z"
      }
    ]
  },
  {
    "name": "JetBrains: Developer Tools for Professionals and Teams – Company Blog | JetBrains",
    "category": "기업",
    "posts": [
      {
        "creator": "Maha Taqi",
        "title": "PyCharm 2024.1.4: What’s New!",
        "link": "https://blog.jetbrains.com/pycharm/2024/06/pycharm-2024-1-4/",
        "pubDate": "Tue, 25 Jun 2024 17:49:37 +0000",
        "content:encodedSnippet": "PyCharm 2024.1.4 is here! View and navigate to URLs directly from the Editor tab, get context-based model suggestions for Hugging Face, and enjoy smart code assistance for TypedDict (PEP 692).\nYou can download the latest version from our download page, or update your current version through our free Toolbox App. \n[banner]\nDownload PyCharm 2024.1.4\n                                                    \nKey Features \nGutter actions and inlay hints for URLs\nNew gutter icons provide an easy way to manage URLs in Flask, FastAPI, and Django projects. In just a few clicks, you can test an endpoint by running the request in the HTTP Client, view all lower-level endpoints, and more. \n\n\n\n\nFurthermore, new inlay hints with endpoint URLs further contribute to code readability for Python web frameworks.\n\n\n\n\nPEP 692: Smart code assistance for TypedDict\nWith support for PEP 692, keyword arguments defined as TypedDict are now available when you invoke parameter info (⌘P / Ctrl+P), in code completion, and in the quick documentation popup (F1 / Ctrl+Q). Now you have access to yet another tool that lets you use the latest type hinting capabilities in Python to develop an easy-to-understand codebase.\n\n\n\n    \nLearn more\n                                                    \nHugging Face: Model suggestions\nPyCharm can now suggest a list of relevant Hugging Face models that best suit your needs. This allows you to decide which model to use and install without ever having to leave the IDE. Decide what you would like the model to do, and PyCharm will provide a list of options to choose from! \nAfter selecting a model, PyCharm will suggest inserting a code snippet that allows you to use this model directly in the open file, and it will download and install all of the missing packages on which this model depends.\n\n\n\n\nWarnings for uninstalled packages in requirements.txt\nPyCharm now underlines packages that are listed in requirements.txt but that are not installed on the current Python interpreter with a yellow squiggly line. Hover over the package name and click Install all missing packages to immediately set up your development environment.\n\n\n\n    \nDownload PyCharm 2024.1.4\n                                                    \nGet all of the details in our release notes so you don’t miss out on anything new!\nIf you come across any bugs, please let us know in our issue tracker so we can fix them right away. Connect with us on X (formerly Twitter) and share your thoughts on PyCharm 2024.1.4!",
        "dc:creator": "Maha Taqi",
        "content": "PyCharm 2024.1.4 is here! View and navigate to URLs directly from the Editor tab, get context-based model suggestions for Hugging Face, and enjoy smart code assistance for TypedDict (PEP 692). You can download the latest version from our download page, or update your current version through our free Toolbox App.&#160; [banner] Key Features&#160; Gutter actions [&#8230;]",
        "contentSnippet": "PyCharm 2024.1.4 is here! View and navigate to URLs directly from the Editor tab, get context-based model suggestions for Hugging Face, and enjoy smart code assistance for TypedDict (PEP 692). You can download the latest version from our download page, or update your current version through our free Toolbox App.  [banner] Key Features  Gutter actions […]",
        "guid": "https://blog.jetbrains.com/?post_type=pycharm&p=486612",
        "categories": [
          "releases",
          "endpoints",
          "hugging-face",
          "pep-692",
          "typeddict"
        ],
        "isoDate": "2024-06-25T17:49:37.000Z"
      },
      {
        "creator": "Alexander Kurakin",
        "title": "Rider 2024.2 EAP 4: Inline Rendering of Doc Comments and Tasks View ",
        "link": "https://blog.jetbrains.com/dotnet/2024/06/25/rider-2024-2-eap-4/",
        "pubDate": "Tue, 25 Jun 2024 13:23:58 +0000",
        "content:encodedSnippet": "Another week, another Rider 2024.2 Early Access Program build! \nThis EAP build is packed with highly anticipated features such as the Tasks view and inline rendering of documentation comments. Let’s have a look!\nInline rendering of documentation comments\nInstead of a wall of text mixed with tags, inline rendering changes these comments into a more visually appealing format, complete with links and code blocks. No more struggling with confusing tags – you’ll be able to read everything more clearly. It can be utilized in any C# or F# files to improve your reading experience. \n\n                        \n\n\nTasks tab and a task dependency graph\nA new Tasks tab has been added to the Debug tool window. This tab presents detailed insights into the System.Threading.Tasks.Task and ValueTask objects, enhancing your ability to manage and monitor asynchronous operations. The tasks can be displayed in either a table or graph format, and you can switch between the two views via a drop-down menu located at the top-right corner of the tab.\nThe table view offers in-depth insights into the current state of each task – whether it is running, completed, blocked, or faulty, along with pertinent metadata. Showing task dependencies and simplifying the identification of deadlocks, the graph view is valuable in the debugging process.\n\n                        \n\n\nFor the full list of changes included in this build, please go to our issue tracker.\nDownload Rider 2024.2 EAP",
        "dc:creator": "Alexander Kurakin",
        "content": "Another week, another Rider 2024.2 Early Access Program build!&#160; This EAP build is packed with highly anticipated features such as the Tasks view and inline rendering of documentation comments. Let’s have a look! Inline rendering of documentation comments Instead of a wall of text mixed with tags, inline rendering changes these comments into a more [&#8230;]",
        "contentSnippet": "Another week, another Rider 2024.2 Early Access Program build!  This EAP build is packed with highly anticipated features such as the Tasks view and inline rendering of documentation comments. Let’s have a look! Inline rendering of documentation comments Instead of a wall of text mixed with tags, inline rendering changes these comments into a more […]",
        "guid": "https://blog.jetbrains.com/?post_type=dotnet&p=486375",
        "categories": [
          "net-tools",
          "eap",
          "rider",
          "debugger",
          "doc-comments",
          "tasks"
        ],
        "isoDate": "2024-06-25T13:23:58.000Z"
      },
      {
        "creator": "Alena Guzharina",
        "title": "What’s New in Datalore 2024.3: Quality Improvements",
        "link": "https://blog.jetbrains.com/datalore/2024/06/25/what-s-new-in-datalore-2024-3-quality-improvements/",
        "pubDate": "Tue, 25 Jun 2024 09:41:56 +0000",
        "content:encodedSnippet": "Following a feature-rich 2024.2 release, we’ve focused on enhancing the quality and reliability of Datalore in our 2024.3 update. Datalore Community, Professional, and Team customers have already received the new update automatically, and Datalore Enterprise customers can upgrade by following these instructions.\nEnhanced file handling for scheduled runs\nWe’ve added an option that lets you save files generated by scheduled runs directly to /data/notebook_files, just like you can for files generated during regular notebook sessions. This new feature allows files to be overwritten, and you can easily find files from all of your previous runs in one directory. The option to save files to isolated artifacts still exists, giving you more flexibility for your scheduled workflows. \n\n\n\n\nSmoother environment setup for R and Scala\nFrom now on, the necessary environment setup for R and Scala will be completed before the execution of init.sh, ensuring a smoother initial run. This enhancement allows you to make meaningful changes to init.sh scripts with the knowledge that they will be applied correctly, streamlining environment configuration.\nEarly terminal access during setup\nThe terminal is now accessible before environment setup is complete, allowing for early troubleshooting.\nOther improvements and bug fixes\nExported workspaces now include both notebooks and their associated reports in the downloaded .zip file. \nDatalore now supports ipydatagrid versions 1.3.0 and 1.3.1. \nA new reportLink field has been added to the top-level metadata of downloaded notebooks. \nWhen refreshing a database schema, you will now see a spinner during the refresh process and receive a clear notification when it is finished, alerting you to any errors. \nFor improved security, Git repository pulls are now limited to 100 MB for Datalore Community, Professional, and Team plans. Datalore Enterprise customers can configure the maximum pull size via the GIT_REPOSITORY_SIZE_LIMIT environment variable.\nNotebooks created by non-owners of a workspace no longer have incorrect report links in the workspace view. \nThe correlation chart in the Visualize tab no longer blinks. \nIn scenarios where PyPi servers are not accessible, Datalore now provides clear messages to admins about connectivity issues, prevents infinite loading, and offers an improved troubleshooting process. Only Enterprise plan users were affected.\nWe’ve fixed a rendering issue that was causing ipywidgets outputs to shake in the report builder and on the report page. Only Community, Professional, and Team plans were affected. \nScheduled reports now update variables in Markdown cells. \n\n\n\n\nDatalore Community, Professional, and Team customers have already received these updates automatically. Datalore Enterprise customers can upgrade by following these instructions:\n      \n      Upgrade to 2024.3\n    \n\n\n\n\nKind regards,\nThe Datalore team",
        "dc:creator": "Alena Guzharina",
        "content": "Following a feature-rich 2024.2 release, we’ve focused on enhancing the quality and reliability of Datalore in our 2024.3 update. Datalore Community, Professional, and Team customers have already received the new update automatically, and Datalore Enterprise customers can upgrade by following these instructions. Enhanced file handling for scheduled runs We&#8217;ve added an option that lets you [&#8230;]",
        "contentSnippet": "Following a feature-rich 2024.2 release, we’ve focused on enhancing the quality and reliability of Datalore in our 2024.3 update. Datalore Community, Professional, and Team customers have already received the new update automatically, and Datalore Enterprise customers can upgrade by following these instructions. Enhanced file handling for scheduled runs We’ve added an option that lets you […]",
        "guid": "https://blog.jetbrains.com/?post_type=datalore&p=486569",
        "categories": [
          "releases"
        ],
        "isoDate": "2024-06-25T09:41:56.000Z"
      },
      {
        "creator": "Alexander Kurakin",
        "title": "Bug Fixes for ReSharper 2024.1.4 and Rider 2024.1.4 Are Now Available! ",
        "link": "https://blog.jetbrains.com/dotnet/2024/06/24/resharper-rider-2024-1-4/",
        "pubDate": "Mon, 24 Jun 2024 20:15:03 +0000",
        "content:encodedSnippet": "The new bug fixes for the 2024.1 release are available to download. \nReSharper\nReSharper 2024.1.4 fixes a couple of notable issues:\nWe’ve fixed a StackOverflow exception on calling Find Usages or Go to Declaration features on strings from resources (RSRP-496787).  \nWe’ve eliminated a code analysis error that occurred when referencing .NET Framework 4.8.1 in a .NET 8 project (RSRP-496682). \nYou can find the full list of fixed issues here.\nDownload ReSharper\n                                                    \nRider\nIn addition to fixed issues from ReSharper, Rider 2024.1.4 delivers several important fixes of its own: \nNo more deadlocks in solutions with .editorconfig files (RIDER-107157).\nRider doesn’t lose Blueprint usages in Unreal Engine solutions if changes are made to Blueprints (RIDER-112251).\nFor the full list of fixed issues, please refer to this page.\nDownload Rider",
        "dc:creator": "Alexander Kurakin",
        "content": "The new bug fixes for the 2024.1 release are available to download.&#160; ReSharper ReSharper 2024.1.4 fixes a couple of notable issues: You can find the full list of fixed issues here. Rider In addition to fixed issues from ReSharper, Rider 2024.1.4 delivers several important fixes of its own:&#160; For the full list of fixed issues, [&#8230;]",
        "contentSnippet": "The new bug fixes for the 2024.1 release are available to download.  ReSharper ReSharper 2024.1.4 fixes a couple of notable issues: You can find the full list of fixed issues here. Rider In addition to fixed issues from ReSharper, Rider 2024.1.4 delivers several important fixes of its own:  For the full list of fixed issues, […]",
        "guid": "https://blog.jetbrains.com/?post_type=dotnet&p=486485",
        "categories": [
          "net-tools",
          "releases",
          "rider",
          "resharper"
        ],
        "isoDate": "2024-06-24T20:15:03.000Z"
      },
      {
        "creator": "Ksenia Shneyveys",
        "title": "All KotlinConf Recordings Are Now Available. Enjoy!",
        "link": "https://blog.jetbrains.com/kotlin/2024/06/kotlinconf24-recordings/",
        "pubDate": "Mon, 24 Jun 2024 11:57:07 +0000",
        "content:encodedSnippet": "You can now find all of the session recordings from KotlinConf’24 on the conference website!\nSessions are also being added to the Kotlin YouTube channel:\n\n\n\n\n\n\nPhotos and slides from some of the sessions are also available – click on a talk and select “Download slides” for the latter.\nDon’t miss these additional materials from the conference:\nA roundup of the highlights from the keynote in this blog post.\nKotlinConfersations, interviews with speakers and attendees hosted by Huyen Tue Dao.\nKotlinConf 2024 was held in Copenhagen, Denmark, from May 22–24, drawing over 2,000 attendees, speakers, and partners. The event began with a workshop day and continued with two days of sessions. The powerful keynote featured Jeffrey van Gogh (Google), Michail Zarečenskij, Julie Gunderson (both Amazon Web Services), Eve Matthaey (Meta), and Egor Tolstoy, Ekaterina Petrova, Vsevolod Tolstopyatov, Sebastian Aigner, Svetlana Isakova (all JetBrains) as speakers. The event concluded with a closing panel hosted by Hadi Hariri, featuring different folks from the Kotlin community answering questions.\nCatch all of the action from KotlinConf’24 and keep up with the latest conference updates by following KotlinConf on X.",
        "dc:creator": "Ksenia Shneyveys",
        "content": "You can now find all of the session recordings from KotlinConf’24 on the conference website! Sessions are also being added to the Kotlin YouTube channel: Photos and slides from some of the sessions are also available – click on a talk and select &#8220;Download slides&#8221; for the latter. Don&#8217;t miss these additional materials from the [&#8230;]",
        "contentSnippet": "You can now find all of the session recordings from KotlinConf’24 on the conference website! Sessions are also being added to the Kotlin YouTube channel: Photos and slides from some of the sessions are also available – click on a talk and select “Download slides” for the latter. Don’t miss these additional materials from the […]",
        "guid": "https://blog.jetbrains.com/?post_type=kotlin&p=486500",
        "categories": [
          "news",
          "kotlinconf"
        ],
        "isoDate": "2024-06-24T11:57:07.000Z"
      },
      {
        "creator": "Maria Kosukhina",
        "title": "IntelliJ IDEA 2024.1.4 Is Out!",
        "link": "https://blog.jetbrains.com/idea/2024/06/intellij-idea-2024-1-4/",
        "pubDate": "Fri, 21 Jun 2024 10:07:04 +0000",
        "content:encodedSnippet": "IntelliJ IDEA 2024.1.4 has arrived with several valuable fixes.\nYou can update to this version from inside the IDE, using the Toolbox App, or using snaps if you are a Ubuntu user. You can also download it from our website.\nHere are the most notable updates included in v2024.1.4:\nCode validation no longer produces incorrect code highlighting caused by freezes while reevaluating inspections. [IJPL-28967]\nWe fixed the issue causing the IDE to lag when editing code. [IJPL-149640, IJPL-156086]\nThe Unknown HTTP header inspection once again loads settings from the inspection profile correctly. [IJPL-65369]\nThe IDE no longer fails to perform operations, citing the Too complex, sorry message. [IJPL-1116]\nUpdating plugins from the Welcome screen now works as expected again. [IJPL-6076]\nThe Enable Project-Wide Analysis action is once again available in the Problems tool window. [IJPL-156560]\n\n\n\n\nTo find out more details about the issues resolved, please refer to the release notes.\nIf you encounter any bugs, please report them to our issue tracker.\nHappy developing!",
        "dc:creator": "Maria Kosukhina",
        "content": "IntelliJ IDEA 2024.1.4 has arrived with several valuable fixes. You can update to this version from inside the IDE, using the&#160;Toolbox App, or using snaps if you are a Ubuntu user. You can also download it from our&#160;website. Here are the most notable updates included in v2024.1.4: To find out more details about the issues [&#8230;]",
        "contentSnippet": "IntelliJ IDEA 2024.1.4 has arrived with several valuable fixes. You can update to this version from inside the IDE, using the Toolbox App, or using snaps if you are a Ubuntu user. You can also download it from our website. Here are the most notable updates included in v2024.1.4: To find out more details about the issues […]",
        "guid": "https://blog.jetbrains.com/?post_type=idea&p=485008",
        "categories": [
          "releases",
          "2024-1",
          "bug-fix-update",
          "intellij-idea",
          "intellij-idea-2024-1"
        ],
        "isoDate": "2024-06-21T10:07:04.000Z"
      },
      {
        "creator": "David Watson",
        "title": "WebStorm 2024.1.5 Is Now Available",
        "link": "https://blog.jetbrains.com/webstorm/2024/06/webstorm-2024-1-5/",
        "pubDate": "Fri, 21 Jun 2024 10:02:03 +0000",
        "content:encodedSnippet": "WebStorm 2024.1.5, the fifth update for WebStorm 2024.1, is now available! It contains several improvements from the platform. \nYou can update to v2024.1.5 by using the Toolbox App, installing it right from the IDE, or downloading it from our website. \nNotable improvements\nHere are some of the notable updates included in v2024.1.5:\nWe’ve fixed the issue causing inspections to show persistent errors that do not refresh during editing (IJPL-28967).\nWe’ve fixed the performance issue causing a delay with caret movement and input (IJPL-149640).\nWe’ve fixed the issue preventing WebStorm from loading profile settings for inspections (IJPL-65369).\nWe’ve fixed a regression issue causing a significant slowdown of the IDE (IJPL-156086).\nWe’ve fixed the severe input and cursor movement lag issues when the show tool window names and background image options are enabled (IJPL-149213).\nWebStorm no longer fails to perform operations, citing the “Too complex, sorry” message (IJPL-1116).\nWe’ve removed the Start Trial confirmation dialog (IJPL-149888).\nWe’ve fixed the issue causing background actions on the main toolbar to get stuck (IJPL-155988).\nUpdating plugins from the Welcome screen works as expected again (IJPL-6076).\nThe Enable Project-Wide Analysis action is again available in the Problems tool window (IJPL-156560).\nThat’s all for today! For the full list of issues addressed in WebStorm 2024.1.5, please see the release notes.\nThe WebStorm team",
        "dc:creator": "David Watson",
        "content": "WebStorm 2024.1.5, the fifth update for WebStorm 2024.1, is now available! It contains several improvements from the platform.&#160; You can update to v2024.1.5 by using the Toolbox App, installing it right from the IDE, or downloading it from our website.&#160; Notable improvements Here are some of the notable updates included in v2024.1.5: That’s all for [&#8230;]",
        "contentSnippet": "WebStorm 2024.1.5, the fifth update for WebStorm 2024.1, is now available! It contains several improvements from the platform.  You can update to v2024.1.5 by using the Toolbox App, installing it right from the IDE, or downloading it from our website.  Notable improvements Here are some of the notable updates included in v2024.1.5: That’s all for […]",
        "guid": "https://blog.jetbrains.com/?post_type=webstorm&p=486104",
        "categories": [
          "releases",
          "webstorm-2024-1"
        ],
        "isoDate": "2024-06-21T10:02:03.000Z"
      },
      {
        "creator": "Maria Kosukhina",
        "title": "IntelliJ IDEA 2024.2 EAP 6: Streamlined Log Management, Enhanced Gradle Build Script Experience, and More",
        "link": "https://blog.jetbrains.com/idea/2024/06/intellij-idea-2024-2-eap-6/",
        "pubDate": "Fri, 21 Jun 2024 06:03:42 +0000",
        "content:encodedSnippet": "IntelliJ IDEA 2024.2 EAP 6 is out! \nYou can get the latest build from our website, through the free Toolbox App, or via snaps for Ubuntu. \nDownload IntelliJ IDEA 2024.2 EAP 6\nThe latest build offers a streamlined user experience with Gradle build scripts, enhanced logging support for Java and Kotlin, and improvements to support Terraform development. To learn about the updates introduced in the previous EAP builds, check out the dedicated 2024.2 EAP section of our blog. \nJava / Kotlin\nEnhanced log management for Java and Kotlin\nContinuing our efforts to improve log handling, IntelliJ IDEA 2024.2 EAP 6 features several notable enhancements for both Java and Kotlin developers, building on the improvements made in the 2024.1 release.\nFirst, we’ve introduced string literal code highlighting, making it easier to distinguish placeholders within string literals. Additionally, the new argument resolution feature allows you to navigate seamlessly from a placeholder in the string literal to its corresponding argument.\n\n\n\n\nThe latest build also includes updated and new inspections for logging statements. We’ve modified the inspection that identifies and warns you about mismatched numbers of parameters in logger calls, taking into account the specifics of the logger being used. For example, the inspection won’t trigger a warning if the extra argument is of the Exception type.\n\n\n\n\nAnother inspection suggests converting System.out.println statements to logger calls, even if the logger is not yet defined.\n\n\n\n\nFurthermore, a new quick-fix is available to add guards for logger calls. The inspection is disabled by default. To enable it in the editor, you can adjust the severity levels in the settings.\n\n\n\n\nBuild tools \nImproved experience with Gradle build scripts\nStarting from IntelliJ IDEA 2024.2 EAP 6, the IDE provides an improved experience with build scripts thanks to the new navigation and highlighting features in Gradle build scripts. First, IntelliJ IDEA now provides smooth and accurate navigation to the Gradle plugins declared in build scripts. \n\n\n\n\nBesides that, we’ve implemented navigation between version catalog files and build scripts in libs.versions.toml file, as well as an option to run registered tasks directly from the gutter.\n\n\n\n\n\nFrameworks and technologies \nEnhanced support for Terraform \nWe’ve significantly extended coding assistance capabilities for Terraform code. \nFirst, full line code completion is now available for Terraform development. Powered by a local large language model (LLM), this functionality predicts entire lines of code, boosting your productivity. \n\n\n\n\nThe in-editor language support for Terraform has been significantly improved in IntelliJ IDEA. This includes essential code insight features such as context-aware code completion, refined syntax highlighting, and an enhanced error detection system with quick-fix suggestions. Additionally, we’ve made the IDE faster by enabling autocompletion and syntax highlighting for Terraform even before indexing is complete, allowing you to start coding more quickly.\nWe’ve also added a feature that shows documentation tooltips when you hover over elements in your Terraform code. This allows you to see relevant information instantly, helping you understand and use Terraform resources more effectively without breaking your workflow.\n\n\n\n\nThese are the most notable updates included in the IntelliJ IDEA 2024.2 EAP 6 build. For the full list of changes, refer to the release notes.\nWe value your feedback during the Early Access Program and eagerly await your comments regarding the new features on X (formerly Twitter) or below. If you find a bug, please report it in our issue tracker.\nHappy developing!",
        "dc:creator": "Maria Kosukhina",
        "content": "IntelliJ IDEA 2024.2 EAP 6 is out!&#160; You can get the latest build from our website, through the free Toolbox App, or via snaps for Ubuntu.&#160; Download IntelliJ IDEA 2024.2 EAP 6 The latest build offers a streamlined user experience with Gradle build scripts, enhanced logging support for Java and Kotlin, and improvements to support [&#8230;]",
        "contentSnippet": "IntelliJ IDEA 2024.2 EAP 6 is out!  You can get the latest build from our website, through the free Toolbox App, or via snaps for Ubuntu.  Download IntelliJ IDEA 2024.2 EAP 6 The latest build offers a streamlined user experience with Gradle build scripts, enhanced logging support for Java and Kotlin, and improvements to support […]",
        "guid": "https://blog.jetbrains.com/?post_type=idea&p=485485",
        "categories": [
          "eap",
          "2024-2-eap",
          "early-access-program",
          "intellij-idea",
          "intellij-idea-2024-2-eap"
        ],
        "isoDate": "2024-06-21T06:03:42.000Z"
      },
      {
        "creator": "Khalid Abuhakmeh",
        "title": "dotCover Command Line Tools for Automation Testing Code Coverage",
        "link": "https://blog.jetbrains.com/dotnet/2024/06/20/dotcover-command-line-tools-for-automation-testing-code-coverage/",
        "pubDate": "Thu, 20 Jun 2024 12:45:54 +0000",
        "content:encodedSnippet": "The ultimate goal of creating a testing strategy is to be confident that you and your team have done their due diligence to deliver a quality product. Test harnesses typically include a mix of unit tests, integration testing, automation, and manual testing, each with advantages and disadvantages for your overall strategy. Determining the effectiveness of your plan requires data collected during test runs, and with code coverage reports, you quantify how each style of testing contributes to your confidence levels.\nWhile code coverage is strongly associated with unit testing libraries such as xUnit, NUnit, or MSTest, the JetBrains tool dotCover can provide code coverage in many scenarios. In this post, we’ll see how to install the dotCover command line tool and gather results from any .NET process, regardless of the execution environment, including local, CI/CD, and production environments.\nDownload and try dotCover\n                                                    \nGetting Started with dotCover Command Line\ndotCover is available as a .NET CLI tool, and you can install it in any environment where you can run a dotnet process. The command line tool supports Windows (x86/x64/ARM64), Linux (x64 / ARM / ARM64 / Musl x64 / Musl ARM64), and macOS (x64 / ARM64). You can install it either as a global or local tool. \nFrom the command line, use the following command to install the tooling.\ndotnet tool install --global JetBrains.dotCover.CommandLineTools\nConfirm the tool is installed globally by running the following command.\ndotnet-dotcover --help\nIssues with global installations typically result from the installation folder for global tools not appearing in the PATH environment variable.\nFor local installation to a .NET Solution, use the following command from the root of the solution folder.\ndotnet new tool-manifest\ndotnet tool install --local JetBrains.dotCover.CommandLineTools\nOnce installed locally, run the following command from the root of your solution to verify a correct installation.\ndotnet dotcover help\nWe’re now ready to start code coverage testing any .NET application. Also, at any point, you can learn more about any command using the following format.\ndotnet dotcover help <command>\nCoverage Analysis for .NET Processes\nI’ve written a straightforward console application with various branching paths for this post. We’ll see if our manual testing can verify that we’ve hit all the branches correctly.\nvar run = true;\nConsole.WriteLine(\"Waiting for input...\");\n\nwhile (run)\n{\n    var key = Console.ReadKey();\n    \n    Console.Clear();\n\n    switch (key.Key)\n    {\n        case ConsoleKey.A:\n            Console.WriteLine(\"A is for 🍎\");\n            break;\n        case ConsoleKey.D:\n            Console.WriteLine(\"D is for 💎\");\n            break;\n        case ConsoleKey.F:\n            Console.WriteLine(\"F is for 👨‍🌾\");\n            break;\n        case ConsoleKey.S:\n            Console.WriteLine(\"S is for 🐍\");\n            break;\n        case ConsoleKey.X:\n            run = false;\n            Console.WriteLine(\"👋 Bye-Bye now\");\n            break;\n        default:\n            Console.WriteLine(\"🤷 I have no clue\");\n            break;\n    }\n}\n\n\n\n\ndotCover allows you to run any dotnet command under coverage analysis, with the most common entry points being exec and test. You can also start code coverage analysis on one or more currently running .NET processes.\nSince this application has no unit tests, we’ll use exec to start our application and then manually attempt to hit each branch. dotCover requires that any process have PDBs available to provide coverage analysis correctly.\nWe’ll use the following command from the command line to start a coverage analysis session on our console application. You’ll want to change the command to point to your project’s dll.\ndotnet dotcover cover-dotnet --ReportType=HTML --TargetArguments=CoverageExample.dll --Output=./\nThis command launches our executable under coverage analysis, waiting for an exit code to stop the session.\n(base) ~/RiderProjects/CoverageExample\ndotnet dotcover cover-dotnet --ReportType=HTML --TargetArguments=CoverageExample.dll --Output=./\n👋 Bye-Bye now\n[JetBrains dotCover] Coverage session finished [5/30/2024 1:25:02PM]\n[JetBrains dotCover] Coverage results post-processing started [5/30/2024 1:25:02PM]\n[JetBrains dotCover] Report generation started [5/30/2024 1:25:02PM]\n[JetBrains dotCover] Report generation finished [5/30/2024 1:25:02PM]\n[JetBrains dotCover] Coverage results post-processing finished [5/30/2024 1:25:02PM]\nOnce completed, we can open our coverage report to see if we’ve hit all the essential parts of our application logic.\n\n\n\n\nVery cool. Let’s talk about some more scenarios.\nNote: You can open dotCover reports directly in ReSharper or dotCover and inspect differences between different runs.\nHelpful Tips and Tricks\nWhile manual testing with coverage analysis is a fun example, you may need more practical testing strategies. The following sections will advise setting up more complex use cases with dotCover.\nDownload OS-specific tooling\nWhile installing the dotCover tooling using the dotnet CLI is convenient for development environments. You can also download an OS-specific version from our download page. The downloaded artifacts can help you build custom Docker images without needing an SDK-based base image or an internet connection to NuGet.\nChange a Container’s Entry Point\nMany folks rely on containerization to provide reliable and repeatable infrastructure for automation testing. As you’ve seen previously, you can use the dotnet-dotcover command to launch your .NET applications. Change your Docker images to use the dotnet-dotcover process as your new EntryPoint and use the --Output flag to write the report to a persistent volume for future retrieval.\nRival Abdrakhmanov describes this process in his blog post, “Diagnostic tools inside Docker container”.\nTerminal Access to Containers\nSometimes, you can log on to running containers and execute the dotnet-dotcover command via orchestration. Terminal access can help you start and stop coverage analysis based on the lifecycle of automated tests such as Selenium or Playwright, which are a breeze to write with JetBrains Aqua.\nMerging Multiple Reports into One\nRegardless of the number of unique processes under coverage analysis, you can always produce a single report. Use the merge command to integrate multiple sources into a single report.\ndotnet dotcover merge --source=Snapshot1.dcvr;Snapshot2.dcvr\nThink about adding an XML Configuration\nYou can manage dotCover settings in XML, making it easier to evolve and fine-tune the coverage analysis process. Create an XML file and pass it as an argument using the --xml flag.\n<ReportParams>\n    <Source><!-- Coverage snapshot file name. --></Source>\n    <Output><!-- Path to the resulting XML report. --></Output>\n    <Output><!-- Path to the resulting JSON report. --></Output>\n    <ReportType>XML</ReportType>\n    <ReportType>JSON</ReportType>\n\n    <!-- Remove auto-implemented properties from report.\n    <HideAutoProperties>True</HideAutoProperties>\n    -->\n\n    <!-- Remove specified files from report. Ant-style patterns are supported here.\n    <ExcludeFileMasks>\n      <Mask>ProjectFolder/**/*.generated.cs</Mask>\n      <Mask>ProjectFolder/**/*.tmp.cs</Mask>\n    </ExcludeFileMasks>\n    -->\n  </ReportParams>    \nDon’t Forget the help command\ndotCover’s built-in help menus offer great advice for every internal command. With the use of the help command, you can quickly figure out what to do, providing the support and confidence you need to use dotCover effectively. \nConclusion\nAs seen in this post, you can add dotCover outside typical unit testing scenarios. Adding automation testing using tools such as Selenium or Playwright doesn’t mean you have to give up on code coverage, especially with the incredible power of dotCover. \nWe hope you found this post helpful. Let us know if you have any questions.\nimage credit: Matthew Henry",
        "dc:creator": "Khalid Abuhakmeh",
        "content": "The ultimate goal of creating a testing strategy is to be confident that you and your team have done their due diligence to deliver a quality product. Test harnesses typically include a mix of unit tests, integration testing, automation, and manual testing, each with advantages and disadvantages for your overall strategy. Determining the effectiveness of [&#8230;]",
        "contentSnippet": "The ultimate goal of creating a testing strategy is to be confident that you and your team have done their due diligence to deliver a quality product. Test harnesses typically include a mix of unit tests, integration testing, automation, and manual testing, each with advantages and disadvantages for your overall strategy. Determining the effectiveness of […]",
        "guid": "https://blog.jetbrains.com/?post_type=dotnet&p=479413",
        "categories": [
          "net-tools",
          "automated-testing",
          "c",
          "code-coverage",
          "dotcover"
        ],
        "isoDate": "2024-06-20T12:45:54.000Z"
      },
      {
        "creator": "Anastasiia Pogorelova",
        "title": "JetBrains Academy for Teams: How to Pitch It to Your Manager",
        "link": "https://blog.jetbrains.com/education/2024/06/20/pitch-jetbrains-academy-to-your-manager/",
        "pubDate": "Thu, 20 Jun 2024 11:58:20 +0000",
        "content:encodedSnippet": "As a passionate coder using JetBrains Academy, you already know the incredible benefits of our project-based learning platform. Now imagine amplifying those benefits across your entire team. \nHere is why and how you should pitch JetBrains Academy for Teams to your employer.\n\n\n\n    \nFREE TRIAL FOR TEAMS\n                                                    \nWhy professional development is essential\nIn the ever-evolving world of tech, keeping up is essential, but staying ahead is even\nbetter. Here’s why investing in learning can be a game-changer for you and your team: \nClose skill gaps: 45% of engineering companies struggle with skill gaps. Upskilling can bridge these gaps, ensuring your team is prepared for new projects and initiatives.\nEnhance engagement and retention: 94% of employees say they would stay longer at a company that invests in their careers. Investing in training can boost morale and reduce turnover.\nFacilitate smooth transitions: Whether onboarding new hires or preparing for tech stack migrations, continuous learning ensures smoother transitions and internal transfers.\nExplore your company’s professional development opportunities\nBefore you make your pitch, it’s a good idea to сheck out what professional development and learning opportunities your company already offers. This can help you frame your request in a way that aligns with existing initiatives. Maybe your company already has a budget for training or a process for approving new tools and resources.\nFind out who is the right person to talk to about the approval process. Is it your direct manager, the HR department, or someone else? Once you’ve done this, you’ll be ready to make a more informed and strategic pitch.\nStructure your pitch\n1. Highlight what you’re learning and its impact\nStart by sharing your personal experience with JetBrains Academy. Explain what you’ve been learning and how it has benefited your current work or future professional goals. \nFor example:\nMaxim, while working in Product Management, decided to learn Python through JetBrains Academy. He completed the Python with Algorithms for Tech Interviews and Python Backend Developer with Flask tracks. \nThe hands-on projects and personalized study plans made it easy to grasp new concepts and apply them. After a few months, he felt confident enough to apply for the Junior Python Developer role at his company and transferred to the development team.\n2. Explain why JetBrains Academy is the right choice\nSo, why JetBrains Academy? For starters, JetBrains Academy taps into JetBrains’ more than 20 years of experience in creating developer tools. JetBrains is trusted by 90 of the Fortune Global Top 100 companies. Chances are good that your company is already using some of our tools for development, which ensures seamless integration and a familiar environment.\nJetBrains Academy offers project-based learning tracks, which are hosted on the Hyperskill platform and backed by JetBrains tools. This means your team will be able to put new knowledge into practice by creating real-world applications while working with professional tools they’ll use daily.\nNow, let’s get to the part where you’re pitching the idea to the whole team. You could say:\n“Imagine if our whole team had access to this! We could all benefit from the seamless integration with the IDEs we already use. Learning how to use the tools is as important as learning how to code. Plus, each of us would get a personalized study plan, so we could learn at our own pace and level.”\nEmphasize the key features that would be beneficial for your whole team:\n\nIntegration with JetBrains IDEsYour team develops their skills in the same IDE editors they use daily. \nPersonalized learning pathsEach team member receives a tailored study plan, progressing at a pace that reinforces knowledge effectively.\nOrganizational dashboardTrack team progress and comply with privacy policies, ensuring a secure and efficient learning environment.\nSSO and free course creation                         Simplify access with single sign-on and create custom courses to align with your company’s specific needs.\nCertificates of completionMotivate and recognize your team’s achievements with personalized certificates.\n\n\n\n\n\n3. Real-life benefits for the team\nThink about how different parts of your organization could benefit. Here are a few use cases your team lead might consider:\nUpskilling tech support, QA departments, or junior developers.\nGetting ready for new projects that require skills the team currently lacks.\nAssisting non-IT employees in gaining a basic understanding of technical skills, useful for collaboration with tech departments.\n\n\n\n\n\nEncouraging your manager to invest in a team-wide subscription to JetBrains Academy can elevate your entire team’s performance and morale. \nStart with the free 10-day trial with full access to JetBrains Academy for Organizations. \nUnlimited yearly access is $399 per user, including a free owner account to manage your team.\n\n\n\n\n\n\nUse these tips to structure your pitch and highlight the awesome benefits of continuous learning with JetBrains Academy. Good luck!\nIf you have any questions or would like to share your feedback, feel free to leave a comment below or contact us at academy@jetbrains.com.\nHappy learning!\nYour JetBrains Academy team",
        "dc:creator": "Anastasiia Pogorelova",
        "content": "As a passionate coder using JetBrains Academy, you already know the incredible benefits of our project-based learning platform. Now imagine amplifying those benefits across your entire team.&#160; Here is why and how you should pitch JetBrains Academy for Teams to your employer. Why professional development is essential In the ever-evolving world of tech, keeping up [&#8230;]",
        "contentSnippet": "As a passionate coder using JetBrains Academy, you already know the incredible benefits of our project-based learning platform. Now imagine amplifying those benefits across your entire team.  Here is why and how you should pitch JetBrains Academy for Teams to your employer. Why professional development is essential In the ever-evolving world of tech, keeping up […]",
        "guid": "https://blog.jetbrains.com/?post_type=education&p=485982",
        "categories": [
          "jetbrains-academy",
          "project-based-learning",
          "teams",
          "hyperskill",
          "jetbrains-academy-for-organizations"
        ],
        "isoDate": "2024-06-20T11:58:20.000Z"
      },
      {
        "creator": "Vitaly Bragilevsky",
        "title": "Custom Fleet Plugins for Your Kotlin Codebase",
        "link": "https://blog.jetbrains.com/fleet/2024/06/custom-fleet-plugins-for-your-kotlin-codebase/",
        "pubDate": "Thu, 20 Jun 2024 09:57:33 +0000",
        "content:encodedSnippet": "Fleet is a code editor designed from scratch to be an extendable platform. Many pieces of Fleet’s functionality are implemented as plugins. While the Fleet team is working toward the Fleet Plugins Public Preview, we decided to share some ideas, details, and examples of why and how external developers are supposed to develop their own plugins for Fleet. This blog post is based on the material from the lightning talk at KotlinConf 2024.\nFleet and Kotlin: Available functionality and ideas for customization\nFleet can be used to work with codebases in Kotlin right from the outset. In Smart Mode, you can employ the power of the IntelliJ backend. On top of Fleet, you can rely on the built-in Kotlin Multiplatform tooling or build your projects with Amper. Although all this functionality is already available out of the box, some customizations might also be needed. \nFor example, your project might employ custom resources. A custom view for those resources might be a helpful feature. If your project relies on external tools, it might be tempting to integrate them into Fleet. If you care about codebase quality, you might enjoy having quick access to source code metrics such as the total size of your codebase, cyclomatic complexity, metrics regarding code abstraction complexity, or relevance of code comments to the code itself. Anything like that might be implemented as a custom plugin for Fleet if unavailable elsewhere.\nIn this blog post, we’ll use a running example of a plugin that counts all the top-level functions declared in a source file and reports them via a custom notification:\n\n\n\n\nNote that changes in the source file are propagated to the notification description immediately. Despite being relatively simple (its implementation takes only a hundred lines of code), this plugin helps us see several crucial ideas behind the Fleet platform. \nFleet architecture for plugin developers\nTo support remote development and collaboration, Fleet has a distributed architecture, clearly separating such components as:\nFrontends – the user-facing parts of any Fleet application.\nWorkspace – the part responsible for registering participating components and maintaining a shared state between all of them.\nBackends – headless services responsible for implementing Smart Mode features (such as static analysis, advanced search, code navigation, and more). \nThe same structure is reflected in Fleet plugins:\n\n\n\n\nFleet plugins typically implement frontend and workspace parts, communicating with backend components. The backend’s implementation depends on the chosen service. For example, when working with Kotlin, the backend part of a Fleet plugin should be an IntelliJ Platform plugin.\nFleet itself is implemented in Kotlin, and plugin developers should write their plugin code in Kotlin. Coding for Fleet therefore requires a good working knowledge of Kotlin and its more advanced concepts, such as DSL (domain-specific language) development or structured concurrency with coroutines. \nTo write efficient code for Fleet, developers have to keep the following two key principles in mind:\n1: Fleet is a transactional distributed database with reactive queries (see the other blog post for more internal details).\n2: Fleet embraces coroutines and structured concurrency.\nEvery change in the UI is technically a transaction over the Fleet state database. Every coroutine launched by a plugin is controlled by Fleet and canceled automatically if the plugin is unloaded. Plugins themselves are also a part of Fleet’s state and are managed according to the same rules as everything else in Fleet. Both loading and unloading a plugin are transactions. Let’s see how these principles affect plugins’ source code.\nImplementing a custom plugin: Counting functions in a Kotlin file\nThe Count Functions plugin defines only the frontend part: It contributes an action that displays a notification with the number of top-level functions in a Kotlin source file. The full source code of the plugin is available on GitHub.\nAll the plugin’s code comes from a single Kotlin file and has the following structure:\n\n\n\n\nWe provide the main plugin’s class FunCounter, which is loaded via the JVM’s ServiceLoader, as well as several functions that perform our desired task.\nGradle configuration\nFleet plugins are developed using Gradle. The main Gradle configuration file specifies the plugin’s most important details:\nversion = \"0.1.0\"\n\nfleetPlugin {\n    id = \"pro.bravit.fleetPlugin.funCounter\"\n    metadata {\n        readableName = \"Count Functions\"\n        description = \"This plugin contributes an action...\"\n    }\n   fleetRuntime {\n       version = \"1.35.115\"\n   }\n   pluginDependencies {\n       plugin(\"fleet.kotlin\")\n   }\n}\nWhile we’re targeting a specific Fleet runtime, it’s also possible to set a range of supported versions. Additionally, we’re declaring dependencies on other Fleet plugins. The fleet.kotlin plugin gives us access to packages providing classes and methods to work with an abstract syntax tree (AST) of a Kotlin source file.\nA plugin class\nThe FunCounter class is an entry point to a frontend part of the plugin. It declares several components required for Fleet plugin bookkeeping and also loads all the contributed functionality.\ntypealias FunCounterAPI = Unit\nclass FunCounter : Plugin<FunCounterAPI> {\n\n   companion object : Plugin.Key<FunCounterAPI>\n   override val key: Plugin.Key<FunCounterAPI> = FunCounter\n\n   override fun ContributionScope.load(pluginScope: PluginScope) {\n       notificationCategory(countFunctionsNotification)\n       actions {\n           setupCountFunctionsAction(pluginScope)\n       }\n   }\n}\nIn this example, we’re contributing a notification category and an action. Our plugin doesn’t provide any APIs, so we’ll use Unit as a generic parameter for the Plugin interface. This class is mentioned in the module-info.java as the one providing a plugin implementation:\nmodule pro.bravit.fleetPlugin.funCounter {\n   // module requirements\n   // exports\n   provides fleet.kernel.plugins.Plugin with pro.bravit.fleetPlugin.funCounter.FunCounter;\n}\nNote the pluginScope argument of the load method: It’s a coroutine scope used by Fleet to control all the coroutines launched by a plugin. If a plugin is unloaded, then all of its coroutines are canceled.\nIn the rest of this article, we’ll look at the implementation of the contributed functionality. \nManaging notifications\nManaging notifications in Fleet usually involves the following two components:\nWe define a notification category so that Fleet can manage all the notifications coming from a plugin.\nWe make Fleet show a notification whenever needed.\nApart from that, we’ll also make it update a notification whenever new information becomes available. Although notifications are not the best place to display information that changes with time, these ones are simple enough to serve as an example here.\nThe notification category is a simple data class value:\nval countFunctionsNotification = NotificationCategory(\n   id = NotificationCategoryId(\"CountFunctions\"),\n   readableName = \"Count Functions\"\n)\nCreating a notification is a bit more complicated: \nprivate suspend fun createCountFunctionsNotification(editor: EditorEntity): NotificationEntity {\n   val fileName = editor.layout?.ownerTab()?.displayName() ?: \"Unknown file\"\n   val title = \"Function counter ($fileName)\"\n   val description = \"Number of top-level functions: ?\"\n   return change {\n       val notification = showNotification(\n           countFunctionsNotification,\n           title, NotificationIcon.Info, description,\n           isSticky = true\n       )\n       cascadeDelete(editor, notification)\n       notification\n   }\n}\nThe code above demonstrates several important features of Fleet’s plugin machinery:\nThe Entity suffix found in EditorEntity and NotificationEntity reminds us that we’re working with database entities. These entities are represented by Kotlin values and are managed by Fleet. They can be created, looked up, and updated as needed. Entities from Fleet itself provide us with information about what’s going on in the Fleet instance (for example, we get the name of the loaded file via editor.layout?.ownerTab()?.displayName())\nChanges in the database are executed in transactions. We introduce transactions with change-blocks.\nNote the cascadeDelete call: With it, we can establish relations between entities in the database. In this case, we ask Fleet to delete a freshly created notification when the corresponding editor is deleted.\nThe showNotification function comes from the Fleet Notification API. It creates and displays a notification and then returns a created notification entity so that we can manage it later.\nIn many cases, Fleet plugin code follows the same pattern, manipulating database entities. Entities are created and updated (in transactions), or we just use the information we’ve extracted from them.  \nTo update the notification, we need to execute another transaction:\nprivate suspend fun updateCountFunctionsNotification(\n\t\t\t\t\tnotification: NotificationEntity, \n\t\t\t\t\tnumberOfFunctions: Int) {\n   val description = \"Number of top-level functions: $numberOfFunctions\"\n   change {\n       notification.description = description\n   }\n}\nNote that both functions above are suspend functions. We have to run them from Kotlin coroutines. We’ll get back to this shortly.\nDeclaring actions\nThe main functionality this plugin provides is the Count Functions action. Fleet actions have the following life cycle:\nThey are registered in the ContributionScope.load plugin’s method.\nFleet shows available actions in the Actions list. Depending on the way an action is defined, it can be missing from that list (if the action’s static prerequisites are not met) or it can be grayed out (if the action’s dynamic prerequisites are not met).\nFleet executes an action whenever the corresponding action is triggered by a user. In most cases, actions are executed by launching a Kotlin coroutine that then provides the required functionality. \nThe Fleet Action API provides a way to declare actions. It starts with the actions block containing action definitions. Now, let’s look at setting up the Count Functions action:\nprivate fun ActionRegistrar.setupCountFunctionsAction(pluginScope: PluginScope) {\n   action(id = \"Count-Functions\", name = \"Count Functions\") {\n       val requiredEditor = required(FleetDataKeys.LastEditorEntity)\n       dynamic {\n           val editor = requiredEditor.value\n           if (editor.document.mediaType == MediaType(\"text\", \"kotlin\")) {\n               callback {\n                   pluginScope.launch {\n                       performCountFunctionsAction(editor)\n                   }\n               }\n           }\n       }\n   }\n}\nThe static requirement for this action is the availability of an editor: If there’s no focused editor, the Count Functions action makes no sense. We specify this requirement by requesting FleetDataKeys.LastEditorEntity in the first line of the action block. Then, we use the dynamic block to specify the dynamic requirement. The document loaded in the editor must contain Kotlin source code. If this requirement is met, we provide a callback block to define code that is executed whenever the action is triggered. As explained above, the execution of the action starts with launching a coroutine in the plugin’s coroutine scope. \nThe Fleet Action API and many other Fleet APIs apply Kotlin DSL builders to describe functionality contributed to Fleet. In this example, we’ve used several Action DSL components, including action, dynamic, and callback, to introduce an action definition, specify its dynamic requirements, and provide an implementation, respectively. \nPerforming actions with reactive queries\nThe central piece of the Count Functions plugin implementation is propagating changes in the source code’s AST to the notification’s description:\n\n\n\n\nThe corresponding code comes in the performCountFunctionsAction function:\nprivate suspend fun performCountFunctionsAction(editor: EditorEntity) {\n   withEntities(editor) {\n       val notification = createCountFunctionsNotification(editor)\n       withEntities(notification) {\n           query {\n               editor.document.syntaxes\n                  ?.firstNotNullOfOrNull(ASTContainer::getDataAsync)\n           }.collectLatest { ast ->\n               val numberOfFunctions = countFunctions(ast?.await())\n               updateCountFunctionsNotification(notification, numberOfFunctions)\n           }\n       }\n   }\n}\nNote the following important parts of this function:\nWe use the withEntities function to introduce suspend blocks that depend on the existence of the referenced entities. If the corresponding entity does not exist anymore, the coroutine is canceled. This approach greatly simplifies implementation, as we have a guarantee that these entities exist in these blocks so that we can safely work with them.\nThe query {} block specifies a subscription request to the Fleet database regarding all the changes occurring to the mentioned entities. These requests are the cornerstone of the Fleet Query API: We can subscribe to changes in the database in order to react to them in a timely manner.\nQueries give us something that closely resembles Kotlin’s asynchronous flows. We collect the flow’s elements and process them as needed. In this case, the flow’s element comes as a Kotlin’s Deferred AST value. We’ll then wait for this value to get an up-to-date AST to count all the top-level functions in it.\nWe don’t need to think about the action’s completion. It will be active until it’s canceled as a result of deleting the entities referenced in withEntities or unloading the plugin itself. \nWorking with a Kotlin source code\nThe final piece of functionality is the countFunctions function. It represents a small exercise in navigating through Kotlin’s source code AST:\nprivate fun countFunctions(tree: AST<*>?): Int =\n   tree?.root()\n       ?.children()\n       ?.count {\n           it.type.toString() == \"FUN\"\n       } ?: 0\nTo use the corresponding types and methods, we had to introduce a dependency to the fleet.kotlin plugin earlier. To simplify the implementation, we return 0 for every unexpected value in AST, without thinking too much about error handling. \nConclusion\nFleet plugin APIs and the corresponding tooling are a work in progress. This blog post offers a sneak peek at how plugin developers are supposed to develop their own plugins for Fleet. The whole plugin development experience is based on two principles: (1) Fleet is a distributed database, and (2) Fleet embraces structured concurrency with coroutines. To make it easier to contribute custom functionality, Fleet provides a set of APIs for database entity and transaction management, actions, notifications, and many other things. Stay tuned for the Fleet Plugins Public Preview to develop your own plugins for Fleet!",
        "dc:creator": "Vitaly Bragilevsky",
        "content": "Fleet is a code editor designed from scratch to be an extendable platform. Many pieces of Fleet’s functionality are implemented as plugins. While the Fleet team is working toward the Fleet Plugins Public Preview, we decided to share some ideas, details, and examples of why and how external developers are supposed to develop their own [&#8230;]",
        "contentSnippet": "Fleet is a code editor designed from scratch to be an extendable platform. Many pieces of Fleet’s functionality are implemented as plugins. While the Fleet team is working toward the Fleet Plugins Public Preview, we decided to share some ideas, details, and examples of why and how external developers are supposed to develop their own […]",
        "guid": "https://blog.jetbrains.com/?post_type=fleet&p=481751",
        "categories": [
          "fleet",
          "kotlin",
          "plugin"
        ],
        "isoDate": "2024-06-20T09:57:33.000Z"
      },
      {
        "creator": "Andrey Gushchin",
        "title": "CLion 2024.1.4 Is Out! Nova Now Available by Default for New Users",
        "link": "https://blog.jetbrains.com/clion/2024/06/2024-1-4-update-is-out/",
        "pubDate": "Wed, 19 Jun 2024 16:26:48 +0000",
        "content:encodedSnippet": "Since November 2023, our team has been diligently working on the implementation of the ReSharper C++/Rider C++ language engine. This project was initiated with a clear focus on our users, aiming to address the long-standing performance and quality issues of CLion, which were caused by the usage of the slowly performing engine.\nThe most notable benefits for our users are as follows:\nFaster highlighting speeds, especially in the case of incremental code updates\nA more responsive UI\nFaster Find Usages\nSignificantly fewer freezes and hangs when refactoring\nFaster test indexing\nToday, we’re thrilled to announce that a new, more powerful language engine (also known as Nova) is now available by default for all new users right after the installation!\nIf, after installation, CLion detects no settings from its previous installation, or if a user chooses not to import them, the new language engine will be enabled.\nDownload build 241.18034.45 from our website, through the Toolbox App, as a snap package for Ubuntu, or via a patch from inside the IDE and experience exceptional performance and efficiency.\nDOWNLOAD CLION\nAs promised in the roadmap for the 2024.2 release, we’re still working hard to close the feature gap between the classic language engine and the new one. Once this work is complete, we’ll make the new engine the default option for existing users.\nWe also encourage you to try the ReSharper C++ language engine, which can be enabled via Advanced Settings. To do so, go to Settings / Preferences | Advanced Settings | CLion | Use the ReSharper C++ language engine (CLion Nova).\n\n\n\n\nYour feedback matters to us! Share your ideas in the comments section below or submit them to our issue tracker. We’re also interested in scheduling short calls with our users to learn about specific cases in more detail. Let us know if you’re willing to share your feedback about the new engine by commenting below!\nDOWNLOAD CLION\nYour CLion team\nJetBrains\nThe Drive to Develop",
        "dc:creator": "Andrey Gushchin",
        "content": "Since November 2023, our team has been diligently working on the implementation of the ReSharper C++/Rider C++ language engine. This project was initiated with a clear focus on our users, aiming to address the long-standing performance and quality issues of CLion, which were caused by the usage of the slowly performing engine. The most notable [&#8230;]",
        "contentSnippet": "Since November 2023, our team has been diligently working on the implementation of the ReSharper C++/Rider C++ language engine. This project was initiated with a clear focus on our users, aiming to address the long-standing performance and quality issues of CLion, which were caused by the usage of the slowly performing engine. The most notable […]",
        "guid": "https://blog.jetbrains.com/?post_type=clion&p=485777",
        "categories": [
          "news",
          "releases",
          "clionnova"
        ],
        "isoDate": "2024-06-19T16:26:48.000Z"
      }
    ]
  },
  {
    "name": "Airbnb Engineering & Data Science",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Instagram Engineering",
    "category": "기업",
    "posts": []
  },
  {
    "name": "PayPal Engineering",
    "category": "기업",
    "posts": []
  },
  {
    "name": "article New updates to Planner comment notifications and settings in Planner Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Visual Studio Blog",
    "category": "기업",
    "posts": [
      {
        "creator": "McKenna Barlow",
        "title": "Code Assessment with .NET Upgrade Assistant",
        "link": "https://devblogs.microsoft.com/visualstudio/code-assessment-with-net-upgrade-assistant/",
        "pubDate": "Tue, 25 Jun 2024 17:00:54 +0000",
        "content:encodedSnippet": "We are thrilled to announce the latest enhancements to the .NET Upgrade Assistant. .NET Upgrade Assistant helps upgrade solutions to newer versions of .NET. Whether you’re upgrading from .NET Framework to .NET 8 or just between .NET Core versions (from .NET 6 or 7 to .NET 8 or 9), .NET Upgrade Assistant can help you understand what changes will be needed. .NET Upgrade Assistant is available as a Visual Studio extension or as a command-line tool. Now as part of your upgrades to modern .NET, you’ll have access to powerful code assessment features.\n\n\nHere’s what’s new:\nOur code assessment tool scans your solution at the source code level and identifies potential issues and dependencies that could pose challenges during the upgrade process. You will be provided with a comprehensive dashboard that provides an overview of issues across your entire solution and estimates of how much effort will be required for each issue to be remediated.\nEach issue in the report is categorized by severity, allowing you to prioritize your efforts. Whether it’s a mandatory issue blocking upgrade or an optional issue identifying opportunities to take advantage of features available in the latest .NET framework, you’ll know exactly where you need to focus your attention.\n\nIssues can provide exact locations within your source code where issues were detected, making troubleshooting and fixing dependencies a breeze. Easily navigate through projects and pinpoint areas that need attention. You can jump to a line that requires your attention, address issues and mark them as fixed, and save the current state of the issues and report so you or your coworkers can pick up exactly where you left off, making collaboration easier than ever. We also include relevant links to documentation, best practices, and community discussions to give you the guidance you need to tackle each challenge.\n \nInstall the Visual Studio Extension\nPre-requisites:\nWindows operating system\nVisual Studio 2022 version 17.1 or later\nInstallation:\nIf you don’t already have the .NET Upgrade Assistant installed, you can download and install the extension from the Visual Studio Marketplace or install it from inside Visual Studio.\nNote: The .NET Upgrade Assistant extension now installs on a per-user basis so there’s no requirement for elevated admin permissions to install. This addresses a need voiced by some users who don’t have admin permissions on their dev machines. More information and troubleshooting suggestions may be found in this blog post. \n \nInstall the CLI tool\nPrerequisites:\n.NET SDK\nThe .NET Upgrade Assistant is also available as a .NET global tool. You can install the tool with the following command:\ndotnet tool install -g upgrade-assistant\nSimilarly, because the .NET Upgrade Assistant is installed as a .NET tool, it can be easily updated by running:\ndotnet tool update -g upgrade-assistant\n \nAnalyze applications with Visual Studio\nOnce you have installed the Visual Studio extension, you are all set to analyze your application in Visual Studio. You can do so by right clicking on your solution in the Solution Explorer and selecting Upgrade.\n\nThe tool will open a window offering you to either create a new report or open an existing one.\n\nTo start a new report, select the projects you want to analyze on the next screen. Web projects are already checked for you, but you may change the selection by checking or unchecking the boxes next to the projects. When the tool runs its assessment, it also analyzes the dependencies for your selected projects. \n\nOn the next screen you can select your preferred target framework for the upgrade. If you need help deciding which target framework is right for you, please check out this blog post.\n\nNext you will choose the components from the previously selected projects that you want to analyze. You can select just your source code and settings, or also include all your code’s binary dependencies for analysis. A custom config file may also be included to change the way that the rules for analysis work. After this selection, hit Analyze to begin the assessment of your application and receive your report\n\n \nAnalyze applications with the CLI\nAfter you’ve installed the .NET Upgrade Assistant CLI tool, open a terminal window and navigate to the directory that contains the project you want to analyze. You can use the following command to see the available options the CLI provides:\nupgrade-assistant --help\nRun analysis on your application with:\nupgrade-assistant analyze\nThe CLI tool provides an interactive way of choosing which project to analyze. Use the arrow keys to select an item, and press Enter to run the item. Select the project you want to analyze. Follow the prompts in the command terminal to make the same selections as outlined above in the Visual Studio extension. When the analysis is done, you will be provided with a report in the format you selected.\n \nInterpret Results\nAfter analysis in Visual Studio, the tool will provide you with a dashboard summary and detailed report. The report will show you information about the number of projects analyzed, issues and incidents found, and an estimate of how much effort each issue, incident, and project will take to be fixed.\n\nYou can view aggregate issues for the whole solution or dig into project specific views. Here you can see detailed information on each incident, get help on how to fix them, jump to their location in code, and so on. You can view these issues by type of issue or by component so that you can understand where the bulk of the issues in the report originate from.\n\n\n \nShare results\nBoth the CLI tool and Visual Studio allow you to save the results of the analysis in various formats so that you may share the report with others. In Visual Studio, just click the export icon in the top right corner of the tool window and select your desired export format. \n\nIn the CLI tool, the final selection before your report is generated will be to select your desired export format. Press Enter to select, then you will be prompted to enter a name for your report. The tool will then generate your report in the format you selected. \n\nThe tool can generate reports in HTML, CSV, and JSON formats. The HTML report looks very similar to the dashboard you’ll see in the Visual Studio after the analysis is completed.\n\nYou can export these reports with various privacy modes so that you can feel confident that those who receive the reports will have exactly the right amount of information required to understand what work needs to be done without risking exposing sensitive information.\n\n \nUseful Materials\nFor supported languages, project types, and upgrade paths, please refer to our Microsoft Learn documentation for the .NET Upgrade Assistant.\nDocumentation: Overview of the .NET Upgrade Assistant – .NET Core | Microsoft Learn\nInstall: How to install the .NET Upgrade Assistant – .NET Core | Microsoft Learn\nGetting started: .NET Upgrade Assistant | Get Started (microsoft.com)\n \nFeedback\nPlease give us your feedback or report any issues either by filing an issue directly in the .NET Upgrade Assistant repository, or via the Visual Studio feedback channel by choosing Help | Send Feedback in Visual Studio. Please mention “.NET Upgrade Assistant” in the title so it will get routed to our team quickly. We appreciate your input and want to build the best tools for you! \n \nThe post Code Assessment with .NET Upgrade Assistant appeared first on Visual Studio Blog.",
        "dc:creator": "McKenna Barlow",
        "content": "<p>We are thrilled to announce the latest enhancements to the .NET Upgrade Assistant. .NET Upgrade Assistant helps upgrade solutions to newer versions of .NET. Whether you’re upgrading from .NET Framework to .NET 8 or just between .NET Core versions (from .NET 6 or 7 to .NET 8 or 9),</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/visualstudio/code-assessment-with-net-upgrade-assistant/\">Code Assessment with .NET Upgrade Assistant</a> appeared first on <a href=\"https://devblogs.microsoft.com/visualstudio\">Visual Studio Blog</a>.</p>\n",
        "contentSnippet": "We are thrilled to announce the latest enhancements to the .NET Upgrade Assistant. .NET Upgrade Assistant helps upgrade solutions to newer versions of .NET. Whether you’re upgrading from .NET Framework to .NET 8 or just between .NET Core versions (from .NET 6 or 7 to .NET 8 or 9),\nThe post Code Assessment with .NET Upgrade Assistant appeared first on Visual Studio Blog.",
        "guid": "https://devblogs.microsoft.com/visualstudio/?p=249505",
        "categories": [
          "Visual Studio",
          ".NET",
          "ASP.NET",
          "Extensions",
          "Update",
          "Upgrade",
          "Upgrade Assistant",
          "Visual Studio 2022"
        ],
        "isoDate": "2024-06-25T17:00:54.000Z"
      },
      {
        "creator": "Christine Ruana",
        "title": "Keep Visual Studio automatically updated and secure through Microsoft Update",
        "link": "https://devblogs.microsoft.com/visualstudio/automatically-install-visual-studio-security-updates-through-microsoft-update/",
        "pubDate": "Wed, 19 Jun 2024 17:30:59 +0000",
        "content:encodedSnippet": "Visual Studio is coming to Microsoft Update! We’re pleased to announce that starting in August 2024, developers who are not part of an organization managed by an IT administrator can choose to receive monthly Visual Studio security updates through the Microsoft Update (MU) system on “patch Tuesdays”.  This new update option will be available for Visual Studio 2022, Visual Studio 2019, and Visual Studio 2017. It won’t be available for the Preview channel.\nHow to enable Visual Studio updates through MU\nOpen Windows Settings and go to Windows Update > Advanced Options. If you can toggle the option at the top “Receive updates for other Microsoft products”, then you control your machine’s update policies and can choose to receive updates for Visual Studio and other Microsoft products from MU. We encourage you to enroll in this capability, as it’s by far the easiest way to stay updated and secure on a monthly cadence. If this option is greyed out and you can’t toggle it, this new feature does not apply to your machine (even if it is turned on) because your administrator controls your update policies.\nFigure 1 – Microsoft Update policy that is controlled by the user\nHow it works\nAs with other Visual Studio update methods, Visual Studio must be closed to apply these updates. MU will never force-close Visual Studio to apply the update. Updates delivered through Microsoft Update typically happen during machine idle time at night. Once you have opted into receiving Visual Studio updates through MU, just remember to periodically save your work and close Visual Studio in the evening to make sure that the update isn’t blocked. The next morning, you can verify in the Windows Update history that the latest Visual Studio security update was successfully applied. You can also initiate the update manually on demand by closing Visual Studio and pressing the Windows Update “Check for updates” button.\nOpting out of MU updates for Visual Studio\nIf you have chosen to receive updates for other Microsoft products, but you don’t want to receive Visual Studio updates from MU, you can set this registry key value to opt out:\n[HKLM\\Software\\Policies\\Microsoft\\VisualStudio\\Setup]\n“VSthroughMUUpdatesOptOut”=dword:1\nPreview the experience through July 2024\nYou can try out this new experience through July 2024 in advance of it rolling out in August. To opt in to the preview, toggle “Receive updates for other Microsoft products” to On, and set this registry key value:\n[HKLM\\Software\\Policies\\Microsoft\\VisualStudio\\Setup]\n“PreviewAutomaticUpdates”=dword:1\nStarting in August 2024, this registry key won’t be necessary.\nDuring June and July, the updates will have a “[Microsoft Update Preview]” prefix in the title. These updates are the same update as if you manually installed it from within the Visual Studio IDE.\nFigure 2 – Visual Studio security updates delivered during the Preview timeframe\n \nPlanned improvements\nThere are a few improvements to this experience that we’re working on.\nIf you rollback an update, you can temporarily use the registry key to opt out of MU updates for Visual Studio to prevent the update from automatically being re-applied. We’re working on a reliable experience for this.\nWe’re working to improve the error messages in the Windows Update UI, such as for the condition that the update was cancelled because Visual Studio was open.\nIf you use the “Check for Updates” button in the Windows Update UI, the progress bar will appear to be stuck at 0% until the update finishes, at which point it will immediately go to 100%. Visual Studio updates can take a while, so we ask for your patience while we improve the progress bar.\nWhat about IT Admin managed machines?\nWe’ve delivered a rich set of managed update solutions via Visual Studio Administrator Updates, which allow IT administrators in organizations to deploy monthly Visual Studio security updates by using Windows Update for Business. Over 1500 organizations, including Microsoft, currently use this solution to automatically install Visual Studio security updates on hundreds of thousands of machines each month.\nConclusion\nIf you’re an individual user that controls your own machine’s update policies, we hope you enroll in this solution to automatically receive and install updates for Visual Studio along with other Microsoft products. We welcome your feedback on this Automatic Update experience.\nWe appreciate the time you’ve spent reporting issues/suggestions and hope you continue to give us feedback when using Visual Studio on what you like and what we can improve. Your feedback is critical to help us make Visual Studio the best tool it can be! You can share feedback with us via Developer Community: report any bugs or issues via report a problem and share your suggestions for new features or improvements to existing ones.\nStay connected with the Visual Studio team by following us on YouTube, Twitter, LinkedIn, Twitch and on Microsoft Learn.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThe post Keep Visual Studio automatically updated and secure through Microsoft Update appeared first on Visual Studio Blog.",
        "dc:creator": "Christine Ruana",
        "content": "<p>Visual Studio is coming to Microsoft Update! We’re pleased to announce that starting in August 2024, developers who are not part of an organization managed by an IT administrator can choose to receive monthly Visual Studio security updates through the Microsoft Update (MU) system on &#8220;patch Tuesdays&#8221;. </p>\n<p>The post <a href=\"https://devblogs.microsoft.com/visualstudio/automatically-install-visual-studio-security-updates-through-microsoft-update/\">Keep Visual Studio automatically updated and secure through Microsoft Update</a> appeared first on <a href=\"https://devblogs.microsoft.com/visualstudio\">Visual Studio Blog</a>.</p>\n",
        "contentSnippet": "Visual Studio is coming to Microsoft Update! We’re pleased to announce that starting in August 2024, developers who are not part of an organization managed by an IT administrator can choose to receive monthly Visual Studio security updates through the Microsoft Update (MU) system on “patch Tuesdays”. \nThe post Keep Visual Studio automatically updated and secure through Microsoft Update appeared first on Visual Studio Blog.",
        "guid": "https://devblogs.microsoft.com/visualstudio/?p=249465",
        "categories": [
          "Visual Studio",
          "Microsoft Update",
          "security",
          "Update"
        ],
        "isoDate": "2024-06-19T17:30:59.000Z"
      }
    ]
  },
  {
    "name": "Instagram Engineering",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Dropbox Tech Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Joshua",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권재명",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김석기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김선영",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권진호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강대명",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권정혁",
    "category": "개인",
    "posts": []
  },
  {
    "name": "줌구",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수보",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김시은",
    "category": "개인",
    "posts": []
  },
  {
    "name": "곽민수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김범진",
    "category": "개인",
    "posts": [
      {
        "creator": "Beejei",
        "title": "Programming in the LLM Era #2 — How",
        "link": "https://beejei.medium.com/programming-in-the-llm-era-2-how-4071ac6e4245?source=rss-9f14ea26d32f------2",
        "pubDate": "Fri, 21 Jun 2024 00:52:51 GMT",
        "content:encodedSnippet": "Programming in the LLM Era #2 — How\nSeamless Collaboration — Dall-E\nEliminating the Understanding Gap Between Development Teams\nWhen developing a service targeted at general users, it’s common knowledge that there are typically Frontend and Backend developers involved. While there are many other categories, focusing on these two can illustrate the point. These roles are often at odds due to their separation of concerns. Even when addressing the same value, such as “efficiency,” their strategies can vastly differ. For a Frontend developer, efficiency means smooth loading and high responsiveness in a web browser. For a Backend developer, efficiency pertains to the server resource usage and cost-effectiveness.\nThese technology-driven areas can be approached in numerous ways, depending on the perspective. However, from a business standpoint, trying to maximize usability and profitability within a reasonable cost often results in vague directives like “cost-effective efficiency.” This makes it challenging to pinpoint specific methodologies, often leading developers to default to familiar technology stacks.\nUsing LLMs can simplify these dilemmas. By providing appropriate conditions and requesting code, the output can serve as a guideline. The key is to create prompts that require minimal technical detail while yielding satisfactory results. This approach ensures that any developer can easily align with the output, fostering better collaboration.\nEliminating the Gap in Code Reviews\nThe second point is about eliminating the gap in traditional code reviews. While code reviews help developers produce higher quality code over time, they can be a source of significant stress. The original author often prefers to stick to their approach, even if there are minor issues. Simple, agreed-upon changes can be made easily, but complex situations make code reviews difficult and time-consuming. This leads to high costs and delays in project completion. Culturally, code reviews can be an opportunity to build camaraderie, but they are still challenging and heavily reliant on individuals’ soft skills.\nBy using LLMs to generate code, everyone can take on the role of a code reviewer. Instead of traditional code review processes, developers can focus on refining the prompts and checking the output. This iterative process allows the team to align more closely with each other’s expectations. As the team reviews and adjusts the prompts together, they maintain a similar vision for the final product. Over time, this repeated collaboration fosters stronger bonds and more effective teamwork.\nBridging the Gap Between Planning and Execution\nThe third point is about closing the gap between planning and execution. Planning is often established by upper management based on the company’s grand vision. However, these plans frequently overlook the day-to-day tasks that need to be done. As plans trickle down to individual teams, they have to contend with ongoing tasks, backlogs, communication with other teams, and internal coordination. This results in increasingly rough plans with declining achievement rates.\nUsing LLMs can help reduce the time needed to reach planned goals. This allows for more detailed planning that aligns with the company’s aspirations. Employees can more easily align with the company’s direction, resulting in a more dynamic organization. With LLMs streamlining the execution of tasks, plans can be more specific and ambitious, fostering a more cohesive and agile workforce.",
        "dc:creator": "Beejei",
        "guid": "https://medium.com/p/4071ac6e4245",
        "categories": [
          "development",
          "chatgpt",
          "llm"
        ],
        "isoDate": "2024-06-21T00:52:51.000Z"
      },
      {
        "creator": "Beejei",
        "title": "Programming in the LLM Era #1 -Why",
        "link": "https://beejei.medium.com/programming-in-the-llm-era-1-why-8965f525c300?source=rss-9f14ea26d32f------2",
        "pubDate": "Thu, 20 Jun 2024 11:59:11 GMT",
        "content:encodedSnippet": "A modern workspace featuring a computer with code — Dall-E generated image\nThe Focus Should Be on Managing Prompts, Not Code\nMany developers find it challenging to create systems integrated with LLMs (Large Language Models) because the outputs from LLMs are not deterministic. However, I believe this very trait is a strength of LLMs, allowing them to adapt to changing realities and produce adaptable code.\nIn traditional development environments, developers strive to meticulously model data and maintain strict types and API information. This rigorous approach can increase the workload and make development more challenging, leading to higher fatigue. But if we shift our focus to managing prompts effectively, and if we can consistently receive improved outputs as code from LLMs, developers can concentrate more on the tasks they want to solve while obtaining high-quality code.\nData Always Exceeds Expectations\nData, unless in a closed environment, always exceeds expectations. Anyone who has modeled database tables has likely encountered user inputs that exceed expected field sizes. Time zones assumed to be fixed for calculations can fail due to political or cultural changes. Exchange rates fluctuate daily, and the cost of cloud resources can change at any moment. Even well-managed data can be unpredictable.\nForms and Excel files, where user freedom is involved, present even greater challenges. They often contain typos or incorrectly copied information, and special characters with unique meanings must be handled cautiously, often using regex functions. In such scenarios, how well can a developer’s data model use real-world information? In the era of LLMs, these issues can be better managed. LLMs can process various forms of unstructured data and respond flexibly to unexpected situations.\nThe Problem of Defining Semantics\nThe incorrect definition of semantics is also a problem. Semantic layers define information for permanent use, but these definitions often fail to capture reality. For example, phone numbers, email addresses, social media account information, names, and genders. Which of these can you be sure will remain unchanged for over 30 years? People are already accustomed to changing personal information.\nMany government agencies’ operations have yet to catch up with this reality, but we must recognize that data and the meanings of the names referring to that data are dynamic. Developers fond of modeling might prefer strong connections like unique keys or foreign keys, but in reality, these values are not always reliable.\nFor instance, it is unclear who might act as your guardian or proxy in a hospital. Transforming ambiguous real-world information into overly rigid definitions in a project is problematic. It is challenging to maintain consensus with such an approach in a constantly changing world.\nThinking Like Humans\nBoth the developers creating programs and the users using them are humans, and thinking like humans is more natural. Hence, future productive activities that require time and effort should be described as naturally as possible in the way humans think to maximize productivity and collaboration.\nDespite the perceived abundance of developers, they are proportionally as rare as endangered species relative to the global population. By changing how developers work, we can unlock immense potential. If we develop technology aligned with human thought processes and improve collaboration efficiency, we can produce better results in less time.",
        "dc:creator": "Beejei",
        "guid": "https://medium.com/p/8965f525c300",
        "categories": [
          "development",
          "llm",
          "chatgpt"
        ],
        "isoDate": "2024-06-20T11:59:11.000Z"
      }
    ]
  },
  {
    "name": "김민서",
    "category": "개인",
    "posts": []
  },
  {
    "name": "I am not Okay",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권창현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권기호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수민",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강태욱",
    "category": "개인",
    "posts": [
      {
        "title": "2024년 오픈소스 대형언어모델 소개",
        "link": "http://daddynkidsmakers.blogspot.com/2024/06/2024.html",
        "pubDate": "2024-06-25T03:51:00.000Z",
        "author": "Daddy Maker",
        "content": "<div style=\"text-align: left;\">이 글은 2024년 오픈소스 대형언어모델을 간략히 소개한다. LLM은 자연어 처리를 위해 개발되었지만, 현재는 멀티모달리티 모델로 발전하고 있다.<div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwkLS_1Gsg670W-75GSRb_2513rWrvDMlJIQ3ll0Sv9PHXcVD_-UnzsmZG_r-R8a0AAXK94Js-0Tw8EI0gozDM0YJElswppM4ObtV5SuLO2msuznv4OMFxqNcas3I8zilQAzNiSoj7746K6CGm6YJJsSYy_8ZvCC8ForT1AOFFeBZqgG8PgFP2HlxiCYgk/s575/f3.PNG\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"448\" data-original-width=\"575\" height=\"311\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwkLS_1Gsg670W-75GSRb_2513rWrvDMlJIQ3ll0Sv9PHXcVD_-UnzsmZG_r-R8a0AAXK94Js-0Tw8EI0gozDM0YJElswppM4ObtV5SuLO2msuznv4OMFxqNcas3I8zilQAzNiSoj7746K6CGm6YJJsSYy_8ZvCC8ForT1AOFFeBZqgG8PgFP2HlxiCYgk/w400-h311/f3.PNG\" width=\"400\" /></a></div><div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiOvu8lJcHmhQISB2l5XMkXtrgZQ7uAGluD2aC2cmqfmtswBca8JJvryOqOrnz1NaVBZh_aMy_qMEwMsqnJfcpfx_jiOaGOpmfKanyjccicUvsy-q4YMPfvJJ8Hpd-5-2txvgf0DZSsYjtJddkGQZa5ews6VK9TLlNyfWC0IBVjCRjxul-vUhON-lPoVuWY/s1655/f5.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"775\" data-original-width=\"1655\" height=\"160\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiOvu8lJcHmhQISB2l5XMkXtrgZQ7uAGluD2aC2cmqfmtswBca8JJvryOqOrnz1NaVBZh_aMy_qMEwMsqnJfcpfx_jiOaGOpmfKanyjccicUvsy-q4YMPfvJJ8Hpd-5-2txvgf0DZSsYjtJddkGQZa5ews6VK9TLlNyfWC0IBVjCRjxul-vUhON-lPoVuWY/w341-h160/f5.png\" width=\"341\" /></a></div></div></div><div class=\"separator\" style=\"clear: both; text-align: center;\">다양한 LLM 모델과 자연어 처리</div><div><b>BERT&nbsp;</b></div><div>BERT 는 Google이 2018년에 도입한 LLM 제품군이다. BERT는 데이터 시퀀스를 다른 데이터 시퀀스로 변환할 수 있는 변환기 기반 모델이다. BERT의 아키텍처는 변압기 인코더의 스택이며 3억 4200만 개의 매개변수를 갖추고 있다. BERT는 대규모 데이터 코퍼스에 대해 사전 훈련된 후 자연어 추론 및 문장 텍스트 유사성과 함께 특정 작업을 수행하도록 미세 조정되었다. 이는 2019년 Google 검색 반복에서 쿼리 이해를 향상시키는 데 사용되었다.</div><div><br /></div><div><b>Claude&nbsp;&nbsp;</b></div><div>Claude&nbsp; LLM은&nbsp; AI 보조자가 유용하고 무해하며 정확하도록 돕는 일련의 원칙에 따라 AI 출력을 형성하는 헌법적 AI에 중점을 둔다. Claude는 Anthropic이라는 회사에서 만들어졌다. Claude LLM의 최신 버전은 Claude 3.5 Sonnet이다. LLM의 이전 버전보다 뉘앙스, 유머 및 복잡한 지침을 더 잘 이해하고 Claude 3 Opus의 두 배 속도로 작동하다. Claude.ai 및 Claude iOS 앱을 통해 무료로 사용할 수 있다.</div><div><br /></div><div><b>Cohere</b></div><div>Cohere는 Command, Rerank 및 Embed를 포함한 여러 LLM을 제공하는 엔터프라이즈 AI 플랫폼이다. 이러한 LLM은 특정 회사의 사용 사례에 맞게 맞춤 학습되고 미세 조정될 수 있다 . Cohere LLM을 만든 회사는 Attention Is All You Need의 저자 중 한 명이 설립했다. Cohere의 강점 중 하나는 Microsoft Azure 에 바인딩된 OpenAI와 달리 하나의 단일 클라우드에 바인딩되지 않는다는 것이다.</div><div><br /></div><div><b>Ernie</b></div><div>Ernie는 Ernie 4.0 챗봇을 지원하는 Baidu의 대규모 언어 모델이다. 이 봇은 2023년 8월에 출시되었으며 4,500만 명 이상의 사용자를 확보했다. 어니는 10조 개의 매개변수를 가지고 있다는 소문이 돌았다. 봇은 중국어에서 가장 잘 작동하지만 다른 언어로도 가능하다.</div><div><br /></div><div><b>Falcon 40B</b></div><div>Falcon 40B는 Technology Innovation Institute에서 개발한 변환기 기반의 인과 디코더 전용 모델이다. 오픈 소스이며 영어 데이터로 교육되었다. 이 모델은 Falcon 1B와 Falcon 7B(10억 및 70억 매개변수)의 두 가지 작은 변형으로도 제공된다. Amazon은 Amazon SageMaker 에서 Falcon 40B를 사용할 수 있게 만들었다 . GitHub에서도 무료로 사용할 수 있다.</div><div><br /></div><div><b>Gemini</b></div><div>Gemini는 동일한 이름의 회사 챗봇을 지원하는 Google의 LLM 제품군이다. 이 모델은 모델 전환 시 Bard에서 Gemini로 브랜드가 변경된 챗봇을 지원하는 데 Palm을 대체했다. Gemini 모델은 다중 모드이므로 이미지, 오디오, 비디오는 물론 텍스트도 처리할 수 있다. Gemini는 많은 Google 애플리케이션 및 제품에도 통합되어 있다. 울트라, 프로, 나노 세 가지 크기로 제공된다. Ultra는 가장 크고 성능이 뛰어난 모델이고 Pro는 중간 계층 모델이며 Nano는 가장 작은 모델로 기기 내 작업 효율성을 위해 설계되었다. Gemini는 대부분의 평가된 벤치마크에서 GPT-4를 능가하다.</div><div><br /></div><div><b>GPT-3</b></div><div>GPT-3는 2020년에 출시된 1,750억 개 이상의 매개변수를 갖춘 OpenAI의 대규모 언어 모델이다. GPT-3은 디코더 전용 변환기 아키텍처를 사용하다. 2022년 9월 Microsoft는 GPT-3의 기본 모델을 독점적으로 사용한다고 발표했다. GPT-3은 이전 제품보다 10배 더 커졌다. GPT-3의 학습 데이터에는 Common Crawl, WebText2, Books1, Books2 및 Wikipedia가 포함된다.</div><div><br /></div><div>GPT-3은 OpenAI가 매개변수 수를 공개적으로 제공한 GPT 시리즈 모델 중 마지막 모델이다. GPT 시리즈는 2018년 OpenAI의 논문 \"Improving Language Understanding by Generative Pre-Training\"으로 처음 소개되었다.</div><div><br /></div><div><b>GPT-4</b></div><div>GPT-4는 2023년에 출시된 OpenAI의 GPT 시리즈 중 가장 큰 모델이다. 다른 모델과 마찬가지로 트랜스포머 기반 모델 이다 . 다른 모델과 달리 매개변수 개수는 공개되지 않았지만 해당 모델에는 170조 개가 넘는다는 소문이 있다. OpenAI는 GPT-4를 다중 모드 모델로 설명하다. 즉, 언어로만 제한되는 것이 아니라 언어와 이미지를 모두 처리하고 생성 할 수 있다는 의미이다. GPT-4에는 사용자가 음성 톤과 작업을 지정할 수 있는 시스템 메시지도 도입되었다.</div><div><br /></div><div>GPT-4는 여러 학업 시험에서 인간 수준의 성능을 보여주었다. 모델 출시 당시 일각에서는 GPT-4가 인공지능 (AGI)에 가까워졌다는 추측이 나왔다. 이는 인간과 동등하거나 더 똑똑하다는 의미다. GPT-4는 Microsoft Bing 검색을 지원하며 ChatGPT Plus에서 사용할 수 있으며 최종적으로 Microsoft Office 제품에 통합될 예정이다.</div><div><br /></div><div><b>GPT-4o</b></div><div>GPT-4 Omni( GPT-4o )는 OpenAI의 GPT-4 후속 제품이며 이전 모델에 비해 몇 가지 향상된 기능을 제공하다. GPT-4o는 ChatGPT를 위한 보다 자연스러운 인간 상호 작용을 생성하며 오디오, 이미지 및 텍스트를 포함한 다양한 입력을 허용하는 대규모 다중 모드 모델이다. 대화를 통해 사용자는 일반적인 인간 대화처럼 참여할 수 있으며 실시간 상호 작용을 통해 감정을 포착할 수도 있다. GPT-4o는 사진이나 화면을 보고 상호 작용 중에 질문을 할 수 있다.</div><div><br /></div><div>GPT-4o는 인간의 응답 시간과 비슷하고 GPT-4 Turbo보다 ​​빠른 232밀리초 내에 응답할 수 있다. GPT-4o 모델은 무료이며 개발자 및 고객 제품에 사용할 수 있다.</div><div><br /></div><div><b>Lamda</b></div><div>Lamda는 Google Brain이 2021년에 발표한 LLM 제품군이다. Lamda는 디코더 전용 변환기 언어 모델을 사용했으며 대규모 텍스트 모음에 대해 사전 학습되었다. 2022년 LaMDA는 당시 Google 엔지니어였던 Blake Lemoine이 프로그램에 지각력이 있다는 주장을 공개하면서 광범위한 주목을 받았다 . Seq2Seq 아키텍처를 기반으로 구축되었다.</div><div><br /></div><div><b>Llama</b></div><div>대형 언어 모델 Meta AI(Llama)는 2023년에 출시된 Meta의 LLM이다. 가장 큰 버전은 매개변수 크기가 650억 개이다. Llama는 원래 승인된 연구원 및 개발자에게 출시되었지만 현재는 오픈 소스이다. Llama는 사용, 테스트 및 실험에 더 적은 컴퓨팅 성능이 필요한 더 작은 크기로 제공된다.</div><div><br /></div><div>Llama는 변환기 아키텍처를 사용하며 CommonCrawl, GitHub, Wikipedia 및 Project Gutenberg의 웹페이지를 포함한 다양한 공개 데이터 소스에 대해 교육을 받았다. Llama는 효과적으로 유출되어 Vicuna와 Orca를 포함한 많은 후손을 낳았다.</div><div><br /></div><div><b>Mistral</b></div><div>Mistral은 평가된 모든 벤치마크에서 비슷한 크기의 Llama의 언어 모델보다 성능이 뛰어난 70억 개의 매개변수 언어 모델이다. Mistral은 또한 지침을 따르도록 특화된 미세 조정 모델을 보유하고 있다. 크기가 작기 때문에 비즈니스 목적에 맞는 자체 호스팅 및 유능한 성능이 가능하다. Apache 2.0 라이센스로 출시되었다.</div><div><br /></div><div><b>Orca</b></div><div>Orca는 Microsoft에서 개발했으며 130억 개의 매개변수를 가지고 있다. 즉, 노트북에서 실행할 수 있을 만큼 작다. LLM이 달성한 추론 절차를 모방하여 다른 오픈 소스 모델의 발전을 개선하는 것을 목표로 하다. Orca는 훨씬 적은 수의 매개변수로 GPT-4와 동일한 성능을 달성하며 많은 작업에서 GPT-3.5와 동등하다. Orca는 LLaMA의 130억 매개변수 버전을 기반으로 구축되었다.</div><div><br /></div><div><b>Palm</b></div><div>Pathways 언어 모델은 AI 챗봇 Bard를 지원하는 Google의 5,400억 매개변수 변환기 기반 모델이다 . Google의 머신러닝용 맞춤 하드웨어인 여러 TPU 4 Pod 에서 학습되었다 . Palm은 코딩, 수학, 분류, 질문 답변 등의 추론 작업을 전문으로 하다. Palm은 복잡한 작업을 간단한 하위 작업으로 분해하는 데에도 탁월하다.</div><div><br /></div><div>PaLM은 Pathways를 구축하여 궁극적으로 여러 사용 사례의 기반이 되는 단일 모델을 만드는 Google 연구 이니셔티브에서 이름을 따왔다. 생명 과학 및 의료 정보를 위한 Med-Palm 2와 위협 분석 속도를 높이기 위한 사이버 보안 배포를 위한 Sec-Palm을 포함하여 여러 가지 정밀 조정된 버전 의 Palm이 있다 .</div><div><br /></div><div><b>Phi</b></div><div>Phi-1은 Microsoft의 변환기 기반 언어 모델이다. 단 13억 개의 매개변수로 Phi-1은 교과서 수준의 데이터 수집에 대해 4일 동안 훈련되었다. Phi-1은 더 나은 품질의 데이터와 합성 데이터에 대해 훈련된 더 작은 모델을 향한 추세의 예이다.</div><div>Phi-1은 Python 코딩을 전문으로 하며 크기가 작기 때문에 일반 기능이 적다.</div><div><br /></div><div><b>StableLM</b></div><div>StableLM은 이미지 생성기 Stable Diffusion을 개발한 Stability AI에서 개발한 오픈 소스 언어 모델 시리즈이다. 이 글을 쓰는 시점에는 30억, 70억 개의 매개변수 모델이 사용 가능하며 150억, 300억, 650억, 1,750억 개의 매개변수 모델이 진행 중이다. StableLM은 투명하고 접근 가능하며 지원을 제공하는 것을 목표로 하다.</div><div><br /></div><div><b>Vicuna</b></div><div>Vicuna는 Llama에서 파생된 또 다른 영향력 있는 오픈 소스 LLM이다. LMSYS에서 개발했으며 sharegpt.com의 데이터를 사용하여 미세 조정되었다. 여러 벤치마크에 따르면 GPT-4보다 작고 성능이 떨어지지만 해당 크기의 모델에는 적합하다. Vicuna에는 330억 개의 매개변수가 있는 반면 GPT-4에는 수조 개의 매개변수가 있다.</div><div><br /></div><div>이외에, Gemma, Phi2와 같이 3B이하 매개변수를 가지는 소형 LLM도 릴리즈되었다. 이들은 노트북과 같은 온디바이스에 설치되어 사용될 수 있다. 다음은 소형 LLM 간 성능 비교를 보여준다.</div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFPomBnh3tYCvOI9OmdIM5whOXPvUVIlQFpNbT-ceNEQoaB-moPo3QJO1ognDXBDbizcKxNxoL-0C9Rn3vs3qQI5WmSLhvMgzBjV8PmIv8PysDy5xSLvnAq_FhKJqzSwK9633cIvE1Jsj3ZmMvdnzfKjL4ETKLrc39mnuRO8n5AV0OkIgT8YXPc5iEb1m1/s774/f1.PNG\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"454\" data-original-width=\"774\" height=\"235\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFPomBnh3tYCvOI9OmdIM5whOXPvUVIlQFpNbT-ceNEQoaB-moPo3QJO1ognDXBDbizcKxNxoL-0C9Rn3vs3qQI5WmSLhvMgzBjV8PmIv8PysDy5xSLvnAq_FhKJqzSwK9633cIvE1Jsj3ZmMvdnzfKjL4ETKLrc39mnuRO8n5AV0OkIgT8YXPc5iEb1m1/w400-h235/f1.PNG\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEbOKobtWUATJ5fdFWfgBFp5sMhFLDFgGK74poXqEFBDzevt7zJw8OoUG-NvCdtNgHC36P4-2gn-j-RtmgHXPOLFO-0KescWfO0MAIAtE-J-RkNieKN6-fixnwNfp-MBg6s_Jje7tA8TmnHdzw3qwkrlKtkpi_6xMDqbzzV9LtukEgc14hdHGTpzMdKaz8/s588/f2.PNG\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"341\" data-original-width=\"588\" height=\"233\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEbOKobtWUATJ5fdFWfgBFp5sMhFLDFgGK74poXqEFBDzevt7zJw8OoUG-NvCdtNgHC36P4-2gn-j-RtmgHXPOLFO-0KescWfO0MAIAtE-J-RkNieKN6-fixnwNfp-MBg6s_Jje7tA8TmnHdzw3qwkrlKtkpi_6xMDqbzzV9LtukEgc14hdHGTpzMdKaz8/w400-h233/f2.PNG\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">소형 LLM 모델 간 성능 비교</div></div><div style=\"text-align: left;\"><br /><b>레퍼런스</b><br /><ul style=\"text-align: left;\"><li><a href=\"https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models\">best large language models</a></li><li><a href=\"https://developers.googleblog.com/ko/large-language-models-on-device-with-mediapipe-and-tensorflow-lite/\">MediaPipe</a></li><li><a href=\"https://medium.com/@nghihuynh_37300/exploring-gemma-a-google-lightweight-and-open-source-large-language-model-f984b1ccca85\">Exploring Gemma: A Google Lightweight and Open-Source Large Language Model</a></li><li><a href=\"https://artificialanalysis.ai/leaderboards/models\">LLM Leaderboard - Comparison of GPT-4o, Llama 3, Mistral, Gemini and over 30 models</a></li><li><a href=\"https://pub.towardsai.net/googles-gemma-vs-microsoft-s-phi-2-vs-mistral-on-summarisation-6877bc7b1a69\">Google’s Gemma vs Microsoft’s Phi-2 vs Mistral on Summarisation</a></li></ul></div>",
        "contentSnippet": "이 글은 2024년 오픈소스 대형언어모델을 간략히 소개한다. LLM은 자연어 처리를 위해 개발되었지만, 현재는 멀티모달리티 모델로 발전하고 있다.\n\n\n\n\n다양한 LLM 모델과 자연어 처리\nBERT \nBERT 는 Google이 2018년에 도입한 LLM 제품군이다. BERT는 데이터 시퀀스를 다른 데이터 시퀀스로 변환할 수 있는 변환기 기반 모델이다. BERT의 아키텍처는 변압기 인코더의 스택이며 3억 4200만 개의 매개변수를 갖추고 있다. BERT는 대규모 데이터 코퍼스에 대해 사전 훈련된 후 자연어 추론 및 문장 텍스트 유사성과 함께 특정 작업을 수행하도록 미세 조정되었다. 이는 2019년 Google 검색 반복에서 쿼리 이해를 향상시키는 데 사용되었다.\n\n\nClaude  \nClaude  LLM은  AI 보조자가 유용하고 무해하며 정확하도록 돕는 일련의 원칙에 따라 AI 출력을 형성하는 헌법적 AI에 중점을 둔다. Claude는 Anthropic이라는 회사에서 만들어졌다. Claude LLM의 최신 버전은 Claude 3.5 Sonnet이다. LLM의 이전 버전보다 뉘앙스, 유머 및 복잡한 지침을 더 잘 이해하고 Claude 3 Opus의 두 배 속도로 작동하다. Claude.ai 및 Claude iOS 앱을 통해 무료로 사용할 수 있다.\n\n\nCohere\nCohere는 Command, Rerank 및 Embed를 포함한 여러 LLM을 제공하는 엔터프라이즈 AI 플랫폼이다. 이러한 LLM은 특정 회사의 사용 사례에 맞게 맞춤 학습되고 미세 조정될 수 있다 . Cohere LLM을 만든 회사는 Attention Is All You Need의 저자 중 한 명이 설립했다. Cohere의 강점 중 하나는 Microsoft Azure 에 바인딩된 OpenAI와 달리 하나의 단일 클라우드에 바인딩되지 않는다는 것이다.\n\n\nErnie\nErnie는 Ernie 4.0 챗봇을 지원하는 Baidu의 대규모 언어 모델이다. 이 봇은 2023년 8월에 출시되었으며 4,500만 명 이상의 사용자를 확보했다. 어니는 10조 개의 매개변수를 가지고 있다는 소문이 돌았다. 봇은 중국어에서 가장 잘 작동하지만 다른 언어로도 가능하다.\n\n\nFalcon 40B\nFalcon 40B는 Technology Innovation Institute에서 개발한 변환기 기반의 인과 디코더 전용 모델이다. 오픈 소스이며 영어 데이터로 교육되었다. 이 모델은 Falcon 1B와 Falcon 7B(10억 및 70억 매개변수)의 두 가지 작은 변형으로도 제공된다. Amazon은 Amazon SageMaker 에서 Falcon 40B를 사용할 수 있게 만들었다 . GitHub에서도 무료로 사용할 수 있다.\n\n\nGemini\nGemini는 동일한 이름의 회사 챗봇을 지원하는 Google의 LLM 제품군이다. 이 모델은 모델 전환 시 Bard에서 Gemini로 브랜드가 변경된 챗봇을 지원하는 데 Palm을 대체했다. Gemini 모델은 다중 모드이므로 이미지, 오디오, 비디오는 물론 텍스트도 처리할 수 있다. Gemini는 많은 Google 애플리케이션 및 제품에도 통합되어 있다. 울트라, 프로, 나노 세 가지 크기로 제공된다. Ultra는 가장 크고 성능이 뛰어난 모델이고 Pro는 중간 계층 모델이며 Nano는 가장 작은 모델로 기기 내 작업 효율성을 위해 설계되었다. Gemini는 대부분의 평가된 벤치마크에서 GPT-4를 능가하다.\n\n\nGPT-3\nGPT-3는 2020년에 출시된 1,750억 개 이상의 매개변수를 갖춘 OpenAI의 대규모 언어 모델이다. GPT-3은 디코더 전용 변환기 아키텍처를 사용하다. 2022년 9월 Microsoft는 GPT-3의 기본 모델을 독점적으로 사용한다고 발표했다. GPT-3은 이전 제품보다 10배 더 커졌다. GPT-3의 학습 데이터에는 Common Crawl, WebText2, Books1, Books2 및 Wikipedia가 포함된다.\n\n\nGPT-3은 OpenAI가 매개변수 수를 공개적으로 제공한 GPT 시리즈 모델 중 마지막 모델이다. GPT 시리즈는 2018년 OpenAI의 논문 \"Improving Language Understanding by Generative Pre-Training\"으로 처음 소개되었다.\n\n\nGPT-4\nGPT-4는 2023년에 출시된 OpenAI의 GPT 시리즈 중 가장 큰 모델이다. 다른 모델과 마찬가지로 트랜스포머 기반 모델 이다 . 다른 모델과 달리 매개변수 개수는 공개되지 않았지만 해당 모델에는 170조 개가 넘는다는 소문이 있다. OpenAI는 GPT-4를 다중 모드 모델로 설명하다. 즉, 언어로만 제한되는 것이 아니라 언어와 이미지를 모두 처리하고 생성 할 수 있다는 의미이다. GPT-4에는 사용자가 음성 톤과 작업을 지정할 수 있는 시스템 메시지도 도입되었다.\n\n\nGPT-4는 여러 학업 시험에서 인간 수준의 성능을 보여주었다. 모델 출시 당시 일각에서는 GPT-4가 인공지능 (AGI)에 가까워졌다는 추측이 나왔다. 이는 인간과 동등하거나 더 똑똑하다는 의미다. GPT-4는 Microsoft Bing 검색을 지원하며 ChatGPT Plus에서 사용할 수 있으며 최종적으로 Microsoft Office 제품에 통합될 예정이다.\n\n\nGPT-4o\nGPT-4 Omni( GPT-4o )는 OpenAI의 GPT-4 후속 제품이며 이전 모델에 비해 몇 가지 향상된 기능을 제공하다. GPT-4o는 ChatGPT를 위한 보다 자연스러운 인간 상호 작용을 생성하며 오디오, 이미지 및 텍스트를 포함한 다양한 입력을 허용하는 대규모 다중 모드 모델이다. 대화를 통해 사용자는 일반적인 인간 대화처럼 참여할 수 있으며 실시간 상호 작용을 통해 감정을 포착할 수도 있다. GPT-4o는 사진이나 화면을 보고 상호 작용 중에 질문을 할 수 있다.\n\n\nGPT-4o는 인간의 응답 시간과 비슷하고 GPT-4 Turbo보다 ​​빠른 232밀리초 내에 응답할 수 있다. GPT-4o 모델은 무료이며 개발자 및 고객 제품에 사용할 수 있다.\n\n\nLamda\nLamda는 Google Brain이 2021년에 발표한 LLM 제품군이다. Lamda는 디코더 전용 변환기 언어 모델을 사용했으며 대규모 텍스트 모음에 대해 사전 학습되었다. 2022년 LaMDA는 당시 Google 엔지니어였던 Blake Lemoine이 프로그램에 지각력이 있다는 주장을 공개하면서 광범위한 주목을 받았다 . Seq2Seq 아키텍처를 기반으로 구축되었다.\n\n\nLlama\n대형 언어 모델 Meta AI(Llama)는 2023년에 출시된 Meta의 LLM이다. 가장 큰 버전은 매개변수 크기가 650억 개이다. Llama는 원래 승인된 연구원 및 개발자에게 출시되었지만 현재는 오픈 소스이다. Llama는 사용, 테스트 및 실험에 더 적은 컴퓨팅 성능이 필요한 더 작은 크기로 제공된다.\n\n\nLlama는 변환기 아키텍처를 사용하며 CommonCrawl, GitHub, Wikipedia 및 Project Gutenberg의 웹페이지를 포함한 다양한 공개 데이터 소스에 대해 교육을 받았다. Llama는 효과적으로 유출되어 Vicuna와 Orca를 포함한 많은 후손을 낳았다.\n\n\nMistral\nMistral은 평가된 모든 벤치마크에서 비슷한 크기의 Llama의 언어 모델보다 성능이 뛰어난 70억 개의 매개변수 언어 모델이다. Mistral은 또한 지침을 따르도록 특화된 미세 조정 모델을 보유하고 있다. 크기가 작기 때문에 비즈니스 목적에 맞는 자체 호스팅 및 유능한 성능이 가능하다. Apache 2.0 라이센스로 출시되었다.\n\n\nOrca\nOrca는 Microsoft에서 개발했으며 130억 개의 매개변수를 가지고 있다. 즉, 노트북에서 실행할 수 있을 만큼 작다. LLM이 달성한 추론 절차를 모방하여 다른 오픈 소스 모델의 발전을 개선하는 것을 목표로 하다. Orca는 훨씬 적은 수의 매개변수로 GPT-4와 동일한 성능을 달성하며 많은 작업에서 GPT-3.5와 동등하다. Orca는 LLaMA의 130억 매개변수 버전을 기반으로 구축되었다.\n\n\nPalm\nPathways 언어 모델은 AI 챗봇 Bard를 지원하는 Google의 5,400억 매개변수 변환기 기반 모델이다 . Google의 머신러닝용 맞춤 하드웨어인 여러 TPU 4 Pod 에서 학습되었다 . Palm은 코딩, 수학, 분류, 질문 답변 등의 추론 작업을 전문으로 하다. Palm은 복잡한 작업을 간단한 하위 작업으로 분해하는 데에도 탁월하다.\n\n\nPaLM은 Pathways를 구축하여 궁극적으로 여러 사용 사례의 기반이 되는 단일 모델을 만드는 Google 연구 이니셔티브에서 이름을 따왔다. 생명 과학 및 의료 정보를 위한 Med-Palm 2와 위협 분석 속도를 높이기 위한 사이버 보안 배포를 위한 Sec-Palm을 포함하여 여러 가지 정밀 조정된 버전 의 Palm이 있다 .\n\n\nPhi\nPhi-1은 Microsoft의 변환기 기반 언어 모델이다. 단 13억 개의 매개변수로 Phi-1은 교과서 수준의 데이터 수집에 대해 4일 동안 훈련되었다. Phi-1은 더 나은 품질의 데이터와 합성 데이터에 대해 훈련된 더 작은 모델을 향한 추세의 예이다.\nPhi-1은 Python 코딩을 전문으로 하며 크기가 작기 때문에 일반 기능이 적다.\n\n\nStableLM\nStableLM은 이미지 생성기 Stable Diffusion을 개발한 Stability AI에서 개발한 오픈 소스 언어 모델 시리즈이다. 이 글을 쓰는 시점에는 30억, 70억 개의 매개변수 모델이 사용 가능하며 150억, 300억, 650억, 1,750억 개의 매개변수 모델이 진행 중이다. StableLM은 투명하고 접근 가능하며 지원을 제공하는 것을 목표로 하다.\n\n\nVicuna\nVicuna는 Llama에서 파생된 또 다른 영향력 있는 오픈 소스 LLM이다. LMSYS에서 개발했으며 sharegpt.com의 데이터를 사용하여 미세 조정되었다. 여러 벤치마크에 따르면 GPT-4보다 작고 성능이 떨어지지만 해당 크기의 모델에는 적합하다. Vicuna에는 330억 개의 매개변수가 있는 반면 GPT-4에는 수조 개의 매개변수가 있다.\n\n\n이외에, Gemma, Phi2와 같이 3B이하 매개변수를 가지는 소형 LLM도 릴리즈되었다. 이들은 노트북과 같은 온디바이스에 설치되어 사용될 수 있다. 다음은 소형 LLM 간 성능 비교를 보여준다.\n\n\n소형 LLM 모델 간 성능 비교\n\n레퍼런스\n\nbest large language models\nMediaPipe\nExploring Gemma: A Google Lightweight and Open-Source Large Language Model\nLLM Leaderboard - Comparison of GPT-4o, Llama 3, Mistral, Gemini and over 30 models\nGoogle’s Gemma vs Microsoft’s Phi-2 vs Mistral on Summarisation",
        "id": "tag:blogger.com,1999:blog-5201956450461596914.post-6277705680864866196",
        "isoDate": "2024-06-25T03:51:00.000Z"
      },
      {
        "title": "GPU 거지 딥러닝 서버 만들기",
        "link": "http://daddynkidsmakers.blogspot.com/2024/06/gpu.html",
        "pubDate": "2024-06-23T05:28:00.000Z",
        "author": "Daddy Maker",
        "content": "<div style=\"text-align: left;\">이 글은&nbsp;GPU 거지?를 위한 딥러닝 서버 구축 방법을 간략히 알아본다.&nbsp;</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEhyY3jxdHp9FTH8y4K8EeVXrPGRlNV_hJRMl4ia3WSSOwjoS4-w4ohJk9dXvFf0SUiqHle0xkCrwXpzbMHf02_7gNkMCJPuQG0aA5_J5a-uEP5IWkohQ4eg79CLQrMl5iFYjeVWTm2njcOWveVPoxpPoy2oDuJtQlOcqa0edQBoot-nUIqJPfW8rwkTBu2U\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"194\" data-original-width=\"259\" height=\"240\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEhyY3jxdHp9FTH8y4K8EeVXrPGRlNV_hJRMl4ia3WSSOwjoS4-w4ohJk9dXvFf0SUiqHle0xkCrwXpzbMHf02_7gNkMCJPuQG0aA5_J5a-uEP5IWkohQ4eg79CLQrMl5iFYjeVWTm2njcOWveVPoxpPoy2oDuJtQlOcqa0edQBoot-nUIqJPfW8rwkTBu2U\" width=\"320\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEgvokHVnzJnaBK5Y12GCjsR13L0rSJBsamOiRPiU7KtgsyBefq6xZelJuuc8jPkUaVRKknODB9ErqEWLuUKDVWvZQzfCBor_nS0S7f_4lQrJpQYGpmnuk28v8FpW88W_-7UwDn0vlpS3P81it8SmXoI-jj6AFGDApB9372RtaZTmW243eLAK7ZC2OdnWnDh\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"960\" data-original-width=\"1280\" height=\"240\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEgvokHVnzJnaBK5Y12GCjsR13L0rSJBsamOiRPiU7KtgsyBefq6xZelJuuc8jPkUaVRKknODB9ErqEWLuUKDVWvZQzfCBor_nS0S7f_4lQrJpQYGpmnuk28v8FpW88W_-7UwDn0vlpS3P81it8SmXoI-jj6AFGDApB9372RtaZTmW243eLAK7ZC2OdnWnDh\" width=\"320\" /></a></div></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://www.ebay.com/itm/225110549185?itmmeta=01J11V603QMATRXW6DK4MJDFMW&amp;hash=item3469a262c1:g:s9EAAOSwjhRi8aPq&amp;itmprp=enc%3AAQAJAAAA8EisE7hYOfIW9NJwekQvSZjEOAVsBQ62JDkJBNADgEw8PyMBbC861ZA98bGSlK6vDL52gunRJrp5d7XOyiXh87qHgckiTqtuqdfWcFx6P4wCg5siVGnCoYY8oLG7Gpf9mTEFcWW%2BFT2paKgmvsHQ2rgpo0aMDs1UushDHyk77%2F2sbk2gfsS8ipOejc7TGzA%2FnPagaEluiSl5Sq6b8LGNHU5H4dK1TZwVcvsmjZjMcu8vpYOHNtJfwXeuIYaosXX8EknELlW%2ByuoJ8N5cH6m2kl9zfkklqGj72192TwMdW8YqKmpMosH55%2FvBL1SWQ9X97A%3D%3D%7Ctkp%3ABk9SR_qBmLuIZA\">ASUS ESC4000 G4 2U 8Bays Dual 2nd Gen Scalable Processor Rackmount GPU Server | eBay</a></div><br /></div><div style=\"text-align: left;\">많은 정부 과제, 용역, LLM 열풍 등으로 인해 NVIDIA GPU 서버 수요가 폭증하고 있다. 이에 덩달아, GPU 인플레이션이 심한데, 업체에서 구매하면 A100 x 4 way server 가격만 약 1억이다. 이를 개별 부품을 사서 조립하면, 6~7천만원 수준에서 GPU 서버를 얻을 수 있다. 물론, 업체에서 서비스로 설치해주는 리눅스 OS, NVIDIA 드라이버 등 소프트웨어는 본인이 모두 해결해야 단점은 있다.&nbsp;</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b>필요성</b></div><div style=\"text-align: left;\">본인이 취미?로 CNN계열 YOLO 모델을 학습, 개발할 때는 그 당시로는 대용량 8GB GPU를 사용하면 나름 빵빵한 스펙이었다.&nbsp;</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">그러나, LLM 기술이 릴리즈되면서, 모든 것이 달라졌다. 예를 들어, 70B 파라메터를 가진 LLM 모델을 학습하거나 튜닝하기 위해서는 700GB 이상의 GPU 메모리가 필요하며, 80GB 용량을 가진 NVIDIA A100이 1,300 ~ 1,500만원이니, 대략 2억 가까이되는 리소스가 필요한 상황이 되어버렸다.&nbsp;</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgS9ocdIrpDwV4Efxhddt-4iZIfSGQeJK95ogW5hy3ThGcDKq-mcJOP-Xniz06_WRVLU6cmA51DZI1aHdvUTXZTfPKWWqWwVSAGIq6XkzEw3yfqB4VjP2lHMR8AbLWv4ptnns9fXn4lSznZn8W9K5b1r_tPYFpSDQodp1S9VAp-YKxFVyd4D_KvBttb4aR-/s694/oom.JPG\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"226\" data-original-width=\"694\" height=\"130\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgS9ocdIrpDwV4Efxhddt-4iZIfSGQeJK95ogW5hy3ThGcDKq-mcJOP-Xniz06_WRVLU6cmA51DZI1aHdvUTXZTfPKWWqWwVSAGIq6XkzEw3yfqB4VjP2lHMR8AbLWv4ptnns9fXn4lSznZn8W9K5b1r_tPYFpSDQodp1S9VAp-YKxFVyd4D_KvBttb4aR-/w400-h130/oom.JPG\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">LLM 모델 튜닝, 학습에서 빈번한 Out of Memory 에러</div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div></div><div style=\"text-align: left;\">실제로, 배치 데이터를 GPU에 올리고 학습하기 위해서는 이 용량의 몇 배가 필요할 수 있어, 금액은 더 올라가게 되어 있다(국내에서도 이 정도 인프라를 갖춘 IT업체가 많지 않다. 대부분 Lora와 같은 작은 메모리를 이용한 파인튜닝 정도이거나, 해외 빅테크 업체가 개발된 LLM의 사전 학습된 모델을 사용해 튜닝하는 수준이다).</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">이런 이유로, 본인이 LLM모델을 직접 개발하고 싶다면, 가성비있게 다중 GPU지원하는 서버를 개발할 필요가 생긴다.&nbsp;</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b>다중 GPU 서버 구축하기</b></div><div style=\"text-align: left;\"><div>다중 GPU 구축할 때 주요 고려 사항은 다음과 같다.</div><div><ul style=\"text-align: left;\"><li>메모리(VRAM)</li><li>성능(Tensor 코어, 클럭 속도)</li><li>슬롯 너비</li><li>사용 전력</li><li>다중 GPU 지원 마더보드</li><li>냉각 처리</li><li>넉넉한 케이스 크기</li></ul></div><div>딥러닝 작업을 위해서는 많은 메모리가 필요하다. LLM은 미세 조정하기에도 방대하며, 컴퓨터 비전 작업은 특히 3D 네트워크에서 메모리 집약적일 수 있다. 당연히 찾아야 할 가장 중요한 측면은 GPU VRAM이다. LLM의 경우 최소 24GB 메모리를 권장하고 컴퓨터 비전 작업의 경우 12GB 이하로 내려가지 않는다.</div><div><br /></div><div>두 번째 기준은 FLOPS(초당 부동 소수점 연산)로 추정할 수 있는 성능이다.</div><div>과거의 중요한 숫자는 회로의 CUDA 코어 수였다. 그러나 딥 러닝의 등장으로 NVIDIA는 클럭당 더 많은 FMA(Fused Multiply-Add) 연산을 수행할 수 있는 특수 텐서 코어를 도입했다.&nbsp;</div><div><br /></div><div>다른 GPU의 성능을 비교할 때는 각별히 주의해야 하다. 다른 세대/아키텍처의 Tensor 코어는 비교할 수 없다. 예를 들어, A100은 256개의 FP16 FMA 작동/클럭을 수행하는 반면 V100은 64개의 \"유일한\" 작업을 수행한다. 또한 이전 아키텍처(Turing, Volta)는 32비트 텐서 연산을 지원하지 않는다. 비교를 더 어렵게 만드는 것은 NVIDIA가 백서에서도 항상 FMA를 보고하는 것은 아니며 동일한 아키텍처의 GPU가 다른 FMA를 가질 수 있다는 것이다. 참고로, GPT는 수많은 A100을 사용해 학습되었다.</div><div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEjs2UWxCl5mkn_YynXEVOMJGs07ub_3-42zoXLFyQ4XBNvofMMVi_jF-35u3AQvKgmYkk5eZt1hfHjnOKBPTyxBYrz8DYQAlLBKw0OV8xabMRkUo6km5Wh58Ng_H8bdvUWP4hSLLGTSJiNKSkTAcMLoQdsZpmue00EKqd-Egz63F-cim8-Wzkotl-m5Hgk3\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"315\" data-original-width=\"310\" height=\"240\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjs2UWxCl5mkn_YynXEVOMJGs07ub_3-42zoXLFyQ4XBNvofMMVi_jF-35u3AQvKgmYkk5eZt1hfHjnOKBPTyxBYrz8DYQAlLBKw0OV8xabMRkUo6km5Wh58Ng_H8bdvUWP4hSLLGTSJiNKSkTAcMLoQdsZpmue00EKqd-Egz63F-cim8-Wzkotl-m5Hgk3\" width=\"236\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">NVIDIA A100 (80GB. 가격은 12,000 ~ 15,000만원 사이)</div><br /></div><div>멀티 GPU 시스템을 구축할 때는 GPU를 PC 케이스에 물리적으로 맞추는 방법을 계획해야 하다. GPU가 점점 더 커지고 있기 때문에, 특히 게임 시리즈에서는 이것이 더 큰 문제가 되고 있다. 소비자용 마더보드에는 최대 7개의 PCIe 슬롯이 있으며 PC 케이스는 이 설정을 기반으로 제작됩니다. 4090은 제조업체에 따라 4개의 슬롯을 쉽게 차지할 수 있으므로 이것이 문제가 되는 이유를 알 수 있다. 또한 과열을 방지하기 위해 송풍기 스타일이 아니거나 수냉식이 아닌 GPU 사이에 최소 1개의 슬롯을 남겨 두어야 하다. 다음과 같은 옵션이 있다.</div><div><br /></div><div>수냉식은 최대 2개의 슬롯을 차지하고 비싸다. AIO(All-in-One) 솔루션을 얻지 못하면 맞춤형 수냉식 루프를 구축해야 하다. AIO 라디에이터가 케이스에 맞지 않을 수 있으므로 여러 개의 수냉식 GPU를 장착하려는 경우에도 마찬가지이다.&nbsp;</div><div><br /></div><div>최신 GPU는 점점 더 많은 전력을 소비하다. 예를 들어, 4090은 450W가 필요하고 H100은 최대 700W를 얻을 수 있다. GPU가 끌어올 수 있는 최대 전력을 줄이는 데 필요한 것은 다음과 같다.</div><div><br /></div><div>sudo nvidia-smi -i &lt;GPU_index&gt; -pl &lt;power_limit&gt;</div><div><br /></div><div>다음 단계는 여러 GPU를 허용하는 마더보드를 선택하는 것이다. 여기서 주요 고려 사항은 PCIe 레인이다. 각 카드에 대해 각각 x8 레인이 있는 최소 PCIe 3.0 슬롯이 필요하다. PCIe 4.0 또는 5.0은 더 드물며 대부분의 딥 러닝 사용 사례에 필요하지 않는다. 간격을 확인하고 GPU가 실제로 원하는 곳으로 이동할 수 있는지 확인하라.&nbsp;</div><div><br /></div><div>마더보드는 여러 GPU를 연동하기 위한 Multiple GPU와, SLI 혹은 NVLink 규약을 지원해야 한다. 이 규약은 다중 GPU 간 메모리를 합치고, 공유하며, 이들간 데이터 전송 속도를 최적화한다. 다음은 그 리스트를 보여준다.&nbsp;</div><div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEhEemEAGq0JVB3Lpbf1J5Lny8_H1bEJvCSZ-vffQuJx2hYAES_dbvg2kFhBDNyvj-iw9u-hb59Q-0-kLofRqfZf5EwiKeMNuMukwTe1NeFxggKaeNPV4I7-PYQ1IR4Gux02IhA2XTamva6pNr5ZllslKPeANFWR0jEpLv_I0XganrIGBHDC7nwaYX-wbKrX\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"456\" data-original-width=\"543\" height=\"269\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEhEemEAGq0JVB3Lpbf1J5Lny8_H1bEJvCSZ-vffQuJx2hYAES_dbvg2kFhBDNyvj-iw9u-hb59Q-0-kLofRqfZf5EwiKeMNuMukwTe1NeFxggKaeNPV4I7-PYQ1IR4Gux02IhA2XTamva6pNr5ZllslKPeANFWR0jEpLv_I0XganrIGBHDC7nwaYX-wbKrX=w320-h269\" width=\"320\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEi9QRBYgkvVSk78f69OEBA52Gbmx6T3uv62a6rfGcqZDMFCS3-mI971atotuU6lxzxpjihLiPPLIAQ3NLVBesw6PxzzMVcIl4RPw5chLhL1Pf-BEjUMQSf6mqyxxIIbeRVxS8uXA_C7on2YHEzpmDK7Mx-a-0uRJ6BHU8PTJmw81Bf7v-lDEHbqAX1zfSN0\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"348\" data-original-width=\"565\" height=\"246\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEi9QRBYgkvVSk78f69OEBA52Gbmx6T3uv62a6rfGcqZDMFCS3-mI971atotuU6lxzxpjihLiPPLIAQ3NLVBesw6PxzzMVcIl4RPw5chLhL1Pf-BEjUMQSf6mqyxxIIbeRVxS8uXA_C7on2YHEzpmDK7Mx-a-0uRJ6BHU8PTJmw81Bf7v-lDEHbqAX1zfSN0=w400-h246\" width=\"400\" /></a></div><br /></div><div><b>마무리</b></div></div><div style=\"text-align: left;\"><div><div>앞의 고려사항이 복잡하다면, GPU 서버 랙 구입을 감안할 수 있다. 더 비싸지만, 안정적이다. 다음은 이런 부분이 고려된 GPU 서버를 보여준다.</div></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><iframe allowfullscreen=\"\" class=\"BLOG_video_class\" height=\"266\" src=\"https://www.youtube.com/embed/MWxXAGFyvTc\" width=\"320\" youtube-src-id=\"MWxXAGFyvTc\"></iframe></div><div><br /></div></div><div style=\"text-align: left;\"><b>레퍼런스</b></div><div style=\"text-align: left;\"><ul style=\"text-align: left;\"><li><a href=\"https://www.asus.com/commercial-servers-workstations/pro-ws-x299-sage-ii/\">Pro WS X299 SAGE II | Servers &amp; Workstations | ASUS Global</a></li><li><a href=\"https://adriangcoder.medium.com/building-a-multi-gpu-deep-learning-machine-on-a-budget-3f3b717d80a9\">Building a Multi-GPU Deep Learning Machine on a budget | by Adrian G | Medium</a></li><li><a href=\"https://www.asus.com/us/site/motherboards/intel-x299-gamer-creator/deeplearning/lambda/\">Lambda | Deep Learning | ASUS US</a></li><li><a href=\"https://towardsdatascience.com/how-to-build-a-multi-gpu-system-for-deep-learning-in-2023-e5bbb905d935#:~:text=The%20next%20step%20of%20the,for%20most%20deep%20learning%20usecases.\">Build a Multi-GPU System for Deep Learning in 2023 | Towards Data Science</a></li><li><a href=\"https://www.asus.com/commercial-servers-workstations/pro-ws-x299-sage-ii/\">Pro WS X299 SAGE II | Servers &amp; Workstations | ASUS Global</a></li></ul></div>",
        "contentSnippet": "이 글은 GPU 거지?를 위한 딥러닝 서버 구축 방법을 간략히 알아본다. \n\n\n\n\n\nASUS ESC4000 G4 2U 8Bays Dual 2nd Gen Scalable Processor Rackmount GPU Server | eBay\n\n많은 정부 과제, 용역, LLM 열풍 등으로 인해 NVIDIA GPU 서버 수요가 폭증하고 있다. 이에 덩달아, GPU 인플레이션이 심한데, 업체에서 구매하면 A100 x 4 way server 가격만 약 1억이다. 이를 개별 부품을 사서 조립하면, 6~7천만원 수준에서 GPU 서버를 얻을 수 있다. 물론, 업체에서 서비스로 설치해주는 리눅스 OS, NVIDIA 드라이버 등 소프트웨어는 본인이 모두 해결해야 단점은 있다. \n\n\n필요성\n본인이 취미?로 CNN계열 YOLO 모델을 학습, 개발할 때는 그 당시로는 대용량 8GB GPU를 사용하면 나름 빵빵한 스펙이었다. \n\n\n그러나, LLM 기술이 릴리즈되면서, 모든 것이 달라졌다. 예를 들어, 70B 파라메터를 가진 LLM 모델을 학습하거나 튜닝하기 위해서는 700GB 이상의 GPU 메모리가 필요하며, 80GB 용량을 가진 NVIDIA A100이 1,300 ~ 1,500만원이니, 대략 2억 가까이되는 리소스가 필요한 상황이 되어버렸다. \n\n\nLLM 모델 튜닝, 학습에서 빈번한 Out of Memory 에러\n\n\n실제로, 배치 데이터를 GPU에 올리고 학습하기 위해서는 이 용량의 몇 배가 필요할 수 있어, 금액은 더 올라가게 되어 있다(국내에서도 이 정도 인프라를 갖춘 IT업체가 많지 않다. 대부분 Lora와 같은 작은 메모리를 이용한 파인튜닝 정도이거나, 해외 빅테크 업체가 개발된 LLM의 사전 학습된 모델을 사용해 튜닝하는 수준이다).\n\n\n이런 이유로, 본인이 LLM모델을 직접 개발하고 싶다면, 가성비있게 다중 GPU지원하는 서버를 개발할 필요가 생긴다. \n\n\n다중 GPU 서버 구축하기\n\n다중 GPU 구축할 때 주요 고려 사항은 다음과 같다.\n\n메모리(VRAM)\n성능(Tensor 코어, 클럭 속도)\n슬롯 너비\n사용 전력\n다중 GPU 지원 마더보드\n냉각 처리\n넉넉한 케이스 크기\n\n딥러닝 작업을 위해서는 많은 메모리가 필요하다. LLM은 미세 조정하기에도 방대하며, 컴퓨터 비전 작업은 특히 3D 네트워크에서 메모리 집약적일 수 있다. 당연히 찾아야 할 가장 중요한 측면은 GPU VRAM이다. LLM의 경우 최소 24GB 메모리를 권장하고 컴퓨터 비전 작업의 경우 12GB 이하로 내려가지 않는다.\n\n\n두 번째 기준은 FLOPS(초당 부동 소수점 연산)로 추정할 수 있는 성능이다.\n과거의 중요한 숫자는 회로의 CUDA 코어 수였다. 그러나 딥 러닝의 등장으로 NVIDIA는 클럭당 더 많은 FMA(Fused Multiply-Add) 연산을 수행할 수 있는 특수 텐서 코어를 도입했다. \n\n\n다른 GPU의 성능을 비교할 때는 각별히 주의해야 하다. 다른 세대/아키텍처의 Tensor 코어는 비교할 수 없다. 예를 들어, A100은 256개의 FP16 FMA 작동/클럭을 수행하는 반면 V100은 64개의 \"유일한\" 작업을 수행한다. 또한 이전 아키텍처(Turing, Volta)는 32비트 텐서 연산을 지원하지 않는다. 비교를 더 어렵게 만드는 것은 NVIDIA가 백서에서도 항상 FMA를 보고하는 것은 아니며 동일한 아키텍처의 GPU가 다른 FMA를 가질 수 있다는 것이다. 참고로, GPT는 수많은 A100을 사용해 학습되었다.\n\n\nNVIDIA A100 (80GB. 가격은 12,000 ~ 15,000만원 사이)\n\n멀티 GPU 시스템을 구축할 때는 GPU를 PC 케이스에 물리적으로 맞추는 방법을 계획해야 하다. GPU가 점점 더 커지고 있기 때문에, 특히 게임 시리즈에서는 이것이 더 큰 문제가 되고 있다. 소비자용 마더보드에는 최대 7개의 PCIe 슬롯이 있으며 PC 케이스는 이 설정을 기반으로 제작됩니다. 4090은 제조업체에 따라 4개의 슬롯을 쉽게 차지할 수 있으므로 이것이 문제가 되는 이유를 알 수 있다. 또한 과열을 방지하기 위해 송풍기 스타일이 아니거나 수냉식이 아닌 GPU 사이에 최소 1개의 슬롯을 남겨 두어야 하다. 다음과 같은 옵션이 있다.\n\n\n수냉식은 최대 2개의 슬롯을 차지하고 비싸다. AIO(All-in-One) 솔루션을 얻지 못하면 맞춤형 수냉식 루프를 구축해야 하다. AIO 라디에이터가 케이스에 맞지 않을 수 있으므로 여러 개의 수냉식 GPU를 장착하려는 경우에도 마찬가지이다. \n\n\n최신 GPU는 점점 더 많은 전력을 소비하다. 예를 들어, 4090은 450W가 필요하고 H100은 최대 700W를 얻을 수 있다. GPU가 끌어올 수 있는 최대 전력을 줄이는 데 필요한 것은 다음과 같다.\n\n\nsudo nvidia-smi -i <GPU_index> -pl <power_limit>\n\n\n다음 단계는 여러 GPU를 허용하는 마더보드를 선택하는 것이다. 여기서 주요 고려 사항은 PCIe 레인이다. 각 카드에 대해 각각 x8 레인이 있는 최소 PCIe 3.0 슬롯이 필요하다. PCIe 4.0 또는 5.0은 더 드물며 대부분의 딥 러닝 사용 사례에 필요하지 않는다. 간격을 확인하고 GPU가 실제로 원하는 곳으로 이동할 수 있는지 확인하라. \n\n\n마더보드는 여러 GPU를 연동하기 위한 Multiple GPU와, SLI 혹은 NVLink 규약을 지원해야 한다. 이 규약은 다중 GPU 간 메모리를 합치고, 공유하며, 이들간 데이터 전송 속도를 최적화한다. 다음은 그 리스트를 보여준다. \n\n\n\n\n마무리\n\n\n앞의 고려사항이 복잡하다면, GPU 서버 랙 구입을 감안할 수 있다. 더 비싸지만, 안정적이다. 다음은 이런 부분이 고려된 GPU 서버를 보여준다.\n\n\n\n\n레퍼런스\n\nPro WS X299 SAGE II | Servers & Workstations | ASUS Global\nBuilding a Multi-GPU Deep Learning Machine on a budget | by Adrian G | Medium\nLambda | Deep Learning | ASUS US\nBuild a Multi-GPU System for Deep Learning in 2023 | Towards Data Science\nPro WS X299 SAGE II | Servers & Workstations | ASUS Global",
        "id": "tag:blogger.com,1999:blog-5201956450461596914.post-3734208096456279404",
        "isoDate": "2024-06-23T05:28:00.000Z"
      },
      {
        "title": "Weights & Biases로  딥러닝 모델 개발 프로세스 기록, 분석, 가시화 및 모델 튜닝하기",
        "link": "http://daddynkidsmakers.blogspot.com/2024/06/weights-biases.html",
        "pubDate": "2024-06-21T07:49:00.000Z",
        "author": "Daddy Maker",
        "content": "<div style=\"text-align: left;\">딥러닝 모델들을 개발하다 보면, 수많은 종류의 데이터셋, 하이퍼모델 파라메터 튜닝 등으로 인해 관리해야 할 자료들이 매우 복잡해진다는 것을 알게 된다. Weights &amp; Biases (W&amp;B) 회사는 이름 그대로 완벽한 모델 학습을 위해 필요한 Weights &amp; Biases를 모니터링, 관리할 수 있는 로그 도구이다. 즉, 딥러닝 모델 개발자를 위한 프로세스 로그 및 가시화 플랫폼을 제공한다.&nbsp;</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEjv5Nez6boz4ciN-vuWPnZ2hGNHcsaKTJWHNikWDMzWWMnRwrM45ut3Jhuc4AOmZaCFkC8Ma1lxMThj4V2zHtS7v6o-0Zvl7iRxEJUXTYcMndbTIiDnDucqh0SK8mZkaifmVtXOkJU50LJywNaaQChC-M3jWTmoUFkCijzBm8Vk79EZDElUcCtswvAa9SQj\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"540\" data-original-width=\"960\" height=\"225\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjv5Nez6boz4ciN-vuWPnZ2hGNHcsaKTJWHNikWDMzWWMnRwrM45ut3Jhuc4AOmZaCFkC8Ma1lxMThj4V2zHtS7v6o-0Zvl7iRxEJUXTYcMndbTIiDnDucqh0SK8mZkaifmVtXOkJU50LJywNaaQChC-M3jWTmoUFkCijzBm8Vk79EZDElUcCtswvAa9SQj=w400-h225\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">W&amp;B(AI Summer)</div><br /></div><div style=\"text-align: left;\">매우 직관적인 이름을 가진 이 스타트업은 Tensorboard와 유사하지만, 적은 코드로 모델 개발에 많은 통찰력을 준다. 이 W&amp;B(WandB) 라이브러리를 사용하면, 딥러닝 모델 학습 시 지저분하게 붙어 나가는 로그 처리를 매우 간단한 함수 몇 개로 처리할 수 있다. 통합된 데시보드 형태로 다양한 모델 학습 품질 지표를 확인 및 비교할 수 있다. 이외, 학습 모델 하이퍼 파라메터 관리와 튜닝 및 비교 보고서 생성 기능을 제공한다. 로그는 숫자, 텍스트, 이미지 등 다양한 포맷을 지원한다.&nbsp;</div><div style=\"text-align: left;\"><div style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEgUPLogcemSukKiuTCB0j37hxU-YrTgXFc-14TlaMAmZRV4QE02guoMmWYevt_8GB6UUkVH2E4hZj-G0bsJbDLwpo0SkXH4rJLbJMkm1K6ovOlujjnkJ9WHQJxe_vp-TBj6JX0uQ1Yav45quf_KXLviQvRQeiGqYV7IQGBApKUj1c6JhEcmZCB6VkweH6wB\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"1042\" data-original-width=\"2033\" height=\"205\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEgUPLogcemSukKiuTCB0j37hxU-YrTgXFc-14TlaMAmZRV4QE02guoMmWYevt_8GB6UUkVH2E4hZj-G0bsJbDLwpo0SkXH4rJLbJMkm1K6ovOlujjnkJ9WHQJxe_vp-TBj6JX0uQ1Yav45quf_KXLviQvRQeiGqYV7IQGBApKUj1c6JhEcmZCB6VkweH6wB=w400-h205\" width=\"400\" /></a></div><div style=\"text-align: center;\">W&amp;B 딥러닝 모델 개발 프로세스 가시화 데쉬보드</div><div style=\"text-align: center;\"><br /></div></div><div style=\"text-align: left;\">이 글은 딥러닝 모델 학습 로그, 가시화만 집중해 살펴본다. 글 마무리에 W&amp;B의 개발 배경도 간단히 알아본다.&nbsp;&nbsp;</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b>사용법</b></div><div style=\"text-align: left;\">다음 링크 방문해 회원 가입한다.&nbsp;</div><div style=\"text-align: left;\"><div><ul style=\"text-align: left;\"><li><a href=\"https://wandb.ai/\">wandb.ai</a>&nbsp;website</li></ul></div></div><div style=\"text-align: left;\">회원 가입하고, 다음 그림과 같이 홈 메뉴에서 키 토큰 값을 얻어 복사한다. 이 키는 wandb API를 사용할 때 필요하다.</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEhisx_F16oiOYwWThwiSeVEsU05yDrZ5xDubg_d_68_aF4Y6YstMxGm5o61LriRMIufrYHy8atkCxc_xM4L0f0aOG7P3fK7df5S16qnmHjC8zKW3yc5uwiDaiIFjIs9n-yAAn1GCVRKI7BxQ2N6kcjhHESKg_J_EjSS49PL5nXfE-JxSTEszBdYo74eU-ul\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"1087\" data-original-width=\"2023\" height=\"215\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEhisx_F16oiOYwWThwiSeVEsU05yDrZ5xDubg_d_68_aF4Y6YstMxGm5o61LriRMIufrYHy8atkCxc_xM4L0f0aOG7P3fK7df5S16qnmHjC8zKW3yc5uwiDaiIFjIs9n-yAAn1GCVRKI7BxQ2N6kcjhHESKg_J_EjSS49PL5nXfE-JxSTEszBdYo74eU-ul=w400-h215\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEjRpfUPW0F6yCOaWncGNCfqGYDO7RwQJlXU7WkIKmeLcDkargQwMniBOlfSNTk6TFf0zjxXvfhYaf9PzCce0DytrcBdqbHnAOEXGHWzJJo_GgTU2yz5q__K42vmi8uul_3kB3bR9WQYjMxgtw7E4VhBVjqklUqlAOExtjSpNqRsviRcdrEQEkkyj8QhflBK\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"608\" data-original-width=\"861\" height=\"283\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjRpfUPW0F6yCOaWncGNCfqGYDO7RwQJlXU7WkIKmeLcDkargQwMniBOlfSNTk6TFf0zjxXvfhYaf9PzCce0DytrcBdqbHnAOEXGHWzJJo_GgTU2yz5q__K42vmi8uul_3kB3bR9WQYjMxgtw7E4VhBVjqklUqlAOExtjSpNqRsviRcdrEQEkkyj8QhflBK=w400-h283\" width=\"400\" /></a></div></div></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">터미널에서 다음을 실행해 설치한다.&nbsp;</div><div style=\"text-align: left;\">pip install wandb</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b>예시 및 결과</b></div><div style=\"text-align: left;\">다음과 같이 cosine 데이터를 학습하는 간단한 코드를 만들어 본다. W&amp;B로 로그를 기록하도록 몇몇 함수를 호출할 것이다. 앞에서 얻은 키 토큰은 다음 코드에 해당 부분에 입력해 준다.</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><div>import os</div><div>from tqdm import tqdm</div><div>import matplotlib.pyplot as plt</div><div>import numpy as np</div><div>import wandb</div><div>import torch</div><div>import torch.nn as nn</div><div>import torch.optim as optim</div><div>from torchviz import make_dot</div><div><br /></div><div>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')</div><div>print(f\"Using device: {device}\")</div><div><br /></div><div>class SimpleMLP(nn.Module):</div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>def __init__(self):</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t</span>super(SimpleMLP, self).__init__()</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t</span>self.fc1 = nn.Linear(1, 50)</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t</span>self.fc2 = nn.Linear(50, 1)</span></div><div><br /></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>def forward(self, x):</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t</span>x = torch.relu(self.fc1(x))</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t</span>x = self.fc2(x)</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t</span>return x</span></div><div><br /></div><div># Generate cosine dataset</div><div>def generate_cosine_data(num_samples=100):</div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>x = torch.linspace(-2 * torch.pi, 2 * torch.pi, num_samples).view(-1, 1)</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>y = torch.cos(x)</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>return x, y</span></div><div><br /></div><div># Instantiate the model, loss function, and optimizer</div><div>model = SimpleMLP()</div><div>model.to(device)</div><div>criterion = nn.MSELoss()</div><div>optimizer = optim.SGD(model.parameters(), lr=0.01)</div><div><br /></div><div>wandb.login(key='') # 여기에 키값 입력</div><div>api = wandb.Api()</div><div><span style=\"white-space: pre;\">\t</span></div><div>wandb.init(project=\"train_cosin\",&nbsp;</div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>config={</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t</span>\"optimizer\": \"SGD\",</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t</span>\"learning_rate\": 0.01,</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t</span>\"architecture\": \"SimpleMLP\",</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t</span>\"dataset\": \"cosine\"<span style=\"white-space: pre;\">\t\t</span></span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>})</span></div><div>wandb.watch(model, criterion, log=\"all\")&nbsp; # 모든 지표 기록</div><div><br /></div><div># 학습 데이터 생성</div><div>x, y = generate_cosine_data()</div><div>x, y = x.to(device), y.to(device)</div><div><br /></div><div># 학습</div><div>prediction = None</div><div>num_epochs = 10000</div><div>image_files = []</div><div>for epoch in tqdm(range(num_epochs)):</div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span># Forward pass</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>outputs = model(x)</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>prediction = outputs</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>loss = criterion(outputs, y)</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>print(f\"Epoch: {epoch}, Loss: {loss.item()}\")</span></div><div><br /></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span># Backward pass and optimization</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>optimizer.zero_grad()</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>loss.backward()</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>optimizer.step()</span></div><div><span style=\"white-space: pre;\">\t</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span># Visualize the gradients of the first epoch using wandb</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>layer_name = ''</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>grad_cpu = data_cpu = None</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>for name, param in model.named_parameters():</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t</span>if param.requires_grad and param.grad is not None:</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t\t</span>layer_name = name</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t\t</span>grad_cpu = param.grad.detach().cpu()</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t\t</span>data_cpu = param.data.detach().cpu()</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t\t</span>break</span></div><div><br /></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>log_data = {\"epoch\": epoch, \"loss\": loss.item(), \"outputs\": outputs.detach().cpu().numpy(),&nbsp;</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t\t\t</span>f'{layer_name}_gradients': wandb.Histogram(grad_cpu),&nbsp;</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t\t\t\t</span>f'{layer_name}_weights': wandb.Histogram(data_cpu)}</span></div><div><span style=\"white-space: normal;\"><span style=\"white-space: pre;\">\t</span>wandb.log(log_data) # 에폭, 로스, 출력, 기울기, 가중치 로그 기록</span></div></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">학습된 결과는 다음과 같다. 잘 학습된 것을 알 수 있다.</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEhmUgttokbUGqTdsJjcqPjNbgHjg_zDL-_EW-q4CEdWLkvFo1iiLQi0NIISQcEJnTZN1mSFq4qvo7HAWJjdwMDNZplPCpatkZUjneq9PSwdNNM1cYz2GDqZaBaKi0zuAIk4Fn2VV-BTVdOZpBJ68ZuenJXu3Ukr1XmFGKMaJS2V0qRwN3c78ICrrfT50t-H\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"524\" data-original-width=\"710\" height=\"236\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEhmUgttokbUGqTdsJjcqPjNbgHjg_zDL-_EW-q4CEdWLkvFo1iiLQi0NIISQcEJnTZN1mSFq4qvo7HAWJjdwMDNZplPCpatkZUjneq9PSwdNNM1cYz2GDqZaBaKi0zuAIk4Fn2VV-BTVdOZpBJ68ZuenJXu3Ukr1XmFGKMaJS2V0qRwN3c78ICrrfT50t-H\" width=\"320\" /></a></div><br /></div><div style=\"text-align: left;\"><a href=\"https://wandb.ai/\">W&amp;B 웹사이트</a>&nbsp;<a href=\"https://wandb.ai/mac999/train_cosin?nw=nwusermac999\">나의 프로젝트</a>의 데쉬보드를 확인한다.&nbsp;</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEgcYlozisJNORfC20z29s8kXJerR5zhr-W8kcmyVn3Rd8KwoHsRSMi5RjWZYJbMWRe7VJlxvVI7n9oKhmjBWiDknpxP5j3OjXbSsTBIET6s0PSjh50J7HA2LpovBlGjR5vFf2ccMKQNLu6xjppvs-yQBFBUFv95kxt8FCMoFVF8H8VM19lwVsarHU6iQTWf\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"1093\" data-original-width=\"982\" height=\"400\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEgcYlozisJNORfC20z29s8kXJerR5zhr-W8kcmyVn3Rd8KwoHsRSMi5RjWZYJbMWRe7VJlxvVI7n9oKhmjBWiDknpxP5j3OjXbSsTBIET6s0PSjh50J7HA2LpovBlGjR5vFf2ccMKQNLu6xjppvs-yQBFBUFv95kxt8FCMoFVF8H8VM19lwVsarHU6iQTWf=w360-h400\" width=\"360\" /></a></div></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">데쉬보드에서는 각 단계 별 기울기, 가중치, 로스 등이 어떻게 변화하는 지를 손쉽게 확인할 수 있다.&nbsp;</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">각 차트 데이터는 엑셀 등 포맷으로 저장 가능하다.&nbsp;</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">로짓 출력값을 확인해 보자. 초기 에폭에서는 학습되지 않은 임의 값을 출력하지만, 학습될 수록 y에 근사한 패턴으로 출력되는 것을 확인할 수 있다.</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEjGYbP23V2GXRIM6RJsx3jv0Fv6THAJH1GOFRMJot88Mf3DEtPsByxLUK70jBuJNkCoByPOD4_VkgL8EMaxUTkX4kuX94qR6Ik-tHV9_XUQfM43Y94XIAsYbPY0AAL7m_H1jGMwskruCdVOr6Y0D3p5pjIBFNGMAqfJBfLXCjPKbEq-jhUqEbDjI6Sae2GI\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"931\" data-original-width=\"1936\" height=\"193\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjGYbP23V2GXRIM6RJsx3jv0Fv6THAJH1GOFRMJot88Mf3DEtPsByxLUK70jBuJNkCoByPOD4_VkgL8EMaxUTkX4kuX94qR6Ik-tHV9_XUQfM43Y94XIAsYbPY0AAL7m_H1jGMwskruCdVOr6Y0D3p5pjIBFNGMAqfJBfLXCjPKbEq-jhUqEbDjI6Sae2GI=w400-h193\" width=\"400\" /></a></div>엑셀 출력된 데이터를 보면, cosine 패턴으로 수렴한 예측값이 점차 많아지는 것을 확인할 수 있다.&nbsp;</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEiesZo-FZEdyAIoH5JTAY3JHPDUHUn6LYVGjZbY_Oie_Fd-FI8oilbxmRY6PUx97kKQVzQ1p-d7el5QwV7wwhSvQe9kLtV7E9fpUzmD3g86_pRLDaOSS7K1GlMIIxcR4rdh0L0VoMf6RqdsAZyOwXkcqeYupGx2PiJpD_0dOG_p_-Gc84gBBfrr-jRtho44\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"403\" data-original-width=\"1580\" height=\"165\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEiesZo-FZEdyAIoH5JTAY3JHPDUHUn6LYVGjZbY_Oie_Fd-FI8oilbxmRY6PUx97kKQVzQ1p-d7el5QwV7wwhSvQe9kLtV7E9fpUzmD3g86_pRLDaOSS7K1GlMIIxcR4rdh0L0VoMf6RqdsAZyOwXkcqeYupGx2PiJpD_0dOG_p_-Gc84gBBfrr-jRtho44=w640-h165\" width=\"640\" /></a></div><br /></div><div style=\"text-align: left;\">다음 경우는 모델의 바이어스 히스토그램 차트 데이터를 보여준다.&nbsp;</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEjPRCnikw_0DGKxAEoa2qmKBGNc1jMbQpewRZBmkfu8ed7XnyvtOVRjU-J6VKhNrgAAGYRUiyJfkDAoH_TFTQvkNTmdDakNm_x7m5wA4ie3Cez95J2iiBKmKWbwU-H54SmmmXQKEXeAw_j38OTaN8WupO_Oc6t2uLCbjANMxpEL0x1o_cySBok6b9_rtVmU\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"866\" data-original-width=\"1926\" height=\"180\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjPRCnikw_0DGKxAEoa2qmKBGNc1jMbQpewRZBmkfu8ed7XnyvtOVRjU-J6VKhNrgAAGYRUiyJfkDAoH_TFTQvkNTmdDakNm_x7m5wA4ie3Cez95J2iiBKmKWbwU-H54SmmmXQKEXeAw_j38OTaN8WupO_Oc6t2uLCbjANMxpEL0x1o_cySBok6b9_rtVmU=w400-h180\" width=\"400\" /></a></div></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">엑셀로 다운로드하면, 모델 학습 데이터의 특정 범위에 속한 값의 누적 데이터를 쉽게 확인할 수 있다.</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEjgRmKXub3EWyf0aVUShKw5YjnTmSfF4AGHbGDj_ZIkhBU39GowrMd_gk5bYRbU6iE8TBqe087xOptmJ8Dx0tyHeDgN_or6lhJIrptj5Jw_fZEnCoGKpsBZeaSlsNoJBzHKXGrt3oclMmFmqLprniDE9Hf2QYm2pPRX6fMSMxHiBbguErFTb7I675W1UJLG\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"412\" data-original-width=\"2012\" height=\"133\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjgRmKXub3EWyf0aVUShKw5YjnTmSfF4AGHbGDj_ZIkhBU39GowrMd_gk5bYRbU6iE8TBqe087xOptmJ8Dx0tyHeDgN_or6lhJIrptj5Jw_fZEnCoGKpsBZeaSlsNoJBzHKXGrt3oclMmFmqLprniDE9Hf2QYm2pPRX6fMSMxHiBbguErFTb7I675W1UJLG=w640-h133\" width=\"640\" /></a></div><br /></div><div style=\"text-align: left;\">다음과 같이 리포트 기능을 사용해, 모델 학습 품질을 검토할 수 있다.</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEjTOwnKp6beo7YTjT9jdDbMNilOyIk0fFKwYsXCoSSUPF8KmwzGNQAKK9zR7llhnc9t8jwHmOa-rTlVKV4Lckv1bUel9ioe89qM4OTQqZKGbg22Ihy6Gygxf6iZnjOPzvwvYJL2zNmssdd4wFuolGZvUqDtR1Sov4XYYEZegCXoI_Calrl_rPNNZlmgYTq4\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"1047\" data-original-width=\"1989\" height=\"210\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjTOwnKp6beo7YTjT9jdDbMNilOyIk0fFKwYsXCoSSUPF8KmwzGNQAKK9zR7llhnc9t8jwHmOa-rTlVKV4Lckv1bUel9ioe89qM4OTQqZKGbg22Ihy6Gygxf6iZnjOPzvwvYJL2zNmssdd4wFuolGZvUqDtR1Sov4XYYEZegCXoI_Calrl_rPNNZlmgYTq4=w400-h210\" width=\"400\" /></a></div><br /></div><div style=\"text-align: left;\"><b>마무리</b></div><div style=\"text-align: left;\">W&amp;B는 루카스 비왈드(Lukas Biewald)가 2017년 설립한 딥러닝 모델 학습 서비스를 제공하는 스타트업이다. 이 회사는 딥러닝 개발자에 필요한 실무적인 도구를 개발한다. 그는 딥러닝 연구 초창기부터 머신러닝 연구자 및 개발자로 있으면서, 실무자의 어려움을 알고 있었다.&nbsp;</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEhfNIluClhuvx4uCwa1PDhLx9ghmMz-NjV8PPaKFT89Najd_lP5N2m-NWLbOToxej3ku-qGu293r1yux9kkhW91g38yIvzaLIbJZssa3IZzC3Sy01_fzoouhFubujYUza_kon9Jg064O8IeFvLu39n6egiptc8tbhGGjZxmrQ5YAoPfu7z-2KZvHZE1XSsC\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"174\" data-original-width=\"431\" height=\"113\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEhfNIluClhuvx4uCwa1PDhLx9ghmMz-NjV8PPaKFT89Najd_lP5N2m-NWLbOToxej3ku-qGu293r1yux9kkhW91g38yIvzaLIbJZssa3IZzC3Sy01_fzoouhFubujYUza_kon9Jg064O8IeFvLu39n6egiptc8tbhGGjZxmrQ5YAoPfu7z-2KZvHZE1XSsC=w280-h113\" width=\"280\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><span style=\"text-align: left;\">Lukas Biewald (2018)</span></div><br /></div><div style=\"text-align: left;\">OpenAI는 W&amp;B의 첫번째 고객이다. 이 도구는 머신러닝 개발자의 많은 수작업 프로세스를 자동화하여, 모델 학습 과정을 모니터링, 추적하고, 직관적인 학습 모델 디버깅, 검사 및 설명이 가능하도록 하여, 체계적으로 이 과정을 관리한다. 이 회사는 현재 700,000명 이상의 유료 사용자를 보유하고 있으며, 대부분, OpenAI와 같이 빅테크 기업들 개발자이 사용자이다.&nbsp;</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEgsFnWMhIr_lTqrAA0haBaRCsTOS6xxmQfEGYYj6jC4QkKJwaJE5mAGr2Ty4ZWaaoqFbRSV8g985BoAb2aXUCl8rTU5TUiLrDUSF4uR0_jxPQYsn2ZiHZHfGoEE9f-I9H_RMgPB2-Hqi3qJ6FnIP7obqnPPDAxVtdLu1-gBuN5-Z21zlvYfdw2td98sZmFo\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"1039\" data-original-width=\"1200\" height=\"240\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEgsFnWMhIr_lTqrAA0haBaRCsTOS6xxmQfEGYYj6jC4QkKJwaJE5mAGr2Ty4ZWaaoqFbRSV8g985BoAb2aXUCl8rTU5TUiLrDUSF4uR0_jxPQYsn2ZiHZHfGoEE9f-I9H_RMgPB2-Hqi3qJ6FnIP7obqnPPDAxVtdLu1-gBuN5-Z21zlvYfdw2td98sZmFo\" width=\"277\" /></a></div></div><div class=\"separator\" style=\"clear: both; text-align: center;\">W&amp;B 창업자 루카드 비왈드 및 핵심 파트너(<a href=\"https://www.bizjournals.com/sanfrancisco/inno/stories/fundings/2023/08/09/san-francisco-ai-observability-startup-raises-50m.html\" style=\"text-align: left;\">San Francisco responsible AI startup Weights &amp; Biases raises $50M - San Francisco Business Times</a>.&nbsp;<a href=\"https://techcrunch.com/2018/05/31/weights-biases-raises-5m-to-build-development-tools-for-machine-learning/\" style=\"text-align: left;\">Weights &amp; Biases raises $5M to build development tools for machine learning</a>)</div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div></div><div style=\"text-align: left;\">이 회사는 현재까지 $250M 펀딩 투자받았다. 이 금액은 원화로 3,500억(환율 1400원 기준)에 해당한다. 현재 W&amp;B는 핵심기술을 개발하며, 기업공개를 준비하고 있다.</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEioHJn1YfJFNPs7FXraDQ_vgsVoKe3pqQECyhSLQI1SV_Apmbi7dB5Ph6fMhdjw08IxE2gYtmr9q33OGkCPC5lRxfnDTQiSCTS-Iy0FxUjBCHWWcPZYBSwOwwFMV36cBN5JblG2sehJwJ8gQLIoVql5Li_de-Yy4umb_kCCtNJ6q3jrNHVJASZRZZWniBUQ\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"890\" data-original-width=\"992\" height=\"358\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEioHJn1YfJFNPs7FXraDQ_vgsVoKe3pqQECyhSLQI1SV_Apmbi7dB5Ph6fMhdjw08IxE2gYtmr9q33OGkCPC5lRxfnDTQiSCTS-Iy0FxUjBCHWWcPZYBSwOwwFMV36cBN5JblG2sehJwJ8gQLIoVql5Li_de-Yy4umb_kCCtNJ6q3jrNHVJASZRZZWniBUQ=w400-h358\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://www.crunchbase.com/organization/weights-biases/company_financials\">Weights &amp; Biases - Funding, Financials, Valuation</a></div><br /></div><div style=\"text-align: left;\"><b>부록: 딥러닝 모델의 해 탐색 과정 탐색 과정 가시화 원리</b></div><div style=\"text-align: left;\">여기서 딥러닝 모델의 해 탐색 과정을 가시화하는 원리를 간략히 살펴보자. 딥러닝 모델은 빅 데이터를 통계적 학습하여, y에 가장 가까운&nbsp;ŷ = w·x + b 를 탐색하는 과정이다. 그러므로, 가장 loss = y - ŷ가 작은 weight w, bais b를 찾는 것이 목적이다. 그러므로, 제대로 해를 탐색하는 지 확인하려면, epoch당 loss와 w:b 차트를 확인하는 것이 중요하다.&nbsp;</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">모델을 구성하는 레이어 유닛이 여러개라면, w:b도 여기에 비례해 많으므로, 원리만 살펴보기 위해, 매우 간단한 ŷ = w·x + b 수식을 학습하는 단순한 딥러닝 모델을 학습한다고 가정한다.</div><div style=\"text-align: left;\"><span style=\"font-size: x-small;\"><br /></span></div><div style=\"text-align: left;\"><div><span style=\"font-size: x-small;\"># 데이터 생성</span></div><div><span style=\"font-size: x-small;\">import numpy as np</span></div><div><span style=\"font-size: x-small;\">import matplotlib.pyplot as plt</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">np.random.seed(20240215)</span></div><div><span style=\"font-size: x-small;\">n = 50</span></div><div><span style=\"font-size: x-small;\">x = np.array(np.random.randn(n), dtype=np.float32)</span></div><div><span style=\"font-size: x-small;\">y = np.array(</span></div><div><span style=\"font-size: x-small;\">&nbsp; 0.75 * x**2 + 1.0 * x + 2.0 + 0.3 * np.random.randn(n),</span></div><div><span style=\"font-size: x-small;\">&nbsp; dtype=np.float32) # 데이터 임의로 생성할 수식</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">plt.scatter(x, y, facecolors='none', edgecolors='b')</span></div><div><span style=\"font-size: x-small;\">plt.scatter(x, y, c='r')</span></div><div><span style=\"font-size: x-small;\">plt.show()</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\"># 데이터 학습 모델 준비</span></div><div><span style=\"font-size: x-small;\">import torch</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">model = torch.nn.Linear(1, 1)</span></div><div><span style=\"font-size: x-small;\">model.weight.data.fill_(6.0)</span></div><div><span style=\"font-size: x-small;\">model.bias.data.fill_(-3.0)</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\"># 손실 함수 준비</span></div><div><span style=\"font-size: x-small;\">loss_fn = torch.nn.MSELoss()</span></div><div><span style=\"font-size: x-small;\">learning_rate = 0.1</span></div><div><span style=\"font-size: x-small;\">epochs = 100</span></div><div><span style=\"font-size: x-small;\">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\"># 학습</span></div><div><span style=\"font-size: x-small;\">models = [[model.weight.item(), model.bias.item()]]</span></div><div><span style=\"font-size: x-small;\">for epoch in range(epochs):</span></div><div><span style=\"font-size: x-small;\">&nbsp; inputs = torch.from_numpy(x).requires_grad_().reshape(-1, 1)</span></div><div><span style=\"font-size: x-small;\">&nbsp; labels = torch.from_numpy(y).reshape(-1, 1)</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">&nbsp; optimizer.zero_grad()</span></div><div><span style=\"font-size: x-small;\">&nbsp; outputs = model(inputs)</span></div><div><span style=\"font-size: x-small;\">&nbsp; loss = loss_fn(outputs, labels)</span></div><div><span style=\"font-size: x-small;\">&nbsp; loss.backward()</span></div><div><span style=\"font-size: x-small;\">&nbsp; optimizer.step()</span></div><div><span style=\"font-size: x-small;\">&nbsp; print('epoch {}, loss {}'.format(epoch, loss.item()))</span></div><div><span style=\"font-size: x-small;\">&nbsp; models.append([model.weight.item(), model.bias.item()])</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\"># 모델 예측 값 비교 출력</span></div><div><span style=\"font-size: x-small;\">weight = model.weight.item()</span></div><div><span style=\"font-size: x-small;\">bias = model.bias.item()</span></div><div><span style=\"font-size: x-small;\">plt.scatter(x, y, facecolors='none', edgecolors='b')</span></div><div><span style=\"font-size: x-small;\">plt.plot(</span></div><div><span style=\"font-size: x-small;\">&nbsp; [x.min(), x.max()],</span></div><div><span style=\"font-size: x-small;\">&nbsp; [weight * x.min() + bias, weight * x.max() + bias],</span></div><div><span style=\"font-size: x-small;\">&nbsp; c='r')</span></div><div><span style=\"font-size: x-small;\">plt.show()</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\"># loss map 작성</span></div><div><span style=\"font-size: x-small;\">def get_loss_map(loss_fn, x, y):</span></div><div><span style=\"font-size: x-small;\">&nbsp; \"\"\"Maps the loss function on a 100-by-100 grid between (-5, -5) and (8, 8).\"\"\"</span></div><div><span style=\"font-size: x-small;\">&nbsp; losses = [[0.0] * 101 for _ in range(101)]</span></div><div><span style=\"font-size: x-small;\">&nbsp; x = torch.from_numpy(x)</span></div><div><span style=\"font-size: x-small;\">&nbsp; y = torch.from_numpy(y)</span></div><div><span style=\"font-size: x-small;\">&nbsp; for wi in range(101):</span></div><div><span style=\"font-size: x-small;\">&nbsp; &nbsp; for wb in range(101):</span></div><div><span style=\"font-size: x-small;\">&nbsp; &nbsp; &nbsp; w = -5.0 + 13.0 * wi / 100.0</span></div><div><span style=\"font-size: x-small;\">&nbsp; &nbsp; &nbsp; b = -5.0 + 13.0 * wb / 100.0</span></div><div><span style=\"font-size: x-small;\">&nbsp; &nbsp; &nbsp; ywb = x * w + b</span></div><div><span style=\"font-size: x-small;\">&nbsp; &nbsp; &nbsp; losses[wi][wb] = loss_fn(ywb, y).item()</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">&nbsp; return list(reversed(losses))&nbsp; # Because y will be reversed.</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\"># w:b 2차원 loss map 상 해 탐색 path 가시화&nbsp;</span></div><div><span style=\"font-size: x-small;\">import pylab</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">loss_fn = torch.nn.MSELoss()</span></div><div><span style=\"font-size: x-small;\">losses = get_loss_map(loss_fn, x, y)</span></div><div><span style=\"font-size: x-small;\">cm = pylab.get_cmap('terrain')</span></div><div><span style=\"font-size: x-small;\">fig, ax = plt.subplots()</span></div><div><span style=\"font-size: x-small;\">plt.xlabel('Bias')</span></div><div><span style=\"font-size: x-small;\">plt.ylabel('Weight')</span></div><div><span style=\"font-size: x-small;\">i = ax.imshow(losses, cmap=cm, interpolation='nearest', extent=[-5, 8, -5, 8])</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">model_weights, model_biases = zip(*models)</span></div><div><span style=\"font-size: x-small;\">ax.scatter(model_biases, model_weights, c='r', marker='+')</span></div><div><span style=\"font-size: x-small;\">ax.plot(model_biases, model_weights, c='r')</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">fig.colorbar(i)</span></div><div><span style=\"font-size: x-small;\">plt.show()</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div>결과는 다음과 같다.&nbsp;</div><div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEjWSWh4nIMWmP2l7Ax4ORoC4mG_yUleJYMyBppC3RuCeUCr7y-CJnYRrhl53p8bXlyTA7CDVUzIGQmgGgTI0FSOdsZtujHsYSCrETRfev1_oT6pjW2oNbknjuH9Qte6hww8eAkBxqGY94Ot8Eq0vplqigY2ip73o75WwdtVsFaSawAfOCx39vNMMSRmpzgL\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"693\" data-original-width=\"828\" height=\"240\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjWSWh4nIMWmP2l7Ax4ORoC4mG_yUleJYMyBppC3RuCeUCr7y-CJnYRrhl53p8bXlyTA7CDVUzIGQmgGgTI0FSOdsZtujHsYSCrETRfev1_oT6pjW2oNbknjuH9Qte6hww8eAkBxqGY94Ot8Eq0vplqigY2ip73o75WwdtVsFaSawAfOCx39vNMMSRmpzgL\" width=\"287\" /></a></div></div></div><div style=\"text-align: left;\">loss함수를 바꿔가며, 해가 어떻게 탐색되는 지 확인해 보면, 다음과 같다. 데이터 특성에 따라 적절한 전략을 써야 한다는 것을 알 수 있다.&nbsp;</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEgxXZ3UN76RCcrA2QSTNHvcWCAOLTS1kIWTTX4tvN8XcgBZdYiZVI4uomOvRkFptC-Aqocr_99ZVAf8CaVzoe6HAI6LCQ4JcACZxy8Io3LmDm3SbUzg-kmk3VnOwIJuGUuFjln0CgM7dr0OylFKOUdYtaA24iEl5KA4KpW7YFuy_vVA0iFduliVS_R2c_mc\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"721\" data-original-width=\"828\" height=\"348\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEgxXZ3UN76RCcrA2QSTNHvcWCAOLTS1kIWTTX4tvN8XcgBZdYiZVI4uomOvRkFptC-Aqocr_99ZVAf8CaVzoe6HAI6LCQ4JcACZxy8Io3LmDm3SbUzg-kmk3VnOwIJuGUuFjln0CgM7dr0OylFKOUdYtaA24iEl5KA4KpW7YFuy_vVA0iFduliVS_R2c_mc=w400-h348\" width=\"400\" /></a></div>이 경우에는 w, b 인 2차원이라 시각화가 쉬웠으나, 3차원 이상이면, 다른 가시화 방법을 사용해야 한다.&nbsp;</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b>레퍼런스</b></div><div style=\"text-align: left;\"><ul style=\"text-align: left;\"><li><a href=\"https://theaisummer.com/weights-and-biases-tutorial/\">A complete Weights and Biases tutorial | AI Summer (theaisummer.com)</a></li><li><a href=\"https://towardsdatascience.com/visualizing-gradient-descent-parameters-in-torch-332a63d1e5c5\">Visualizing Gradient Descent Parameters in Torch | by P.G. Baumstarck | Towards Data Science</a></li><li><a href=\"https://www.geeksforgeeks.org/what-is-automl-in-machine-learning/\">What is AutoML in Machine Learning? - GeeksforGeeks</a></li><li><a href=\"https://cruizbran.medium.com/auto-ml-49f065978c78\">Auto ML. The options | by Ignacio Ruiz | Medium</a></li></ul></div>",
        "contentSnippet": "딥러닝 모델들을 개발하다 보면, 수많은 종류의 데이터셋, 하이퍼모델 파라메터 튜닝 등으로 인해 관리해야 할 자료들이 매우 복잡해진다는 것을 알게 된다. Weights & Biases (W&B) 회사는 이름 그대로 완벽한 모델 학습을 위해 필요한 Weights & Biases를 모니터링, 관리할 수 있는 로그 도구이다. 즉, 딥러닝 모델 개발자를 위한 프로세스 로그 및 가시화 플랫폼을 제공한다. \n\n\nW&B(AI Summer)\n\n매우 직관적인 이름을 가진 이 스타트업은 Tensorboard와 유사하지만, 적은 코드로 모델 개발에 많은 통찰력을 준다. 이 W&B(WandB) 라이브러리를 사용하면, 딥러닝 모델 학습 시 지저분하게 붙어 나가는 로그 처리를 매우 간단한 함수 몇 개로 처리할 수 있다. 통합된 데시보드 형태로 다양한 모델 학습 품질 지표를 확인 및 비교할 수 있다. 이외, 학습 모델 하이퍼 파라메터 관리와 튜닝 및 비교 보고서 생성 기능을 제공한다. 로그는 숫자, 텍스트, 이미지 등 다양한 포맷을 지원한다. \n\n\nW&B 딥러닝 모델 개발 프로세스 가시화 데쉬보드\n\n\n이 글은 딥러닝 모델 학습 로그, 가시화만 집중해 살펴본다. 글 마무리에 W&B의 개발 배경도 간단히 알아본다.  \n\n\n사용법\n다음 링크 방문해 회원 가입한다. \n\n\nwandb.ai website\n\n\n회원 가입하고, 다음 그림과 같이 홈 메뉴에서 키 토큰 값을 얻어 복사한다. 이 키는 wandb API를 사용할 때 필요하다.\n\n\n\n\n\n\n\n터미널에서 다음을 실행해 설치한다. \npip install wandb\n\n\n예시 및 결과\n다음과 같이 cosine 데이터를 학습하는 간단한 코드를 만들어 본다. W&B로 로그를 기록하도록 몇몇 함수를 호출할 것이다. 앞에서 얻은 키 토큰은 다음 코드에 해당 부분에 입력해 준다.\n\n\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport wandb\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchviz import make_dot\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n\nclass SimpleMLP(nn.Module):\n\tdef __init__(self):\n\t\tsuper(SimpleMLP, self).__init__()\n\t\tself.fc1 = nn.Linear(1, 50)\n\t\tself.fc2 = nn.Linear(50, 1)\n\n\n\tdef forward(self, x):\n\t\tx = torch.relu(self.fc1(x))\n\t\tx = self.fc2(x)\n\t\treturn x\n\n\n# Generate cosine dataset\ndef generate_cosine_data(num_samples=100):\n\tx = torch.linspace(-2 * torch.pi, 2 * torch.pi, num_samples).view(-1, 1)\n\ty = torch.cos(x)\n\treturn x, y\n\n\n# Instantiate the model, loss function, and optimizer\nmodel = SimpleMLP()\nmodel.to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n\nwandb.login(key='') # 여기에 키값 입력\napi = wandb.Api()\n\t\nwandb.init(project=\"train_cosin\", \n\tconfig={\n\t\t\"optimizer\": \"SGD\",\n\t\t\"learning_rate\": 0.01,\n\t\t\"architecture\": \"SimpleMLP\",\n\t\t\"dataset\": \"cosine\"\t\t\n\t})\nwandb.watch(model, criterion, log=\"all\")  # 모든 지표 기록\n\n\n# 학습 데이터 생성\nx, y = generate_cosine_data()\nx, y = x.to(device), y.to(device)\n\n\n# 학습\nprediction = None\nnum_epochs = 10000\nimage_files = []\nfor epoch in tqdm(range(num_epochs)):\n\t# Forward pass\n\toutputs = model(x)\n\tprediction = outputs\n\tloss = criterion(outputs, y)\n\tprint(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n\n\n\t# Backward pass and optimization\n\toptimizer.zero_grad()\n\tloss.backward()\n\toptimizer.step()\n\t\n\t# Visualize the gradients of the first epoch using wandb\n\tlayer_name = ''\n\tgrad_cpu = data_cpu = None\n\tfor name, param in model.named_parameters():\n\t\tif param.requires_grad and param.grad is not None:\n\t\t\tlayer_name = name\n\t\t\tgrad_cpu = param.grad.detach().cpu()\n\t\t\tdata_cpu = param.data.detach().cpu()\n\t\t\tbreak\n\n\n\tlog_data = {\"epoch\": epoch, \"loss\": loss.item(), \"outputs\": outputs.detach().cpu().numpy(), \n\t\t\t\tf'{layer_name}_gradients': wandb.Histogram(grad_cpu), \n\t\t\t\tf'{layer_name}_weights': wandb.Histogram(data_cpu)}\n\twandb.log(log_data) # 에폭, 로스, 출력, 기울기, 가중치 로그 기록\n\n\n학습된 결과는 다음과 같다. 잘 학습된 것을 알 수 있다.\n\n\n\nW&B 웹사이트 나의 프로젝트의 데쉬보드를 확인한다. \n\n\n\n\n데쉬보드에서는 각 단계 별 기울기, 가중치, 로스 등이 어떻게 변화하는 지를 손쉽게 확인할 수 있다. \n\n\n각 차트 데이터는 엑셀 등 포맷으로 저장 가능하다. \n\n\n로짓 출력값을 확인해 보자. 초기 에폭에서는 학습되지 않은 임의 값을 출력하지만, 학습될 수록 y에 근사한 패턴으로 출력되는 것을 확인할 수 있다.\n\n\n엑셀 출력된 데이터를 보면, cosine 패턴으로 수렴한 예측값이 점차 많아지는 것을 확인할 수 있다. \n\n\n\n다음 경우는 모델의 바이어스 히스토그램 차트 데이터를 보여준다. \n\n\n\n\n엑셀로 다운로드하면, 모델 학습 데이터의 특정 범위에 속한 값의 누적 데이터를 쉽게 확인할 수 있다.\n\n\n\n다음과 같이 리포트 기능을 사용해, 모델 학습 품질을 검토할 수 있다.\n\n\n\n마무리\nW&B는 루카스 비왈드(Lukas Biewald)가 2017년 설립한 딥러닝 모델 학습 서비스를 제공하는 스타트업이다. 이 회사는 딥러닝 개발자에 필요한 실무적인 도구를 개발한다. 그는 딥러닝 연구 초창기부터 머신러닝 연구자 및 개발자로 있으면서, 실무자의 어려움을 알고 있었다. \n\n\nLukas Biewald (2018)\n\nOpenAI는 W&B의 첫번째 고객이다. 이 도구는 머신러닝 개발자의 많은 수작업 프로세스를 자동화하여, 모델 학습 과정을 모니터링, 추적하고, 직관적인 학습 모델 디버깅, 검사 및 설명이 가능하도록 하여, 체계적으로 이 과정을 관리한다. 이 회사는 현재 700,000명 이상의 유료 사용자를 보유하고 있으며, 대부분, OpenAI와 같이 빅테크 기업들 개발자이 사용자이다. \n\n\n\nW&B 창업자 루카드 비왈드 및 핵심 파트너(San Francisco responsible AI startup Weights & Biases raises $50M - San Francisco Business Times. Weights & Biases raises $5M to build development tools for machine learning)\n\n\n이 회사는 현재까지 $250M 펀딩 투자받았다. 이 금액은 원화로 3,500억(환율 1400원 기준)에 해당한다. 현재 W&B는 핵심기술을 개발하며, 기업공개를 준비하고 있다.\n\n\nWeights & Biases - Funding, Financials, Valuation\n\n부록: 딥러닝 모델의 해 탐색 과정 탐색 과정 가시화 원리\n여기서 딥러닝 모델의 해 탐색 과정을 가시화하는 원리를 간략히 살펴보자. 딥러닝 모델은 빅 데이터를 통계적 학습하여, y에 가장 가까운 ŷ = w·x + b 를 탐색하는 과정이다. 그러므로, 가장 loss = y - ŷ가 작은 weight w, bais b를 찾는 것이 목적이다. 그러므로, 제대로 해를 탐색하는 지 확인하려면, epoch당 loss와 w:b 차트를 확인하는 것이 중요하다. \n\n\n모델을 구성하는 레이어 유닛이 여러개라면, w:b도 여기에 비례해 많으므로, 원리만 살펴보기 위해, 매우 간단한 ŷ = w·x + b 수식을 학습하는 단순한 딥러닝 모델을 학습한다고 가정한다.\n\n\n\n# 데이터 생성\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nnp.random.seed(20240215)\nn = 50\nx = np.array(np.random.randn(n), dtype=np.float32)\ny = np.array(\n  0.75 * x**2 + 1.0 * x + 2.0 + 0.3 * np.random.randn(n),\n  dtype=np.float32) # 데이터 임의로 생성할 수식\n\n\nplt.scatter(x, y, facecolors='none', edgecolors='b')\nplt.scatter(x, y, c='r')\nplt.show()\n\n\n# 데이터 학습 모델 준비\nimport torch\n\n\nmodel = torch.nn.Linear(1, 1)\nmodel.weight.data.fill_(6.0)\nmodel.bias.data.fill_(-3.0)\n\n\n# 손실 함수 준비\nloss_fn = torch.nn.MSELoss()\nlearning_rate = 0.1\nepochs = 100\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n\n# 학습\nmodels = [[model.weight.item(), model.bias.item()]]\nfor epoch in range(epochs):\n  inputs = torch.from_numpy(x).requires_grad_().reshape(-1, 1)\n  labels = torch.from_numpy(y).reshape(-1, 1)\n\n\n  optimizer.zero_grad()\n  outputs = model(inputs)\n  loss = loss_fn(outputs, labels)\n  loss.backward()\n  optimizer.step()\n  print('epoch {}, loss {}'.format(epoch, loss.item()))\n  models.append([model.weight.item(), model.bias.item()])\n\n\n# 모델 예측 값 비교 출력\nweight = model.weight.item()\nbias = model.bias.item()\nplt.scatter(x, y, facecolors='none', edgecolors='b')\nplt.plot(\n  [x.min(), x.max()],\n  [weight * x.min() + bias, weight * x.max() + bias],\n  c='r')\nplt.show()\n\n\n# loss map 작성\ndef get_loss_map(loss_fn, x, y):\n  \"\"\"Maps the loss function on a 100-by-100 grid between (-5, -5) and (8, 8).\"\"\"\n  losses = [[0.0] * 101 for _ in range(101)]\n  x = torch.from_numpy(x)\n  y = torch.from_numpy(y)\n  for wi in range(101):\n    for wb in range(101):\n      w = -5.0 + 13.0 * wi / 100.0\n      b = -5.0 + 13.0 * wb / 100.0\n      ywb = x * w + b\n      losses[wi][wb] = loss_fn(ywb, y).item()\n\n\n  return list(reversed(losses))  # Because y will be reversed.\n\n\n# w:b 2차원 loss map 상 해 탐색 path 가시화 \nimport pylab\n\n\nloss_fn = torch.nn.MSELoss()\nlosses = get_loss_map(loss_fn, x, y)\ncm = pylab.get_cmap('terrain')\nfig, ax = plt.subplots()\nplt.xlabel('Bias')\nplt.ylabel('Weight')\ni = ax.imshow(losses, cmap=cm, interpolation='nearest', extent=[-5, 8, -5, 8])\n\n\nmodel_weights, model_biases = zip(*models)\nax.scatter(model_biases, model_weights, c='r', marker='+')\nax.plot(model_biases, model_weights, c='r')\n\n\nfig.colorbar(i)\nplt.show()\n\n\n결과는 다음과 같다. \n\n\n\nloss함수를 바꿔가며, 해가 어떻게 탐색되는 지 확인해 보면, 다음과 같다. 데이터 특성에 따라 적절한 전략을 써야 한다는 것을 알 수 있다. \n\n\n이 경우에는 w, b 인 2차원이라 시각화가 쉬웠으나, 3차원 이상이면, 다른 가시화 방법을 사용해야 한다. \n\n\n레퍼런스\n\nA complete Weights and Biases tutorial | AI Summer (theaisummer.com)\nVisualizing Gradient Descent Parameters in Torch | by P.G. Baumstarck | Towards Data Science\nWhat is AutoML in Machine Learning? - GeeksforGeeks\nAuto ML. The options | by Ignacio Ruiz | Medium",
        "id": "tag:blogger.com,1999:blog-5201956450461596914.post-5395180824671467701",
        "isoDate": "2024-06-21T07:49:00.000Z"
      },
      {
        "title": "도메인 모델 성능개선을 위한 Lora 기반 LLAMA3 모델 파인튜닝하기",
        "link": "http://daddynkidsmakers.blogspot.com/2024/06/lora-llama3.html",
        "pubDate": "2024-06-21T01:47:00.000Z",
        "author": "Daddy Maker",
        "content": "<div style=\"text-align: left;\">이 글은&nbsp;Lora 기반 LLAMA3 모델 파인튜닝하는 방법을 간략히 보여준다. 이를 통해, 특정 도메인의 LLM 모델 생성 정확도를 향상시킬 수 있다.</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEivO-JPv31asTFlMJ9hxAv_0n63icDloIzHGL81i5REKv4gsbyn76oMOTRCWBgQQvOO_gGs8SkOe1Nr1Mg-yYyPTa228TExPwnArl19qmm5Xhoyp2qIQNV5_2EEs9f7DbU2sNxcHkvMAis1hEy3vARody2nihhXZHv8J_ncm-SmjMFRHlJWnppTxqcDewnt\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"600\" data-original-width=\"1200\" height=\"160\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEivO-JPv31asTFlMJ9hxAv_0n63icDloIzHGL81i5REKv4gsbyn76oMOTRCWBgQQvOO_gGs8SkOe1Nr1Mg-yYyPTa228TExPwnArl19qmm5Xhoyp2qIQNV5_2EEs9f7DbU2sNxcHkvMAis1hEy3vARody2nihhXZHv8J_ncm-SmjMFRHlJWnppTxqcDewnt\" width=\"320\" /></a></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b><span style=\"font-size: medium;\">머리말</span></b></div><div style=\"text-align: left;\"><div>LLAMA3는 메타가 개발한 LLM 제품이다. 모델은 15조 개의 토큰으로 구성된 광범위한 데이터 세트에서 훈련되었다(라마 2의 경우 2T 토큰과 비교). 700억 개의 파라미터 모델과 더 작은 80억 개의 파라미터 모델의 두 가지 모델 크기가 출시되었다. 70B 모델은 MMLU 벤치마크에서 82점, HumanEval 벤치마크에서 81.7점을 기록하며 이미 인상적인 성능을 보여주었다.</div><div><br /></div><div>라마 3 모델은 컨텍스트 길이를 최대 8,192개 토큰(라마 2의 경우 4,096개 토큰)까지 늘렸으며, RoPE를 통해 최대 32k까지 확장할 수 있다. 또한 이 모델은 128K 토큰 어휘가 있는 새로운 토크나이저를 사용하여 텍스트를 인코딩하는 데 필요한 토큰 수를 15% 줄이다.&nbsp;</div><div><br /></div><div><b><span style=\"font-size: medium;\">파인튜닝 개념</span></b></div><div>파인튜닝(fine turning. 미세 조정)전략은 활용된 데이터에 따라 달라지며, 이는 네 가지 유형으로 분류할 수 있다.</div><div><ul style=\"text-align: left;\"><li>감독 미세 조정</li><li>퓨샷 학습(Few-shot Learning)</li><li>전체 전이 학습</li><li>도메인별 미세 조정</li><li>감독 미세 조정</li></ul></div><div>이 방법은 미세 조정에 대한 표준 접근 방식을 나타낸다. 모델은 텍스트 분류, 질문 답변 또는 명명된 엔터티 인식과 같이 수행하려는 특정 작업에 맞게 조정된 레이블이 지정된 데이터 세트를 사용하여 추가 학습을 거친다. 예를 들어 감정 분석에서 모델은 해당 감정으로 주석이 달린 텍스트 샘플로 구성된 데이터 세트에서 학습된다.</div><div><br /></div><div><b>퓨샷 학습(Few-Shot Learning)</b></div><div>레이블이 지정된 대규모 데이터 세트를 어셈블하는 것이 비현실적인 것으로 판명된 시나리오에서는 솔루션을 제공하기 위해 몇 가지 학습 단계가 개입한다. 이 기술은 입력 프롬프트를 시작할 때 원하는 작업에 대한 몇 가지 <a href=\"https://medium.com/thedeephub/a-practical-guide-for-rag-and-few-shot-prompting-in-langchain-0b0e18dc9df5\">예제</a>(또는 샷)를 모델에 제공한다. 이렇게 함으로써 모델은 철저한 미세 조정 요법 없이 작업에 대한 더 나은 컨텍스트 이해를 얻을 수 있다.</div><div><br /></div></div></div><blockquote style=\"border: none; margin: 0px 0px 0px 40px; padding: 0px; text-align: left;\"><div style=\"text-align: left;\"><div style=\"text-align: left;\"><div><div><span style=\"font-size: x-small;\">few_shot_examples = [</span></div></div></div></div><div style=\"text-align: left;\"><div style=\"text-align: left;\"><div><div><span style=\"font-size: x-small;\">{\"input\":\"Could you please clarify the terms outlined in section 3.2 of the contract?\",</span></div></div></div></div><div style=\"text-align: left;\"><div style=\"text-align: left;\"><div><div><span style=\"font-size: x-small;\">\"output\":\"Certainly, I will provide clarification on the terms in section 3.2.\"},</span></div></div></div></div><div style=\"text-align: left;\"><div style=\"text-align: left;\"><div><div><span style=\"font-size: x-small;\">{\"input\":\"We are interested in extending the payment deadline to 30 days instead of the current 15 days. Additionally, we would like to add a clause regarding late payment penalties.\",</span></div></div></div></div><div style=\"text-align: left;\"><div style=\"text-align: left;\"><div><div><span style=\"font-size: x-small;\">\"output\":\"Our request is to extend the payment deadline to 30 days and include a clause on late payment penalties.\"},</span></div></div></div></div><div style=\"text-align: left;\"><div style=\"text-align: left;\"><div><div><span style=\"font-size: x-small;\">{\"input\":\"\"\"The current indemnification clause seems too broad. We would like to narrow it down to cover only direct damages and exclude consequential damages.</span></div></div></div></div><div style=\"text-align: left;\"><div style=\"text-align: left;\"><div><div><span style=\"font-size: x-small;\">Additionally, we propose including a dispute resolution clause specifying arbitration as the preferred method of resolving disputes.\"\"\",</span></div></div></div></div><div style=\"text-align: left;\"><div style=\"text-align: left;\"><div><div><span style=\"font-size: x-small;\">\"output\":\"\"\"We suggest revising the indemnification clause to limit it to covering direct damages and excluding consequential damages.</span></div></div></div></div><div style=\"text-align: left;\"><div style=\"text-align: left;\"><div><div><span style=\"font-size: x-small;\">Furthermore, we recommend adding a dispute resolution clause that specifies arbitration as the preferred method of resolving disputes.\"\"\"},</span></div></div></div></div><div style=\"text-align: left;\"><div style=\"text-align: left;\"><div><div><span style=\"font-size: x-small;\">{\"input\":\"I believe the proposed changes are acceptable.\",</span></div></div></div></div><div style=\"text-align: left;\"><div style=\"text-align: left;\"><div><div><span style=\"font-size: x-small;\">\"output\":\"Thank you for your feedback. I will proceed with implementing the proposed changes.\"}</span></div></div></div></div><div style=\"text-align: left;\"><div style=\"text-align: left;\"><div><div><span style=\"font-size: x-small;\">]</span></div></div></div></div></blockquote><div style=\"text-align: left;\"><div style=\"text-align: left;\"><div><br /></div><div><b>전체 전이 학습</b></div><div>모든 미세 조정 방법에는 일종의 전이 학습이 포함되지만, 이 범주는 특히 모델이 원래 학습 목표와 다른 작업을 수행할 수 있도록 한다. 핵심은 광범위하고 일반적인 데이터 세트에서 모델이 축적한 지식을 활용하여 보다 전문적이거나 관련 작업에 적용하는 데 있다.</div><div><br /></div><div><b>도메인별 미세 조정</b></div><div>이 미세 조정 변형은 특정 도메인 또는 산업과 관련된 텍스트를 이해하고 생성하도록 모델을 적응시키는 것을 목표로 한다. 모델은 대상 도메인과 관련된 텍스트로 구성된 데이터 세트를 사용하여 미세 조정을 거치므로 도메인별 작업에 대한 컨텍스트 파악과 숙련도가 향상된다. 예를 들어, 의료 애플리케이션용 챗봇을 개발하기 위해 모델은 의료 기록에 대해 훈련되어 의료 영역 내에서 언어 이해 능력을 개선해야 한다.</div><div><br /></div><div>미세 조정 과정에서 업데이트되는 모델 가중치에 따라 두 가지 유형의 미세 조정이 있다.</div><div><br /></div><div><b>파라미터 효율 미세 조정(PEFT)</b></div><div>전체 미세 조정이라고 하는 이 포괄적인 미세 조정 방법에는 모든 모델 가중치를 업데이트하여 최적화된 버전을 만드는 작업이 포함된다. 그러나 사전 학습과 유사한 메모리 및 계산 리소스에 대한 상당한 요구 사항을 부과하므로 학습 중에 저장 및 처리를 관리하기 위한 강력한 인프라가 필요한다.</div><div><br /></div><div><b>파라미터 효율 미세 조정(PEFT)</b></div><div>매개 변수 효율적인 미세 조정 또는 간단히 <a href=\"https://www.datacamp.com/tutorial/llama3-fine-tuning-locally\">PEFT</a>는 명령 미세 조정 방법론에서 전체 미세 조정에 대한 보다 리소스 효율적인 대안을 나타낸다. 전체 LLM 미세 조정은 상당한 계산 오버헤드를 수반하여 메모리 할당에 문제를 제기하는 반면, PEFT는 매개변수의 하위 집합만 업데이트하고 나머지는 효과적으로 \"고정\"하는 솔루션을 제공한다. 이 접근 방식은 학습 가능한 매개 변수의 수를 줄여 메모리 요구 사항을 완화하고 치명적인 망각을 방지한다. 완전한 미세 조정과 달리 PEFT는 이전에 습득한 지식을 유지하면서 원래 LLM 가중치를 보존한다. 이 기능은 여러 작업을 미세 조정할 때 스토리지 제약 조건을 완화하는 데 유리한 것으로 입증되었다. LoRA(Low-Rank Adaptation) 및 QLoRA(Quantized Low-Rank Adaptation)와 같이 널리 채택된 기술은 파라미터 효율적인 미세 조정을 달성하기 위한 효과적인 방법을 보여준다.</div><div><br /></div><div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEh1xBGvzoSB-DvUMUFOseSfHViN1xtXuCZuL03N2Nb8is9B3ID-5uQpAtMjPd0PmaS6p_M7ikJcp_C3am5p6AnGoZHMY1xq0ZCCfKOEQRUQdHthMebvzR_25Jgltb9vrPQE2c4cFjmXlxvKLccTYpJv-iwA3P0iWZ1iCcgCzj5QDPWj7cBUPLNaXmbiA4e9\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"635\" data-original-width=\"1586\" height=\"160\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEh1xBGvzoSB-DvUMUFOseSfHViN1xtXuCZuL03N2Nb8is9B3ID-5uQpAtMjPd0PmaS6p_M7ikJcp_C3am5p6AnGoZHMY1xq0ZCCfKOEQRUQdHthMebvzR_25Jgltb9vrPQE2c4cFjmXlxvKLccTYpJv-iwA3P0iWZ1iCcgCzj5QDPWj7cBUPLNaXmbiA4e9=w400-h160\" width=\"400\" /></a></div><div><b>ReFT</b></div><div>언어 모델은 토큰 시퀀스의 상황에 맞는 표현을 생성해야 한다.&nbsp;</div><div><a href=\"https://medium.com/@syed_hasan/finetuning-llama-3-using-reft-representation-fine-tuning-technique-00f4fe1f497c\">ReFT</a>(표현 미세 조정)은 n개 입력 토큰 x = (x1, . . . , xn)의 시퀀스가 주어지면 모델은 먼저 이러한 토큰을 표현 목록에 포함한다. 그런 다음, 모델 계층은 은닉 표현의 j번째 리스트 h(j)를 이전 은닉 표현 리스트 h(j−1)의 함수로 연속적으로 계산한다. 각 은닉 표현은 벡터입니다. LM은 최종 은닉 표현을 사용하여 예측을 생성한다.</div><div><br /></div><div>PyReFT는 학습 개입을 통해 내부 언어모델 표현적응을 지원하는 표현 미세 조정(ReFT) 라이브러리이다. 적은 수의 미세 조정 매개변수와 강력한 성능을 통해 Pyreft는 미세 조정 효율성을 높이고 미세 조정 비용을 줄일 수 있다.</div></div><div><br /></div><div><b>LoRA 및 QLoRa</b></div><div>LoRA는 향상된 미세 조정 접근 방식으로, 사전 훈련된 대규모 언어 모델의 가중치 행렬에 근접한 두 개의 작은 행렬만 미세 조정하여 LoRA 어댑터를 형성함으로써 기존 방법에서 벗어난다. 그런 다음 이 미세 조정된 어댑터는 후속 추론 작업을 위해 미리 학습된 모델에 통합된다. 특정 작업 또는 사용 사례에 대한 LoRA 미세 조정이 완료되면 그 결과는 변경되지 않은 원본 LLM과 함께 훨씬 더 작은 \"LoRA 어댑터\"의 출현으로 이어지며, 이는 종종 원래 LLM 크기(GB가 아닌 MB로 측정됨)의 일부에 불과한다. 추론하는 동안 LoRA 어댑터는 원래 LLM과 융합되어야 한다. 이 접근 방식은 많은 LoRA 어댑터가 원래 LLM의 용도를 효과적으로 변경할 수 있으므로 여러 작업 및 사용 사례를 처리할 때 전체 메모리 요구 사항을 줄일 수 있다는 주요 이점을 제공한다.</div><div><br /></div><div>QLoRA는 LoRA에 비해 메모리 효율성이 더욱 향상되었음을 나타낸다. LoRA 어댑터의 가중치를 더 낮은 정밀도(일반적으로 원래 4비트 대신 8비트)로 양자화하여 LoRA 기술을 개선한다. 이 추가 최적화는 메모리 공간과 스토리지 오버헤드를 크게 줄이다. QLoRA에서 사전 훈련된 모델은 양자화된 4비트 가중치로 GPU 메모리에 로드되며, 이는 LoRA에서 사용되는 8비트 정밀도에서 벗어난다. 이러한 비트 정밀도 감소에도 불구하고 QLoRA는 이전 제품과 비슷한 수준의 효율성을 유지하여 성능 저하 없이 메모리 사용을 최적화하는 능력을 보여준다.</div></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b><span style=\"font-size: medium;\">라마3 파인튜닝 모델 개발&nbsp;</span></b></div><div style=\"text-align: left;\"><b>개발 환경</b></div><div style=\"text-align: left;\">앞의 내용을 고려해, 라마3를 파인튜닝한다. 개발환경은 다음과 같다.</div><div style=\"text-align: left;\"><div><div>pip install -U transformers&nbsp;</div><div>pip install -U datasets&nbsp;</div><div>pip install -U accelerate&nbsp;</div><div>pip install -U peft&nbsp;</div><div>pip install -U trl&nbsp;</div><div>pip install -U bitsandbytes&nbsp;</div><div>pip install -U wandb</div></div></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">저렴한 비용으로 학습 모델을 개발하기 위해, Lora, 양자화를 사용한다. 사전 학습모델은 허깅페이스(HF)에서 제공하는 LLAMA3-8B모델을 사용한다.&nbsp;</div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEiIRKarbfJMJAC07a2AqVryhdCBM0qBcpXmwk9T2SbC0fnvUABY-3JvegiZZNNBt61pzvReABWqlQrGP6xMdBSq67CNah5tjPPKxCYpWY_HeEZzcnZ_ckoCLcnYsJkjtpELXtr1Hti-7R5Uufl5Ig1eOTPlot5H9aY6RpmRB_B1sqjJYED4wJyre1ZXm5xU\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"958\" data-original-width=\"968\" height=\"240\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEiIRKarbfJMJAC07a2AqVryhdCBM0qBcpXmwk9T2SbC0fnvUABY-3JvegiZZNNBt61pzvReABWqlQrGP6xMdBSq67CNah5tjPPKxCYpWY_HeEZzcnZ_ckoCLcnYsJkjtpELXtr1Hti-7R5Uufl5Ig1eOTPlot5H9aY6RpmRB_B1sqjJYED4wJyre1ZXm5xU\" width=\"243\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://huggingface.co/Undi95/Meta-Llama-3-8B-hf/tree/main\">Meta-Llama-3-8B-hf at main (huggingface.co)</a></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">파인튜닝할 데이터는&nbsp;<a href=\"https://huggingface.co/datasets/ruslanmv/ai-medical-chatbot\">ruslanmv/ai-medical-chatbot · Datasets at Hugging Face</a>&nbsp;를 사용한다.&nbsp;</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEgijKnkUoBzTd8S0lzed7MX60IfoielH-Z5F1tM8oydUwzi1WkWVa0rjOc_JE3htfrvcpCm8DqawnKdBLTMM2B_FdiANxL_vGGk_N8N5b62yEJ5g-pyrRYgjwwxV0wkkxhMZpl9M8-Tvt2kBtX-Fo02-rpbBXfMki_LvToIR0yZWEzptjL9NfsqiSUijUyK\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"914\" data-original-width=\"1811\" height=\"263\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEgijKnkUoBzTd8S0lzed7MX60IfoielH-Z5F1tM8oydUwzi1WkWVa0rjOc_JE3htfrvcpCm8DqawnKdBLTMM2B_FdiANxL_vGGk_N8N5b62yEJ5g-pyrRYgjwwxV0wkkxhMZpl9M8-Tvt2kBtX-Fo02-rpbBXfMki_LvToIR0yZWEzptjL9NfsqiSUijUyK=w519-h263\" width=\"519\" /></a></div><br /></div><div style=\"text-align: left;\">구현 코드는 다음과 같다.</div><div style=\"text-align: left;\"># 파이썬 패키지 임포트</div><div style=\"text-align: left;\"><div>from transformers import (</div><div>&nbsp; &nbsp; AutoModelForCausalLM,</div><div>&nbsp; &nbsp; AutoTokenizer,</div><div>&nbsp; &nbsp; BitsAndBytesConfig,</div><div>&nbsp; &nbsp; HfArgumentParser,</div><div>&nbsp; &nbsp; TrainingArguments,</div><div>&nbsp; &nbsp; pipeline,</div><div>&nbsp; &nbsp; logging,</div><div>)</div><div>from peft import (</div><div>&nbsp; &nbsp; LoraConfig,</div><div>&nbsp; &nbsp; PeftModel,</div><div>&nbsp; &nbsp; prepare_model_for_kbit_training,</div><div>&nbsp; &nbsp; get_peft_model,</div><div>)</div><div>import os, torch, wandb</div><div>from datasets import load_dataset</div><div>from trl import SFTTrainer, setup_chat_format</div><div><br /></div><div># 허깅페이스및 W&amp;B 키 토큰 이용해 로그인. 미리 사전 발급받아야 함.</div><div>from huggingface_hub import login # https://www.kaggle.com/discussions/product-feedback/114053</div><div>login(token = 'input your HF token')&nbsp; # HF 키 토큰 입력</div><div><br /></div><div>wandb.login(key='input your WandB token')&nbsp; # W&amp;B 키 토큰 입력</div><div>run = wandb.init(</div><div>&nbsp; &nbsp; project='Fine-tune Llama 3 8B on Medical Dataset',&nbsp;</div><div>&nbsp; &nbsp; job_type=\"training\",&nbsp;</div><div>&nbsp; &nbsp; anonymous=\"allow\"</div><div>)</div><div><br /></div><div># 사전학습모델 설정</div><div>from transformers import AutoTokenizer, AutoModelForCausalLM</div><div><br /></div><div>base_model = \"Undi95/Meta-Llama-3-8B-hf\"&nbsp;</div><div>dataset_name = \"ruslanmv/ai-medical-chatbot\" # 사용할 데이터셋</div><div>new_model = \"llama-3-8b-chat-doctor\"&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# 파인튜닝 모델 이름</div><div><br /></div><div>torch_dtype = torch.float16</div><div>attn_implementation = \"eager\"</div><div><br /></div><div># QLoRA 설정</div><div>bnb_config = BitsAndBytesConfig(</div><div>&nbsp; &nbsp; load_in_4bit=True, # 4bit 양자화</div><div>&nbsp; &nbsp; bnb_4bit_quant_type=\"nf4\",</div><div>&nbsp; &nbsp; bnb_4bit_compute_dtype=torch_dtype,</div><div>&nbsp; &nbsp; bnb_4bit_use_double_quant=True,</div><div>)</div><div><br /></div><div>model = AutoModelForCausalLM.from_pretrained(</div><div>&nbsp; &nbsp; base_model,</div><div>&nbsp; &nbsp; quantization_config=bnb_config,</div><div>&nbsp; &nbsp; device_map=\"auto\",</div><div>&nbsp; &nbsp; attn_implementation=attn_implementation</div><div>)</div><div><br /></div><div># 토크나이저 및 모델 로딩</div><div><div>tokenizer = AutoTokenizer.from_pretrained(base_model)</div><div>model, tokenizer = setup_chat_format(model, tokenizer)</div></div><div><br /></div><div># Lora 설정</div><div>peft_config = LoraConfig(</div><div>&nbsp; &nbsp; r=16,</div><div>&nbsp; &nbsp; lora_alpha=32,</div><div>&nbsp; &nbsp; lora_dropout=0.05,</div><div>&nbsp; &nbsp; bias=\"none\",</div><div>&nbsp; &nbsp; task_type=\"CAUSAL_LM\",</div><div>&nbsp; &nbsp; target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']</div><div>)</div><div>model = get_peft_model(model, peft_config)&nbsp; # 파라메터 튜닝 설정</div><div><br /></div><div># 데이터셋 로딩</div><div>dataset = load_dataset(dataset_name, split=\"all\")</div><div>dataset = dataset.shuffle(seed=65).select(range(1000)) # 파인튜닝 예시 위해 1000개만 사용</div><div><br /></div><div>def format_chat_template(row):</div><div>&nbsp; &nbsp; row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},</div><div>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]</div><div>&nbsp; &nbsp; row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)</div><div>&nbsp; &nbsp; return row</div><div><br /></div><div>dataset = dataset.map(</div><div>&nbsp; &nbsp; format_chat_template,</div><div>&nbsp; &nbsp; num_proc=4,</div><div>)</div><div><br /></div><div>print(dataset['text'][3])</div><div><br /></div><div><div>dataset = dataset.train_test_split(test_size=0.1) # 데이터 검증 세트 분할</div></div><div><br /></div><div># 모델 하이퍼파라메터 설정. 참고.&nbsp;<a href=\"https://www.datacamp.com/tutorial/fine-tuning-llama-2\">Fine-Tuning LLaMA 2: A Step-by-Step Guide to Customizing the Large Language Model | DataCamp</a></div><div>training_arguments = TrainingArguments(</div><div>&nbsp; &nbsp; output_dir=new_model,</div><div>&nbsp; &nbsp; per_device_train_batch_size=1,</div><div>&nbsp; &nbsp; per_device_eval_batch_size=1,</div><div>&nbsp; &nbsp; gradient_accumulation_steps=2,</div><div>&nbsp; &nbsp; optim=\"paged_adamw_32bit\",</div><div>&nbsp; &nbsp; num_train_epochs=1,</div><div>&nbsp; &nbsp; evaluation_strategy=\"steps\",</div><div>&nbsp; &nbsp; eval_steps=0.2,</div><div>&nbsp; &nbsp; logging_steps=1,</div><div>&nbsp; &nbsp; warmup_steps=10,</div><div>&nbsp; &nbsp; logging_strategy=\"steps\",</div><div>&nbsp; &nbsp; learning_rate=2e-4,</div><div>&nbsp; &nbsp; fp16=False,</div><div>&nbsp; &nbsp; bf16=False,</div><div>&nbsp; &nbsp; group_by_length=True,</div><div>&nbsp; &nbsp; report_to=\"wandb\"</div><div>)</div><div><br /></div><div># 지도미세조정(SFT) 설정</div><div>trainer = SFTTrainer(</div><div>&nbsp; &nbsp; model=model,</div><div>&nbsp; &nbsp; train_dataset=dataset[\"train\"],</div><div>&nbsp; &nbsp; eval_dataset=dataset[\"test\"],</div><div>&nbsp; &nbsp; peft_config=peft_config,</div><div>&nbsp; &nbsp; max_seq_length=512,</div><div>&nbsp; &nbsp; dataset_text_field=\"text\",</div><div>&nbsp; &nbsp; tokenizer=tokenizer,</div><div>&nbsp; &nbsp; args=training_arguments,</div><div>&nbsp; &nbsp; packing= False,</div><div>)</div><div><br /></div><div># 미세조정 모델학습</div><div>trainer.train()</div><div><br /></div><div># W&amp;B 로그 종료</div><div>wandb.finish()</div><div>model.config.use_cache = True</div><div><br /></div><div># 테스트</div><div>messages = [</div><div>&nbsp; &nbsp; {</div><div>&nbsp; &nbsp; &nbsp; &nbsp; \"role\": \"user\",</div><div>&nbsp; &nbsp; &nbsp; &nbsp; \"content\": \"Hello doctor, I have bad acne. How do I get rid of it?\"</div><div>&nbsp; &nbsp; }</div><div>]</div><div>prompt = tokenizer.apply_chat_template(messages, tokenize=False,&nbsp;</div><div>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;add_generation_prompt=True)</div><div>inputs = tokenizer(prompt, return_tensors='pt', padding=True,&nbsp;</div><div>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;truncation=True).to(\"cuda\")</div><div>outputs = model.generate(**inputs, max_length=150,&nbsp;</div><div>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;num_return_sequences=1)</div><div>text = tokenizer.decode(outputs[0], skip_special_tokens=True)</div><div>print(text.split(\"assistant\")[1])</div><div><br /></div><div># 파인튜닝된 모델을 허깅페이스 허브에 업로드함</div><div>trainer.model.save_pretrained(new_model)</div><div>trainer.model.push_to_hub(new_model, use_temp_dir=False)</div></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">실행 결과는 다음과 같다.</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEgOsJ6wlSBJrG-7S4k6ELsO2YLj_VZGjOeQl7GP332_PW7La37gcrgLg7auFnTzCBJQHw4x3rpxkEJa_xp3qlo0btkLqlQ0LpsttT2iSHd57YMvW4Dbim5fTgZRyHNBQEjbxms6Fk6XW_rL9hPRf1vQoHsMEynVMz7z1yORTZsxSKYqV-5ay0Ay1oJ3CrLo\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"372\" data-original-width=\"1853\" height=\"64\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEgOsJ6wlSBJrG-7S4k6ELsO2YLj_VZGjOeQl7GP332_PW7La37gcrgLg7auFnTzCBJQHw4x3rpxkEJa_xp3qlo0btkLqlQ0LpsttT2iSHd57YMvW4Dbim5fTgZRyHNBQEjbxms6Fk6XW_rL9hPRf1vQoHsMEynVMz7z1yORTZsxSKYqV-5ay0Ay1oJ3CrLo\" width=\"320\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEhiXjEIhuTMcYBwwc9TUtjo2sVu6JmYbD_TApKmuXHnXISkYSVj-RCO2kCG0Px3GNZTvsj0y1pHNtxsD23wXdWLyv59hEhlx5vL-94UO2tmt7Q64A7IdN7nNafUMMLMfeVWbqp4GKHmvQvWV7GjzF75ZYyk0YNteX-hAJsDpB_8RMd87VV8V2CrNbRbahOM\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"430\" data-original-width=\"819\" height=\"168\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEhiXjEIhuTMcYBwwc9TUtjo2sVu6JmYbD_TApKmuXHnXISkYSVj-RCO2kCG0Px3GNZTvsj0y1pHNtxsD23wXdWLyv59hEhlx5vL-94UO2tmt7Q64A7IdN7nNafUMMLMfeVWbqp4GKHmvQvWV7GjzF75ZYyk0YNteX-hAJsDpB_8RMd87VV8V2CrNbRbahOM\" width=\"320\" /></a></div><br /><br /></div><b>레퍼런스<br /></b><ul style=\"text-align: left;\"><li><a href=\"https://medium.com/@miloszivic99/finetuning-large-language-models-customize-llama-3-8b-for-your-needs-bfe0f43cd239\">Finetuning Large Language Models: Customize Llama 3 8B For Your Needs | by Milos Zivic | Apr, 2024 | Medium</a></li><li><a href=\"https://huggingface.co/Undi95/Meta-Llama-3-8B-hf\">Undi95/Meta-Llama-3-8B-hf · Hugging Face</a></li><li><a href=\"https://mlops.community/budget-instruction-fine-tuning-of-llama-3-8b-instructon-medical-data-with-hugging-face-google-colab-and-unsloth/\">Budget Instruction Fine-tuning of Llama 3 8B Instruct(on Medical Data) with Hugging Face, Google Colab and Unsloth - MLOps Community</a></li><li><a href=\"https://pytorch.org/torchtune/main/install.html\">torchtune main documentation (pytorch.org)</a>, <a href=\"https://arxiv.org/pdf/2305.11206\">LIMA</a></li><li><a href=\"https://github.com/pytorch/torchtune\">pytorch/torchtune: A Native-PyTorch Library for LLM Fine-tuning (github.com)</a></li><li><a href=\"https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B\">llama-3-Korean-Bllossom-8B · Hugging Face</a></li><li><a href=\"https://www.datacamp.com/tutorial/llama3-fine-tuning-locally\">Fine-Tuning Llama 3 and Using It Locally: A Step-by-Step Guide | DataCamp</a></li><li><a href=\"https://huggingface.co/blog/mlabonne/orpo-llama-3\">Fine-tune Llama 3 with ORPO (huggingface.co)</a></li><li><a href=\"https://medium.com/@syed_hasan/finetuning-llama-3-using-reft-representation-fine-tuning-technique-00f4fe1f497c\">Finetuning Llama-3 using ReFT (Representation Fine-Tuning) Technique | by Syed Hasan | Apr, 2024 | Medium</a></li><li><a href=\"https://www.philschmid.de/fsdp-qlora-llama3\">Efficiently fine-tune Llama 3 with PyTorch FSDP and Q-Lora (philschmid.de)</a></li><li><a href=\"https://llama.meta.com/docs/how-to-guides/fine-tuning/\">Fine-tuning | How-to guides (meta.com)</a></li><li><a href=\"https://github.com/ggerganov/llama.cpp/discussions/2904\">Fine tuning GPU memory requirements · ggerganov/llama.cpp · Discussion #2904 (github.com)</a></li><li><a href=\"https://anakin.ai/blog/how-to-fine-tune-llama-3/\">How to Fine-Tune LLaMA 3: An Easy Guide (anakin.ai)</a></li><li><a href=\"https://huggingface.co/datasets/heegyu/kowikitext\">kowikitext · Datasets at Hugging Face</a>,&nbsp;<a href=\"https://github.com/lovit/kowikitext/\">kowikitext (github.com)</a></li></ul><div>다음은 파인튜닝 후 RAG처리 전략에 대한 레퍼런스이다.&nbsp;</div><div style=\"text-align: left;\"><ul style=\"text-align: left;\"><li><a href=\"https://blog.langchain.dev/applying-openai-rag/\">Applying OpenAI's RAG Strategies (langchain.dev)</a></li><li><a href=\"https://blog.langchain.dev/query-transformations/\">Query Transformations (langchain.dev)</a></li></ul></div></div>",
        "contentSnippet": "이 글은 Lora 기반 LLAMA3 모델 파인튜닝하는 방법을 간략히 보여준다. 이를 통해, 특정 도메인의 LLM 모델 생성 정확도를 향상시킬 수 있다.\n\n\n\n\n\n머리말\n\nLLAMA3는 메타가 개발한 LLM 제품이다. 모델은 15조 개의 토큰으로 구성된 광범위한 데이터 세트에서 훈련되었다(라마 2의 경우 2T 토큰과 비교). 700억 개의 파라미터 모델과 더 작은 80억 개의 파라미터 모델의 두 가지 모델 크기가 출시되었다. 70B 모델은 MMLU 벤치마크에서 82점, HumanEval 벤치마크에서 81.7점을 기록하며 이미 인상적인 성능을 보여주었다.\n\n\n라마 3 모델은 컨텍스트 길이를 최대 8,192개 토큰(라마 2의 경우 4,096개 토큰)까지 늘렸으며, RoPE를 통해 최대 32k까지 확장할 수 있다. 또한 이 모델은 128K 토큰 어휘가 있는 새로운 토크나이저를 사용하여 텍스트를 인코딩하는 데 필요한 토큰 수를 15% 줄이다. \n\n\n파인튜닝 개념\n파인튜닝(fine turning. 미세 조정)전략은 활용된 데이터에 따라 달라지며, 이는 네 가지 유형으로 분류할 수 있다.\n\n감독 미세 조정\n퓨샷 학습(Few-shot Learning)\n전체 전이 학습\n도메인별 미세 조정\n감독 미세 조정\n\n이 방법은 미세 조정에 대한 표준 접근 방식을 나타낸다. 모델은 텍스트 분류, 질문 답변 또는 명명된 엔터티 인식과 같이 수행하려는 특정 작업에 맞게 조정된 레이블이 지정된 데이터 세트를 사용하여 추가 학습을 거친다. 예를 들어 감정 분석에서 모델은 해당 감정으로 주석이 달린 텍스트 샘플로 구성된 데이터 세트에서 학습된다.\n\n\n퓨샷 학습(Few-Shot Learning)\n레이블이 지정된 대규모 데이터 세트를 어셈블하는 것이 비현실적인 것으로 판명된 시나리오에서는 솔루션을 제공하기 위해 몇 가지 학습 단계가 개입한다. 이 기술은 입력 프롬프트를 시작할 때 원하는 작업에 대한 몇 가지 예제(또는 샷)를 모델에 제공한다. 이렇게 함으로써 모델은 철저한 미세 조정 요법 없이 작업에 대한 더 나은 컨텍스트 이해를 얻을 수 있다.\n\n\n\n\n\nfew_shot_examples = [\n\n\n\n{\"input\":\"Could you please clarify the terms outlined in section 3.2 of the contract?\",\n\n\n\n\"output\":\"Certainly, I will provide clarification on the terms in section 3.2.\"},\n\n\n\n{\"input\":\"We are interested in extending the payment deadline to 30 days instead of the current 15 days. Additionally, we would like to add a clause regarding late payment penalties.\",\n\n\n\n\"output\":\"Our request is to extend the payment deadline to 30 days and include a clause on late payment penalties.\"},\n\n\n\n{\"input\":\"\"\"The current indemnification clause seems too broad. We would like to narrow it down to cover only direct damages and exclude consequential damages.\n\n\n\nAdditionally, we propose including a dispute resolution clause specifying arbitration as the preferred method of resolving disputes.\"\"\",\n\n\n\n\"output\":\"\"\"We suggest revising the indemnification clause to limit it to covering direct damages and excluding consequential damages.\n\n\n\nFurthermore, we recommend adding a dispute resolution clause that specifies arbitration as the preferred method of resolving disputes.\"\"\"},\n\n\n\n{\"input\":\"I believe the proposed changes are acceptable.\",\n\n\n\n\"output\":\"Thank you for your feedback. I will proceed with implementing the proposed changes.\"}\n\n\n\n]\n\n\n\n\n\n전체 전이 학습\n모든 미세 조정 방법에는 일종의 전이 학습이 포함되지만, 이 범주는 특히 모델이 원래 학습 목표와 다른 작업을 수행할 수 있도록 한다. 핵심은 광범위하고 일반적인 데이터 세트에서 모델이 축적한 지식을 활용하여 보다 전문적이거나 관련 작업에 적용하는 데 있다.\n\n\n도메인별 미세 조정\n이 미세 조정 변형은 특정 도메인 또는 산업과 관련된 텍스트를 이해하고 생성하도록 모델을 적응시키는 것을 목표로 한다. 모델은 대상 도메인과 관련된 텍스트로 구성된 데이터 세트를 사용하여 미세 조정을 거치므로 도메인별 작업에 대한 컨텍스트 파악과 숙련도가 향상된다. 예를 들어, 의료 애플리케이션용 챗봇을 개발하기 위해 모델은 의료 기록에 대해 훈련되어 의료 영역 내에서 언어 이해 능력을 개선해야 한다.\n\n\n미세 조정 과정에서 업데이트되는 모델 가중치에 따라 두 가지 유형의 미세 조정이 있다.\n\n\n파라미터 효율 미세 조정(PEFT)\n전체 미세 조정이라고 하는 이 포괄적인 미세 조정 방법에는 모든 모델 가중치를 업데이트하여 최적화된 버전을 만드는 작업이 포함된다. 그러나 사전 학습과 유사한 메모리 및 계산 리소스에 대한 상당한 요구 사항을 부과하므로 학습 중에 저장 및 처리를 관리하기 위한 강력한 인프라가 필요한다.\n\n\n파라미터 효율 미세 조정(PEFT)\n매개 변수 효율적인 미세 조정 또는 간단히 PEFT는 명령 미세 조정 방법론에서 전체 미세 조정에 대한 보다 리소스 효율적인 대안을 나타낸다. 전체 LLM 미세 조정은 상당한 계산 오버헤드를 수반하여 메모리 할당에 문제를 제기하는 반면, PEFT는 매개변수의 하위 집합만 업데이트하고 나머지는 효과적으로 \"고정\"하는 솔루션을 제공한다. 이 접근 방식은 학습 가능한 매개 변수의 수를 줄여 메모리 요구 사항을 완화하고 치명적인 망각을 방지한다. 완전한 미세 조정과 달리 PEFT는 이전에 습득한 지식을 유지하면서 원래 LLM 가중치를 보존한다. 이 기능은 여러 작업을 미세 조정할 때 스토리지 제약 조건을 완화하는 데 유리한 것으로 입증되었다. LoRA(Low-Rank Adaptation) 및 QLoRA(Quantized Low-Rank Adaptation)와 같이 널리 채택된 기술은 파라미터 효율적인 미세 조정을 달성하기 위한 효과적인 방법을 보여준다.\n\n\n\nReFT\n언어 모델은 토큰 시퀀스의 상황에 맞는 표현을 생성해야 한다. \nReFT(표현 미세 조정)은 n개 입력 토큰 x = (x1, . . . , xn)의 시퀀스가 주어지면 모델은 먼저 이러한 토큰을 표현 목록에 포함한다. 그런 다음, 모델 계층은 은닉 표현의 j번째 리스트 h(j)를 이전 은닉 표현 리스트 h(j−1)의 함수로 연속적으로 계산한다. 각 은닉 표현은 벡터입니다. LM은 최종 은닉 표현을 사용하여 예측을 생성한다.\n\n\nPyReFT는 학습 개입을 통해 내부 언어모델 표현적응을 지원하는 표현 미세 조정(ReFT) 라이브러리이다. 적은 수의 미세 조정 매개변수와 강력한 성능을 통해 Pyreft는 미세 조정 효율성을 높이고 미세 조정 비용을 줄일 수 있다.\n\n\nLoRA 및 QLoRa\nLoRA는 향상된 미세 조정 접근 방식으로, 사전 훈련된 대규모 언어 모델의 가중치 행렬에 근접한 두 개의 작은 행렬만 미세 조정하여 LoRA 어댑터를 형성함으로써 기존 방법에서 벗어난다. 그런 다음 이 미세 조정된 어댑터는 후속 추론 작업을 위해 미리 학습된 모델에 통합된다. 특정 작업 또는 사용 사례에 대한 LoRA 미세 조정이 완료되면 그 결과는 변경되지 않은 원본 LLM과 함께 훨씬 더 작은 \"LoRA 어댑터\"의 출현으로 이어지며, 이는 종종 원래 LLM 크기(GB가 아닌 MB로 측정됨)의 일부에 불과한다. 추론하는 동안 LoRA 어댑터는 원래 LLM과 융합되어야 한다. 이 접근 방식은 많은 LoRA 어댑터가 원래 LLM의 용도를 효과적으로 변경할 수 있으므로 여러 작업 및 사용 사례를 처리할 때 전체 메모리 요구 사항을 줄일 수 있다는 주요 이점을 제공한다.\n\n\nQLoRA는 LoRA에 비해 메모리 효율성이 더욱 향상되었음을 나타낸다. LoRA 어댑터의 가중치를 더 낮은 정밀도(일반적으로 원래 4비트 대신 8비트)로 양자화하여 LoRA 기술을 개선한다. 이 추가 최적화는 메모리 공간과 스토리지 오버헤드를 크게 줄이다. QLoRA에서 사전 훈련된 모델은 양자화된 4비트 가중치로 GPU 메모리에 로드되며, 이는 LoRA에서 사용되는 8비트 정밀도에서 벗어난다. 이러한 비트 정밀도 감소에도 불구하고 QLoRA는 이전 제품과 비슷한 수준의 효율성을 유지하여 성능 저하 없이 메모리 사용을 최적화하는 능력을 보여준다.\n\n\n라마3 파인튜닝 모델 개발 \n개발 환경\n앞의 내용을 고려해, 라마3를 파인튜닝한다. 개발환경은 다음과 같다.\n\npip install -U transformers \npip install -U datasets \npip install -U accelerate \npip install -U peft \npip install -U trl \npip install -U bitsandbytes \npip install -U wandb\n\n\n\n\n저렴한 비용으로 학습 모델을 개발하기 위해, Lora, 양자화를 사용한다. 사전 학습모델은 허깅페이스(HF)에서 제공하는 LLAMA3-8B모델을 사용한다. \n\nMeta-Llama-3-8B-hf at main (huggingface.co)\n\n\n파인튜닝할 데이터는 ruslanmv/ai-medical-chatbot · Datasets at Hugging Face 를 사용한다. \n\n\n\n구현 코드는 다음과 같다.\n# 파이썬 패키지 임포트\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format\n\n\n# 허깅페이스및 W&B 키 토큰 이용해 로그인. 미리 사전 발급받아야 함.\nfrom huggingface_hub import login # https://www.kaggle.com/discussions/product-feedback/114053\nlogin(token = 'input your HF token')  # HF 키 토큰 입력\n\n\nwandb.login(key='input your WandB token')  # W&B 키 토큰 입력\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on Medical Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)\n\n\n# 사전학습모델 설정\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\nbase_model = \"Undi95/Meta-Llama-3-8B-hf\" \ndataset_name = \"ruslanmv/ai-medical-chatbot\" # 사용할 데이터셋\nnew_model = \"llama-3-8b-chat-doctor\"           # 파인튜닝 모델 이름\n\n\ntorch_dtype = torch.float16\nattn_implementation = \"eager\"\n\n\n# QLoRA 설정\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, # 4bit 양자화\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)\n\n\n# 토크나이저 및 모델 로딩\n\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n\n\n# Lora 설정\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)  # 파라메터 튜닝 설정\n\n\n# 데이터셋 로딩\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=65).select(range(1000)) # 파인튜닝 예시 위해 1000개만 사용\n\n\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc=4,\n)\n\n\nprint(dataset['text'][3])\n\n\ndataset = dataset.train_test_split(test_size=0.1) # 데이터 검증 세트 분할\n\n\n# 모델 하이퍼파라메터 설정. 참고. Fine-Tuning LLaMA 2: A Step-by-Step Guide to Customizing the Large Language Model | DataCamp\ntraining_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)\n\n\n# 지도미세조정(SFT) 설정\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)\n\n\n# 미세조정 모델학습\ntrainer.train()\n\n\n# W&B 로그 종료\nwandb.finish()\nmodel.config.use_cache = True\n\n\n# 테스트\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello doctor, I have bad acne. How do I get rid of it?\"\n    }\n]\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\noutputs = model.generate(**inputs, max_length=150, \n                         num_return_sequences=1)\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(text.split(\"assistant\")[1])\n\n\n# 파인튜닝된 모델을 허깅페이스 허브에 업로드함\ntrainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)\n\n\n실행 결과는 다음과 같다.\n\n\n\n\n레퍼런스\n\nFinetuning Large Language Models: Customize Llama 3 8B For Your Needs | by Milos Zivic | Apr, 2024 | Medium\nUndi95/Meta-Llama-3-8B-hf · Hugging Face\nBudget Instruction Fine-tuning of Llama 3 8B Instruct(on Medical Data) with Hugging Face, Google Colab and Unsloth - MLOps Community\ntorchtune main documentation (pytorch.org), LIMA\npytorch/torchtune: A Native-PyTorch Library for LLM Fine-tuning (github.com)\nllama-3-Korean-Bllossom-8B · Hugging Face\nFine-Tuning Llama 3 and Using It Locally: A Step-by-Step Guide | DataCamp\nFine-tune Llama 3 with ORPO (huggingface.co)\nFinetuning Llama-3 using ReFT (Representation Fine-Tuning) Technique | by Syed Hasan | Apr, 2024 | Medium\nEfficiently fine-tune Llama 3 with PyTorch FSDP and Q-Lora (philschmid.de)\nFine-tuning | How-to guides (meta.com)\nFine tuning GPU memory requirements · ggerganov/llama.cpp · Discussion #2904 (github.com)\nHow to Fine-Tune LLaMA 3: An Easy Guide (anakin.ai)\nkowikitext · Datasets at Hugging Face, kowikitext (github.com)\n\n다음은 파인튜닝 후 RAG처리 전략에 대한 레퍼런스이다. \n\nApplying OpenAI's RAG Strategies (langchain.dev)\nQuery Transformations (langchain.dev)",
        "id": "tag:blogger.com,1999:blog-5201956450461596914.post-6440721014895432690",
        "isoDate": "2024-06-21T01:47:00.000Z"
      },
      {
        "title": "NLP의 핵심. 토큰, 임베딩 모델 파인튜닝",
        "link": "http://daddynkidsmakers.blogspot.com/2024/06/nlp.html",
        "pubDate": "2024-06-20T02:42:00.000Z",
        "author": "Daddy Maker",
        "content": "<div style=\"text-align: left;\">이 글은 LLM NLP처리의 핵심인 토큰, 임베딩 모델 파인튜닝에 대한 내용을 간략히 다룬다.&nbsp;<br /></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEjdtFPzVXJo-FpMzg755V6l-C1vSvouzcyrxYR9JU7pRd3R7_lQLCUDlMIGQrhcDV5eZ6iddZsgM8FHl5sBSEw2ssbdrGxP_vn8spPvkYHSB7fUVbeWUEXRIYunL2-3V9c3l4RzSDUPiHNmsy_39IyyGhDdqCL-aogV_EY6ENs4DYKH9qk7zzFjriBdVgR-\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"360\" data-original-width=\"480\" height=\"240\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjdtFPzVXJo-FpMzg755V6l-C1vSvouzcyrxYR9JU7pRd3R7_lQLCUDlMIGQrhcDV5eZ6iddZsgM8FHl5sBSEw2ssbdrGxP_vn8spPvkYHSB7fUVbeWUEXRIYunL2-3V9c3l4RzSDUPiHNmsy_39IyyGhDdqCL-aogV_EY6ENs4DYKH9qk7zzFjriBdVgR-\" width=\"320\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">TIKTOKEN 라이브러리</div></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b>배경 - 도메인 의존 정보</b></div><div style=\"text-align: left;\">의학과 같은 특별한 분야에서는 기존 LLM이 제대로 정보를 생성하지 못하는 경우가 많다. 이런 문제는 파인튜닝할 때 크게 나타난다. 사실, 토큰과 임베딩은 입력 시퀀스에 대한 출력을 학습, 예측할 때 훈련의 전제가 되는 LLM의 기본조건이다.&nbsp;</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">토큰 사전과 임베딩 모델이 다르면, 제대로 된 모델 학습, 예측, 패턴 계산 결과를 얻기 어렵다. 그러므로, LLM을 사용하기 전에 미리 이 점을 명확히 확인해야 한다. 만약, 이런 전제가 만족될 수 없다면, <a href=\"https://huggingface.co/tomaszki/llama-3-a/raw/main/vocab.json\">LLM 토큰</a> 개발, 파인튜닝 및 사전 모델학습이 필요하다.&nbsp;</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGJkuNmb9VoaDwHakxUDZ7fTa7IJ2ToAqTyozvTunuRes3oE_4pIUzTBbGZTyypHvexXaLGwB1h0sUWwck0VTIH-VwU5qjk52k-H1179vy6bxOJgyMnJJ7j46MAyHh2dZz5BkARRHARkp4k60JYncGrvkKDV2NWvGdpm62j6omjW6z9PxCVjz_iqcgSG0F/s364/a16.JPG\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"241\" data-original-width=\"364\" height=\"171\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGJkuNmb9VoaDwHakxUDZ7fTa7IJ2ToAqTyozvTunuRes3oE_4pIUzTBbGZTyypHvexXaLGwB1h0sUWwck0VTIH-VwU5qjk52k-H1179vy6bxOJgyMnJJ7j46MAyHh2dZz5BkARRHARkp4k60JYncGrvkKDV2NWvGdpm62j6omjW6z9PxCVjz_iqcgSG0F/w257-h171/a16.JPG\" width=\"257\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">숫자 토큰화 결과(OOV. Out-Of-Vocabulary 문제)</div><div><br /></div></div><div style=\"text-align: left;\"><b>개발환경</b></div><div style=\"text-align: left;\">실습을 위해 다음을 설치한다.</div><div style=\"text-align: left;\"><div>pip install transformers torch</div><div><br /></div><div><div>참고로, 다음은 파인튜닝에 사용하는 도구를 보여준다.</div><div><ul><li>Torch: 텐서 계산 및 딥 러닝을 위한 핵심 라이브러리이다.</li><li>peft: 낮은 순위의 적응 기술을 사용하여 대규모 언어 모델을 효율적으로 미세 조정할 수 있다. 특히 리소스가 제한된 장치에서 학습 가능한 매개 변수의 수를 줄여 모델을 압축하고 더 빠르게 미세 조정할 수 있다.</li><li>bitsandbytes: 신경망에 대한 양자화 및 이진화 기술을 제공하여 모델 압축을 지원한다. 모델 압축에 도움이 되므로 메모리와 계산 능력이 제한된 에지 장치에 모델을 보다 실현 가능하게 만들 수 있다.</li><li>Transformers: 대규모 언어 모델 작업을 간소화하여 사전 학습된 모델 및 학습 파이프라인을 제공한다.</li><li>trl: 대규모 언어 모델의 경우 효율적인 모델 학습 및 최적화에 중점을 둔다.</li><li>accelerate: 다양한 하드웨어 플랫폼에서 학습 및 추론을 가속화한다.</li><li>dataset: 기계 학습 작업을 위한 데이터 세트 로드 및 준비를 간소화한다.</li><li>pipeline: 사용자 지정 학습 없이 일반적인 NLP 작업에 대해 사전 학습된 모델의 사용을 간소화한다.</li><li>pyarrow: 효율적인 데이터 로드 및 처리를 위해 사용될 수 있다.</li><li>LoraConfig: LoRA 기반 미세 조정을 위한 구성 매개변수를 보유한다.</li><li>SFTTrainer: 모델 학습, 최적화 및 평가를 처리한다.</li></ul></div><div><b>토큰 사전&nbsp;</b></div></div></div><div style=\"text-align: left;\">LLM 파인튜닝이나 RAG 시 토큰 사전이 없으면, 제대로 학습되지 않는다. 입력 시퀀스의 토큰이 사전에 없으면, 토큰은 분리된다. 분리된 토큰들은 각자 다른 맥락을 가지도록 학습된다. 다음 코드를 실행하면, 그 내용을 확인할 수 있다. 이런 문제는 유전자 해석 등 다양한 문제에서 발생된다.&nbsp;</div><div style=\"text-align: left;\">from transformers import DistilBertTokenizerFast, DistilBertModel</div><div style=\"text-align: left;\"><div><br /></div><div>tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")</div><div>tokens = tokenizer.encode('This is a IfcBuilding.', return_tensors='pt')</div><div>print(\"These are tokens!\", tokens)</div><div>for token in tokens[0]:</div><div>&nbsp; &nbsp; print(\"This are decoded tokens!\", tokenizer.decode([token]))</div><div><br /></div><div>model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")</div><div>print(model.embeddings.word_embeddings(tokens))</div><div>for e in model.embeddings.word_embeddings(tokens)[0]:</div><div>&nbsp; &nbsp; print(\"This is an embedding!\", e)</div></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">다음 코드를 실행해보면, 좀 더 많은 문제점을 확인할 수 있다.</div><div style=\"text-align: left;\"><div>from transformers import BertTokenizer, BertModel</div><div>bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')</div><div><br /></div><div>example_sen = (</div><div>&nbsp; &nbsp; \"\"\"</div><div>&nbsp; &nbsp; The United States and Russia sought to lower the temperature in a&nbsp;</div><div>&nbsp; &nbsp; heated standoff over Ukraine,even as they reported no breakthroughs&nbsp;</div><div>&nbsp; &nbsp; in high-stakes talks on Friday aimed at preventing a feared Russian invasion</div><div>&nbsp; &nbsp; \"\"\"</div><div>)</div><div>print(bert_tokenizer.tokenize(example_sen))</div><div><br /></div><div>결과는 다음과 같다.&nbsp;</div><div>['the', 'united', 'states', 'and', 'russia', 'sought', 'to', 'lower', 'the', 'temperature', 'in', 'a', 'heated', 'stand', '##off', 'over', 'ukraine', ',', 'even', 'as', 'they', 'reported', 'no', 'breakthrough', '##s', 'in', 'high', '-', 'stakes', 'talks', 'on', 'friday', 'aimed', 'at', 'preventing', 'a', 'feared', 'russian', 'invasion']</div></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">이 경우, 토큰을 추가하거나, 하나로 합치는 것을 고려할 필요가 있다.</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div></div><div style=\"text-align: left;\"><b>토큰 추가와 임베딩 공간</b></div><div style=\"text-align: left;\">BERT를 이용해 토큰 사전과 임베딩을 실습해본다.</div><div style=\"text-align: left;\">일반적으로 허깅페이스 라이브러리에서 LLM모델에 대한 토큰 추가가 가능한 tokenizer를 제공해 준다. 토큰을 추가하면, 임베딩 차원에 영향을 주므로, 해당 크기를 수정해야 한다. 다음은 이를 고려한 코드를 보여준다.&nbsp;</div><div style=\"text-align: left;\"><div>from transformers import AutoTokenizer, AutoModelForCausalLM</div><div><br /></div><div># 사전 학습모델 및 토크나이저 로딩</div><div>model = AutoModelForCausalLM.from_pretrained('model-name')</div><div><div>tokenizer = AutoTokenizer.from_pretrained('model-name')</div></div><div><br /></div><div># 토큰 추가</div><div>new_tokens = ['newword1', 'newword2']</div><div>tokenizer.add_tokens(new_tokens)</div><div><br /></div><div># 임베딩 공간 리사이즈&nbsp;</div><div>model.resize_token_embeddings(len(tokenizer))</div><div><br /></div><div># 추가된 토큰과 함께 파인튜닝.&nbsp;</div><div># (fine-tuning code here)</div><div><br /></div><div><b>BPE(Byte Pair Encoding) 토큰 압축</b></div><div><div>BPE는 바이트 쌍 인코딩(Byte pair Encoding)을 의미하며, 데이터의 가장 일반적인 연속 바이트 쌍을 해당 데이터 내에 발생하지 않는 바이트로 대체하는 데이터 압축 형태이다. 결과 데이터에 대해 프로세스가 반복적으로 반복된다. 자연어 처리(NLP) 및 기계 학습의 맥락에서 BPE는 하위 단어 토큰화 방법으로 사용된다.&nbsp;</div><div><br /></div><div>단어를 보다 관리하기 쉬운 하위 단어나 기호로 분할하여 대규모 어휘를 효율적으로 인코딩할 수 있다. 이 접근 방식은 어휘의 크기를 크게 줄이고 희귀 단어나 OOV(어휘에서 벗어난) 용어를 처리하는 모델의 능력을 향상시킬 수 있다.</div><div><br /></div><div>NLP에 BPE를 적용하는 기본 단계는 다음과 같다.</div><div><ul style=\"text-align: left;\"><li>텍스트를 단어로 나눈 다음 문자로 나누고 각 문자(또는 문자 시퀀스)의 빈도를 계산한다.</li><li>인접한 문자 또는 문자 시퀀스의 가장 빈번한 쌍을 반복적으로 찾아, 이를 새로운 문자 시퀀스로 병합한다.</li><li>원하는 어휘 크기에 도달할 때까지 또는 더 이상 병합으로 인해 어휘 크기가 크게 줄어들 때까지 병합 프로세스를 반복한다.</li></ul></div><div>BPE는 언어 모델링, 텍스트 생성, 특히 BERT(Bidirection Encoder Representations from Transformers)와 같은 변환기 기반 모델과 같은 다양한 NLP 모델 및 작업에 널리 채택되어 광범위한 어휘를 효율적으로 처리하는 데 도움이 된다.</div></div><div><br /></div><div>다음은 그 예제를 보여준다.</div><div>from tokenizers import Tokenizer, models, pre_tokenizers, trainers</div><div><div><br /></div><div>tokenizer = Tokenizer(models.BPE()) # 토큰화 얻기</div><div><br /></div><div>tokenizer.pre_tokenizer = pre_tokenizers.Whitespace() # 사용자 토큰 처리 객체</div><div>def custom_pre_tokenizer(sequence): # 사용자 토큰 정의</div><div>&nbsp; &nbsp; # Define rules to combine tokens, e.g., \"new word\" -&gt; \"newword\"</div><div>&nbsp; &nbsp; combined_sequence = sequence.replace(\"new word\", \"newword\")</div><div>&nbsp; &nbsp; return combined_sequence</div><div><br /></div><div># 토큰 훈련. custom pre-tokenizer 활용함.</div><div>trainer = trainers.BpeTrainer()</div><div>tokenizer.train(files=[\"path/to/training/data.txt\"], trainer=trainer, pre_tokenizer=custom_pre_tokenizer)</div><div><br /></div><div># 훈련된 토큰 저장</div><div>tokenizer.save(\"path/to/customized_tokenizer.json\")</div></div><div><br /></div><div><div><b>임베딩 모델 파인튜닝</b></div></div><div>다음은 토큰을 추가하고, 임베딩 모델을 파인튜닝하는&nbsp;보여준다.</div><div><br /></div><div><div>from transformers import BertTokenizerFast, BertModel</div><div>import torch</div><div>from torch import nn</div><div><br /></div><div># BERT 토크나이저 사전학습모델 로딩</div><div>tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')</div><div>print(tokenizer.tokenize(\"[CLS] Hello world, how are you?\"))</div><div><br /></div><div>print(tokenizer.tokenize(\"[newtoken] Hello world, how are you?\"))</div><div>tokenizer.add_tokens(['[newtoken]'])</div><div><br /></div><div><div>다음과 같이, [newtoken] 토큰 추가 전 테스트. 토큰이 한단어가 아닌 분할 출력된 것 확인</div><div>['[',</div><div>&nbsp;'newt',</div><div>&nbsp;'##oke',</div><div>&nbsp;'##n',</div><div>&nbsp;']',</div><div>&nbsp;'hello',</div><div>&nbsp;'world',</div><div>&nbsp;',',</div><div>&nbsp;'how',</div><div>&nbsp;'are',</div><div>&nbsp;'you',</div><div>&nbsp;'?']</div></div><div><br /></div><div>토큰을 추가하고 다시 토큰화를 한다.</div><div><div>tokenizer.add_tokens(['[newtoken]'])</div><div>tokenizer.tokenize(\"[newtoken] Hello world, how are you?\")</div></div><div><br /></div><div>제대로 토큰화가 된다.&nbsp;</div><div>['[newtoken]', 'hello', 'world', ',', 'how', 'are', 'you', '?']</div><div><br /></div><div>토큰값을 확인해 본다.</div><div>tokenized = tokenizer(\"[newtoken] Hello world, how are you?\", add_special_tokens=False, return_tensors=\"pt\")</div><div>print(tokenized['input_ids'])</div><div><br /></div><div>tkn = tokenized['input_ids'][0, 0]</div><div>print(\"First token:\", tkn)</div><div>print(\"Decoded:\", tokenizer.decode(tkn))</div><div><br /></div><div>다음과 같이, 토큰값이 잘 할당된 것을 알 수 있다.</div><div><div>tensor([[30522,&nbsp; 7592,&nbsp; 2088,&nbsp; 1010,&nbsp; 2129,&nbsp; 2024,&nbsp; 2017,&nbsp; 1029]])</div><div>First token: tensor(30522)</div><div>Decoded: [newtoken]</div></div><div><br /></div><div>임베딩 모델 학습을 위한 BERT 로딩하고, 앞의 토큰 리스트를 모델에 입력한다.</div><div>model = BertModel.from_pretrained('bert-base-uncased')</div><div>print(model.embeddings)</div><div><br /></div><div>try:</div><div>&nbsp; &nbsp; out = model(**tokenized)</div><div>&nbsp; &nbsp; out.last_hidden_state</div><div>except Exception as e:</div><div>&nbsp; &nbsp; print(e)</div><div><br /></div><div>임베딩 모델이 추가된 토큰을 학습하지 않았으므로, out of range 에러가 출력될 것이다.&nbsp;</div><div>다음 코드로 BERT 모델의토큰 공간 크기를 확인해 본다.</div><div>weights = model.embeddings.word_embeddings.weight.data</div><div>print(weights.shape)</div><div><br /></div><div>출력은 다음과 같이 30522이다.</div><div><div>torch.Size([30522, 768])</div></div><div><br /></div><div>이제 [CLS] 토큰을 임베딩 모델에 추가해보자.&nbsp;</div><div>new_weights = torch.cat((weights, weights[101:102]), 0)</div><div>new_emb = nn.Embedding.from_pretrained(new_weights, padding_idx=0, freeze=False)</div><div>print(new_emb)</div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiOFsQaKRDg14K_tV9HjnNhajL96t-NH-TXtnuwVdvailYbH6radvdePHtRZ9sY44tvAjGi7WBJhEtRF-Q82223dlK4wUg0zULyInMPfGdATBT-CRkZp5VnpTXs0XQF-0aDPjb9qnKePF7TTweBTDhmcKAeMb_f18iy2pQYL6AUy4RZ8ItPDQIGlkbI1Upv/s592/T1.JPG\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"239\" data-original-width=\"592\" height=\"161\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiOFsQaKRDg14K_tV9HjnNhajL96t-NH-TXtnuwVdvailYbH6radvdePHtRZ9sY44tvAjGi7WBJhEtRF-Q82223dlK4wUg0zULyInMPfGdATBT-CRkZp5VnpTXs0XQF-0aDPjb9qnKePF7TTweBTDhmcKAeMb_f18iy2pQYL6AUy4RZ8ItPDQIGlkbI1Upv/w400-h161/T1.JPG\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://tinkerd.net/blog/machine-learning/bert-tokenization/\">Understanding BERT | Tokenization (tinkerd.net)</a></div><div><br /></div><div>다음과 같이 30523으로 토큰 크기가 증가되었다.&nbsp;</div><div><div>Embedding(30523, 768, padding_idx=0)</div></div><div><br /></div><div>새 레이어를 모델 마지막에 추가한다.</div><div>model.embeddings.word_embeddings = new_emb</div><div>print(model.embeddings)</div><div><br /></div><div>그 결과로 임베딩 모델의 word_embeddings가 업데이트된다.</div><div><div>BertEmbeddings(</div><div>&nbsp; (word_embeddings): Embedding(30523, 768, padding_idx=0)</div><div>&nbsp; (position_embeddings): Embedding(512, 768)</div><div>&nbsp; (token_type_embeddings): Embedding(2, 768)</div><div>&nbsp; (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</div><div>&nbsp; (dropout): Dropout(p=0.1, inplace=False)</div><div>)</div></div><div><br /></div><div>앞의 토큰 시퀀스 리스트를 입력한다. 그럼, 제대로 결과가 출력될 것이다.</div><div>out = model(**tokenized)</div><div>print(out.last_hidden_state)</div><div><br /></div><div>다음 코드를 실행하면, 추가된 모델이 동일한 결과를 가지는 것을 알 수 있다.</div><div>model = BertModel.from_pretrained('bert-base-uncased')</div><div>out2 = model(</div><div>&nbsp; &nbsp; **tokenizer(\"[CLS] Hello world, how are you?\", add_special_tokens=False, return_tensors=\"pt\")</div><div>)</div><div><br /></div><div>out3 = torch.all(out.last_hidden_state == out2.last_hidden_state)</div><div>output(out3)</div></div></div><div style=\"text-align: left;\"><b><br /></b></div><div style=\"text-align: left;\"><b>마무리</b></div><div style=\"text-align: left;\"><div>LLM 파인튜닝이나 RAG 시 학습 데이터에 포함된 토큰에 대한 적절한 사전이 없으면, 제대로 학습되지 않는다. 이 글은 LLM NLP처리의 핵심인 토큰, 임베딩 모델 파인튜닝에 대한 내용을 간략히 다룬다.&nbsp;</div><div><br /></div></div><div style=\"text-align: left;\"><b>레퍼런스</b></div><div style=\"text-align: left;\"><ul style=\"text-align: left;\"><li><a href=\"https://medium.com/@ilyesrezgui46/enhance-your-gpt-experience-tiktoken-unveiled-free-token-counting-for-prompts-with-python-code-51851883825b\">Enhance Your GPT Experience: Tiktoken Unveiled — Free Token Counting for Prompts (with Python Code!) | by Ilyes Rezgui | Medium</a></li><li><a href=\"https://www.ai-bio.info/huggingface-bertformaskedlm-finetuning\">huggingface BertForMaskedLM fine-tuning (ai-bio.info)</a></li><li><a href=\"https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/split_by_token/\">Split by tokens | 🦜️🔗 LangChain</a></li><li><a href=\"https://huggingface.co/tomaszki/llama-3-a/raw/main/vocab.json\">huggingface.co/tomaszki/llama-3-a/raw/main/vocab.json</a></li><li><a href=\"https://wikidocs.net/234002\">TokenTextSplitter</a></li><li><a href=\"https://medium.com/@pierre_guillou/nlp-how-to-add-a-domain-specific-vocabulary-new-tokens-to-a-subword-tokenizer-already-trained-33ab15613a41\">NLP | How to add a domain-specific vocabulary (new tokens) to a subword tokenizer already trained like BERT WordPiece | by Pierre Guillou | Medium</a></li><li><a href=\"https://huggingface.co/blog/lmassaron/fine-tuning-llms-on-kaggle-notebooks\">Fine-tuning a large language model on Kaggle Notebooks (or even on your own computer) for solving real-world tasks (huggingface.co)</a></li><li><a href=\"https://medium.com/@gobishangar11/llama-2-a-detailed-guide-to-fine-tuning-the-large-language-model-8968f77bcd15\">LLaMA 2: A Detailed Guide to Fine-Tuning the Large Language Model | by Gobi Shangar | Medium</a></li><li><a href=\"https://tuanatran.medium.com/fine-tuning-large-language-model-with-hugging-face-pytorch-adce80dce2ad\">Fine-Tuning Large Language Model with Hugging Face &amp; PyTorch | by Tuan Tran | Medium</a></li><li><a href=\"https://www.youtube.com/watch?v=McTuvQCXbRY\">Instruction Fine-tuning GEMMA-2</a>B.&nbsp;<a href=\"https://github.com/tsdata/langchain-ollama/blob/main/006_gemma_peft_qlora_colab/gemma_finetuning_koalpaca.ipynb\">langchain-ollama/006_gemma_peft_qlora_colab/gemma_finetuning_koalpaca.ipynb at main · tsdata/langchain-ollama (github.com)</a></li><li><a href=\"https://www.youtube.com/watch?v=f8pDjOkXIZo\">LLAMA3 Model FineTune</a>, <a href=\"https://www.youtube.com/watch?v=eNxAgYBdJ2A\">KoLLAMA3</a></li><li><a href=\"https://www.youtube.com/watch?v=GjZ1a0OJqGk\">LLAMA2 finetuning #1</a>, <a href=\"https://www.youtube.com/watch?v=ZVYpQRJBKDs\">#2</a>,&nbsp;</li><li><a href=\"https://www.youtube.com/watch?v=mljLcQ_N9h0\">LLAMA2 custom dataset</a>&nbsp;and <a href=\"https://www.youtube.com/watch?v=vziygFrRlZ4\">fine-tune</a></li><li><a href=\"https://www.youtube.com/watch?v=GLmMKl3OSD8\">LLAMA2 gradient</a></li><li><a href=\"https://www.youtube.com/watch?v=tX75-O3abmQ\">LLAMA2 finetune and llama3 unsloth</a></li><li><a href=\"https://www.youtube.com/watch?v=vziygFrRlZ4\">ChatGPT train</a></li><li><a href=\"https://www.youtube.com/watch?v=RAAKwB4niCg\">MLOps</a></li><li><a href=\"https://www.youtube.com/watch?v=66GD0Bj5Whk\">LoRA</a></li><li><a href=\"https://colab.research.google.com/drive/1eq_DmpiN3hXil5N4r-7GHqA75zkgBSWW#scrollTo=LaK1l1mczz93\">Phi-3 colab</a>, <a href=\"https://entreprenerdly.com/fine-tuning-phi-3-with-huggingface/\">tuning</a>&nbsp;in <a href=\"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\">huggingface</a></li><li><a href=\"https://entreprenerdly.com/fine-tuning-phi-3-with-huggingface/\">Fine-Tuning Phi-3 with Hugging Face - Entreprenerdly</a></li><li><a href=\"https://github.com/datawhalechina/self-llm/blob/master/phi-3/Phi-3-mini-4k-Instruct-Lora.ipynb\">self-llm/phi-3/Phi-3-mini-4k-Instruct-Lora.ipynb at master · datawhalechina/self-llm (github.com)</a></li><li><a href=\"https://github.com/microsoft/Phi-3CookBook/blob/main/code/04.Finetuning/Phi-3-finetune-qlora-python.ipynb\">Phi-3CookBook/code/04.Finetuning/Phi-3-finetune-qlora-python.ipynb at main · microsoft/Phi-3CookBook (github.com)</a></li></ul></div>",
        "contentSnippet": "이 글은 LLM NLP처리의 핵심인 토큰, 임베딩 모델 파인튜닝에 대한 내용을 간략히 다룬다. \n\n\n\n\nTIKTOKEN 라이브러리\n\n\n배경 - 도메인 의존 정보\n의학과 같은 특별한 분야에서는 기존 LLM이 제대로 정보를 생성하지 못하는 경우가 많다. 이런 문제는 파인튜닝할 때 크게 나타난다. 사실, 토큰과 임베딩은 입력 시퀀스에 대한 출력을 학습, 예측할 때 훈련의 전제가 되는 LLM의 기본조건이다. \n\n\n토큰 사전과 임베딩 모델이 다르면, 제대로 된 모델 학습, 예측, 패턴 계산 결과를 얻기 어렵다. 그러므로, LLM을 사용하기 전에 미리 이 점을 명확히 확인해야 한다. 만약, 이런 전제가 만족될 수 없다면, LLM 토큰 개발, 파인튜닝 및 사전 모델학습이 필요하다. \n\n\n숫자 토큰화 결과(OOV. Out-Of-Vocabulary 문제)\n\n\n개발환경\n실습을 위해 다음을 설치한다.\n\npip install transformers torch\n\n\n참고로, 다음은 파인튜닝에 사용하는 도구를 보여준다.\n\nTorch: 텐서 계산 및 딥 러닝을 위한 핵심 라이브러리이다.\npeft: 낮은 순위의 적응 기술을 사용하여 대규모 언어 모델을 효율적으로 미세 조정할 수 있다. 특히 리소스가 제한된 장치에서 학습 가능한 매개 변수의 수를 줄여 모델을 압축하고 더 빠르게 미세 조정할 수 있다.\nbitsandbytes: 신경망에 대한 양자화 및 이진화 기술을 제공하여 모델 압축을 지원한다. 모델 압축에 도움이 되므로 메모리와 계산 능력이 제한된 에지 장치에 모델을 보다 실현 가능하게 만들 수 있다.\nTransformers: 대규모 언어 모델 작업을 간소화하여 사전 학습된 모델 및 학습 파이프라인을 제공한다.\ntrl: 대규모 언어 모델의 경우 효율적인 모델 학습 및 최적화에 중점을 둔다.\naccelerate: 다양한 하드웨어 플랫폼에서 학습 및 추론을 가속화한다.\ndataset: 기계 학습 작업을 위한 데이터 세트 로드 및 준비를 간소화한다.\npipeline: 사용자 지정 학습 없이 일반적인 NLP 작업에 대해 사전 학습된 모델의 사용을 간소화한다.\npyarrow: 효율적인 데이터 로드 및 처리를 위해 사용될 수 있다.\nLoraConfig: LoRA 기반 미세 조정을 위한 구성 매개변수를 보유한다.\nSFTTrainer: 모델 학습, 최적화 및 평가를 처리한다.\n\n토큰 사전 \n\nLLM 파인튜닝이나 RAG 시 토큰 사전이 없으면, 제대로 학습되지 않는다. 입력 시퀀스의 토큰이 사전에 없으면, 토큰은 분리된다. 분리된 토큰들은 각자 다른 맥락을 가지도록 학습된다. 다음 코드를 실행하면, 그 내용을 확인할 수 있다. 이런 문제는 유전자 해석 등 다양한 문제에서 발생된다. \nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ntokens = tokenizer.encode('This is a IfcBuilding.', return_tensors='pt')\nprint(\"These are tokens!\", tokens)\nfor token in tokens[0]:\n    print(\"This are decoded tokens!\", tokenizer.decode([token]))\n\n\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\nprint(model.embeddings.word_embeddings(tokens))\nfor e in model.embeddings.word_embeddings(tokens)[0]:\n    print(\"This is an embedding!\", e)\n\n\n다음 코드를 실행해보면, 좀 더 많은 문제점을 확인할 수 있다.\n\nfrom transformers import BertTokenizer, BertModel\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n\nexample_sen = (\n    \"\"\"\n    The United States and Russia sought to lower the temperature in a \n    heated standoff over Ukraine,even as they reported no breakthroughs \n    in high-stakes talks on Friday aimed at preventing a feared Russian invasion\n    \"\"\"\n)\nprint(bert_tokenizer.tokenize(example_sen))\n\n\n결과는 다음과 같다. \n['the', 'united', 'states', 'and', 'russia', 'sought', 'to', 'lower', 'the', 'temperature', 'in', 'a', 'heated', 'stand', '##off', 'over', 'ukraine', ',', 'even', 'as', 'they', 'reported', 'no', 'breakthrough', '##s', 'in', 'high', '-', 'stakes', 'talks', 'on', 'friday', 'aimed', 'at', 'preventing', 'a', 'feared', 'russian', 'invasion']\n\n\n이 경우, 토큰을 추가하거나, 하나로 합치는 것을 고려할 필요가 있다.\n\n\n\n토큰 추가와 임베딩 공간\nBERT를 이용해 토큰 사전과 임베딩을 실습해본다.\n일반적으로 허깅페이스 라이브러리에서 LLM모델에 대한 토큰 추가가 가능한 tokenizer를 제공해 준다. 토큰을 추가하면, 임베딩 차원에 영향을 주므로, 해당 크기를 수정해야 한다. 다음은 이를 고려한 코드를 보여준다. \n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\n# 사전 학습모델 및 토크나이저 로딩\nmodel = AutoModelForCausalLM.from_pretrained('model-name')\n\ntokenizer = AutoTokenizer.from_pretrained('model-name')\n\n\n# 토큰 추가\nnew_tokens = ['newword1', 'newword2']\ntokenizer.add_tokens(new_tokens)\n\n\n# 임베딩 공간 리사이즈 \nmodel.resize_token_embeddings(len(tokenizer))\n\n\n# 추가된 토큰과 함께 파인튜닝. \n# (fine-tuning code here)\n\n\nBPE(Byte Pair Encoding) 토큰 압축\n\nBPE는 바이트 쌍 인코딩(Byte pair Encoding)을 의미하며, 데이터의 가장 일반적인 연속 바이트 쌍을 해당 데이터 내에 발생하지 않는 바이트로 대체하는 데이터 압축 형태이다. 결과 데이터에 대해 프로세스가 반복적으로 반복된다. 자연어 처리(NLP) 및 기계 학습의 맥락에서 BPE는 하위 단어 토큰화 방법으로 사용된다. \n\n\n단어를 보다 관리하기 쉬운 하위 단어나 기호로 분할하여 대규모 어휘를 효율적으로 인코딩할 수 있다. 이 접근 방식은 어휘의 크기를 크게 줄이고 희귀 단어나 OOV(어휘에서 벗어난) 용어를 처리하는 모델의 능력을 향상시킬 수 있다.\n\n\nNLP에 BPE를 적용하는 기본 단계는 다음과 같다.\n\n텍스트를 단어로 나눈 다음 문자로 나누고 각 문자(또는 문자 시퀀스)의 빈도를 계산한다.\n인접한 문자 또는 문자 시퀀스의 가장 빈번한 쌍을 반복적으로 찾아, 이를 새로운 문자 시퀀스로 병합한다.\n원하는 어휘 크기에 도달할 때까지 또는 더 이상 병합으로 인해 어휘 크기가 크게 줄어들 때까지 병합 프로세스를 반복한다.\n\nBPE는 언어 모델링, 텍스트 생성, 특히 BERT(Bidirection Encoder Representations from Transformers)와 같은 변환기 기반 모델과 같은 다양한 NLP 모델 및 작업에 널리 채택되어 광범위한 어휘를 효율적으로 처리하는 데 도움이 된다.\n\n\n다음은 그 예제를 보여준다.\nfrom tokenizers import Tokenizer, models, pre_tokenizers, trainers\n\n\ntokenizer = Tokenizer(models.BPE()) # 토큰화 얻기\n\n\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace() # 사용자 토큰 처리 객체\ndef custom_pre_tokenizer(sequence): # 사용자 토큰 정의\n    # Define rules to combine tokens, e.g., \"new word\" -> \"newword\"\n    combined_sequence = sequence.replace(\"new word\", \"newword\")\n    return combined_sequence\n\n\n# 토큰 훈련. custom pre-tokenizer 활용함.\ntrainer = trainers.BpeTrainer()\ntokenizer.train(files=[\"path/to/training/data.txt\"], trainer=trainer, pre_tokenizer=custom_pre_tokenizer)\n\n\n# 훈련된 토큰 저장\ntokenizer.save(\"path/to/customized_tokenizer.json\")\n\n\n\n임베딩 모델 파인튜닝\n\n다음은 토큰을 추가하고, 임베딩 모델을 파인튜닝하는 보여준다.\n\n\nfrom transformers import BertTokenizerFast, BertModel\nimport torch\nfrom torch import nn\n\n\n# BERT 토크나이저 사전학습모델 로딩\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\nprint(tokenizer.tokenize(\"[CLS] Hello world, how are you?\"))\n\n\nprint(tokenizer.tokenize(\"[newtoken] Hello world, how are you?\"))\ntokenizer.add_tokens(['[newtoken]'])\n\n\n다음과 같이, [newtoken] 토큰 추가 전 테스트. 토큰이 한단어가 아닌 분할 출력된 것 확인\n['[',\n 'newt',\n '##oke',\n '##n',\n ']',\n 'hello',\n 'world',\n ',',\n 'how',\n 'are',\n 'you',\n '?']\n\n\n토큰을 추가하고 다시 토큰화를 한다.\n\ntokenizer.add_tokens(['[newtoken]'])\ntokenizer.tokenize(\"[newtoken] Hello world, how are you?\")\n\n\n제대로 토큰화가 된다. \n['[newtoken]', 'hello', 'world', ',', 'how', 'are', 'you', '?']\n\n\n토큰값을 확인해 본다.\ntokenized = tokenizer(\"[newtoken] Hello world, how are you?\", add_special_tokens=False, return_tensors=\"pt\")\nprint(tokenized['input_ids'])\n\n\ntkn = tokenized['input_ids'][0, 0]\nprint(\"First token:\", tkn)\nprint(\"Decoded:\", tokenizer.decode(tkn))\n\n\n다음과 같이, 토큰값이 잘 할당된 것을 알 수 있다.\n\ntensor([[30522,  7592,  2088,  1010,  2129,  2024,  2017,  1029]])\nFirst token: tensor(30522)\nDecoded: [newtoken]\n\n\n임베딩 모델 학습을 위한 BERT 로딩하고, 앞의 토큰 리스트를 모델에 입력한다.\nmodel = BertModel.from_pretrained('bert-base-uncased')\nprint(model.embeddings)\n\n\ntry:\n    out = model(**tokenized)\n    out.last_hidden_state\nexcept Exception as e:\n    print(e)\n\n\n임베딩 모델이 추가된 토큰을 학습하지 않았으므로, out of range 에러가 출력될 것이다. \n다음 코드로 BERT 모델의토큰 공간 크기를 확인해 본다.\nweights = model.embeddings.word_embeddings.weight.data\nprint(weights.shape)\n\n\n출력은 다음과 같이 30522이다.\n\ntorch.Size([30522, 768])\n\n\n이제 [CLS] 토큰을 임베딩 모델에 추가해보자. \nnew_weights = torch.cat((weights, weights[101:102]), 0)\nnew_emb = nn.Embedding.from_pretrained(new_weights, padding_idx=0, freeze=False)\nprint(new_emb)\n\nUnderstanding BERT | Tokenization (tinkerd.net)\n\n\n다음과 같이 30523으로 토큰 크기가 증가되었다. \n\nEmbedding(30523, 768, padding_idx=0)\n\n\n새 레이어를 모델 마지막에 추가한다.\nmodel.embeddings.word_embeddings = new_emb\nprint(model.embeddings)\n\n\n그 결과로 임베딩 모델의 word_embeddings가 업데이트된다.\n\nBertEmbeddings(\n  (word_embeddings): Embedding(30523, 768, padding_idx=0)\n  (position_embeddings): Embedding(512, 768)\n  (token_type_embeddings): Embedding(2, 768)\n  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\n\n\n앞의 토큰 시퀀스 리스트를 입력한다. 그럼, 제대로 결과가 출력될 것이다.\nout = model(**tokenized)\nprint(out.last_hidden_state)\n\n\n다음 코드를 실행하면, 추가된 모델이 동일한 결과를 가지는 것을 알 수 있다.\nmodel = BertModel.from_pretrained('bert-base-uncased')\nout2 = model(\n    **tokenizer(\"[CLS] Hello world, how are you?\", add_special_tokens=False, return_tensors=\"pt\")\n)\n\n\nout3 = torch.all(out.last_hidden_state == out2.last_hidden_state)\noutput(out3)\n\n\n\n마무리\n\nLLM 파인튜닝이나 RAG 시 학습 데이터에 포함된 토큰에 대한 적절한 사전이 없으면, 제대로 학습되지 않는다. 이 글은 LLM NLP처리의 핵심인 토큰, 임베딩 모델 파인튜닝에 대한 내용을 간략히 다룬다. \n\n\n레퍼런스\n\nEnhance Your GPT Experience: Tiktoken Unveiled — Free Token Counting for Prompts (with Python Code!) | by Ilyes Rezgui | Medium\nhuggingface BertForMaskedLM fine-tuning (ai-bio.info)\nSplit by tokens | 🦜️🔗 LangChain\nhuggingface.co/tomaszki/llama-3-a/raw/main/vocab.json\nTokenTextSplitter\nNLP | How to add a domain-specific vocabulary (new tokens) to a subword tokenizer already trained like BERT WordPiece | by Pierre Guillou | Medium\nFine-tuning a large language model on Kaggle Notebooks (or even on your own computer) for solving real-world tasks (huggingface.co)\nLLaMA 2: A Detailed Guide to Fine-Tuning the Large Language Model | by Gobi Shangar | Medium\nFine-Tuning Large Language Model with Hugging Face & PyTorch | by Tuan Tran | Medium\nInstruction Fine-tuning GEMMA-2B. langchain-ollama/006_gemma_peft_qlora_colab/gemma_finetuning_koalpaca.ipynb at main · tsdata/langchain-ollama (github.com)\nLLAMA3 Model FineTune, KoLLAMA3\nLLAMA2 finetuning #1, #2, \nLLAMA2 custom dataset and fine-tune\nLLAMA2 gradient\nLLAMA2 finetune and llama3 unsloth\nChatGPT train\nMLOps\nLoRA\nPhi-3 colab, tuning in huggingface\nFine-Tuning Phi-3 with Hugging Face - Entreprenerdly\nself-llm/phi-3/Phi-3-mini-4k-Instruct-Lora.ipynb at master · datawhalechina/self-llm (github.com)\nPhi-3CookBook/code/04.Finetuning/Phi-3-finetune-qlora-python.ipynb at main · microsoft/Phi-3CookBook (github.com)",
        "id": "tag:blogger.com,1999:blog-5201956450461596914.post-5784072672755351339",
        "isoDate": "2024-06-20T02:42:00.000Z"
      }
    ]
  },
  {
    "name": "권용진",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권영재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김승호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김병환",
    "category": "개인",
    "posts": []
  },
  {
    "name": "RSS feed for boyism Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성준의 린스타트업과 디자인씽킹",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권혁우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김준형",
    "category": "개인",
    "posts": [
      {
        "creator": "김준형",
        "title": "고소득자는 세금을 많이 내서 존중받아야 한다?!",
        "link": "https://medium.com/@ghilbut/%EA%B3%A0%EC%86%8C%EB%93%9D%EC%9E%90%EB%8A%94-%EC%84%B8%EA%B8%88%EC%9D%84-%EB%A7%8E%EC%9D%B4-%EB%82%B4%EC%84%9C-%EC%A1%B4%EC%A4%91%EB%B0%9B%EC%95%84%EC%95%BC-%ED%95%9C%EB%8B%A4-a1db27fa5086?source=rss-8482dec25ee1------2",
        "pubDate": "Sat, 22 Jun 2024 05:38:57 GMT",
        "content:encodedSnippet": "링크드인에 이름있는 스타트업의 대표가 쓴 글이다. 한국 사회의 단면을 보여준다. 주장의 빈약한 부분들을 짚어보자.\n\n사람은 모두 존중 받아야 한다. 소득과 세금에 따라 차등하는 가치가 아니다. 돈을 냈으니 반드시 존중을 받아내겠다는 자세가 지금 한국에 만연한 갑질 문화의 근원이다. 마이클 샐던의 ‘돈으로 살 수 없는 것들’에서 돈으로 가치 매김 할 수 없는 것에 가격표를 붙이면 부패한다고 했다. 존중은 대체 얼만큼의 고소득 및 세금과 교환할 수 있을까?\n연예인들과 유명인들도 구설수에 오르고 무고를 당한다. 우연히 유명해진 일반인들도 그렇다. 대기업 대표나 임원도 다르지 않다. 유명세에 따라오는 현상일 뿐이다. 단지, 구설수에 오른다고 사람들이 그들을 존중하지 않는다고 일반화 하는 것은 확대 해석의 오류이며, 소득과 세금을 이유로 존중을 강요하는 것은 매우 이상하다. 오히려 그들이 구설수에 오르는 이유는 그 대가를 받기에 충분한 역할과 결과가 있었는지 의문이 들기 때문일 수 있다. 큰 폭의 주가 하락이나 주주가치 훼손, 실적 악화와 대규모 정리해고 와중에도 대주주와 특정 임원들만 높은 연봉과 보너스를 받는다면 구설수에 오를 수 있지 않을까?\n존중은 받는 것이지 달라고 요구하는 것이 아니다. 우리는 한국이 여전히 유전무죄무전유죄의 국가라고 자평한다. 탈세에 관대하고 경제사범의 처벌이 매우 가벼우며 사기에 의한 수익의 환수와 피해의 복구가 불가능하다. 부당경쟁과 담합, 불공정 행위의 과징금이 매우 가벼워 오히려 불법을 장려한다. 땅콩회항, 라면상무, 네이버 직원 자살, 대기업들의 물적 분할에 의한 주식가치 훼손과 소액주주의 피해, 주식회사 임원들의 부도덕한 주식 처분에 의한 임직원들과 주주들의 피해, 유명인들과 부자들의 음주운전에 대한 관대한 처벌, 고가차량 소유주들의 각종 패악질 등 부를 쌓은 자들의 사회적 해악이 매일매일 기사에 오른다. 한줌 권력과 돈만 있으면 무슨 짓이든 해도 된다는 사회 부적응자들이 나날이 늘어난다. 반면에 고소득층의 부정적 인식을 강화하는 사건사고들에 대한 자정작용은 전무하다. 워런 버핏이 빌 게이츠와 같은 부자들에게 기부를 독려하고 기부하지 않는 부자들을 폄훼하는 것과 대조적이다. 이러한 상황에서 고소득자라는 이유만으로 존경심을 보이라는 것은 흡사 멕시코에서 마약상들을 존경하라는 것과 같다. 멕시코 마약 카르텔은 멕시코 경제에 매우 큰 기여를 한다.\n세금은 국가에 내는 지대비용이다. 고소득자가 세금을 더 많이 내는 이유 중에는 국가 인프라에 의한 이익을 더 많이 보았기 때문이 있다. 국가 인프라는 정부의 공공시스템과 도로/항만/공항/전기/수도 등 기간시설들이다. 한국의 가장 중요한 기간 시설들은 박정희 시대에 독일과 사우디 등에 파견되었던 노동자들이 벌어온 외화와 일제 강점기 피해의 대가로 얻어온 일본의 차관, 베트남 참전의 대가 등으로 구축되었다. 즉, 닭이 먼저냐 달걀이 먼저냐를 따지자면 고소득자들은 부모 세대의 피와 땀으로 구축된 인프라를 이용해 돈을 벌어 세금으로 사용료를 지불하는 것이다. 국가는 사용료를 받아 인프라를 유지/보수/확장하고 사회 안전망을 강화한다. 이는 기업의 영리활동과 다르지 않다.\n사회 안전망에 사용되는 세금은 고소득층이 저소득층에 내리는 은혜가 아니다. 사회 안전망이 무너지고 치안이 나빠지면 고소득자의 재산을 목표로 하는 폭력 범죄도 증가한다. 극단적으로 브라질이나 필리핀처럼 무장한 사설 경호원들이 가족들을 24시간 밀착 보호해야 할 수 있다. 그렇다고 사설 경호원이 모든 강력 범죄를 막아내지도 못한다. 필리핀에서 경찰들이 돈을 목적으로 한인 사업가를 납치 및 살해한 사건이 있었다. 심지어 무장 경비업체가 보호하는 타운하우스에서 일어난 일이다. 사회 안전망의 붕괴는 국내에서도 이런 범죄를 가능하게 할 수 있다. 세금을 내서 이런 불상사를 막는 것은 고소득층 자신들을 위한 것이기도 하다.\n전 세계적으로 양극화가 심화 될수록 소득에 따른 거주지 구분이 뚜렷해 진다. 그리고 돈이 많은 지역의 인프라와 편의 시설이 상대적으로 수준 높게 유지되고 개선된다. 한국에서 조사된 바는 없지만, 선진국의 연구에 의하면 고소득층의 환경에 쓰이는 세금이 그들이 내는 세금보다 많다. 즉, 낙후된 지역을 개선하는데는 재개발과 같이 매우 큰 재원이 필요하기 때문에 세금 투입에 큰 장벽이 있는 반면, 고소득층의 환경은 점진적 개선이 용이하기 때문에 꾸준하게 세금이 투입되고 있다. 저소득층의 거주지가 재개발된다고 해도 저소득층에게 이익이 돌아가는 것이 아니라 오히려 그들은 거주지에서 쫒겨나고 쾌적한 환경은 더 부유한 사람들에게 돌아간다. 즉, 고소득자들이 내는 세금은 그들의 주거 환경을 위해 재사용되는 부분이 많다.\n한국은 부동산 정책을 부양하고 금융기업들과 대기업들을 지원하는데 막대한 세금을 쓰고 있다. 이는 고소득층의 수익과 재산을 보전하고 증식하는데 기여한다. 즉, 자신들의 세금은 자신들의 재산을 위해 쓰이고 있다.\n세금에는 재산세와 소득세 같은 직접세도 있지만, 유류와 공산품등에 붙는 간접세도 있다. 일부 경제학자들과 사회학자들에 의하면 직접세와 간접세를 모두 합하면, 고소득자와 고액자산가의 세금이 저소득층과 취약계층에게 충분히 분배되고 있는가에 대해 난색을 표하는 의견들도 있다. 오히려 소비의 상대적 가치를 계산하면 가난할수록 단위 소비에 드는 비용과 세금이 더 비싸다는 주장도 있다. 이런 연구들은 높은 세금을 내는 사람들이 충분히 많은 기여를 한다는 주장을 약화시킨다.\n세금을 통한 부의 재분배는 전적으로 국가의 예산 집행에 달려있다. 정부의 예산 배분에 따라 고소득층의 세금으로 저소득층이 받는 혜택은 미미할 수 있다. 또한, 채권을 통해 적자 재정을 운영한다면 저소득층 지원이 세금에 의한 것인지 국가 채무에 의한 것인지도 모호하다. 따라서 높은 세금에 대한 대가는 저소득층이 아닌 국가에게 주장해야 맞다.\n고소득층이 세금을 많이 내는 만큼 존경받고 싶다면, 부정축재와 부패를 배격하고 그들의 소득이 투명하고 정당한 경제활동을 통해 축적되었다는 이미지를 구축하는 것이 우선이다. 또한, 세금이 부동산 부양이나 포퓰리즘보다는 국가 시스템과 사회 안전망을 위해 올바르게 쓰이도록 정치적인 압력을 행사하는 모습을 보이는 것 또한 필요하다. 그리고 소득과 재산이 많더라도 윤리와 품격을 갖추지 않는 자들은 무리에 어울릴 수 없다는 모습을 보여야 한다. 이러한 품격을 대중이 알게 된다면 없던 존중과 존경도 절로 샘솟을 것이다. 현재의 한국은 무슨 짓을 해도 돈만 많으면 된다는 인식이 확대되고 있기 때문에 고소득자의 이미지도 마냥 긍정적일 수만은 없다.\n\n옳은 일은 꾸준히 해도 주변에서 알기 힘들다. 보통 100을 잘해도 자연스럽게 드러나는 것은 겨우 1 정도 뿐이며, 그마저도 드러나는데 매우 긴 시간이 걸린다. 그리고 존경 받는 사람들은 그 꾸준함의 끝에서 우연히 대중에게 발견되기 때문에 가치가 있는 것이다. 세금을 많이 내기 때문이 아니라…",
        "dc:creator": "김준형",
        "guid": "https://medium.com/p/a1db27fa5086",
        "categories": [
          "noblesse-oblige",
          "wealth",
          "taxes",
          "respect"
        ],
        "isoDate": "2024-06-22T05:38:57.000Z"
      }
    ]
  },
  {
    "name": "강동혁",
    "category": "개인",
    "posts": []
  },
  {
    "name": "고종범",
    "category": "개인",
    "posts": []
  },
  {
    "name": "cheese10yun",
    "category": "개인",
    "posts": []
  },
  {
    "name": "구자철",
    "category": "개인",
    "posts": []
  },
  {
    "name": "FSS",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권동준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김용일",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김도균",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김상훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민석",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김동우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권윤학",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강성훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김만수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "엘키",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권태환",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김슬기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김광현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성빈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김영우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강명훈",
    "category": "개인",
    "posts": [
      {
        "title": "Splunk의 eval과 rex - 4th",
        "link": "https://kangmyounghun.blogspot.com/2024/06/splunk-eval-rex-4th.html",
        "pubDate": "2024-06-23T05:25:00.001Z",
        "author": "강명훈",
        "content": "<div><span style=\"font-family: courier;\">.</span>을 기준으로 첫 번째 문자열을 추출하는 정규표현식.</div><div><br /></div>\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxAzokGS4mQzX9sc7bwFX8S1rEWWROzmoEzRNaxExcyHy0YR-JbTxaOxIvtkAQWG8222PvTUOuUnCOqJR6ErRHgyNo5bP9TIY8KqwxQj2IbOZZzMiCvCy5xlTn11cFbQwrmP-y4eD-IVVotoyE2EG_IAPjmJKXzq7mZG5oGdbYFL2tzslRNvlzZibmtqVF/s1280/regex.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"613\" data-original-width=\"1280\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxAzokGS4mQzX9sc7bwFX8S1rEWWROzmoEzRNaxExcyHy0YR-JbTxaOxIvtkAQWG8222PvTUOuUnCOqJR6ErRHgyNo5bP9TIY8KqwxQj2IbOZZzMiCvCy5xlTn11cFbQwrmP-y4eD-IVVotoyE2EG_IAPjmJKXzq7mZG5oGdbYFL2tzslRNvlzZibmtqVF/s520/regex.png\" width=\"520\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>한 줄 테스트</b></td></tr></tbody></table><div><br /></div><div><span><a name='more'></a></span>rex에서는 잘 동작한다.</div><div><br /></div><div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjILbQ_pVKUvQygEgXFWZVzDYu0_UuPjj37PeXph-RHPIMTdUZFuNfdW2s-4II_ePPAjSAbDHS12vPBedNOSECy94enrnWACEC42kEKKoDMOVMGT7fenwJNvnapJ7JE4A24lCAvuwzlHcX9_cK7pV2v7s21KqvIexLbDFmnXUDMBqAze8ic9oPCCyyGqZ1a/s991/splunk_rex.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"720\" data-original-width=\"991\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjILbQ_pVKUvQygEgXFWZVzDYu0_UuPjj37PeXph-RHPIMTdUZFuNfdW2s-4II_ePPAjSAbDHS12vPBedNOSECy94enrnWACEC42kEKKoDMOVMGT7fenwJNvnapJ7JE4A24lCAvuwzlHcX9_cK7pV2v7s21KqvIexLbDFmnXUDMBqAze8ic9oPCCyyGqZ1a/s520/splunk_rex.png\" width=\"520\" /></a></div><br /><div>그런데 같은 정규표현식이 eval replace 함수에서는 다른 결과를 가져옴.</div></div><br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfc-ZwLVQ2t8YBN2DLQKNb9KTbhCIkpEVH7AWJNlhnOm9wFVz0mUPoJOp0p29CIOvaJaqvQRNR43JzPaFCsrQu3zPXrE-9NwV8mLBdAerDSKIncAS2m88Q4wM1J1wGMP9w8kbo6LSVAhYxAExde2Tg4dB9DuvGXf5JPvHB48SZvBQaCMeV_G01vvpBmGD0/s991/splunk_eval_replace.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"720\" data-original-width=\"991\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfc-ZwLVQ2t8YBN2DLQKNb9KTbhCIkpEVH7AWJNlhnOm9wFVz0mUPoJOp0p29CIOvaJaqvQRNR43JzPaFCsrQu3zPXrE-9NwV8mLBdAerDSKIncAS2m88Q4wM1J1wGMP9w8kbo6LSVAhYxAExde2Tg4dB9DuvGXf5JPvHB48SZvBQaCMeV_G01vvpBmGD0/s520/splunk_eval_replace.png\" width=\"520\" /></a></div><div><br /></div><div>이유는 Message 필드가 단일값이 아닌 다중값이기 때문. 정규표현식이 모든 줄 단위 검색 결과를 가져왔다는 얘기.</div><div><br /></div>\n<div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiB7wdA22j-aXegKk__HCcyjifYdzi7ySAfNdN5SzzQG6EJ5RAtQgn97YYdputJZR9irpM6PgMMM64jz2Oy11Cvpvf2wPnu_dZMUshdSzKNcJdgETgNh0lodaqNnogEjeRLMBiMTaFI7hFS3Bkr2Zm_nh5QNY8YcWIEdj3-uzr2xAsFkjTDfyQMs0FduiDU/s1280/multi_values.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"667\" data-original-width=\"1280\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiB7wdA22j-aXegKk__HCcyjifYdzi7ySAfNdN5SzzQG6EJ5RAtQgn97YYdputJZR9irpM6PgMMM64jz2Oy11Cvpvf2wPnu_dZMUshdSzKNcJdgETgNh0lodaqNnogEjeRLMBiMTaFI7hFS3Bkr2Zm_nh5QNY8YcWIEdj3-uzr2xAsFkjTDfyQMs0FduiDU/s520/multi_values.png\" width=\"520\" /></a></div><div><br /></div><b><span style=\"font-size: x-large;\">rex는 왜 다를까?</span></b><div><br /></div><div>정규표현식은 패턴 일치가 발생하는 순간 검색을 중단하는 게 기본 동작. 일치하는 모든 패턴을 검색하려면 글로벌<span style=\"font-size: x-small;\">(g)</span> 모드를 사용해야 한다. <span style=\"font-size: x-small;\">(글로벌 모드 지원 여부는 환경에 따라 다름)</span></div><div><br /></div>\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgI8YhyiKZmu9Td0LikklKil85u6Jxp46BmWjVI52AhC_cSBqYc_YZZpgnqQVZX8cTonCBIvqId8i_YdPVNZOA22YSVXfoQuigbOFxdrFFkk3qt-0xhyphenhyphenN3wZGRYZdWN9dCtMXFsoHvLrqrZfGOTlekYzy37HoXrACBfxayamWt9XQv4h80wiRjCHctAUaC5/s1105/regex_global.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"720\" data-original-width=\"1105\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgI8YhyiKZmu9Td0LikklKil85u6Jxp46BmWjVI52AhC_cSBqYc_YZZpgnqQVZX8cTonCBIvqId8i_YdPVNZOA22YSVXfoQuigbOFxdrFFkk3qt-0xhyphenhyphenN3wZGRYZdWN9dCtMXFsoHvLrqrZfGOTlekYzy37HoXrACBfxayamWt9XQv4h80wiRjCHctAUaC5/s520/regex_global.png\" width=\"520\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>두 줄 테스트</b></td></tr></tbody></table><br />\n<div>다음은 rex <a href=\"https://docs.splunk.com/Documentation/Splunk/9.2.1/SearchReference/Rex#Optional_arguments\" target=\"_blank\">max_match</a> 제한<span style=\"font-size: x-small;\">(기본값 1: 1회 검색)</span>을 해제한 결과. 정규표현식에서 글로벌 모드를 사용한 결과, eval replace 함수와 같은 결과를 보여준다.&nbsp;</div><div><br /></div><div>\n<div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEpXKQJKK0I7BFkSsmu6_idtmk7hCJTtt7ivGLF_dT2Omg3qx6-59jOELEfE7ce-cqSY8OcPYCgx1odgl6EJAucTIjsACp8Z78o0FL9C-SUV48DBRh8Zx2tFr43_2h-bqIiaHC4x_P9HkSSWpBonkCIlGHV8JCnKK93eTHpwzTWOn0JC54MpdfHRZ6_jHj/s988/splunk_rex_max_match.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"720\" data-original-width=\"988\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEpXKQJKK0I7BFkSsmu6_idtmk7hCJTtt7ivGLF_dT2Omg3qx6-59jOELEfE7ce-cqSY8OcPYCgx1odgl6EJAucTIjsACp8Z78o0FL9C-SUV48DBRh8Zx2tFr43_2h-bqIiaHC4x_P9HkSSWpBonkCIlGHV8JCnKK93eTHpwzTWOn0JC54MpdfHRZ6_jHj/s520/splunk_rex_max_match.png\" width=\"520\" /></a></div><br />\n<div>eval replace 함수는 글로벌 모드를 기본 사용한다는 얘기. 그런데 이걸 조절하는 옵션은 따로 제공하지 않는다. 정규표현식이 줄 구분을 무시하도록 <a href=\"https://www.regular-expressions.info/modifiers.html\" target=\"_blank\">한 줄<span style=\"font-size: x-small;\">(single line)</span> 검색 모드</a> 적용.</div><div><br /></div>\n<div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8xvPtLe10yw46YkxldZJU-rqxqN_THWDJW-xw3Wxzh4yG6seEMWRoMU1HyLpaS1RBkT4O3H2_34Tqg37tEMwG9rGGVddLBSHXLwc6dPilNnqs1NpyzadFsh67veujJEbr6oSYH06dIZvVVYL8o6vhbYD5SkFte5tKXIKDwXC5iSxEb7DviFAiLwoZPQbF/s960/splunk_eval_replace_single_line.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"720\" data-original-width=\"960\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8xvPtLe10yw46YkxldZJU-rqxqN_THWDJW-xw3Wxzh4yG6seEMWRoMU1HyLpaS1RBkT4O3H2_34Tqg37tEMwG9rGGVddLBSHXLwc6dPilNnqs1NpyzadFsh67veujJEbr6oSYH06dIZvVVYL8o6vhbYD5SkFte5tKXIKDwXC5iSxEb7DviFAiLwoZPQbF/s520/splunk_eval_replace_single_line.png\" width=\"520\" /></a></div><div><br /></div>\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifN0DRpbgIEBOVpJRsvdscGnb5GR-SkqDoFnGJRD5w6hRM8OD0skGDy5oEKW0vqBXq9H8NfEeMDxJQUhsUR6Im5BlRDrwrsHiBUZyvW-ckRtw7Jlhq0eJ57e8EvMmnjnTyAXmFXTxgPAMiPEv3xTblPtMCFVD7nCENY-6GIC5uCiCcpl3HdSleMBTZg64p/s1280/regex_single_line.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"616\" data-original-width=\"1280\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifN0DRpbgIEBOVpJRsvdscGnb5GR-SkqDoFnGJRD5w6hRM8OD0skGDy5oEKW0vqBXq9H8NfEeMDxJQUhsUR6Im5BlRDrwrsHiBUZyvW-ckRtw7Jlhq0eJ57e8EvMmnjnTyAXmFXTxgPAMiPEv3xTblPtMCFVD7nCENY-6GIC5uCiCcpl3HdSleMBTZg64p/s520/regex_single_line.png\" width=\"520\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>두 줄 테스트</b></td></tr></tbody></table><div><br /></div><div>참고로 rex는 이렇게 해도 된다. 1회 검색이 기본이라 첫 번째 줄만 검색하기 때문.</div><div><br /></div>\n<div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjD9nOQAhoEDbROg2eZ2IAnaKOz_AOWbCNtoNK0SQvYXaN4qS1DPzuyIUI88781zi9MEEuQlgow-G7IMOnHkpF6haqc22GR7lweKatcVm-bj6U16TD8ZmcKmn40mwpsjOd2xJc0ACkQz1aeBbmvMiHCfPNPorBdw6ZwgX3VBsKTCub97gQBlDOxvlovD9aP/s905/splunk_rex2.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"720\" data-original-width=\"905\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjD9nOQAhoEDbROg2eZ2IAnaKOz_AOWbCNtoNK0SQvYXaN4qS1DPzuyIUI88781zi9MEEuQlgow-G7IMOnHkpF6haqc22GR7lweKatcVm-bj6U16TD8ZmcKmn40mwpsjOd2xJc0ACkQz1aeBbmvMiHCfPNPorBdw6ZwgX3VBsKTCub97gQBlDOxvlovD9aP/s520/splunk_rex2.png\" width=\"520\" /></a></div><div><br /></div><div><div><b>관련 글</b></div><div><ul><li><a href=\"https://kangmyounghun.blogspot.com/2024/05/splunk-eval-rex-3rd.html\">Splunk의 eval과 rex - 3rd</a></li><li><a href=\"https://kangmyounghun.blogspot.com/2021/08/splunk-eval-rex.html\">Splunk의 eval과 rex</a></li><li><a href=\"https://kangmyounghun.blogspot.com/2024/02/splunk_25.html\">Splunk의 조건문</a></li></ul></div></div></div>",
        "contentSnippet": ".을 기준으로 첫 번째 문자열을 추출하는 정규표현식.\n\n\n\n\n한 줄 테스트\n\n\n\nrex에서는 잘 동작한다.\n\n\n\n\n그런데 같은 정규표현식이 eval replace 함수에서는 다른 결과를 가져옴.\n\n\n\n\n이유는 Message 필드가 단일값이 아닌 다중값이기 때문. 정규표현식이 모든 줄 단위 검색 결과를 가져왔다는 얘기.\n\n\n\n\nrex는 왜 다를까?\n\n정규표현식은 패턴 일치가 발생하는 순간 검색을 중단하는 게 기본 동작. 일치하는 모든 패턴을 검색하려면 글로벌(g) 모드를 사용해야 한다. (글로벌 모드 지원 여부는 환경에 따라 다름)\n\n\n\n\n두 줄 테스트\n\n\n다음은 rex max_match 제한(기본값 1: 1회 검색)을 해제한 결과. 정규표현식에서 글로벌 모드를 사용한 결과, eval replace 함수와 같은 결과를 보여준다. \n\n\n\n\neval replace 함수는 글로벌 모드를 기본 사용한다는 얘기. 그런데 이걸 조절하는 옵션은 따로 제공하지 않는다. 정규표현식이 줄 구분을 무시하도록 한 줄(single line) 검색 모드 적용.\n\n\n\n\n\n\n\n두 줄 테스트\n\n\n\n참고로 rex는 이렇게 해도 된다. 1회 검색이 기본이라 첫 번째 줄만 검색하기 때문.\n\n\n\n\n\n관련 글\n\nSplunk의 eval과 rex - 3rd\nSplunk의 eval과 rex\nSplunk의 조건문",
        "id": "tag:blogger.com,1999:blog-2597780270996323853.post-4087014396549553775",
        "isoDate": "2024-06-23T05:25:00.001Z"
      }
    ]
  },
  {
    "name": "김민장",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김덕홍",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성중",
    "category": "개인",
    "posts": [
      {
        "title": "데이터 중심 애플리케이션 설계(Designing Data-Intensive Applications)",
        "link": "https://sungjk.github.io/2024/06/22/ddia.html",
        "pubDate": "2024-06-22T00:00:00+00:00",
        "content": "\n            \n            &lt;h2 id=&quot;1장-신뢰할-수-있고-확장-가능하며-유지보수하기-쉬운-애플리케이션&quot;&gt;1장. 신뢰할 수 있고 확장 가능하며 유지보수하기 쉬운 애플리케이션&lt;/h2&gt;\n\n&lt;p&gt;p.6&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;신뢰성(Reliability)&lt;/strong&gt;\n하드웨어나 소프트웨여 결함, 심지어 인적 오류(human error) 같은 역경에 직면하더라도 시스템은 지속적으로 올바르게 동작(원하는 성능 수준에서 정확한 기능을 수행)해야 한다.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;확장성(Scalability)&lt;/strong&gt;\n시스템의 데이터 양, 트래픽 양, 복잡도가 증가하면서 이를 처리할 수 있는 적절한 방법이 있어야 한다.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;유지보수성(Maintainability)&lt;/strong&gt;\n시간이 지남에 따라 여러 다양한 사람들이 시스템 상에서 작업(현재 작업을 유지보수하고 새로운 사용 사례를 시스템에 적용하는 엔지니어링과 운영)할 것이기 때문에 모든 사용자가 시스템 상에서 생산적으로 작업할 수 있게 해야 한다.&lt;/p&gt;\n\n&lt;p&gt;p.6-7&lt;/p&gt;\n\n&lt;p&gt;소프트웨어의 일반적인 기대치&lt;/p&gt;\n&lt;ul&gt;\n  &lt;li&gt;애플리케이션 사용자가 거대한 기능을 수행한다.&lt;/li&gt;\n  &lt;li&gt;시스템은 사용자가 범한 실수나 예상치 못한 소프트웨어 사용법을 허용할 수 있다.&lt;/li&gt;\n  &lt;li&gt;시스템 성능은 예상된 부하와 데이터 양에서 필수적인 사용 사례를 충분히 만족한다.&lt;/li&gt;\n  &lt;li&gt;시스템은 허가되지 않은 접근과 오남용을 방지한다.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;잘못될 수 있는 일을 결함(fault)이라 부른다. 그리고 결함을 예측하고 대처할 수 있는 시스템을 내결함성(fault-tolerant) 또는 탄력성(resilient)을 지녔다고 말한다.&lt;/p&gt;\n\n&lt;p&gt;결함은 장애(failure)와 동일하지 않다. 일반적으로 결함은 사양에서 벗어난 시스템의 한 구성 요소로 정의되지만, 장애는 사용자에게 필요한 서비스를 제공하지 못하고 시스템 전체가 멈춘 경우다. … 대개 결함으로 인해 장애가 발생하지 않게끔 내결함성 구조를 설계하는 것이 가장 좋다.&lt;/p&gt;\n\n&lt;p&gt;p.9-10&lt;/p&gt;\n\n&lt;p&gt;대규모 인터넷 서비스에 대한 한 연구에 따르면 운영자의 설정 오류가 중단의 주요원인인 반면 하드웨어(서버나 네트워크) 결함은 중단 원인의 10~25% 정도에 그친다.&lt;/p&gt;\n&lt;ul&gt;\n  &lt;li&gt;오류의 가능성을 최소화하는 방향으로 시스템을 설계하라&lt;/li&gt;\n  &lt;li&gt;사람의 실수로 장애가 발생할 수 있는 부분을 분리하라&lt;/li&gt;\n  &lt;li&gt;모든 수준에서 철저하게 테스트하라&lt;/li&gt;\n  &lt;li&gt;인적 오류를 빠르고 쉽게 복구할 수 있게 하라&lt;/li&gt;\n  &lt;li&gt;모니터링 대책을 마연하라&lt;/li&gt;\n  &lt;li&gt;조작 교육과 실슴을 시행하라&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;p.11\n부하는 &lt;strong&gt;부하 매개변수(load paramter)&lt;/strong&gt;라 부르는 몇 개의 숫자로 나타낼 수 있다. … 부하 매개변수로 웹 서버의 초당 요청 수, 데이터베이스의 읽기 대 쓰기 비율, 대화방의 동시 활성 사용자(active user), 캐시 적중률 등이 될 수 있다.&lt;/p&gt;\n\n&lt;p&gt;p.19 유지보수성&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;운용성(operability)&lt;/strong&gt;\n운영팀이 시스템을 원활하게 운영할 수 있게 쉽게 만들어라.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;단순성(Simplicity)&lt;/strong&gt;\n시스템에서 복잡도를 최대한 제거해 새로운 엔지니어가 시스템을 이해하기 쉽게 만들어라(사용자 인터페이스의 단순성과는 다르다는 점에 유의하라).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;발전성(Evolvability)&lt;/strong&gt;\n엔지니어가 이후에 시스템을 쉽게 변경할 수 있게 하라. 그래야 요구사항 변경 같은 예기치 않은 사용 사례를 적용하기 쉽다. 이 속성은 유연성(extensibility), 수정 가능성(modifiability), 적응성(plasticity)으로 알려져 있다.&lt;/p&gt;\n\n&lt;p&gt;p.20 좋은 운영성&lt;/p&gt;\n&lt;ul&gt;\n  &lt;li&gt;좋은 모니터링으로 런타임(runtime) 동작과 시스템의 내부에 대한 가시성 제공&lt;/li&gt;\n  &lt;li&gt;표준 도구를 이용해 자동화와 통합을 위한 우수한 지원을 제공&lt;/li&gt;\n  &lt;li&gt;개별 장비 의존성을 회피. 유지보수를 위해 장비를 내리더라도 시스템 전체에 영향을 주지 않고 계속해서 운영 가능해야 함&lt;/li&gt;\n  &lt;li&gt;좋은 문서와 이해하기 쉬운 운영 모델(예를 들어 “X를 하면 Y가 발생한다”) 제공&lt;/li&gt;\n  &lt;li&gt;만족할 만한 기본 동작을 제공하고, 필요할 때 기본값을 다시 정의할 수 있는 자유를 관리자에게 부여&lt;/li&gt;\n  &lt;li&gt;적절하게 자기 회복(self-healing)이 가능할 뿐 아니라 필요에 따라 관리자가 시스템 상태를 수동으로 제어할 수 있게 함&lt;/li&gt;\n  &lt;li&gt;예측 가능하게 동작하고 예기치 않은 상황을 최소화함&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n\n&lt;h2 id=&quot;2장-데이터-모델과-질의-언어&quot;&gt;2장. 데이터 모델과 질의 언어&lt;/h2&gt;\n\n&lt;p&gt;p.32 지역성(locality). JSON 표현에서는 모든 관련 정보가 한 곳에 있어 질의 하나로 충분하다.&lt;/p&gt;\n\n&lt;p&gt;p.40 쓰기 스키마(schema-on-write)(관계형 데이터베이스의 전통적인 접근 방식으로 스키마는 명시적이고 데이터 베이스는 쓰여진 모든 데이터가 스키마를 따르고 있음을 보장한다)와 반대되는 읽기 스키마(schema-on-read)(데이터 구조는 암묵적이고 데이터를 읽을 때만 해석된다)&lt;/p&gt;\n\n&lt;p&gt;p.43 선언형 질의\nSQL이나 관계 대수 같은 선언형 질의 언어에서는 목표를 달성하기 위한 방법이 아니라,  알고자 하는 데이터의 패턴, 즉 결과가 충족해야 하는 조건과 데이터를 어떻게 변환(예를 들어 정렬, 그룹화, 집계)할지를 지정하기만 하면 된다.&lt;/p&gt;\n\n&lt;hr /&gt;\n\n&lt;h2 id=&quot;3장-저장소와-검색&quot;&gt;3장: 저장소와 검색&lt;/h2&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p.72 특정 작업부(workload) 유형에서 좋은 성능을 내게끔 저장소 엔진을 조정하려면 저장소 엔진이 내부에서 수행되는 작업에 대해 대략적인 개념을 이해할 필요가 있다.&lt;/li&gt;\n  &lt;li&gt;p.74 색인을 잘 선택했다면 읽기 질의 속도가 향상된다. 하지만 모든 색인은 쓰기 속도를 떨어뜨린다.&lt;/li&gt;\n  &lt;li&gt;p.74 키를 데이터 파일의 바이트 오프셋에 매핑해 인메모리 해시 맵을 유지하는 전략이다.&lt;/li&gt;\n  &lt;li&gt;p.74 값을 조회하려면 해시 맵을 사용해 데이터 파일에서 오프셋을 찾아 해당 위치를 구하고 값을 읽는다.&lt;/li&gt;\n  &lt;li&gt;p.75 컴팩션은 로그에서 중복된 키를 버리고 각 키의 최신 갱신 값만 유지하는 것을 의미한다.&lt;/li&gt;\n  &lt;li&gt;p.77 불행하게도 디스크 상의 해시 맵에 좋은 성능을 기대하기란 어렵다. 이는 무작위 접근 I/O가 많이 필요하고 디스크가 가득 찼을 때 확장하는 비용이 비싸며 해시 충돌 해소를 위해 성가신 로직이 필요하다.&lt;/li&gt;\n  &lt;li&gt;p.78 해시 테이블은 범위 질의(range query)에 효율적이지 않다.&lt;/li&gt;\n  &lt;li&gt;p.79 그래도 handbag과 handsome 키의 오프셋을 알고 있고 정렬돼 있으므로 handiwork는 두 키 사이에 있다는 사실을 알 수 있다.&lt;/li&gt;\n  &lt;li&gt;p.80 저장소 엔진\n    &lt;ul&gt;\n      &lt;li&gt;쓰기가 들어오면 인메모리 균형 트리(balanced tree) 데이터 구조(예를 들어 레드 블랙 트리)에 추가한다. 이 인메모리 트리는 멤테이블(memtable)이라고도 한다.&lt;/li&gt;\n      &lt;li&gt;멤테이블이 보통 수 메가바이트 정도의 임곗값보다 커지면 SS테이블 파일로 디스크에 기록한다. 트리가 이미 키로 정렬된 키-값 쌍을 유지하고 있기 때문에 효율적으로 수행할 수 있다. 새로운 SS테이블 파일은 데이터베이스의 가장 최신 세그먼트가 된다. SS테이블을 디스크에 기록하는 동안 쓰기는 새로운 멤테이블 인스턴스에 기록한다.&lt;/li&gt;\n      &lt;li&gt;읽기 요청을 제공하려면 먼저 멤테이블에서 키를 찾아야 한다. 그다음 디스크 상의 가장 최신 세그먼트에서 찾는다. 그다음으로 두 번째 오래된 세그먼트, 세 번째 오래된 세그먼트 등에서 찾는다.&lt;/li&gt;\n      &lt;li&gt;가끔 세그먼트 파일을 합치고 덮어 쓰여지거나 삭제된 값을 버리는 병합과 컴팩션 과정을 수행한다. 이 과정은 백그라운드에서 수행된다.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;p.80 정렬된 파일 병합과 컴팩션 원리를 기반으로 하는 저장소 엔진을 LSM 저장소 엔진이라 부른다.&lt;/li&gt;\n  &lt;li&gt;p.81 키를 단어(용어)로, 값은 단어를 포함한 모든 문서의 ID 목록(포스팅 목록(postings list))으로 하는 키-값 구조로 구현한다.&lt;/li&gt;\n  &lt;li&gt;p.81 블룸 필터는 키가 데이터베이스에 존재하지 않음을 알려주므로 존재하지 않는 키를 위한 불필요한 디스크 읽기를 많이 절약할 수 있다.&lt;/li&gt;\n  &lt;li&gt;p.81 백그라운드에서 연쇄적으로 SS테이블을 지속적으로 병합하는 것이다. 이 개념은 데이터셋이 가능한 메모리보다 훨씬 더 크더라도 여전히 효과적이다. 데이터가 정렬된 순서로 저장돼 있다면 범위 질의를 효율적으로 실행할 수 있다.&lt;/li&gt;\n  &lt;li&gt;p.82 B 트리는 전통적으로 4KB 크기(때로는 더 큰)의 고정 크기 블록이나 페이지로 나누고 한 번에 하나의 페이지에 읽기 또는 쓰기를 한다. 디스크가 고정 크기 블록으로 배열되기 때문에 이런 설계는 근본적으로 하드웨어와 조금 더 밀접한 관련이 있다.&lt;/li&gt;\n  &lt;li&gt;p.83 새로운 키를 수용한 페이지에 충분한 여유 공간이 없다면 페이지 하나를 반쯤 채워진 페이지 둘로 나누고 상위 페이지가 새로운 키 범위의 하위 부분들을 알 수 있게 갱신한다.&lt;/li&gt;\n  &lt;li&gt;p.99 칼럼 지향 저장소. 모든 값을 하나의 로우에 함께 저장하지 않는 대신 각 칼럼별로 모든 값을 함께 저장한다. 각 칼럼을 개별 파일에 저장하면 질의에 사용되는 칼럼만 읽고 구분 분석하면 된다. 이 방식을 사용하면 작업량이 많이 줄어든다.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n\n&lt;h2 id=&quot;05장-복제&quot;&gt;05장: 복제&lt;/h2&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p.156 동기식 복제의 장점은 팔로워가 리더와 일관성 있게 최신 데이터 복사본을 가지는 것을 보장한다. 갑자기 리더가 작동하지 않아도 데이터는 팔로워에서 계속 사용할 수 있음을 확신할 수 있다. 단점은 (팔로워가 죽거나 네트워크 문제나 다른 어떤 이유로 인해) 동기 팔로워가 응답하지 않는다면 쓰기가 처리될 수 없다는 것이다. 리더는 모든 쓰기를 차단(block)하고 동기 복제 서버가 다시 사용할 수 있을때까지 기다려야 한다.&lt;/li&gt;\n  &lt;li&gt;p.159 팔로워 중 하나를 새로운 리더로 승격해야 하고 클라이언트는 새로운 리더로 쓰기를 전송하기 위해 재설정이 필요하며 다른 팔로워는 새로운 리더로부터 데이터 변경을 소비하기 시작해야 한다. 이 과정을 장애 복구(failover)라 한다.&lt;/li&gt;\n  &lt;li&gt;p.167 Monotonic Read: 각 사용자의 읽기가 항상 동일한 복제 서버에서 수행되게끔 하는 것&lt;/li&gt;\n  &lt;li&gt;p.168 Consistent Prefix Read: 일련의 쓰기가 특정 순서로 발생한다면 이 쓰기를 읽는 모든 사용자는 같은 순서로 쓰여진 내용을 보게 됨을 보장한다.&lt;/li&gt;\n  &lt;li&gt;p.174 충돌을 처리하는 제일 간단한 전략은 충돌을 피하는 것이다. 특정 레코드의 모든 쓰기가 동일한 리더를 거치도록 애플리케이션이 보장한다면 충돌은 발생하지 않는다.&lt;/li&gt;\n  &lt;li&gt;p.175 Automatic Conflict Resolution\n    &lt;ul&gt;\n      &lt;li&gt;&lt;a href=&quot;https://assets.amazon.science/ac/1d/eb50c4064c538c8ac440ce6a1d91/dynamo-amazons-highly-available-key-value-store.pdf&quot;&gt;Dynamo: Amazon’s Highly Available Key-value Store&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;p.179 Dynamo Style: 리더의 개념을 버리고 모든 복제 서버가 클라이언트로부터 쓰기를 직접 받을 수 있게 허용하는 접근 방식&lt;/li&gt;\n  &lt;li&gt;p.180 Outdated 해결: 클라이언트가 데이터베이스에서 읽을 때 하나의 복제 서버로 요청을 보내지 않고 읽기 요청을 병렬로 여러 노드에 전송. 클라이언트는 여러 노드에서 다른 응답을 받음. 즉, 한 노드에서는 최신 값을 받고 다른 노드에서는 오래된 값을 받음. 이 때 버전 숫자를 사용해 어떤 값이 최신 내용인지 결정.&lt;/li&gt;\n  &lt;li&gt;p.180 Anti-entropy\n    &lt;ul&gt;\n      &lt;li&gt;&lt;a href=&quot;https://medium.com/@adityashete009/anti-entropy-and-merkel-trees-amazon-dynamodb-part-4-efbf1f7285c0&quot;&gt;Anti-Entropy and Merkel Trees: Amazon DynamoDB (Part 4)&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=&quot;https://www.hemantkgupta.com/p/insights-from-paper-part-ii-dynamo&quot;&gt;Insights from paper (Part II) — Dynamo: Amazon’s Highly Available Key-value Store&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=&quot;https://efficientcodeblog.wordpress.com/2017/12/26/read-repair-and-anti-entropy-two-ways-to-remedy-replication-lag-in-dynamo-style-datastores-leaderless-replication/&quot;&gt;Read Repair and Anti-Entropy : Two Ways To Remedy Replication Lag in Dynamo-style Datastores (Leaderless Replication)&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;p.188. Concurrent: 한 작업이 다른 작업 이전에 발생했는지가 동시성의 의미를 정의하는 핵심. 사실 작업이 다른 작업보다 먼저 발생하지 않으면 (즉 어느 작업도 다른 작업에 대해 알지 못한다면) 단순히 동시 작업이라 말한다.\n    &lt;ul&gt;\n      &lt;li&gt;동시성을 정의하기 위해 정확한 시각은 중요하지 않다. 두 작업이 발생한 물리적인 시각보다 각 작업이 서로 알지 못하면 단순히 두 작업은 동시에 수행됐다 말한다.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n\n&lt;h2 id=&quot;06장-파티셔닝&quot;&gt;06장: 파티셔닝&lt;/h2&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p199 데이터셋이 매우 크거나 질의 처리량이 매우 높다면 복제만으로는 부족하고 데이터를 파티션으로 쪼갤 필요가 있다. 이 작업을 샤딩이라고도 한다.&lt;/li&gt;\n  &lt;li&gt;p200 파티셔닝하는 주된 이유는 확장성.  단일 파티션에 실행되는 질의를 생각해 보면 각 노드에서 자신의 파티션에 해당하는 질의를 독립적으로 실행할 수 있으므로 노드를 추가함으로써 질의 처리량을 늘릴 수 있다. 크고 복잡한 질의는 훨씬 더 어렵기는 하지만 여러 노드에서 병렬 실행이 가능하다.&lt;/li&gt;\n  &lt;li&gt;p203 키 범위 기준 파티셔닝은 특정한 접근 패턴이 핫스팟을 유발하는 단점이 있다.&lt;/li&gt;\n  &lt;li&gt;p203 좋은 해시 함수는 쏠린 데이터를 입력으로 받아 균일하게 분산되게 한다.&lt;/li&gt;\n  &lt;li&gt;p204 파티셔닝에 키의 해시값을 사용해서 파티셔닝하면 키 범위 파티셔닝의 좋은 속성을 잃어 버린다. 바로 범위 질의를 효율적으로 실행할 수 있는 능력이다. 전에는 인접했던 키들이 이제는 모든 파티션에 흩어져서 정렬 순서가 유지되지 않는다.&lt;/li&gt;\n  &lt;li&gt;p204 복합 키의 첫 번째 컬럼에 값 범위로 검색하는 질의를 쓸 수 없지만,  첫 번째 칼럼에 고정된 값을 지정하면 키의 다른 컬럼에 대해서는 범위 스캔 가능&lt;/li&gt;\n  &lt;li&gt;Partitioning Secondary Indexes by Document:\n    &lt;ul&gt;\n      &lt;li&gt;p207 각 파티션이 독립적. 자신의 보조 인덱스를 유지하며 그 파티션에 속하는 문서만 담당. 다른 파티션에 어떤 데이터가 저장되는지는 신경 쓰지 않음.&lt;/li&gt;\n      &lt;li&gt;p207 보조 인덱스를 써서 읽는 질의는 큰 비용 발생. 여러 파티션에서 질의를 병렬 실행하더라도 꼬리 지연 시간 증폭이 발생하기 쉬움.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Partitioning Secondary Indexes by Term:\n    &lt;ul&gt;\n      &lt;li&gt;p208 모든 파티션의 데이터를 담당하는 전역 색인을 만들 수도 있음&lt;/li&gt;\n      &lt;li&gt;p208 용어 자체로 파티셔닝하면 범위 스캔에 유용함.  반면 용어의 해시값을 사용해 파티셔닝하면 부하가 좀 더 고르게 분산됨.&lt;/li&gt;\n      &lt;li&gt;p208 읽기 효율적.  쓰기가 느리고 복잡함.&lt;/li&gt;\n      &lt;li&gt;p209 대개 비동기로 갱신됨.  쓰기를 실행한 후 바로 인덱스를 읽으면 변경 사항이 반영되지 않을 수도 있다.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Rebalancing Partitions:\n    &lt;ul&gt;\n      &lt;li&gt;p209 리밸런싱:  클러스터에서 한 노드가 담당하던 부하를 다른 노드로 옮기는 과정&lt;/li&gt;\n      &lt;li&gt;p210 mod 연산 쓰지 마라.  리밸런싱 비용 지나치게 커지고, 데이터를 필요 이상으로 이동하지 않는 방법 필요.&lt;/li&gt;\n      &lt;li&gt;p211 파티션 개수 고정:  파티션이 너무 크면 리밸런싱을 실행할 때와 노드 장애로부터 복구 비용이 큼.  그러나 파티션이 너무 작으면 오버헤드가 너무 커짐.&lt;/li&gt;\n      &lt;li&gt;p212 동적 파티셔닝:  파티션 개수가 전체 데이터 용량에 맞춰 조정되는 이점.  파티션 개수가 데이터셋 크기에 비례.  데이터 양이 작으면 파티션 개수가 적어도 되므로 오버헤드 적다.   HBase, MongoDB는 빈 디비에 초기 파티션 집합 설정(pre-splitting)&lt;/li&gt;\n      &lt;li&gt;p213 노드 비례 파티셔닝:  노드 대수가 변함 없는 동안은 개별 파티션 크기가 데이터셋 크기에 비례.  노드 대수 늘리면 파티션 크기 다시 작아짐.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;카산드라\n    &lt;ul&gt;\n      &lt;li&gt;https://d2.naver.com/helloworld/1039&lt;/li&gt;\n      &lt;li&gt;https://tjddnjs.tistory.com/91&lt;/li&gt;\n      &lt;li&gt;p.213 리밸런싱 알고리즘 &lt;a href=&quot;https://www.datastax.com/blog/new-token-allocation-algorithm-cassandra-30&quot;&gt;New token allocation algorithm in Cassandra 3.0&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;p.216 가십 프로토콜  &lt;a href=&quot;https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/architecture/archGossipAbout.html&quot;&gt;Apache Cassandra™ 3.x - Internode communications (gossip)&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n\n&lt;h2 id=&quot;07장-트랜잭션&quot;&gt;07장: 트랜잭션&lt;/h2&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p222 트랜잭션은 전체가 성공(commit)하거나 실패(abort, rollback).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h4 id=&quot;p223-acid&quot;&gt;p223 ACID&lt;/h4&gt;\n\n&lt;p&gt;&lt;strong&gt;Atomicity&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;Not\n    &lt;ul&gt;\n      &lt;li&gt;In the context of ACID, atomicity is &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; about concurrency.&lt;/li&gt;\n      &lt;li&gt;It does not describe what happens if several processes try to access the same data at the same time, because that is covered under the letter &lt;em&gt;I&lt;/em&gt;, for &lt;em&gt;&lt;strong&gt;isolation&lt;/strong&gt;.&lt;/em&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;여러 쓰기 작업이 하나의 원자적인 트랜잭션으로 묶여 있는데 결함 때문에 완료(commit)될 수 없다면 abort되고 데이터베이스는 이 트랜잭션에서 지금까지 실행한 쓰기를 무시하거나 취소(undo) 해야 한다.&lt;/li&gt;\n  &lt;li&gt;Perhaps &lt;strong&gt;&lt;em&gt;abortability&lt;/em&gt;&lt;/strong&gt; would have been a better term than &lt;em&gt;&lt;strong&gt;atomicity&lt;/strong&gt;.&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Consistency&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;The idea of ACID consistency is that you have certain statements about your data (&lt;strong&gt;&lt;em&gt;invariants&lt;/em&gt;&lt;/strong&gt;) that must always be true.&lt;/li&gt;\n  &lt;li&gt;it’s the application’s responsibility to define its transactions correctly so that they preserve consistency.&lt;/li&gt;\n  &lt;li&gt;The letter C doesn’t really belong in ACID.\n    &lt;ul&gt;\n      &lt;li&gt;&lt;strong&gt;Atomicity&lt;/strong&gt;, &lt;strong&gt;Isolation&lt;/strong&gt;, &lt;strong&gt;Durability&lt;/strong&gt; are properties of the database, whereas &lt;strong&gt;Consistency&lt;/strong&gt; (in the ACID sense) is a property of the application.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Isolation&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;In the sense of ACID means that concurrently executing transactions are isolated from each other: they cannot step on each other’s toes.&lt;/li&gt;\n  &lt;li&gt;The database ensures that when the transactions have committed, the result is the same as if they had run &lt;strong&gt;&lt;em&gt;serially&lt;/em&gt;&lt;/strong&gt; (one after another), even though in reality they may have run concurrently.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/07-transaction-race-conditon.png&quot; alt=&quot;transaction-race-conditon&quot; title=&quot;transaction-race-conditon&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Durability&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;any data it has written will not be forgotten.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Single-Object and Multi-Object Operations&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;&lt;strong&gt;atomicity&lt;/strong&gt; and &lt;strong&gt;isolation&lt;/strong&gt; describe what the database should do if a client makes several writes within the same transaction.\n    &lt;ul&gt;\n      &lt;li&gt;&lt;strong&gt;&lt;em&gt;Atomicity:&lt;/em&gt;&lt;/strong&gt;  If an error occurs halfway through a sequence of writes, the transaction should be aborted, and the writes made up to that point should be discarded. In other words, the database saves you from having to worry about partial failure, by giving an all-or-nothing guarantee.&lt;/li&gt;\n      &lt;li&gt;&lt;em&gt;&lt;strong&gt;Isolation&lt;/strong&gt;:&lt;/em&gt;  Concurrently running transactions shouldn’t interfere with each other. For example, if one transaction makes several writes, then another transaction should see either all or none of those writes, but not some subset.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;A transaction is usually understood as a mechanism for grouping multiple operations on multiple objects into one unit of execution.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Handling errors and aborts&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;&lt;strong&gt;best effort&lt;/strong&gt;:  “the database will do as much as it can, and if it runs into an error, it won’t undo something it has already done”.  so it’s the application’s responsibility to recover from errors.&lt;/li&gt;\n  &lt;li&gt;the whole point of aborts is to enable safe retries.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;p232-weak-isolation-levels&quot;&gt;p232 &lt;strong&gt;Weak Isolation Levels&lt;/strong&gt;&lt;/h3&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;&lt;strong&gt;&lt;em&gt;serializable&lt;/em&gt; isolation&lt;/strong&gt;:  the database guarantees that transactions have the same effect as if they ran &lt;em&gt;serially&lt;/em&gt; (i.e., one at a time, without any concurrency).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Read Committed&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n  &lt;li&gt;When reading from the database, you will only see data that has been committed\n(no &lt;em&gt;dirty reads&lt;/em&gt;).\n    &lt;ul&gt;\n      &lt;li&gt;아직 커밋되지 않은 롤백 데이터 읽을 수 있음  If the database allows dirty reads, that means a transaction may see data that is later rolled back—i.e., which is never actually committed to the database. Reasoning about the consequences quickly becomes mind-bending.&lt;/li&gt;\n      &lt;li&gt;새 값이 커밋돼야만 다른 트랜잭션들이 새 값을 읽을 수 있음. While the transaction is ongoing, any other transactions that read the object are simply given the old value. Only when the new value is committed do transactions switch over to reading the new value.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;When writing to the database, you will only overwrite data that has been committed (no &lt;em&gt;dirty writes&lt;/em&gt;).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;p236-240. Snapshot Isolation and Repeatable Read&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;Read skew is considered acceptable under read committed isolation.\n    &lt;ul&gt;\n      &lt;li&gt;skew; &lt;em&gt;timing anomaly&lt;/em&gt;.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;&lt;strong&gt;&lt;em&gt;readers never block writers, and writers never block readers&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;\n  &lt;li&gt;&lt;strong&gt;&lt;em&gt;multi-version concurrency control&lt;/em&gt; (MVCC)&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/07-transaction-multi-version.png&quot; alt=&quot;transaction-multi-version&quot; title=&quot;transaction-multi-version&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;객체를 볼 수 있는 조건:\n    &lt;ul&gt;\n      &lt;li&gt;읽기 트랜잭션 실행 시점에 이미 커밋된 상태여야함. At the time when the reader’s transaction started, the transaction that created the\n  object had already committed.&lt;/li&gt;\n      &lt;li&gt;The object is not marked for deletion, or if it is, the transaction that requested\n  deletion had not yet committed at the time when the reader’s transaction started.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;p242-preventing-lost-updates&quot;&gt;&lt;strong&gt;p242~ Preventing Lost Updates&lt;/strong&gt;&lt;/h3&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;The read committed and snapshot isolation levels:  동시 쓰기할 때 read-only 트랜잭션이 무엇을 볼 수 있는지. The guarantees of what a read-only transaction can see in the presence of concurrent writes.&lt;/li&gt;\n  &lt;li&gt;If two transactions do this concurrently, one of the modifications can be lost, because the second write does not include the first modification. (later write &lt;em&gt;clobbers&lt;/em&gt; the earlier write.)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Atomic write operations&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;MongoDB(document db);  atomic operations for making local modifications to a part of a JSON document.&lt;/li&gt;\n  &lt;li&gt;Redis;  atomic operations for modifying data structures such as priority queues.&lt;/li&gt;\n  &lt;li&gt;Atomic operations are usually implemented by taking an exclusive lock on the object.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Explicit locking&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SELECT **FOR UPDATE**;&lt;/code&gt;&lt;/li&gt;\n  &lt;li&gt;It’s easy to forget to add a necessary lock somewhere in the code, and thus introduce a race condition.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Compare-and-set&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;마지막으로 읽은 후로 변경되지 않았을 때만 갱신 허용.  To avoid lost updates by allowing an update to happen only if the value has not changed since you last read it.&lt;/li&gt;\n  &lt;li&gt;if the database allows the WHERE clause to read from an old snapshot, this statement may not prevent lost updates.\n    &lt;ul&gt;\n      &lt;li&gt;because the condition may be true even though another concurrent write is occurring.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Conflict resolution and replication&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;여러 충돌 버전 생성을 허용하고 사후에 충돌 해소. Allow concurrent writes to create several conflicting versions of a value (also known as &lt;em&gt;siblings&lt;/em&gt;), and to use application code or\nspecial data structures to resolve and merge these versions after the fact.&lt;/li&gt;\n  &lt;li&gt;최종 쓰기 승리는 갱신 손실 발생하기 쉬움.  The &lt;strong&gt;&lt;em&gt;last write wins&lt;/em&gt; (LWW)&lt;/strong&gt; conflict resolution method is prone to lost updates.  LWW is the default in many replicated databases.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;p246 Write Skew and Phantoms&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Characterizing write skew&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;쓰기 스큐는 두 트랜잭션이 다른 객체 갱신.  dirty write와 lost update는 다른 트랜잭션이 하나의 동일 객체 갱신.   Write skew can occur if two transactions read the same objects, and then update some of those objects (different transactions may update different objects). In the special case where different transactions update the same object, you get a dirty write or lost update anomaly (depending on the timing).&lt;/li&gt;\n  &lt;li&gt;직렬성 격리로 write skew 자동 방지.  Automatically preventing write skew requires true serializable isolation.&lt;/li&gt;\n  &lt;li&gt;차선책; serializable isolation 사용할 수 없으면 트랜잭션이 의존하는 로우 잠그기.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Phantoms causing write skew&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;&lt;em&gt;&lt;strong&gt;phantom&lt;/strong&gt;:&lt;/em&gt;  쓰기가 다른 트랜잭션의 검색 질의 결과를 바구는 현상.  Where a write in one transaction changes the result of a search query in another transaction.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;AWS Summit &amp;lt;채널톡의 RDBMS에서 NoSQL 전환&amp;gt; 세션 내용&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;특정 시점 스파이크 트래픽과 리소스 비효율 문제 해결을 위해 DynamoDB 도입&lt;/li&gt;\n  &lt;li&gt;채팅 뱃지 카운트 트랜잭션 동시성 처리: Optimistic lock 이용해서 conflicts 발생시 exponential backoff&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/07-aws-summit-dynamodb-0.png&quot; alt=&quot;aws-summit-dynamodb&quot; title=&quot;aws-summit-dynamodb&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/07-aws-summit-dynamodb-1.png&quot; alt=&quot;aws-summit-dynamodb&quot; title=&quot;aws-summit-dynamodb&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/07-aws-summit-dynamodb-2.png&quot; alt=&quot;aws-summit-dynamodb&quot; title=&quot;aws-summit-dynamodb&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/07-aws-summit-dynamodb-3.png&quot; alt=&quot;aws-summit-dynamodb&quot; title=&quot;aws-summit-dynamodb&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;h3 id=&quot;p251-serializability&quot;&gt;&lt;strong&gt;p251 Serializability&lt;/strong&gt;&lt;/h3&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;Testing for concurrency issues is hard, because they are usually nondeterministic — problems only occur if you get unlucky with the timing.&lt;/li&gt;\n  &lt;li&gt;the strongest isolation level.&lt;/li&gt;\n  &lt;li&gt;It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, &lt;strong&gt;&lt;em&gt;serially&lt;/em&gt;&lt;/strong&gt;, without any concurrency.&lt;/li&gt;\n  &lt;li&gt;the database prevents &lt;strong&gt;&lt;em&gt;all&lt;/em&gt;&lt;/strong&gt; possible race conditions.&lt;/li&gt;\n  &lt;li&gt;2007년경이 돼서야 단일 스레드 루프에서 트랜잭션 실행하는게 실현 가능하다고 결론 내림\n    &lt;ul&gt;\n      &lt;li&gt;램 가격.  모든 데이터를 메모리 적재해서 트랜잭션 빠르게 실행.&lt;/li&gt;\n      &lt;li&gt;OLTP 트랜잭션이 보통 짧고 실행하는 읽기와 쓰기 개수가 적음.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;단일 스레드 기반 시스템이 동시성을 지원하는 시스템보다 성능이 나을때가 있음.  잠금 오버헤드 피할 수 있기 때문.  A system designed for single-threaded execution can sometimes perform better than a system that supports concurrency, because it can avoid the coordination overhead of locking.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Encapsulating transactions in stored procedures&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;데이터가 모두 메모리에 있고 프로시저는 네트워크나 디스크 I/O 대기 없이 매우 빠르게 실행된다고 가정.  Provided that all data required by a transaction is in memory, the stored procedure can execute very fast, without waiting for any network or disk I/O.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/07-transaction-difference.png&quot; alt=&quot;transaction-difference&quot; title=&quot;transaction-difference&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Partitioning&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;동시성 제어는 간단해지지만 단일 장비의 단일 CPU 코어 속도 제한. Executing all transactions serially makes concurrency control much simpler, but limits the transaction throughput of the database to the speed of a single CPU core on a\nsingle machine.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Two-Phase Locking (2PL)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;2PL:  쓰기는 다른 쓰기와 읽기 진행하지 못하게 막음. writers don’t just block other writers; they also block readers and vice versa.&lt;/li&gt;\n  &lt;li&gt;Snapshot isolation:  읽기와 쓰기 양쪽 모두 막지 못하는 원칙.  the mantra &lt;em&gt;readers never block writers, and writers never block.&lt;/em&gt;&lt;/li&gt;\n  &lt;li&gt;lock: &lt;strong&gt;&lt;em&gt;shared mode&lt;/em&gt;&lt;/strong&gt; or in &lt;strong&gt;&lt;em&gt;exclusive mode&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;\n  &lt;li&gt;Performance\n    &lt;ul&gt;\n      &lt;li&gt;This is partly due to the overhead of acquiring and releasing all those locks, but more importantly due to reduced concurrency.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Pessimistic vs. optimistic concurrency control\n    &lt;ul&gt;\n      &lt;li&gt;&lt;strong&gt;Pessimistic&lt;/strong&gt;: if anything might possibly go wrong (as indicated by a lock held by another transaction), it’s better to wait until the situation is safe again before doing anything.&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;Optimistic&lt;/strong&gt;: instead of blocking if something potentially dangerous happens, transactions continue anyway, in the hope that everything will turn out all right.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Performance of serializable snapshot isolation&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;잠금 때문에 트랜잭션 차단될 필요 없음. Compared to two-phase locking, the big advantage of serializable snapshot isolation is that one transaction doesn’t need to block waiting for locks held by another transaction.&lt;/li&gt;\n  &lt;li&gt;The rate of aborts significantly affects the overall performance of SSI. For example, a transaction that reads and writes data over a long period of time is likely to run into conflicts and abort, so SSI requires that read-write transactions be fairly short (long- running read-only transactions may be okay)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n\n&lt;h2 id=&quot;08장-분산-시스템의-골칫거리&quot;&gt;08장: 분산 시스템의 골칫거리&lt;/h2&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p274 엔지니어로서의 우리의 임부는 모든 게 잘못되더라도 제 역할을 해내는 시스템을 구축하는것.  In the end, our task as engineers is to build systems that do their job (i.e., meet the guarantees that users are expecting), in spite of everything going wrong.&lt;/li&gt;\n  &lt;li&gt;&lt;strong&gt;Faults and Partial Failures&lt;/strong&gt;\n    &lt;ul&gt;\n      &lt;li&gt;&lt;strong&gt;&lt;em&gt;deterministic&lt;/em&gt;&lt;/strong&gt;: ex. 하드웨어가 올바르게 동작하면 같은 연산은 항상 같은 결과를 낸다.&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;&lt;em&gt;partial failure&lt;/em&gt;&lt;/strong&gt;: 분산 시스템에서 시스템의 일부만 고장. &lt;strong&gt;&lt;em&gt;nondeterministic&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;p276 분산 시스템이 동작하게 만들려면 partial failure 가능성을 받아들이고 소프트웨어에 &lt;strong&gt;내결함성 메커니즘&lt;/strong&gt;(fault-tolerance mechanisms)을 넣어야 한다. 신뢰성 없는 구성 요소를 사용해 신뢰성 있는 시스템을 구축해야 함.   In other words, we need to build a reliable system from unreliable components.&lt;/li&gt;\n  &lt;li&gt;&lt;strong&gt;Unreliable Networks&lt;/strong&gt;\n    &lt;ul&gt;\n      &lt;li&gt;p279 비동기 네트워크.. 유일한 정보는 응답을 아직 받지 못했다는 것. 응답을 다른 노드로 요청을 보내서 응답을 받지 못했다면 그 이유를 아는 것은 불가능.   이런 문제를 다루는 흔한 방법이 타임아웃.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;&lt;strong&gt;Timeouts and Unbounded Delays&lt;/strong&gt;\n    &lt;ul&gt;\n      &lt;li&gt;타임아웃이 길면 노드가 죽었다고 선언될 때까지 기다리는 시간이 길어짐.  타임아웃이 짧으면 결함을 빨리 발견하지만  노드가 일시적으로 느려졌을 뿐인데도 죽었다고 잘못 선언할 가능성.&lt;/li&gt;\n      &lt;li&gt;2d + r;  d: 전송 시간, r: 요청 처리 시간&lt;/li&gt;\n      &lt;li&gt;기약 없는 지연(unbounded delay): 패킷을 가능한 한 빨리 보내려고 하지만 패킷이 도착하는데 걸리는 시간에 상한치는 없다.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Network congestion and queueing&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p282 TCP 흐름제어(flow control). 노드가 네트워크 링크나 수신 노드에 과부하를 가하지 않도록 자신의 송신율을 제한.&lt;/li&gt;\n  &lt;li&gt;TCP는 어떤 타임아웃(왕복 시간을 관찰해서 계산)  안에 확인 응답을 받지 않으면 패킷이 손실됐다고 간주하고 손실된 패킷은 자동으로 재전송.  애플리케이션에게는 패킷 손실이나 재전송이 보이지 않지만 그 결과로 생기는 지연 발생(타임아웃 만료 + 재전송 패킷 확인 응답)\n    &lt;ul&gt;\n      &lt;li&gt;&lt;a href=&quot;https://alden-kang.tistory.com/20&quot;&gt;Connection Timeout과 Read Timeout 살펴보기&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=&quot;https://brunch.co.kr/@alden/15&quot;&gt;TCP retransmission과 튜닝 포인트&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;두 글 요약하자면 → 커넥션 타임아웃은 3초로 적당한거같다. TCP 는 재전송 메커니즘이 있어서 이 때 타임아웃이 1초임. 그래서 3초 정도가 적덩한거같다. Read Timeout 은 300ms 였어서 1초&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;p284  고정된 타임아웃을 설정하는 대신 시스템이 지속적으로 응답 시간과 그들의 변동성을 측정,   관찰된 응답 시간 분포에 따라 타임아웃을 자동 조정.\n    &lt;ul&gt;\n      &lt;li&gt;&lt;a href=&quot;https://doc.akka.io/docs/akka/current/typed/failure-detector.html&quot;&gt;Phi Accrual Failure Detector - akka&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=&quot;https://medium.com/@arpitbhayani/phi-%CF%86-accrual-failure-detection-79c21ce53a7a&quot;&gt;Phi φ Accrual Failure Detection&lt;/a&gt;\n        &lt;ol&gt;\n          &lt;li&gt;&lt;strong&gt;Suspicion Level (φ value):&lt;/strong&gt;\n            &lt;ul&gt;\n              &lt;li&gt;φ 값은 수신된 하트비트(heartbeat) 사이의 시간 간격을 기반으로 계산. 이 값은 현재 네트워크 상태를 반영하도록 지속적으로 조정. φ 값이 높을수록 노드가 실패했을 가능성이 높다는 것을 의미.&lt;/li&gt;\n            &lt;/ul&gt;\n          &lt;/li&gt;\n          &lt;li&gt;&lt;strong&gt;Heartbeat Intervals&lt;/strong&gt;:\n            &lt;ul&gt;\n              &lt;li&gt;노드로부터 모니터링. interval은 샘플 윈도우에 저장되어 분포를 추정하고 의심 수준을 계산하는 데 사용&lt;/li&gt;\n            &lt;/ul&gt;\n          &lt;/li&gt;\n          &lt;li&gt;&lt;strong&gt;Dynamic Thresholds&lt;/strong&gt;:\n            &lt;ul&gt;\n              &lt;li&gt;φ 값은 시스템이 다양한 동적 임계값을 설정. 예를 들어, φ 값이 특정 임계값을 초과하면 노드를 실패한 것으로 간주할 수 있지만, 더 낮은 φ 값에서는 예방 조치를 시작할 수 있다.&lt;/li&gt;\n            &lt;/ul&gt;\n          &lt;/li&gt;\n        &lt;/ol&gt;\n      &lt;/li&gt;\n      &lt;li&gt;A → B → C 순으로 호출한다고 하자. A를 관리하고있다면 B에서 지연된건지 C에서 지연된건지 찾는데, 데이터독으로 트레이싱하고있어서, 최근 n일 기준으로 API 의 latency 를 보고. 가령 B→C가 50ms 라면 Read Timeout 고려해서 500ms 정도로 잡고, A→B 구간에서는 500보다 좀 더 크게 잡고.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Synchronous Versus Asynchronous Networks&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p284 전화 네트워크. 고정 회선.\n    &lt;ul&gt;\n      &lt;li&gt;bounded delay(제한 있는 지연):  네트워크의 다음 홉에 통화당 16비트 공간을 미리 할당.  그리고 큐 대기가 없으므로 종단 지연 시간의 최대치가 고정되어 있음.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Can we not simply make network delays predictable?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;데이터센터 네트워크와 인터넷은 패킨 교환 사용;  bursty traffic 에 최적화&lt;/li&gt;\n  &lt;li&gt;회선; 통화를 하는 동안 보내는 초당 비트 개수가 상당히 고정돼 있는 음성과 영상 통화에 적합&lt;/li&gt;\n  &lt;li&gt;웹 페이지 요청, 이메일 전송, 파일 전송은 특별한 대역폭 요구사항 없음. 단지 가능하면 빨리 완료되기를 바람.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;p287 Unreliable Clocks&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;시간, 시계와 관련된 질문을 Durations(지속 시간), Points in time(시점)에 따라 기술&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Synchronized clocks for global snapshots&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p295 스패너에 대한 이야기(feat. ChatGPT):\n    &lt;ul&gt;\n      &lt;li&gt;데이터베이스 내의 여러 트랜잭션 간의 인과 관계를 고려하여 타임스탬프를 부여\n        &lt;ul&gt;\n          &lt;li&gt;&lt;strong&gt;트랜잭션 타임스탬프:&lt;/strong&gt; 각 트랜잭션이 완료된 시점(시간). 트랜잭션의 순서를 정하고, 데이터의 일관성을 유지하는 데 중요&lt;/li&gt;\n          &lt;li&gt;&lt;strong&gt;인과성(Causality)&lt;/strong&gt;: 한 트랜잭션이 다른 트랜잭션에 영향을 미치는 관계.  예를 들어, 트랜잭션 A가 트랜잭션 B보다 먼저 실행되고, B가 A의 결과에 의존하는 경우, A는 B의 원인이 된다.\n      1. &lt;strong&gt;TrueTime API&lt;/strong&gt;:&lt;/li&gt;\n          &lt;li&gt;TrueTime API를 사용하여 각 트랜잭션에 정확한 타임스탬프를 부여. TrueTime은 GPS 및 원자시계 정보를 활용하여 매우 정확한 시각 정보를 제공.&lt;/li&gt;\n          &lt;li&gt;TrueTime의 사용으로 인해 각 트랜잭션의 완료 시점에 대해 상한(earliest) 및 하한(latest) 시간을 알 수 있으며, 이를 통해 트랜잭션 간의 정확한 순서를 결정.\n      2. &lt;strong&gt;트랜잭션 순서 보장&lt;/strong&gt;:&lt;/li&gt;\n          &lt;li&gt;트랜잭션 간의 인과 관계를 보장하기 위해, 이전 트랜잭션이 완료된 후에만 다음 트랜잭션을 수행. 이는 인과 관계가 반영된 타임스탬프를 부여하는데 중요한 역할을 한다.\n      3. &lt;strong&gt;Globally Consistent Reads and Writes&lt;/strong&gt;:&lt;/li&gt;\n          &lt;li&gt;이러한 타임스탬프 메커니즘 덕분에, Spanner는 전 세계에 분산된 데이터베이스에서도 강력한 일관성을 유지할 수 있습니다. 각 트랜잭션의 타임스탬프가 인과성을 반영하기 때문에, 사용자는 일관된 데이터를 읽고 쓸 수 있습니다.&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n      &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/spanner/docs/true-time-external-consistency&quot;&gt;Spanner: TrueTime and external consistency&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/e3d0/9699c66bd0b89f663954bf8b043491368620.pdf&quot;&gt;Spanner: Google’s Globally-Distributed Database&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Process Pauses&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;note) p297. 실행 중인 스레드를 어떤 시점에 선점(preempt)하고 얼마만의 시간이 흐른후 재개할 수 있다.  선점된 스레드는 이를 알아채지 못한다.  이 문제는 단일 장비에서 다중 스레드 코드를 thread-safe 하게 만드는 것과 비슷.   컨텍스트 스위치가 임의로 발생할 수 있고 병렬(parallelism)이 발생할 수 도 있으므로 타이밍에 대해 어떤 가정도 할 수 없다.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Knowledge, Truth, and Lies&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p300 인식하고 측정하는 수단을 믿을 수 없다면 그 지식을 어떻게 확신할 수 있나? How sure can we be of that knowledge, if the mechanisms for perception and measurement are unreliable?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The Truth Is Defined by the Majority&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p301 노드가 상황에 대한 자신의 판단을 반드시 믿을 수 있는 것은 아님.  분산 시스템은 한 노드에만 의존할 수는 없다. 노드에 언제든 장애가 나서 잠재적으로 시스템이 멈추고 복구할 수 없게 될 수도 있기 때문이다.  &lt;strong&gt;정족수(quorum)&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Fencing tokens&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p303 잠금이 승인될 때마다 증가하는 숫자.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/08-fencing-token.png&quot; alt=&quot;fencing-token&quot; title=&quot;fencing-token&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;자원 자체가 이미 처리된 것보다 오래된 토큰을 사용해서 쓰는 것을 거부함으로써 토큰을 확인하는 활동적인 역할을 맡아야 함.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Byzantine Faults&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p304 어떤 노드가 실제로는 받지 않은 특정 메시지를 받았다고 주장…&lt;/li&gt;\n  &lt;li&gt;비잔틴 결함(&lt;em&gt;Byzantine fault&lt;/em&gt;), 비잔틴 장군 문제(&lt;em&gt;Byzantine Generals Problem&lt;/em&gt;)\n    &lt;ul&gt;\n      &lt;li&gt;&lt;a href=&quot;https://monday9pm.com/%EB%A9%94%EC%8B%9C%EC%A7%80-%EC%A0%84%EB%8B%AC-%EC%A0%84%EB%9E%B5%EA%B3%BC-%EB%91%90-%EC%9E%A5%EA%B5%B0-%EB%AC%B8%EC%A0%9C-message-delivery-semantics-and-two-generals-problem-f8f1c7646c0b&quot;&gt;&lt;strong&gt;메시지 전달 전략과 두 장군 문제(Message Delivery Semantics and Two Generals’ Problem)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=&quot;https://lwn.net/Articles/540368/&quot;&gt;&lt;strong&gt;ELC: SpaceX lessons learned&lt;/strong&gt;&lt;/a&gt;\n        &lt;ul&gt;\n          &lt;li&gt;SpaceX에서는 Falcon, Dragon, Grasshopper 등의 비행 제어, 지상 스테이션, 개발자 데스크톱까지 모든 것이 리눅스.&lt;/li&gt;\n          &lt;li&gt;fault-tolerant(내결함성/결함 허용)을 위해 3중으로 시스템을 구성하고 비잔틴 장군 알고리즘을 사용. 우주 정거장(ISS)에 접근할 때, 결함 허용 수준을 만족해야만 정거장에 접근할 수 있음.  이를 위해 삼중 중복 컴퓨터를 사용해서 필요한 수준의 결함 허용을 달성.&lt;/li&gt;\n          &lt;li&gt;비잔틴 장군 알고리즘은 컴퓨터들이 동의하지 않는 상황을 처리하는 데 사용. 이런 상황은 방사선 사건으로 인해 메모리나 레지스터 값이 변경되는 경우 발생.&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Byzantine fault-tolerant&lt;/em&gt;&lt;/strong&gt; :  일부 노드가 오작동하고 프로토콜을 준수하지 않거나 악의적인 공격자가 네트워크를 방해하더라도 시스템이 계속 올바르게 동작&lt;/li&gt;\n  &lt;li&gt;보통 비잔틴 결함이 없다고 가정할 수 있다. 우리 조직이 모든 노드 제어.. 방사선 수준은 큰 문제 아님..&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Safety and liveness&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p308  안전성(나쁜 일은 일어나지 않는다), 활동성(좋은 일은 결국 일어난다).  Safety is often informally defined as &lt;strong&gt;&lt;em&gt;nothing bad happens&lt;/em&gt;&lt;/strong&gt;, and liveness as &lt;strong&gt;&lt;em&gt;something good eventually happens&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;정리&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;결함을 견뎌내려면 그것을 감지하는 게 첫 걸음이지만 그것조차 어렵다.&lt;/li&gt;\n  &lt;li&gt;대부분의 시스템은 노드에 장애가 발생했는지 알 수 있는 정확한 메커니즘이 없어서 대부분의 분산 알고리즘은 원격 노드를 아직 쓸 수 있는지 결정하기 위해 타임아웃을 사용.&lt;/li&gt;\n  &lt;li&gt;그러나 타임아웃은 네트워크 장애와 노드 장애를 구별할 수 없고 변동이 큰 네트워크 지연 때문에 때때로 노드가 죽은 것으로 잘못 의심받을 수 있다.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n\n&lt;h2 id=&quot;09장-일관성과-합의p349&quot;&gt;09장: 일관성과 합의(~p.349)&lt;/h2&gt;\n\n&lt;p&gt;&lt;strong&gt;p.321 Consistency Guarantees&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;약한 보장(weak guarantee): 복제 데이터 베이스는 대부분 최소한 &lt;strong&gt;최종적 일관성&lt;/strong&gt;(&lt;strong&gt;&lt;em&gt;eventual consistency**)&lt;/em&gt;을 제공한다. 하지만 언제 **수렴&lt;/strong&gt;(&lt;strong&gt;&lt;em&gt;convergence&lt;/em&gt;&lt;/strong&gt;) 될 지 모름.&lt;/li&gt;\n  &lt;li&gt;강한 보장(strongest consistency):  데이터 시스템이 선택적으로 제공. 성능이 나쁘거나 약한 보장을 제공하는 시스템보다 내결함성이 약할 수 있음.&lt;/li&gt;\n  &lt;li&gt;트랜잭션 격리 수준과의 차이점;\n    &lt;ul&gt;\n      &lt;li&gt;트랜잭션 격리:  주로 동시에 실행되는 트랜잭션 때문에 발생하는 경쟁 조건을 회피하는 것에 관한 것.&lt;/li&gt;\n      &lt;li&gt;분산 일관성:  대개 지연과 결함이 있더라도 복제본의 상태를 코디네이션하는 것&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;p322 &lt;strong&gt;Linearizability&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;“데이터베이스가 복제본이 하나만 있다면 훨씬 더 단순해 지지 않을까?”  원자적 일관성(&lt;em&gt;atomic consistency&lt;/em&gt;), 강한 일관성(&lt;em&gt;strong consistency&lt;/em&gt;), 즉각 일관성(&lt;em&gt;immediate consistency&lt;/em&gt;), 외부 일관성(&lt;em&gt;external consistency&lt;/em&gt;)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/09-consistency.png&quot; alt=&quot;consistency&quot; title=&quot;consistency&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;최신성 보장(recency guarantee)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;시스템에 선형성을 부여하는 것&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*cas*(*x*, *v*old, *v*new) ⇒ *r*&lt;/code&gt;:\n    &lt;ul&gt;\n      &lt;li&gt;클라이언트가 cas(compare-and-set) 연산 요청&lt;/li&gt;\n      &lt;li&gt;레지스터 x의 현재 값이 Vold과 같으면 Vnew로 설정하고 아니면 오류 발환(error)&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/09-linearizability.png&quot; alt=&quot;linearizability&quot; title=&quot;linearizability&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p332 요약; 다이나모 스타일 복제를 하는 리더 없는 시스템은 선형성을 제공하지 않는다고 보는 게 안전.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;p333 선형성의 비용&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;단일 리더 설정에서 데이터센터 사이에 연결 끊기면 선형성 읽기 불가능.  팔로워로부터 읽을 수는 있지만 데이터가 뒤쳐졌을 수 있다(비선형적).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;p335 CAP&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;네트워크 결함이 생기면 선형성과 완전한 가용성 사이에서 선택해야 함.  CAP는 &lt;strong&gt;네트워크 분단이 생겼을때 일관성과 가용성 중 하나를 선택&lt;/strong&gt;하라는 의미.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;p342 비인과적 일련번호 생성기(Noncausal sequence number generators)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;노드별) 각 노드 개별로 독립적인 일련번호 집합 생성. 예) 한 노드는 홀수만, 다른 노드는 짝수만 생성.&lt;/li&gt;\n  &lt;li&gt;잘 동작하지만 생성한 일련번호가 인과성에 일관적이지 않음(timestamp 불일치)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;p343 Lamport timestamps&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;&amp;lt;카운터, 노드&amp;gt; ID 조합.&lt;/li&gt;\n  &lt;li&gt;때때로 카운터 값이 같을 수 있지만 타임스탬프에 노드 ID를 포함시켜서 각 타임스탬프 유일.&lt;/li&gt;\n  &lt;li&gt;물리적 일 기준 시계와 아무 관련이 없지만 전체 순서화를 제공.&lt;/li&gt;\n  &lt;li&gt;버전 벡터와의 차이;\n    &lt;ul&gt;\n      &lt;li&gt;버전 벡터:  두 연산이 동시적인지 또는 어떤 연산이 다른 연산에 인과적으로 의존하는지 구별&lt;/li&gt;\n      &lt;li&gt;램포트 타임스탬프:  항상 전체 순서화를 강제화&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Total Order Broadcast&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p346  후속 메시지가 이미 전달됐다면 노드는 그 순서의 앞에 메시지를 소급적으로 끼워넣는 게 허용되지 않는다.  전체 순서 브로드캐스트가 타임스탬프 순서화보다 강하다.&lt;/li&gt;\n  &lt;li&gt;p347 선형성과의 차이\n    &lt;ul&gt;\n      &lt;li&gt;전체 순서 브로드캐스트는 비동기식.  메시지는 고정된 순서로 신뢰성 있게 전달되도록 보장.   하지만 언제 메시지가 전달되는지 보장되지 않음.&lt;/li&gt;\n      &lt;li&gt;선형성은 최신성 보장. 읽기가 최근에 쓰여진 값을 보는 게 보장.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n\n&lt;h2 id=&quot;09장-분산-트랜잭션과-합의p349&quot;&gt;09장: 분산 트랜잭션과 합의(p.349~)&lt;/h2&gt;\n\n&lt;p&gt;p349 Consensus&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;합의:  여러 노드들이 뭔가에 동의하게 만드는 것&lt;/li&gt;\n  &lt;li&gt;리더 선출(Leader election):  단일 리더 복제를 사용하는 DB에서 모든 노드는 어떤 노드가 리더인지 동의해야 함.&lt;/li&gt;\n  &lt;li&gt;원자적 커밋(Atomic commit): 트랜잭션 원자성을 유지하고 싶다면 모든 노드가 트랜잭션의 결과에 동의해야 함.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;p351 원자적 커밋과 2PC(Two-Phase Commit) why?&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;트랜잭션의 결과는 항상 커밋 Success 이거나 Abort.  The outcome of a transaction is either a &lt;strong&gt;successful&lt;/strong&gt; commit, in which case all of the transaction’s writes are made durable, or an &lt;strong&gt;abort&lt;/strong&gt;, in which case all of the transaction’s writes are rolled back (i.e., undone or discarded).&lt;/li&gt;\n  &lt;li&gt;트랜잭션에 여러 노드가 관여하게 되면 어떤 노드에서는 커밋이 성공하고 다른 노드에서는 실패해서 원자성 보장을 위반하기 쉽다.  어떤 노드가 트랜잭션을 Commit 하지만 다른 노드는 Abort 한다면 노드들이 서로 일관성이 없어진다.&lt;/li&gt;\n  &lt;li&gt;트랜잭션 커밋은 되돌릴 수 없어야 한다.  보상 트랜잭션(compensating transaction)은 취소 가능하지만 분리된 트랜잭션 개념으로 이해해야 한다.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;p.352-355  2PC&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;2PC는 여러 노드에 걸친 원자적 트랜잭션 Commit 되게 함. 즉 모든 노드가 커밋되거나 모든 노드가 Abort 되도록 보장하는 알고리즘&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/09-2pc.png&quot; alt=&quot;2pc&quot; title=&quot;2pc&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;코디네이터(coordinator, transaction manager), 데이터베이스 노드들을 트랜잭션 참여자(participant)&lt;/li&gt;\n  &lt;li&gt;애플리케이션이 Commit할 준비가 되면 코디네이터가 1단계 시작 → 각 노드에 준비 요청을 보내서 커밋할 수 있는지 물어본다 → 그 후 코디네이터는 참여자들의 응답을 추적\n    &lt;ul&gt;\n      &lt;li&gt;모든 참여자가 커밋할 준비가 됐다면 “yes” 응답 → 코디네이터는 2단계에서 커밋 요청(commit request)하고 실제로 커밋 발생&lt;/li&gt;\n      &lt;li&gt;참여자 중 누구라도 “no” 응답 → 코디네이터는 2단게에서 모든 노드에 Abort 요청&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;코디네이터 장애시,  2PC가 완료될 수 있는 유일한 방법은 코디네이터가 복구되기를 기다리는것 뿐.\n    &lt;ul&gt;\n      &lt;li&gt;코디네이터가 Commit 이나 Abort 요청 보내기 전에 디스크에 있는 트랜잭션 로그에 기록&lt;/li&gt;\n      &lt;li&gt;코디네이터가 복구될 때 트랜잭션 로그를 읽어서 모든 의심스러운 트랜잭션들의 상태를 결정&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;p358-360  XA transactions&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;트랜잭션 코디네이터와 연결되는 인터페이스를 제공하는 API.  트랜잭션 코디네이터는 XA API를 구현.&lt;/li&gt;\n  &lt;li&gt;애플리케이션이 네트워크 드라이버나 클라이언트 라이브러리를 사용해 참여자 DB나 메시징 서비스와 통신한다고 가정.&lt;/li&gt;\n  &lt;li&gt;애플리케이션 프로세그가 죽거나 애플리케이션이 실행중인 장비가 죽으면 코디네이터도 함께 사라짐&lt;/li&gt;\n  &lt;li&gt;코디네이터 장애 복구 문제.  유일한 방법은 관리자가 수동으로 트랜잭션을 커밋하거나 롤백할지 결정하는 것.&lt;/li&gt;\n  &lt;li&gt;\n    &lt;p&gt;JDBC 분산 트랜잭션&lt;/p&gt;\n\n    &lt;p&gt;JDBC(Java Database Connectivity)의 트랜잭션은 로컬. 이는 단일 연결이 트랜잭션의 모든 작업을 수행하며 연결은 한 번에 하나의 트랜잭션에 대해서만 작업할 수 있음을 의미.&lt;/p&gt;\n\n    &lt;p&gt;하나의 트랜잭션에 대한 모든 작업이 완료되거나 실패한 경우, 작업을 영구적으로 만들기 위해 커밋 또는 롤백이 호출되고 새 트랜잭션이 시작된다.  그러나 Java에서는 로컬 트랜잭션을 넘어서는 기능을 제공하는 고급 트랜잭션 지원도 있고, Java 트랜잭션 API에서 제공한다.&lt;/p&gt;\n\n    &lt;p&gt;Java Transaction API(JTA)는 복잡한 트랜잭션을 지원.  또한 연결 객체에서 트랜잭션을 분리하는 기능도 제공합니다.  JDBC가 객체 데이터베이스 연결(ODBC) 및 X/Open 호출 수준 인터페이스(CLI) 명세를 모델로 삼았듯이, JTA는 X/Open eXtended Architecture(XA) 명세를 모델로 삼습니다. JTA와 JDBC는 연결 객체에서 트랜잭션을 분리하기 위해 함께 작동. 연결 객체에서 트랜잭션을 분리함으로써 단일 연결이 여러 트랜잭션을 동시에 처리할 수 있다.  반대로 여러 연결이 단일 트랜잭션을 처리할 수도 있음.&lt;/p&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;p360 분산 트랜잭션의 제약&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;XA 트랜잭션은 여러 참여 데이터 시스템이 서로 일관성을 유지하게 하는 실제적이고 중요한 문제를 해결해 주지만, XA 트랜잭션도 중요한 운영상 문제를 가져온다.&lt;/li&gt;\n  &lt;li&gt;핵심 구현은 트랜잭션의 코디네이터 자체가 일종의 데이터베이스여야 하고, 따라서 다른 중요한 데이터베이스와 동일하게 신경 써서 접근해야함.\n    &lt;ul&gt;\n      &lt;li&gt;분산 트랜잭션은 &lt;strong&gt;장애를 증폭&lt;/strong&gt;시키는 경향이 있으며 이는 내결함성을 지닌 시스템을 구축하려는 목적에 어긋난다.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;p361 내결함성을 지닌 합의(Fault-Tolerant Consensus)&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;합의 알고리즘이 만족해야 할 속성\n    &lt;ul&gt;\n      &lt;li&gt;균일한 동의(Uniform agreement):  어떤 두 노드도 다르게 결정하지 않음&lt;/li&gt;\n      &lt;li&gt;무결성(Integrity): 어떤 노드도 두 번 결정하지 않음&lt;/li&gt;\n      &lt;li&gt;유효성(Validity): 한 노드가 값 v를 결정한다면 v는 어떤 노드에서 제안된 것이다.&lt;/li&gt;\n      &lt;li&gt;종료(Termination): 죽지 않은 모든 노드는 결국 어떤 값을 결정한다.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;카프카 Zookeeper → Raft consensus\n    &lt;ul&gt;\n      &lt;li&gt;&lt;a href=&quot;https://seongjin.me/raft-consensus-algorithm/&quot;&gt;https://seongjin.me/raft-consensus-algorithm/&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=&quot;https://devocean.sk.com/blog/techBoardDetail.do?ID=165711&quot;&gt;https://devocean.sk.com/blog/techBoardDetail.do?ID=165711&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=&quot;https://devocean.sk.com/blog/techBoardDetail.do?ID=165737&quot;&gt;https://devocean.sk.com/blog/techBoardDetail.do?ID=165737&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;p365 에포크 번호 붙이기와 정족수&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;현재 리더가 죽었다고 생각될 때마다 새 노드를 선출하기 위해 노드 사이에서 투표를 해서 에포크 번호가 높은 리더가 이겨서 리더가 된다.&lt;/li&gt;\n  &lt;li&gt;리더가 뭔가를 결정하도록 허용하기 전에 충돌되는 결정을 할 지도 모르는 에포크 번호가 더 높은 리더가 없는지 먼저 확인해야 한다.&lt;/li&gt;\n  &lt;li&gt;노드의 정족수로부터 투표를 받아서 리더를 판단합니다. 정족수는 항상은 아니지만 노드의 과반수로 구성된다.&lt;/li&gt;\n  &lt;li&gt;2PC는 “yes” 투표가 필요하지만,  내결함성을 지닌 합의 알고리즘은 노드의 과반수로부터만 투표.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;p367 멤버십과 코디네이션 서비스&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;HBase, 하둡, 노바, 카프카는 모두 &lt;strong&gt;주키퍼&lt;/strong&gt;에 의존.&lt;/li&gt;\n  &lt;li&gt;주키퍼 기능\n    &lt;ul&gt;\n      &lt;li&gt;&lt;strong&gt;선형적 원자적 연산(Linearizable atomic operations):&lt;/strong&gt;  원자적 compare-and-set 연산을 사용해 잠금을 구현&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;연산의 전체 순서화(Total ordering of operations):&lt;/strong&gt;  펜싱 토큰 등을 사용해 클라이언트 충돌 막기&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;장애 감지(Failure detection):&lt;/strong&gt; 세션에서 획득한 잠금은 세션이 타임아웃 됐을 때 자동으로 해제되도록 설정&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;변경 알림(Change notifications):&lt;/strong&gt;  클라이언트는 다른 클라이언트가 생성한 잠금과 값을 읽을 수 있을 뿐만 아니라 거기에 변경이 있는지 감시할 수 있다.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;p369 Service discovery,  Membership services&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;Service discovery: 특정 서비스에 연결하려면 어떤 IP 주소로 접속해야 하는지 알아내는 용도로도 자주 사용.&lt;/li&gt;\n  &lt;li&gt;Membership services:  클러스터에서 어떤 노드가 현재 활성화된 살아 있는 멤버인지 결정.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n\n&lt;h2 id=&quot;11장-스트림-처리p453&quot;&gt;11장: 스트림 처리(~p453)&lt;/h2&gt;\n\n&lt;p&gt;p382 레코드 시스템과 파생 데이터 시스템&lt;/p&gt;\n\n&lt;table&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt;레코드 시스템(Systems of record)&lt;/th&gt;\n      &lt;th&gt;파생 데이터 시스템(Derived data systems)&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr&gt;\n      &lt;td&gt;믿을 수 있는 데이터 버전을 저장&lt;/td&gt;\n      &lt;td&gt;다른 시스템에 존재하는 데이터를 가져와 특정 방식으로 변환하고 처리한 결과&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;td&gt;진실의 근원(source of truth). 사용자의 입력과 같은 새로운 데이터가 들어오면 먼저 레코드 시스템에 저장&lt;/td&gt;\n      &lt;td&gt;데이터를 잃게 되더라도 원천 데이터로부터 다시 생성 가능&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;td&gt;일반적으로 정규화를 거쳐 정확하게 한 번 표현된다.  레코드 시스템과 다른 시스템 간에 차이가 난다면 레코드 시스템이 옳다.&lt;/td&gt;\n      &lt;td&gt;필요한 데이터가 캐시에 있다면 캐시에서 제공하고, 그렇지 않다면 기반 데이터베이스를 거쳐 제공할 수 있다.&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n\n&lt;p&gt;p436 &lt;strong&gt;이벤트 스트림 전송(Transmitting Event Streams)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;&lt;strong&gt;이벤트&lt;/strong&gt;\n    &lt;ul&gt;\n      &lt;li&gt;스트림 처리 문맥(stream processing context)에서 레코드는 보통 &lt;strong&gt;이벤트&lt;/strong&gt;라고 한다.&lt;/li&gt;\n      &lt;li&gt;특정 시점에 일어난 사건에 대한 세부 사항을 포함하는, 작고 독립된 불변 객체라는 점에서 본질적으로 동일하다.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;생성자(producer)와 소비자(consumer)\n    &lt;ul&gt;\n      &lt;li&gt;생산자(producer), 발행자(publisher), 발송자(sender)&lt;/li&gt;\n      &lt;li&gt;소비자(consumer), 구독자(subscriber), 수신자(recipient)&lt;/li&gt;\n      &lt;li&gt;레코드 식별(그룹핑)\n        &lt;ul&gt;\n          &lt;li&gt;파일 시스템: 파일 이름으로 관련 레코드 식별&lt;/li&gt;\n          &lt;li&gt;스트림 시스템: 토픽(topic)이나 스트림으로 관련 이벤트 그룹핑&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;메시징-시스템-p438&quot;&gt;메시징 시스템 p438&lt;/h3&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;\n    &lt;p&gt;메시지 시스템에 대한 질문(Publish, Subscribe 모델)&lt;/p&gt;\n\n    &lt;p&gt;Q 소비자가 메시지 처리하는 속도보다 생산자가 메시지 전송하는게 빠른 경우(생산자 &amp;gt; 소비자)&lt;/p&gt;\n\n    &lt;ul&gt;\n      &lt;li&gt;메시지를 버리기&lt;/li&gt;\n      &lt;li&gt;큐에 메시지 버퍼링하기&lt;/li&gt;\n      &lt;li&gt;배압(backpressure) 적용:\n        &lt;ul&gt;\n          &lt;li&gt;생산자가 메시지를 더 보내지 못하게 막기&lt;/li&gt;\n          &lt;li&gt;유닉스 파이프와 TCP가 사용&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n\n    &lt;p&gt;Q 노드가 죽거나 일시적으로 오프라인이 된다면?&lt;/p&gt;\n\n    &lt;ul&gt;\n      &lt;li&gt;디스크에 기록하거나 복제본 생성을 하거나 둘 모두를 해야 한다.&lt;/li&gt;\n      &lt;li&gt;메시지를 잃어도 괜찮다면 같은 하드웨어에서 처리량은 높이고 지연 시간은 낮출 수 있다.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;생산자에서 소비자로 메시지 직접 전달:  프로토콜이 네트워크 상에서 패킷 유실을 감지하고 재전송하더라도 직접 메시지 시스템은 일반적으로 생산자와 소비자가 항상 온라인 상태라고 가정&lt;/li&gt;\n  &lt;li&gt;\n    &lt;p&gt;p440 메시지 브로커와 데이터베이스 비교&lt;/p&gt;\n\n    &lt;table&gt;\n      &lt;thead&gt;\n        &lt;tr&gt;\n          &lt;th&gt;데이터베이스(Databases)&lt;/th&gt;\n          &lt;th&gt;메시지 브로커(Message brokers)&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;td&gt;명시적으로 데이터가 삭제될 때까지 데이터 보관&lt;/td&gt;\n          &lt;td&gt;대부분 소비자에게 데이터 배달이 성공할 경우 자동으로 메시지 삭제. 대부분 메시지를 빨리 지우기 때문에 작업 집합이 상당히 작다고 가정(큐 크기가 작다). 소비자 처리가 느려서 메시지 브로커가 많은 메시지를 버퍼링해야 한다면, 개별 메시지 처리 시간이 길어지고 전체 처리량이 저하됨&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td&gt;보조 색인을 지원. 데이터 검색을 위한 다양한 방법 지원&lt;/td&gt;\n          &lt;td&gt;특정 패턴과 부합하는 토픽의 부분 집합을 구독하는 방식 지원&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td&gt;일반적으로 질의 시점의 데이터 스냅숏을 기준으로 질의&lt;/td&gt;\n          &lt;td&gt;임의 질의를 지원하지 않음. 데이터가 변하면 클라이언트에게 전달&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n  &lt;/li&gt;\n  &lt;li&gt;p441 복수 소비자(Multiple consumers)\n    &lt;ul&gt;\n      &lt;li&gt;로드 밸런싱(Load balancing):\n        &lt;ul&gt;\n          &lt;li&gt;각 메시지는 소비자 중 &lt;strong&gt;하나&lt;/strong&gt;로 전달&lt;/li&gt;\n          &lt;li&gt;메시지를 처리하는 비용이 비싸서 처리를 병렬화하기 위해 소비자를 추가하고 싶을때 유용&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n      &lt;li&gt;팬 아웃(Fan-out):\n        &lt;ul&gt;\n          &lt;li&gt;각 메시지는 &lt;strong&gt;모든&lt;/strong&gt; 소비자에게 전달&lt;/li&gt;\n          &lt;li&gt;여러 독립적인 소비자가 브로드캐스팅된 동일한 메시지를 서로 간섭 없이 리스닝 가능&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/11-fanout.png&quot; alt=&quot;fanout&quot; title=&quot;fanout&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;p&gt;p442 확인 응답과 재전송(Acknowledgments and redelivery)&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;생산자: m1 → m2 → m3 → m4 → m5&lt;/li&gt;\n  &lt;li&gt;소비자2가 m3 처리중 장애 발생해서 소비자1로 m3 메시지 재전송.&lt;/li&gt;\n  &lt;li&gt;소비자 1에서는 m4 → m3 → m5 순으로 메시지 처리&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/11-consumer.png&quot; alt=&quot;consumer&quot; title=&quot;consumer&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;소비자마다 독립된 큐를 사용하면, 즉 부하 균형 분산 기능을 사용하지 않는다면 이 문제를 피할 수 있다.\n    &lt;ul&gt;\n      &lt;li&gt;카프카 토픽의 파티션 키(해시) - stickness&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;메시지가 서로 완전히 독립적이라면 메시지 순서가 바뀌는 것은 문제가 되지 않는다.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;p443-파티셔닝된-로그partitioned-logs&quot;&gt;p443 파티셔닝된 로그(Partitioned Logs)&lt;/h3&gt;\n\n&lt;p&gt;&lt;strong&gt;로그 기반 메시지 브로커(log- based message brokers)&lt;/strong&gt;:  데이터베이스의 지속성 있는 저장 방법과 메시징 시스템의 지연 시간이 짧은 알림 기능을 조합&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;\n    &lt;p&gt;로그를 사용한 메시지 저장소&lt;/p&gt;\n\n    &lt;p&gt;로그 == 파티션&lt;/p&gt;\n\n    &lt;ul&gt;\n      &lt;li&gt;\n        &lt;p&gt;&lt;strong&gt;p445 로그 파티셔닝(the log can be partitioned)&lt;/strong&gt;: 디스크 하나를 쓸 때보다 처리량을 높이기 위해 확장하는 방법&lt;/p&gt;\n\n        &lt;blockquote&gt;\n          &lt;ul&gt;\n            &lt;li&gt;하나의 파티션을 처리하는 작업을 공유하는 소비자 둘로 나누어 로드 밸런싱하는 방식을 사용할 수 있다.  양쪽 소비자 모두 모든 메시지를 읽지만 그 중 하나는 짝수 오프셋 메시지만 처리하고, 다른 소비자는 홀수 오프셋 메시지만 처리하는 방식.&lt;/li&gt;\n            &lt;li&gt;스레드 풀 사용을 사용해 메시지 처리 분산하는 방식. 소비자 오프셋 관리가 복잡해진다. 일반적으로 한 파티션은 단일 스레드가 처리하는 것이 적절하고,  병렬성을 높이고 싶다면 파티션 수를 늘리는 게 좋다.&lt;/li&gt;\n          &lt;/ul&gt;\n        &lt;/blockquote&gt;\n      &lt;/li&gt;\n      &lt;li&gt;\n        &lt;p&gt;단 다른 파티션 간 메시지의 순서는 보장하지 않는다.&lt;/p&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/11-partition.png&quot; alt=&quot;partition&quot; title=&quot;partition&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;\n    &lt;p&gt;p444 로그 방식과 전통적인 메시징 방식의 비교&lt;/p&gt;\n\n    &lt;p&gt;로그 기반 접근법은 팬 아웃 메시지 방식 제공:&lt;/p&gt;\n\n    &lt;ul&gt;\n      &lt;li&gt;소비자가 서로 영향 없이 독립적으로 로그를 읽을 수 있고 메시지를 읽어도 로그에서 삭제되지 않는다.&lt;/li&gt;\n    &lt;/ul&gt;\n\n    &lt;p&gt;각 클라이언트는 할당된 파티션의 메시지를 모두 소비:&lt;/p&gt;\n\n    &lt;ul&gt;\n      &lt;li&gt;토픽 하나를 소비하는 작업을 공유하는 노드 수는 많아야 해당 토픽의 로그 파티션 수로 제한.  같은 파티션 내 메시지는 같은 노드로 전달되기 때문.&lt;/li&gt;\n      &lt;li&gt;특정 메시지 처리가 느리면 파티션 내 후속 메시지 처리가 지연된다(head-of-line blocking).&lt;/li&gt;\n    &lt;/ul&gt;\n\n    &lt;table&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;td&gt;메시지 처리 비용 비싼 경우, 메시지 단위로 병렬화 처리 하고 싶은 경우, 메시지 순서는 그렇게 중요하지 않은 경우&lt;/td&gt;\n          &lt;td&gt;JMS / AMQP 방식의 메시지 브로커 적합&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td&gt;처리량이 많은 경우, 메시지를 처리하는 속도가 빠른 경우, 메시지 순서가 중요한 경우&lt;/td&gt;\n          &lt;td&gt;로그 기반 접근법 적합&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n  &lt;/li&gt;\n  &lt;li&gt;p445 소비자 오프셋(Consumer offsets)\n    &lt;ul&gt;\n      &lt;li&gt;브로커는 주기적으로 소비자 오프셋을 기록&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;p446 디스크 공간 사용\n    &lt;ul&gt;\n      &lt;li&gt;로그는 크기가 제한된 버퍼로 구현하고,  버퍼가 가득 차면 오래된 메시지 순서대로 버린다.&lt;/li&gt;\n      &lt;li&gt;원형 버퍼(circular buffer), 링 버퍼(ring buffer)&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;데이터베이스와-스트림&quot;&gt;데이터베이스와 스트림&lt;/h3&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;p448 데이터베이스에 뭔가를 기록한다는 사실은 캡처해서 저장하고 처리할 수 있는 이벤트다.&lt;/li&gt;\n  &lt;li&gt;p448 시스템 동기화 유지하기&lt;/li&gt;\n  &lt;li&gt;이중 기록(dual write)의 심각한 문제\n    &lt;ul&gt;\n      &lt;li&gt;문제 1) 타이밍이 좋지 않아 요청이 서로 교차한 경우, 두 시스템은 오류가 발생하지 않았음에도 영원히 서로 불일치 상태로 남음.&lt;/li&gt;\n      &lt;li&gt;문제2) 한쪽 쓰기가 성공할 때, 다른쪽 쓰기는 실패할 수 있다.  동시성 문제라기보다는 내결함성 문제로 두 시스템 간 불일치가 발생하는 현상.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/11-dual-write.png&quot; alt=&quot;dual-write&quot; title=&quot;dual-write&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;&lt;strong&gt;p450 변경 데이터 캡처(Change Data Capture)&lt;/strong&gt;\n    &lt;ul&gt;\n      &lt;li&gt;데이터베이스에 기록하는 모든 데이터의 변화를 관찰해 다른 시스템으로 데이터를 복제할 수 있는 형태로 추출하는 과정&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;img src=&quot;/images/2024/06/22/11-cdc.png&quot; alt=&quot;cdc&quot; title=&quot;cdc&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;파생 데이터 시스템이 레코드 시스템의 정확한 데이터 복제본을 가지게 하기 위해,  레코드 시스템에 발생하는 모든 변경 사항을 파생 데이터 시스템에 반영하는 것을 보장하는 메커니즘&lt;/li&gt;\n  &lt;li&gt;\n    &lt;p&gt;멕스웰(maxwell), 디비지움(debezium): binlog를 파싱해 유사한 방식으로 mysql용 변경 데이터 캡처를 구현&lt;/p&gt;\n  &lt;/li&gt;\n  &lt;li&gt;p452 초기 스냅숏(Initial snapshot):\n    &lt;ul&gt;\n      &lt;li&gt;데이터베이스에서 발생한 모든 변경 로그가 있다면,  로그를 재현해서 데이터베이스의 전체 상태를 재구축 가능&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;p452 로그 컴팩션(Log compaction):\n    &lt;ul&gt;\n      &lt;li&gt;저장 엔진은 주기적으로 같은 키의 로그 레코드를 찾아 중복을 제거&lt;/li&gt;\n      &lt;li&gt;각 키에 대해 가장 최근에 갱신된 내용만 유지&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;p453 변경 스트림용 API 지원\n    &lt;ul&gt;\n      &lt;li&gt;kafka connect;\n        &lt;ul&gt;\n          &lt;li&gt;변경 이벤트를 스트림하는데 카프카 사용&lt;/li&gt;\n          &lt;li&gt;검색 색인과 같은 파생 데이터 시스템을 갱신하는데 사용 가능&lt;/li&gt;\n          &lt;li&gt;스트림 처리 시스템에도 이벤트 공급이 가능&lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n\n&lt;h2 id=&quot;11장-스트림-처리p45411장-끝&quot;&gt;11장: 스트림 처리(p454~11장 끝)&lt;/h2&gt;\n\n&lt;p&gt;&lt;strong&gt;이벤트 소싱(event sourcing)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;CDC:\n    &lt;ul&gt;\n      &lt;li&gt;데이터베이스를 변경 가능한 방식으로 사용해 레코드를 자유롭게 갱신하고 삭제&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;이벤트 소싱:\n    &lt;ul&gt;\n      &lt;li&gt;이벤트 로그에 기록된 불변 이벤트를 기반으로 명시적으로 구축&lt;/li&gt;\n      &lt;li&gt;이벤트 저장은 추가만 가능하고 갱신이나 삭제는 권장하지 않거나 금지&lt;/li&gt;\n      &lt;li&gt;어떤 상황이 발생한 후에 상황 파악이 쉽기 때문에 디버깅에 도움이 되고 버그 방지&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;이벤트 로그에서 현재 상태 파생하기&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;시스템에 기록한 데이터를 표현한 이벤트 로그를 가져와 사용자에게 보여주기에 적당한 애플리케이션 상태(시스템에서 데이터를 읽는 방식)로 변환해야 한다.\n    &lt;ul&gt;\n      &lt;li&gt;예시; 현재 장바구니 내용(변경 사항은 관심사 x)&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;CDC: 레코드의 가장 새로운 버전만 보유.  같은 키의 이전 이벤트는 로그 컴팩션을 통해 버린다&lt;/li&gt;\n  &lt;li&gt;이벤트 소싱: 뒤에 발생한 이벤트가 앞선 이벤트를 덮어쓰지 않는다. 마지막 상태 재구축을 위해서는 이벤트의 전체 히스토리가 필요하다.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;p455 명령과 이벤트&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;명령이 실행 가능한지 확인.&lt;/li&gt;\n  &lt;li&gt;무결성이 검증되고 명령이 승인되면 명령은 지속성 있는 불변 이벤트가 된다.&lt;/li&gt;\n  &lt;li&gt;명령의 유효성은 이벤트가 되기 전에 동기식으로 검증해야 한다.\n    &lt;ul&gt;\n      &lt;li&gt;좌석 예약 예시.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;p456 상태와 스트림 그리고 불변성&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;상태의 본질을 변하는 것이다. 상태가 어떻게 바뀌었든 항상 이런 변화를 일으킨 일련의 이벤트가 있다.&lt;/li&gt;\n  &lt;li&gt;변경 가능한 상태와 추가 전용 불변 이벤트 로그는 마치 동전의 양면과 같이 서로 모순이다. 모든 &lt;strong&gt;변경 로그(changelog)&lt;/strong&gt;는 시간이 지남에 따라 바뀌는 상태를 나타낸다.&lt;/li&gt;\n  &lt;li&gt;변경 로그를 지속성 있게 저장한다면 상태를 간단히 재생성할 수 있는 효과가 있다. 이벤트 로그를 레코드 시스템으로 생각하고 모든 변경 가능 상태를 이벤트 로그로부터 파생된 것으로 생각하면 시스템을 거치는 데이터 흐름에 관해 추론하기 쉽다.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;p458 동일한 이벤트 로그로 여러 가지 뷰 만들기&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;CQRS(Command Query Responsibility Segregation)&lt;/li&gt;\n  &lt;li&gt;읽기 최적화된 뷰(read-optimized views)는 데이터를 비정규화하는 것이 전적으로 합리적이다.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;동시성 제어&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;이벤트가 아직 읽기 뷰에 반영되지 않았을 가능성&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;p460 불변성의 한계&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;성능적인 이유 외에도. 사생활 침해 규제로 인해 개인 정보 지울 필요가 있다든지, 데이터 보호법에 따라 잘못된 정보를 삭제해야 한다든지, 민감한 정보가 우발적으로 노출되는 것을 방지해야 하는 경우&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;p461-스트림-처리&quot;&gt;&lt;strong&gt;p461 스트림 처리&lt;/strong&gt;&lt;/h3&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;몇 분 동안 실행된 일괄 처리 작업은 실패한 태스크를 처음부터 재시작하는 것으로 충분,  하지만 몇 년 동안 실행 중인 스트림 작업은 장애 발생 이후 처음부터 재시작하는 방법은 비현실적.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;p462 복잡한 이벤트 처리&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;CEP(Complex event processing): 특정 이벤트 패턴을 검색해야 하는 애플리케이션에 적합&lt;/li&gt;\n  &lt;li&gt;스트림 분석(Stream analytics): 연속한 특정 이벤트 패턴을 찾는 것보다 대량의 이벤트를 집계하고 통계적 지표를 뽑는 것을 더 우선.\n    &lt;ul&gt;\n      &lt;li&gt;특정 유형의 이벤트 빈도 측정(시간당 얼마나 자주 발생하는지)&lt;/li&gt;\n      &lt;li&gt;특정 기간에 걸친 값의 이동 평균(ROLLING AVERAGE) 계산&lt;/li&gt;\n      &lt;li&gt;이전 시간 간격과 현재 통계값의 비교(추세 감지,  지난주 대비 비정상적으로 높거나 낮은 지표에 대해 경고)&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;p464 구체화 뷰(materialized views) 유지하기&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;이벤트 소싱에서 애플리케이션 상태는 이벤트 로그를 적용함으로써 유지된다.  여기서 애플리케이션 상태는 일종의 구체화 뷰다.&lt;/li&gt;\n  &lt;li&gt;구체화 뷰를 만들려면 잠재적으로 임의의 시간 범위에 발생한 모든 이벤트가 필요하다.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;p464 스트림 상에서 검색하기&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;질의를 먼저 저장.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;메시지 전달과 RPC&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;메시지 전달이 stream 처리, RPC는 액터.  액터는 1:1로 메시지 주고 받는 형태.  스트림 처리는 카프카처럼 데이터 파이프라인 구축.  스트림 처리는 메시지 관심있는 사람 여러명이 관심.  1:1 / 1:n 또는 n:m&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;액터:\n    &lt;ul&gt;\n      &lt;li&gt;주로 동시성 관리, 통신 모듈을 분산 실행하는 매커니즘&lt;/li&gt;\n      &lt;li&gt;액터간 통신은 주로 단기적이고 일대일&lt;/li&gt;\n      &lt;li&gt;임의의 방식으로 통신 가능&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;스트림 처리:\n    &lt;ul&gt;\n      &lt;li&gt;기본적으로 데이터 관리 기법&lt;/li&gt;\n      &lt;li&gt;지속성 있고 다중 구독 가능&lt;/li&gt;\n      &lt;li&gt;비순환 파이프라인에 설정됨.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;p468 어떤 시계를 사용할 것인가?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;잘못된 장치 시계를 조정하는 한 가지 방법: 세 가지 타임스탬프를 로그로 남기기&lt;/p&gt;\n\n&lt;ol&gt;\n  &lt;li&gt;이벤트가 발생한 시간 - 장치 시계&lt;/li&gt;\n  &lt;li&gt;이벤트를 서버로 보낸 시간 - 장치 시계&lt;/li&gt;\n  &lt;li&gt;서버에서 이벤트를 받은 시간 - 서버 시계&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;2와 3 차이를 구하면 장치 시계와 서버 시계 간의 오프셋 추정 가능.  계산한 오프셋을 이벤트 타임스탬프에 적용해 이벤트가 실제로 발생한 시간을 추정.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;p469 윈도우 유형&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;텀블링 윈도우(Tumbling window)&lt;/li&gt;\n  &lt;li&gt;홉핑 윈도우(Hopping window)&lt;/li&gt;\n  &lt;li&gt;슬라이딩 윈도우(Sliding window)&lt;/li&gt;\n  &lt;li&gt;세션 윈도우(Session window)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;p474-내결함성fault-tolerance&quot;&gt;p474 내결함성(&lt;strong&gt;Fault Tolerance)&lt;/strong&gt;&lt;/h3&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;태스크를 재시작하는 것은 사실 레코드를 여러 번 처리할 수도 있다는 뜻. 출력은 한 번만 처리된 것으로 보이는 효과.\n    &lt;ul&gt;\n      &lt;li&gt;&lt;strong&gt;exactly-once semantics(정확히 한 번 시맨틱), effectively-once(결과적으로 한 번)&lt;/strong&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Microbatching and checkpointing(마이크로 일괄 처리와 체크포인트)&lt;/li&gt;\n  &lt;li&gt;Atomic commit revisited(원자적 커밋 재검토)&lt;/li&gt;\n  &lt;li&gt;멱등성(Idempotence)&lt;/li&gt;\n  &lt;li&gt;실패 후에 상태 재구축하기(Rebuilding state after a failure)&lt;/li&gt;\n&lt;/ul&gt;\n\n\n            \n          ",
        "contentSnippet": "<h2 id=\"1장-신뢰할-수-있고-확장-가능하며-유지보수하기-쉬운-애플리케이션\">1장. 신뢰할 수 있고 확장 가능하며 유지보수하기 쉬운 애플리케이션</h2>\n\n<p>p.6</p>\n\n<p><strong>신뢰성(Reliability)</strong>\n하드웨어나 소프트웨여 결함, 심지어 인적 오류(human error) 같은 역경에 직면하더라도 시스템은 지속적으로 올바르게 동작(원하는 성능 수준에서 정확한 기능을 수행)해야 한다.</p>\n\n<p><strong>확장성(Scalability)</strong>\n시스템의 데이터 양, 트래픽 양, 복잡도가 증가하면서 이를 처리할 수 있는 적절한 방법이 있어야 한다.</p>\n\n<p><strong>유지보수성(Maintainability)</strong>\n시간이 지남에 따라 여러 다양한 사람들이 시스템 상에서 작업(현재 작업을 유지보수하고 새로운 사용 사례를 시스템에 적용하는 엔지니어링과 운영)할 것이기 때문에 모든 사용자가 시스템 상에서 생산적으로 작업할 수 있게 해야 한다.</p>\n\n<p>p.6-7</p>\n\n<p>소프트웨어의 일반적인 기대치</p>\n<ul>\n  <li>애플리케이션 사용자가 거대한 기능을 수행한다.</li>\n  <li>시스템은 사용자가 범한 실수나 예상치 못한 소프트웨어 사용법을 허용할 수 있다.</li>\n  <li>시스템 성능은 예상된 부하와 데이터 양에서 필수적인 사용 사례를 충분히 만족한다.</li>\n  <li>시스템은 허가되지 않은 접근과 오남용을 방지한다.</li>\n</ul>\n\n<p>잘못될 수 있는 일을 결함(fault)이라 부른다. 그리고 결함을 예측하고 대처할 수 있는 시스템을 내결함성(fault-tolerant) 또는 탄력성(resilient)을 지녔다고 말한다.</p>\n\n<p>결함은 장애(failure)와 동일하지 않다. 일반적으로 결함은 사양에서 벗어난 시스템의 한 구성 요소로 정의되지만, 장애는 사용자에게 필요한 서비스를 제공하지 못하고 시스템 전체가 멈춘 경우다. … 대개 결함으로 인해 장애가 발생하지 않게끔 내결함성 구조를 설계하는 것이 가장 좋다.</p>\n\n<p>p.9-10</p>\n\n<p>대규모 인터넷 서비스에 대한 한 연구에 따르면 운영자의 설정 오류가 중단의 주요원인인 반면 하드웨어(서버나 네트워크) 결함은 중단 원인의 10~25% 정도에 그친다.</p>\n<ul>\n  <li>오류의 가능성을 최소화하는 방향으로 시스템을 설계하라</li>\n  <li>사람의 실수로 장애가 발생할 수 있는 부분을 분리하라</li>\n  <li>모든 수준에서 철저하게 테스트하라</li>\n  <li>인적 오류를 빠르고 쉽게 복구할 수 있게 하라</li>\n  <li>모니터링 대책을 마연하라</li>\n  <li>조작 교육과 실슴을 시행하라</li>\n</ul>\n\n<p>p.11\n부하는 <strong>부하 매개변수(load paramter)</strong>라 부르는 몇 개의 숫자로 나타낼 수 있다. … 부하 매개변수로 웹 서버의 초당 요청 수, 데이터베이스의 읽기 대 쓰기 비율, 대화방의 동시 활성 사용자(active user), 캐시 적중률 등이 될 수 있다.</p>\n\n<p>p.19 유지보수성</p>\n\n<p><strong>운용성(operability)</strong>\n운영팀이 시스템을 원활하게 운영할 수 있게 쉽게 만들어라.</p>\n\n<p><strong>단순성(Simplicity)</strong>\n시스템에서 복잡도를 최대한 제거해 새로운 엔지니어가 시스템을 이해하기 쉽게 만들어라(사용자 인터페이스의 단순성과는 다르다는 점에 유의하라).</p>\n\n<p><strong>발전성(Evolvability)</strong>\n엔지니어가 이후에 시스템을 쉽게 변경할 수 있게 하라. 그래야 요구사항 변경 같은 예기치 않은 사용 사례를 적용하기 쉽다. 이 속성은 유연성(extensibility), 수정 가능성(modifiability), 적응성(plasticity)으로 알려져 있다.</p>\n\n<p>p.20 좋은 운영성</p>\n<ul>\n  <li>좋은 모니터링으로 런타임(runtime) 동작과 시스템의 내부에 대한 가시성 제공</li>\n  <li>표준 도구를 이용해 자동화와 통합을 위한 우수한 지원을 제공</li>\n  <li>개별 장비 의존성을 회피. 유지보수를 위해 장비를 내리더라도 시스템 전체에 영향을 주지 않고 계속해서 운영 가능해야 함</li>\n  <li>좋은 문서와 이해하기 쉬운 운영 모델(예를 들어 “X를 하면 Y가 발생한다”) 제공</li>\n  <li>만족할 만한 기본 동작을 제공하고, 필요할 때 기본값을 다시 정의할 수 있는 자유를 관리자에게 부여</li>\n  <li>적절하게 자기 회복(self-healing)이 가능할 뿐 아니라 필요에 따라 관리자가 시스템 상태를 수동으로 제어할 수 있게 함</li>\n  <li>예측 가능하게 동작하고 예기치 않은 상황을 최소화함</li>\n</ul>\n\n<hr />\n\n<h2 id=\"2장-데이터-모델과-질의-언어\">2장. 데이터 모델과 질의 언어</h2>\n\n<p>p.32 지역성(locality). JSON 표현에서는 모든 관련 정보가 한 곳에 있어 질의 하나로 충분하다.</p>\n\n<p>p.40 쓰기 스키마(schema-on-write)(관계형 데이터베이스의 전통적인 접근 방식으로 스키마는 명시적이고 데이터 베이스는 쓰여진 모든 데이터가 스키마를 따르고 있음을 보장한다)와 반대되는 읽기 스키마(schema-on-read)(데이터 구조는 암묵적이고 데이터를 읽을 때만 해석된다)</p>\n\n<p>p.43 선언형 질의\nSQL이나 관계 대수 같은 선언형 질의 언어에서는 목표를 달성하기 위한 방법이 아니라,  알고자 하는 데이터의 패턴, 즉 결과가 충족해야 하는 조건과 데이터를 어떻게 변환(예를 들어 정렬, 그룹화, 집계)할지를 지정하기만 하면 된다.</p>\n\n<hr />\n\n<h2 id=\"3장-저장소와-검색\">3장: 저장소와 검색</h2>\n\n<ul>\n  <li>p.72 특정 작업부(workload) 유형에서 좋은 성능을 내게끔 저장소 엔진을 조정하려면 저장소 엔진이 내부에서 수행되는 작업에 대해 대략적인 개념을 이해할 필요가 있다.</li>\n  <li>p.74 색인을 잘 선택했다면 읽기 질의 속도가 향상된다. 하지만 모든 색인은 쓰기 속도를 떨어뜨린다.</li>\n  <li>p.74 키를 데이터 파일의 바이트 오프셋에 매핑해 인메모리 해시 맵을 유지하는 전략이다.</li>\n  <li>p.74 값을 조회하려면 해시 맵을 사용해 데이터 파일에서 오프셋을 찾아 해당 위치를 구하고 값을 읽는다.</li>\n  <li>p.75 컴팩션은 로그에서 중복된 키를 버리고 각 키의 최신 갱신 값만 유지하는 것을 의미한다.</li>\n  <li>p.77 불행하게도 디스크 상의 해시 맵에 좋은 성능을 기대하기란 어렵다. 이는 무작위 접근 I/O가 많이 필요하고 디스크가 가득 찼을 때 확장하는 비용이 비싸며 해시 충돌 해소를 위해 성가신 로직이 필요하다.</li>\n  <li>p.78 해시 테이블은 범위 질의(range query)에 효율적이지 않다.</li>\n  <li>p.79 그래도 handbag과 handsome 키의 오프셋을 알고 있고 정렬돼 있으므로 handiwork는 두 키 사이에 있다는 사실을 알 수 있다.</li>\n  <li>p.80 저장소 엔진\n    <ul>\n      <li>쓰기가 들어오면 인메모리 균형 트리(balanced tree) 데이터 구조(예를 들어 레드 블랙 트리)에 추가한다. 이 인메모리 트리는 멤테이블(memtable)이라고도 한다.</li>\n      <li>멤테이블이 보통 수 메가바이트 정도의 임곗값보다 커지면 SS테이블 파일로 디스크에 기록한다. 트리가 이미 키로 정렬된 키-값 쌍을 유지하고 있기 때문에 효율적으로 수행할 수 있다. 새로운 SS테이블 파일은 데이터베이스의 가장 최신 세그먼트가 된다. SS테이블을 디스크에 기록하는 동안 쓰기는 새로운 멤테이블 인스턴스에 기록한다.</li>\n      <li>읽기 요청을 제공하려면 먼저 멤테이블에서 키를 찾아야 한다. 그다음 디스크 상의 가장 최신 세그먼트에서 찾는다. 그다음으로 두 번째 오래된 세그먼트, 세 번째 오래된 세그먼트 등에서 찾는다.</li>\n      <li>가끔 세그먼트 파일을 합치고 덮어 쓰여지거나 삭제된 값을 버리는 병합과 컴팩션 과정을 수행한다. 이 과정은 백그라운드에서 수행된다.</li>\n    </ul>\n  </li>\n  <li>p.80 정렬된 파일 병합과 컴팩션 원리를 기반으로 하는 저장소 엔진을 LSM 저장소 엔진이라 부른다.</li>\n  <li>p.81 키를 단어(용어)로, 값은 단어를 포함한 모든 문서의 ID 목록(포스팅 목록(postings list))으로 하는 키-값 구조로 구현한다.</li>\n  <li>p.81 블룸 필터는 키가 데이터베이스에 존재하지 않음을 알려주므로 존재하지 않는 키를 위한 불필요한 디스크 읽기를 많이 절약할 수 있다.</li>\n  <li>p.81 백그라운드에서 연쇄적으로 SS테이블을 지속적으로 병합하는 것이다. 이 개념은 데이터셋이 가능한 메모리보다 훨씬 더 크더라도 여전히 효과적이다. 데이터가 정렬된 순서로 저장돼 있다면 범위 질의를 효율적으로 실행할 수 있다.</li>\n  <li>p.82 B 트리는 전통적으로 4KB 크기(때로는 더 큰)의 고정 크기 블록이나 페이지로 나누고 한 번에 하나의 페이지에 읽기 또는 쓰기를 한다. 디스크가 고정 크기 블록으로 배열되기 때문에 이런 설계는 근본적으로 하드웨어와 조금 더 밀접한 관련이 있다.</li>\n  <li>p.83 새로운 키를 수용한 페이지에 충분한 여유 공간이 없다면 페이지 하나를 반쯤 채워진 페이지 둘로 나누고 상위 페이지가 새로운 키 범위의 하위 부분들을 알 수 있게 갱신한다.</li>\n  <li>p.99 칼럼 지향 저장소. 모든 값을 하나의 로우에 함께 저장하지 않는 대신 각 칼럼별로 모든 값을 함께 저장한다. 각 칼럼을 개별 파일에 저장하면 질의에 사용되는 칼럼만 읽고 구분 분석하면 된다. 이 방식을 사용하면 작업량이 많이 줄어든다.</li>\n</ul>\n\n<hr />\n\n<h2 id=\"05장-복제\">05장: 복제</h2>\n\n<ul>\n  <li>p.156 동기식 복제의 장점은 팔로워가 리더와 일관성 있게 최신 데이터 복사본을 가지는 것을 보장한다. 갑자기 리더가 작동하지 않아도 데이터는 팔로워에서 계속 사용할 수 있음을 확신할 수 있다. 단점은 (팔로워가 죽거나 네트워크 문제나 다른 어떤 이유로 인해) 동기 팔로워가 응답하지 않는다면 쓰기가 처리될 수 없다는 것이다. 리더는 모든 쓰기를 차단(block)하고 동기 복제 서버가 다시 사용할 수 있을때까지 기다려야 한다.</li>\n  <li>p.159 팔로워 중 하나를 새로운 리더로 승격해야 하고 클라이언트는 새로운 리더로 쓰기를 전송하기 위해 재설정이 필요하며 다른 팔로워는 새로운 리더로부터 데이터 변경을 소비하기 시작해야 한다. 이 과정을 장애 복구(failover)라 한다.</li>\n  <li>p.167 Monotonic Read: 각 사용자의 읽기가 항상 동일한 복제 서버에서 수행되게끔 하는 것</li>\n  <li>p.168 Consistent Prefix Read: 일련의 쓰기가 특정 순서로 발생한다면 이 쓰기를 읽는 모든 사용자는 같은 순서로 쓰여진 내용을 보게 됨을 보장한다.</li>\n  <li>p.174 충돌을 처리하는 제일 간단한 전략은 충돌을 피하는 것이다. 특정 레코드의 모든 쓰기가 동일한 리더를 거치도록 애플리케이션이 보장한다면 충돌은 발생하지 않는다.</li>\n  <li>p.175 Automatic Conflict Resolution\n    <ul>\n      <li><a href=\"https://assets.amazon.science/ac/1d/eb50c4064c538c8ac440ce6a1d91/dynamo-amazons-highly-available-key-value-store.pdf\">Dynamo: Amazon’s Highly Available Key-value Store</a></li>\n    </ul>\n  </li>\n  <li>p.179 Dynamo Style: 리더의 개념을 버리고 모든 복제 서버가 클라이언트로부터 쓰기를 직접 받을 수 있게 허용하는 접근 방식</li>\n  <li>p.180 Outdated 해결: 클라이언트가 데이터베이스에서 읽을 때 하나의 복제 서버로 요청을 보내지 않고 읽기 요청을 병렬로 여러 노드에 전송. 클라이언트는 여러 노드에서 다른 응답을 받음. 즉, 한 노드에서는 최신 값을 받고 다른 노드에서는 오래된 값을 받음. 이 때 버전 숫자를 사용해 어떤 값이 최신 내용인지 결정.</li>\n  <li>p.180 Anti-entropy\n    <ul>\n      <li><a href=\"https://medium.com/@adityashete009/anti-entropy-and-merkel-trees-amazon-dynamodb-part-4-efbf1f7285c0\">Anti-Entropy and Merkel Trees: Amazon DynamoDB (Part 4)</a></li>\n      <li><a href=\"https://www.hemantkgupta.com/p/insights-from-paper-part-ii-dynamo\">Insights from paper (Part II) — Dynamo: Amazon’s Highly Available Key-value Store</a></li>\n      <li><a href=\"https://efficientcodeblog.wordpress.com/2017/12/26/read-repair-and-anti-entropy-two-ways-to-remedy-replication-lag-in-dynamo-style-datastores-leaderless-replication/\">Read Repair and Anti-Entropy : Two Ways To Remedy Replication Lag in Dynamo-style Datastores (Leaderless Replication)</a></li>\n    </ul>\n  </li>\n  <li>p.188. Concurrent: 한 작업이 다른 작업 이전에 발생했는지가 동시성의 의미를 정의하는 핵심. 사실 작업이 다른 작업보다 먼저 발생하지 않으면 (즉 어느 작업도 다른 작업에 대해 알지 못한다면) 단순히 동시 작업이라 말한다.\n    <ul>\n      <li>동시성을 정의하기 위해 정확한 시각은 중요하지 않다. 두 작업이 발생한 물리적인 시각보다 각 작업이 서로 알지 못하면 단순히 두 작업은 동시에 수행됐다 말한다.</li>\n    </ul>\n  </li>\n</ul>\n\n<hr />\n\n<h2 id=\"06장-파티셔닝\">06장: 파티셔닝</h2>\n\n<ul>\n  <li>p199 데이터셋이 매우 크거나 질의 처리량이 매우 높다면 복제만으로는 부족하고 데이터를 파티션으로 쪼갤 필요가 있다. 이 작업을 샤딩이라고도 한다.</li>\n  <li>p200 파티셔닝하는 주된 이유는 확장성.  단일 파티션에 실행되는 질의를 생각해 보면 각 노드에서 자신의 파티션에 해당하는 질의를 독립적으로 실행할 수 있으므로 노드를 추가함으로써 질의 처리량을 늘릴 수 있다. 크고 복잡한 질의는 훨씬 더 어렵기는 하지만 여러 노드에서 병렬 실행이 가능하다.</li>\n  <li>p203 키 범위 기준 파티셔닝은 특정한 접근 패턴이 핫스팟을 유발하는 단점이 있다.</li>\n  <li>p203 좋은 해시 함수는 쏠린 데이터를 입력으로 받아 균일하게 분산되게 한다.</li>\n  <li>p204 파티셔닝에 키의 해시값을 사용해서 파티셔닝하면 키 범위 파티셔닝의 좋은 속성을 잃어 버린다. 바로 범위 질의를 효율적으로 실행할 수 있는 능력이다. 전에는 인접했던 키들이 이제는 모든 파티션에 흩어져서 정렬 순서가 유지되지 않는다.</li>\n  <li>p204 복합 키의 첫 번째 컬럼에 값 범위로 검색하는 질의를 쓸 수 없지만,  첫 번째 칼럼에 고정된 값을 지정하면 키의 다른 컬럼에 대해서는 범위 스캔 가능</li>\n  <li>Partitioning Secondary Indexes by Document:\n    <ul>\n      <li>p207 각 파티션이 독립적. 자신의 보조 인덱스를 유지하며 그 파티션에 속하는 문서만 담당. 다른 파티션에 어떤 데이터가 저장되는지는 신경 쓰지 않음.</li>\n      <li>p207 보조 인덱스를 써서 읽는 질의는 큰 비용 발생. 여러 파티션에서 질의를 병렬 실행하더라도 꼬리 지연 시간 증폭이 발생하기 쉬움.</li>\n    </ul>\n  </li>\n  <li>Partitioning Secondary Indexes by Term:\n    <ul>\n      <li>p208 모든 파티션의 데이터를 담당하는 전역 색인을 만들 수도 있음</li>\n      <li>p208 용어 자체로 파티셔닝하면 범위 스캔에 유용함.  반면 용어의 해시값을 사용해 파티셔닝하면 부하가 좀 더 고르게 분산됨.</li>\n      <li>p208 읽기 효율적.  쓰기가 느리고 복잡함.</li>\n      <li>p209 대개 비동기로 갱신됨.  쓰기를 실행한 후 바로 인덱스를 읽으면 변경 사항이 반영되지 않을 수도 있다.</li>\n    </ul>\n  </li>\n  <li>Rebalancing Partitions:\n    <ul>\n      <li>p209 리밸런싱:  클러스터에서 한 노드가 담당하던 부하를 다른 노드로 옮기는 과정</li>\n      <li>p210 mod 연산 쓰지 마라.  리밸런싱 비용 지나치게 커지고, 데이터를 필요 이상으로 이동하지 않는 방법 필요.</li>\n      <li>p211 파티션 개수 고정:  파티션이 너무 크면 리밸런싱을 실행할 때와 노드 장애로부터 복구 비용이 큼.  그러나 파티션이 너무 작으면 오버헤드가 너무 커짐.</li>\n      <li>p212 동적 파티셔닝:  파티션 개수가 전체 데이터 용량에 맞춰 조정되는 이점.  파티션 개수가 데이터셋 크기에 비례.  데이터 양이 작으면 파티션 개수가 적어도 되므로 오버헤드 적다.   HBase, MongoDB는 빈 디비에 초기 파티션 집합 설정(pre-splitting)</li>\n      <li>p213 노드 비례 파티셔닝:  노드 대수가 변함 없는 동안은 개별 파티션 크기가 데이터셋 크기에 비례.  노드 대수 늘리면 파티션 크기 다시 작아짐.</li>\n    </ul>\n  </li>\n  <li>카산드라\n    <ul>\n      <li>https://d2.naver.com/helloworld/1039</li>\n      <li>https://tjddnjs.tistory.com/91</li>\n      <li>p.213 리밸런싱 알고리즘 <a href=\"https://www.datastax.com/blog/new-token-allocation-algorithm-cassandra-30\">New token allocation algorithm in Cassandra 3.0</a></li>\n      <li>p.216 가십 프로토콜  <a href=\"https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/architecture/archGossipAbout.html\">Apache Cassandra™ 3.x - Internode communications (gossip)</a></li>\n    </ul>\n  </li>\n</ul>\n\n<hr />\n\n<h2 id=\"07장-트랜잭션\">07장: 트랜잭션</h2>\n\n<ul>\n  <li>p222 트랜잭션은 전체가 성공(commit)하거나 실패(abort, rollback).</li>\n</ul>\n\n<h4 id=\"p223-acid\">p223 ACID</h4>\n\n<p><strong>Atomicity</strong></p>\n\n<ul>\n  <li>Not\n    <ul>\n      <li>In the context of ACID, atomicity is <strong><em>not</em></strong> about concurrency.</li>\n      <li>It does not describe what happens if several processes try to access the same data at the same time, because that is covered under the letter <em>I</em>, for <em><strong>isolation</strong>.</em></li>\n    </ul>\n  </li>\n  <li>여러 쓰기 작업이 하나의 원자적인 트랜잭션으로 묶여 있는데 결함 때문에 완료(commit)될 수 없다면 abort되고 데이터베이스는 이 트랜잭션에서 지금까지 실행한 쓰기를 무시하거나 취소(undo) 해야 한다.</li>\n  <li>Perhaps <strong><em>abortability</em></strong> would have been a better term than <em><strong>atomicity</strong>.</em></li>\n</ul>\n\n<p>Consistency</p>\n\n<ul>\n  <li>The idea of ACID consistency is that you have certain statements about your data (<strong><em>invariants</em></strong>) that must always be true.</li>\n  <li>it’s the application’s responsibility to define its transactions correctly so that they preserve consistency.</li>\n  <li>The letter C doesn’t really belong in ACID.\n    <ul>\n      <li><strong>Atomicity</strong>, <strong>Isolation</strong>, <strong>Durability</strong> are properties of the database, whereas <strong>Consistency</strong> (in the ACID sense) is a property of the application.</li>\n    </ul>\n  </li>\n</ul>\n\n<p><strong>Isolation</strong></p>\n\n<ul>\n  <li>In the sense of ACID means that concurrently executing transactions are isolated from each other: they cannot step on each other’s toes.</li>\n  <li>The database ensures that when the transactions have committed, the result is the same as if they had run <strong><em>serially</em></strong> (one after another), even though in reality they may have run concurrently.</li>\n</ul>\n\n<p><img src=\"/images/2024/06/22/07-transaction-race-conditon.png\" alt=\"transaction-race-conditon\" title=\"transaction-race-conditon\" class=\"center-image\" /></p>\n\n<p><strong>Durability</strong></p>\n\n<ul>\n  <li>any data it has written will not be forgotten.</li>\n</ul>\n\n<p><strong>Single-Object and Multi-Object Operations</strong></p>\n\n<ul>\n  <li><strong>atomicity</strong> and <strong>isolation</strong> describe what the database should do if a client makes several writes within the same transaction.\n    <ul>\n      <li><strong><em>Atomicity:</em></strong>  If an error occurs halfway through a sequence of writes, the transaction should be aborted, and the writes made up to that point should be discarded. In other words, the database saves you from having to worry about partial failure, by giving an all-or-nothing guarantee.</li>\n      <li><em><strong>Isolation</strong>:</em>  Concurrently running transactions shouldn’t interfere with each other. For example, if one transaction makes several writes, then another transaction should see either all or none of those writes, but not some subset.</li>\n    </ul>\n  </li>\n  <li>A transaction is usually understood as a mechanism for grouping multiple operations on multiple objects into one unit of execution.</li>\n</ul>\n\n<p><strong>Handling errors and aborts</strong></p>\n\n<ul>\n  <li><strong>best effort</strong>:  “the database will do as much as it can, and if it runs into an error, it won’t undo something it has already done”.  so it’s the application’s responsibility to recover from errors.</li>\n  <li>the whole point of aborts is to enable safe retries.</li>\n</ul>\n\n<h3 id=\"p232-weak-isolation-levels\">p232 <strong>Weak Isolation Levels</strong></h3>\n\n<ul>\n  <li><strong><em>serializable</em> isolation</strong>:  the database guarantees that transactions have the same effect as if they ran <em>serially</em> (i.e., one at a time, without any concurrency).</li>\n</ul>\n\n<p><strong>Read Committed</strong></p>\n\n<ol>\n  <li>When reading from the database, you will only see data that has been committed\n(no <em>dirty reads</em>).\n    <ul>\n      <li>아직 커밋되지 않은 롤백 데이터 읽을 수 있음  If the database allows dirty reads, that means a transaction may see data that is later rolled back—i.e., which is never actually committed to the database. Reasoning about the consequences quickly becomes mind-bending.</li>\n      <li>새 값이 커밋돼야만 다른 트랜잭션들이 새 값을 읽을 수 있음. While the transaction is ongoing, any other transactions that read the object are simply given the old value. Only when the new value is committed do transactions switch over to reading the new value.</li>\n    </ul>\n  </li>\n  <li>When writing to the database, you will only overwrite data that has been committed (no <em>dirty writes</em>).</li>\n</ol>\n\n<p><strong>p236-240. Snapshot Isolation and Repeatable Read</strong></p>\n\n<ul>\n  <li>Read skew is considered acceptable under read committed isolation.\n    <ul>\n      <li>skew; <em>timing anomaly</em>.</li>\n    </ul>\n  </li>\n  <li><strong><em>readers never block writers, and writers never block readers</em></strong>.</li>\n  <li><strong><em>multi-version concurrency control</em> (MVCC)</strong></li>\n</ul>\n\n<p><img src=\"/images/2024/06/22/07-transaction-multi-version.png\" alt=\"transaction-multi-version\" title=\"transaction-multi-version\" class=\"center-image\" /></p>\n\n<ul>\n  <li>객체를 볼 수 있는 조건:\n    <ul>\n      <li>읽기 트랜잭션 실행 시점에 이미 커밋된 상태여야함. At the time when the reader’s transaction started, the transaction that created the\n  object had already committed.</li>\n      <li>The object is not marked for deletion, or if it is, the transaction that requested\n  deletion had not yet committed at the time when the reader’s transaction started.</li>\n    </ul>\n  </li>\n</ul>\n\n<h3 id=\"p242-preventing-lost-updates\"><strong>p242~ Preventing Lost Updates</strong></h3>\n\n<ul>\n  <li>The read committed and snapshot isolation levels:  동시 쓰기할 때 read-only 트랜잭션이 무엇을 볼 수 있는지. The guarantees of what a read-only transaction can see in the presence of concurrent writes.</li>\n  <li>If two transactions do this concurrently, one of the modifications can be lost, because the second write does not include the first modification. (later write <em>clobbers</em> the earlier write.)</li>\n</ul>\n\n<p><strong>Atomic write operations</strong></p>\n\n<ul>\n  <li>MongoDB(document db);  atomic operations for making local modifications to a part of a JSON document.</li>\n  <li>Redis;  atomic operations for modifying data structures such as priority queues.</li>\n  <li>Atomic operations are usually implemented by taking an exclusive lock on the object.</li>\n</ul>\n\n<p><strong>Explicit locking</strong></p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">SELECT **FOR UPDATE**;</code></li>\n  <li>It’s easy to forget to add a necessary lock somewhere in the code, and thus introduce a race condition.</li>\n</ul>\n\n<p><strong>Compare-and-set</strong></p>\n\n<ul>\n  <li>마지막으로 읽은 후로 변경되지 않았을 때만 갱신 허용.  To avoid lost updates by allowing an update to happen only if the value has not changed since you last read it.</li>\n  <li>if the database allows the WHERE clause to read from an old snapshot, this statement may not prevent lost updates.\n    <ul>\n      <li>because the condition may be true even though another concurrent write is occurring.</li>\n    </ul>\n  </li>\n</ul>\n\n<p><strong>Conflict resolution and replication</strong></p>\n\n<ul>\n  <li>여러 충돌 버전 생성을 허용하고 사후에 충돌 해소. Allow concurrent writes to create several conflicting versions of a value (also known as <em>siblings</em>), and to use application code or\nspecial data structures to resolve and merge these versions after the fact.</li>\n  <li>최종 쓰기 승리는 갱신 손실 발생하기 쉬움.  The <strong><em>last write wins</em> (LWW)</strong> conflict resolution method is prone to lost updates.  LWW is the default in many replicated databases.</li>\n</ul>\n\n<p><strong>p246 Write Skew and Phantoms</strong></p>\n\n<p><strong>Characterizing write skew</strong></p>\n\n<ul>\n  <li>쓰기 스큐는 두 트랜잭션이 다른 객체 갱신.  dirty write와 lost update는 다른 트랜잭션이 하나의 동일 객체 갱신.   Write skew can occur if two transactions read the same objects, and then update some of those objects (different transactions may update different objects). In the special case where different transactions update the same object, you get a dirty write or lost update anomaly (depending on the timing).</li>\n  <li>직렬성 격리로 write skew 자동 방지.  Automatically preventing write skew requires true serializable isolation.</li>\n  <li>차선책; serializable isolation 사용할 수 없으면 트랜잭션이 의존하는 로우 잠그기.</li>\n</ul>\n\n<p><strong>Phantoms causing write skew</strong></p>\n\n<ul>\n  <li><em><strong>phantom</strong>:</em>  쓰기가 다른 트랜잭션의 검색 질의 결과를 바구는 현상.  Where a write in one transaction changes the result of a search query in another transaction.</li>\n</ul>\n\n<p>AWS Summit &lt;채널톡의 RDBMS에서 NoSQL 전환&gt; 세션 내용</p>\n\n<ul>\n  <li>특정 시점 스파이크 트래픽과 리소스 비효율 문제 해결을 위해 DynamoDB 도입</li>\n  <li>채팅 뱃지 카운트 트랜잭션 동시성 처리: Optimistic lock 이용해서 conflicts 발생시 exponential backoff</li>\n</ul>\n\n<p><img src=\"/images/2024/06/22/07-aws-summit-dynamodb-0.png\" alt=\"aws-summit-dynamodb\" title=\"aws-summit-dynamodb\" class=\"center-image\" /></p>\n\n<p><img src=\"/images/2024/06/22/07-aws-summit-dynamodb-1.png\" alt=\"aws-summit-dynamodb\" title=\"aws-summit-dynamodb\" class=\"center-image\" /></p>\n\n<p><img src=\"/images/2024/06/22/07-aws-summit-dynamodb-2.png\" alt=\"aws-summit-dynamodb\" title=\"aws-summit-dynamodb\" class=\"center-image\" /></p>\n\n<p><img src=\"/images/2024/06/22/07-aws-summit-dynamodb-3.png\" alt=\"aws-summit-dynamodb\" title=\"aws-summit-dynamodb\" class=\"center-image\" /></p>\n\n<h3 id=\"p251-serializability\"><strong>p251 Serializability</strong></h3>\n\n<ul>\n  <li>Testing for concurrency issues is hard, because they are usually nondeterministic — problems only occur if you get unlucky with the timing.</li>\n  <li>the strongest isolation level.</li>\n  <li>It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, <strong><em>serially</em></strong>, without any concurrency.</li>\n  <li>the database prevents <strong><em>all</em></strong> possible race conditions.</li>\n  <li>2007년경이 돼서야 단일 스레드 루프에서 트랜잭션 실행하는게 실현 가능하다고 결론 내림\n    <ul>\n      <li>램 가격.  모든 데이터를 메모리 적재해서 트랜잭션 빠르게 실행.</li>\n      <li>OLTP 트랜잭션이 보통 짧고 실행하는 읽기와 쓰기 개수가 적음.</li>\n    </ul>\n  </li>\n  <li>단일 스레드 기반 시스템이 동시성을 지원하는 시스템보다 성능이 나을때가 있음.  잠금 오버헤드 피할 수 있기 때문.  A system designed for single-threaded execution can sometimes perform better than a system that supports concurrency, because it can avoid the coordination overhead of locking.</li>\n</ul>\n\n<p><strong>Encapsulating transactions in stored procedures</strong></p>\n\n<ul>\n  <li>데이터가 모두 메모리에 있고 프로시저는 네트워크나 디스크 I/O 대기 없이 매우 빠르게 실행된다고 가정.  Provided that all data required by a transaction is in memory, the stored procedure can execute very fast, without waiting for any network or disk I/O.</li>\n</ul>\n\n<p><img src=\"/images/2024/06/22/07-transaction-difference.png\" alt=\"transaction-difference\" title=\"transaction-difference\" class=\"center-image\" /></p>\n\n<p><strong>Partitioning</strong></p>\n\n<ul>\n  <li>동시성 제어는 간단해지지만 단일 장비의 단일 CPU 코어 속도 제한. Executing all transactions serially makes concurrency control much simpler, but limits the transaction throughput of the database to the speed of a single CPU core on a\nsingle machine.</li>\n</ul>\n\n<p><strong>Two-Phase Locking (2PL)</strong></p>\n\n<ul>\n  <li>2PL:  쓰기는 다른 쓰기와 읽기 진행하지 못하게 막음. writers don’t just block other writers; they also block readers and vice versa.</li>\n  <li>Snapshot isolation:  읽기와 쓰기 양쪽 모두 막지 못하는 원칙.  the mantra <em>readers never block writers, and writers never block.</em></li>\n  <li>lock: <strong><em>shared mode</em></strong> or in <strong><em>exclusive mode</em></strong></li>\n  <li>Performance\n    <ul>\n      <li>This is partly due to the overhead of acquiring and releasing all those locks, but more importantly due to reduced concurrency.</li>\n    </ul>\n  </li>\n  <li>Pessimistic vs. optimistic concurrency control\n    <ul>\n      <li><strong>Pessimistic</strong>: if anything might possibly go wrong (as indicated by a lock held by another transaction), it’s better to wait until the situation is safe again before doing anything.</li>\n      <li><strong>Optimistic</strong>: instead of blocking if something potentially dangerous happens, transactions continue anyway, in the hope that everything will turn out all right.</li>\n    </ul>\n  </li>\n</ul>\n\n<p>Performance of serializable snapshot isolation</p>\n\n<ul>\n  <li>잠금 때문에 트랜잭션 차단될 필요 없음. Compared to two-phase locking, the big advantage of serializable snapshot isolation is that one transaction doesn’t need to block waiting for locks held by another transaction.</li>\n  <li>The rate of aborts significantly affects the overall performance of SSI. For example, a transaction that reads and writes data over a long period of time is likely to run into conflicts and abort, so SSI requires that read-write transactions be fairly short (long- running read-only transactions may be okay)</li>\n</ul>\n\n<hr />\n\n<h2 id=\"08장-분산-시스템의-골칫거리\">08장: 분산 시스템의 골칫거리</h2>\n\n<ul>\n  <li>p274 엔지니어로서의 우리의 임부는 모든 게 잘못되더라도 제 역할을 해내는 시스템을 구축하는것.  In the end, our task as engineers is to build systems that do their job (i.e., meet the guarantees that users are expecting), in spite of everything going wrong.</li>\n  <li><strong>Faults and Partial Failures</strong>\n    <ul>\n      <li><strong><em>deterministic</em></strong>: ex. 하드웨어가 올바르게 동작하면 같은 연산은 항상 같은 결과를 낸다.</li>\n      <li><strong><em>partial failure</em></strong>: 분산 시스템에서 시스템의 일부만 고장. <strong><em>nondeterministic</em></strong></li>\n    </ul>\n  </li>\n  <li>p276 분산 시스템이 동작하게 만들려면 partial failure 가능성을 받아들이고 소프트웨어에 <strong>내결함성 메커니즘</strong>(fault-tolerance mechanisms)을 넣어야 한다. 신뢰성 없는 구성 요소를 사용해 신뢰성 있는 시스템을 구축해야 함.   In other words, we need to build a reliable system from unreliable components.</li>\n  <li><strong>Unreliable Networks</strong>\n    <ul>\n      <li>p279 비동기 네트워크.. 유일한 정보는 응답을 아직 받지 못했다는 것. 응답을 다른 노드로 요청을 보내서 응답을 받지 못했다면 그 이유를 아는 것은 불가능.   이런 문제를 다루는 흔한 방법이 타임아웃.</li>\n    </ul>\n  </li>\n  <li><strong>Timeouts and Unbounded Delays</strong>\n    <ul>\n      <li>타임아웃이 길면 노드가 죽었다고 선언될 때까지 기다리는 시간이 길어짐.  타임아웃이 짧으면 결함을 빨리 발견하지만  노드가 일시적으로 느려졌을 뿐인데도 죽었다고 잘못 선언할 가능성.</li>\n      <li>2d + r;  d: 전송 시간, r: 요청 처리 시간</li>\n      <li>기약 없는 지연(unbounded delay): 패킷을 가능한 한 빨리 보내려고 하지만 패킷이 도착하는데 걸리는 시간에 상한치는 없다.</li>\n    </ul>\n  </li>\n</ul>\n\n<p><strong>Network congestion and queueing</strong></p>\n\n<ul>\n  <li>p282 TCP 흐름제어(flow control). 노드가 네트워크 링크나 수신 노드에 과부하를 가하지 않도록 자신의 송신율을 제한.</li>\n  <li>TCP는 어떤 타임아웃(왕복 시간을 관찰해서 계산)  안에 확인 응답을 받지 않으면 패킷이 손실됐다고 간주하고 손실된 패킷은 자동으로 재전송.  애플리케이션에게는 패킷 손실이나 재전송이 보이지 않지만 그 결과로 생기는 지연 발생(타임아웃 만료 + 재전송 패킷 확인 응답)\n    <ul>\n      <li><a href=\"https://alden-kang.tistory.com/20\">Connection Timeout과 Read Timeout 살펴보기</a></li>\n      <li><a href=\"https://brunch.co.kr/@alden/15\">TCP retransmission과 튜닝 포인트</a></li>\n      <li>두 글 요약하자면 → 커넥션 타임아웃은 3초로 적당한거같다. TCP 는 재전송 메커니즘이 있어서 이 때 타임아웃이 1초임. 그래서 3초 정도가 적덩한거같다. Read Timeout 은 300ms 였어서 1초</li>\n    </ul>\n  </li>\n  <li>p284  고정된 타임아웃을 설정하는 대신 시스템이 지속적으로 응답 시간과 그들의 변동성을 측정,   관찰된 응답 시간 분포에 따라 타임아웃을 자동 조정.\n    <ul>\n      <li><a href=\"https://doc.akka.io/docs/akka/current/typed/failure-detector.html\">Phi Accrual Failure Detector - akka</a></li>\n      <li><a href=\"https://medium.com/@arpitbhayani/phi-%CF%86-accrual-failure-detection-79c21ce53a7a\">Phi φ Accrual Failure Detection</a>\n        <ol>\n          <li><strong>Suspicion Level (φ value):</strong>\n            <ul>\n              <li>φ 값은 수신된 하트비트(heartbeat) 사이의 시간 간격을 기반으로 계산. 이 값은 현재 네트워크 상태를 반영하도록 지속적으로 조정. φ 값이 높을수록 노드가 실패했을 가능성이 높다는 것을 의미.</li>\n            </ul>\n          </li>\n          <li><strong>Heartbeat Intervals</strong>:\n            <ul>\n              <li>노드로부터 모니터링. interval은 샘플 윈도우에 저장되어 분포를 추정하고 의심 수준을 계산하는 데 사용</li>\n            </ul>\n          </li>\n          <li><strong>Dynamic Thresholds</strong>:\n            <ul>\n              <li>φ 값은 시스템이 다양한 동적 임계값을 설정. 예를 들어, φ 값이 특정 임계값을 초과하면 노드를 실패한 것으로 간주할 수 있지만, 더 낮은 φ 값에서는 예방 조치를 시작할 수 있다.</li>\n            </ul>\n          </li>\n        </ol>\n      </li>\n      <li>A → B → C 순으로 호출한다고 하자. A를 관리하고있다면 B에서 지연된건지 C에서 지연된건지 찾는데, 데이터독으로 트레이싱하고있어서, 최근 n일 기준으로 API 의 latency 를 보고. 가령 B→C가 50ms 라면 Read Timeout 고려해서 500ms 정도로 잡고, A→B 구간에서는 500보다 좀 더 크게 잡고.</li>\n    </ul>\n  </li>\n</ul>\n\n<p><strong>Synchronous Versus Asynchronous Networks</strong></p>\n\n<ul>\n  <li>p284 전화 네트워크. 고정 회선.\n    <ul>\n      <li>bounded delay(제한 있는 지연):  네트워크의 다음 홉에 통화당 16비트 공간을 미리 할당.  그리고 큐 대기가 없으므로 종단 지연 시간의 최대치가 고정되어 있음.</li>\n    </ul>\n  </li>\n</ul>\n\n<p><strong>Can we not simply make network delays predictable?</strong></p>\n\n<ul>\n  <li>데이터센터 네트워크와 인터넷은 패킨 교환 사용;  bursty traffic 에 최적화</li>\n  <li>회선; 통화를 하는 동안 보내는 초당 비트 개수가 상당히 고정돼 있는 음성과 영상 통화에 적합</li>\n  <li>웹 페이지 요청, 이메일 전송, 파일 전송은 특별한 대역폭 요구사항 없음. 단지 가능하면 빨리 완료되기를 바람.</li>\n</ul>\n\n<p><strong>p287 Unreliable Clocks</strong></p>\n\n<ul>\n  <li>시간, 시계와 관련된 질문을 Durations(지속 시간), Points in time(시점)에 따라 기술</li>\n</ul>\n\n<p><strong>Synchronized clocks for global snapshots</strong></p>\n\n<ul>\n  <li>p295 스패너에 대한 이야기(feat. ChatGPT):\n    <ul>\n      <li>데이터베이스 내의 여러 트랜잭션 간의 인과 관계를 고려하여 타임스탬프를 부여\n        <ul>\n          <li><strong>트랜잭션 타임스탬프:</strong> 각 트랜잭션이 완료된 시점(시간). 트랜잭션의 순서를 정하고, 데이터의 일관성을 유지하는 데 중요</li>\n          <li><strong>인과성(Causality)</strong>: 한 트랜잭션이 다른 트랜잭션에 영향을 미치는 관계.  예를 들어, 트랜잭션 A가 트랜잭션 B보다 먼저 실행되고, B가 A의 결과에 의존하는 경우, A는 B의 원인이 된다.\n      1. <strong>TrueTime API</strong>:</li>\n          <li>TrueTime API를 사용하여 각 트랜잭션에 정확한 타임스탬프를 부여. TrueTime은 GPS 및 원자시계 정보를 활용하여 매우 정확한 시각 정보를 제공.</li>\n          <li>TrueTime의 사용으로 인해 각 트랜잭션의 완료 시점에 대해 상한(earliest) 및 하한(latest) 시간을 알 수 있으며, 이를 통해 트랜잭션 간의 정확한 순서를 결정.\n      2. <strong>트랜잭션 순서 보장</strong>:</li>\n          <li>트랜잭션 간의 인과 관계를 보장하기 위해, 이전 트랜잭션이 완료된 후에만 다음 트랜잭션을 수행. 이는 인과 관계가 반영된 타임스탬프를 부여하는데 중요한 역할을 한다.\n      3. <strong>Globally Consistent Reads and Writes</strong>:</li>\n          <li>이러한 타임스탬프 메커니즘 덕분에, Spanner는 전 세계에 분산된 데이터베이스에서도 강력한 일관성을 유지할 수 있습니다. 각 트랜잭션의 타임스탬프가 인과성을 반영하기 때문에, 사용자는 일관된 데이터를 읽고 쓸 수 있습니다.</li>\n        </ul>\n      </li>\n      <li><a href=\"https://cloud.google.com/spanner/docs/true-time-external-consistency\">Spanner: TrueTime and external consistency</a></li>\n      <li><a href=\"https://pdfs.semanticscholar.org/e3d0/9699c66bd0b89f663954bf8b043491368620.pdf\">Spanner: Google’s Globally-Distributed Database</a></li>\n    </ul>\n  </li>\n</ul>\n\n<p><strong>Process Pauses</strong></p>\n\n<ul>\n  <li>note) p297. 실행 중인 스레드를 어떤 시점에 선점(preempt)하고 얼마만의 시간이 흐른후 재개할 수 있다.  선점된 스레드는 이를 알아채지 못한다.  이 문제는 단일 장비에서 다중 스레드 코드를 thread-safe 하게 만드는 것과 비슷.   컨텍스트 스위치가 임의로 발생할 수 있고 병렬(parallelism)이 발생할 수 도 있으므로 타이밍에 대해 어떤 가정도 할 수 없다.</li>\n</ul>\n\n<p><strong>Knowledge, Truth, and Lies</strong></p>\n\n<ul>\n  <li>p300 인식하고 측정하는 수단을 믿을 수 없다면 그 지식을 어떻게 확신할 수 있나? How sure can we be of that knowledge, if the mechanisms for perception and measurement are unreliable?</li>\n</ul>\n\n<p><strong>The Truth Is Defined by the Majority</strong></p>\n\n<ul>\n  <li>p301 노드가 상황에 대한 자신의 판단을 반드시 믿을 수 있는 것은 아님.  분산 시스템은 한 노드에만 의존할 수는 없다. 노드에 언제든 장애가 나서 잠재적으로 시스템이 멈추고 복구할 수 없게 될 수도 있기 때문이다.  <strong>정족수(quorum)</strong></li>\n</ul>\n\n<p><strong>Fencing tokens</strong></p>\n\n<ul>\n  <li>p303 잠금이 승인될 때마다 증가하는 숫자.</li>\n</ul>\n\n<p><img src=\"/images/2024/06/22/08-fencing-token.png\" alt=\"fencing-token\" title=\"fencing-token\" class=\"center-image\" /></p>\n\n<ul>\n  <li>자원 자체가 이미 처리된 것보다 오래된 토큰을 사용해서 쓰는 것을 거부함으로써 토큰을 확인하는 활동적인 역할을 맡아야 함.</li>\n</ul>\n\n<p><strong>Byzantine Faults</strong></p>\n\n<ul>\n  <li>p304 어떤 노드가 실제로는 받지 않은 특정 메시지를 받았다고 주장…</li>\n  <li>비잔틴 결함(<em>Byzantine fault</em>), 비잔틴 장군 문제(<em>Byzantine Generals Problem</em>)\n    <ul>\n      <li><a href=\"https://monday9pm.com/%EB%A9%94%EC%8B%9C%EC%A7%80-%EC%A0%84%EB%8B%AC-%EC%A0%84%EB%9E%B5%EA%B3%BC-%EB%91%90-%EC%9E%A5%EA%B5%B0-%EB%AC%B8%EC%A0%9C-message-delivery-semantics-and-two-generals-problem-f8f1c7646c0b\"><strong>메시지 전달 전략과 두 장군 문제(Message Delivery Semantics and Two Generals’ Problem)</strong></a></li>\n      <li><a href=\"https://lwn.net/Articles/540368/\"><strong>ELC: SpaceX lessons learned</strong></a>\n        <ul>\n          <li>SpaceX에서는 Falcon, Dragon, Grasshopper 등의 비행 제어, 지상 스테이션, 개발자 데스크톱까지 모든 것이 리눅스.</li>\n          <li>fault-tolerant(내결함성/결함 허용)을 위해 3중으로 시스템을 구성하고 비잔틴 장군 알고리즘을 사용. 우주 정거장(ISS)에 접근할 때, 결함 허용 수준을 만족해야만 정거장에 접근할 수 있음.  이를 위해 삼중 중복 컴퓨터를 사용해서 필요한 수준의 결함 허용을 달성.</li>\n          <li>비잔틴 장군 알고리즘은 컴퓨터들이 동의하지 않는 상황을 처리하는 데 사용. 이런 상황은 방사선 사건으로 인해 메모리나 레지스터 값이 변경되는 경우 발생.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong><em>Byzantine fault-tolerant</em></strong> :  일부 노드가 오작동하고 프로토콜을 준수하지 않거나 악의적인 공격자가 네트워크를 방해하더라도 시스템이 계속 올바르게 동작</li>\n  <li>보통 비잔틴 결함이 없다고 가정할 수 있다. 우리 조직이 모든 노드 제어.. 방사선 수준은 큰 문제 아님..</li>\n</ul>\n\n<p><strong>Safety and liveness</strong></p>\n\n<ul>\n  <li>p308  안전성(나쁜 일은 일어나지 않는다), 활동성(좋은 일은 결국 일어난다).  Safety is often informally defined as <strong><em>nothing bad happens</em></strong>, and liveness as <strong><em>something good eventually happens</em></strong>.</li>\n</ul>\n\n<p>정리</p>\n\n<ul>\n  <li>결함을 견뎌내려면 그것을 감지하는 게 첫 걸음이지만 그것조차 어렵다.</li>\n  <li>대부분의 시스템은 노드에 장애가 발생했는지 알 수 있는 정확한 메커니즘이 없어서 대부분의 분산 알고리즘은 원격 노드를 아직 쓸 수 있는지 결정하기 위해 타임아웃을 사용.</li>\n  <li>그러나 타임아웃은 네트워크 장애와 노드 장애를 구별할 수 없고 변동이 큰 네트워크 지연 때문에 때때로 노드가 죽은 것으로 잘못 의심받을 수 있다.</li>\n</ul>\n\n<hr />\n\n<h2 id=\"09장-일관성과-합의p349\">09장: 일관성과 합의(~p.349)</h2>\n\n<p><strong>p.321 Consistency Guarantees</strong></p>\n\n<ul>\n  <li>약한 보장(weak guarantee): 복제 데이터 베이스는 대부분 최소한 <strong>최종적 일관성</strong>(<strong><em>eventual consistency**)</em>을 제공한다. 하지만 언제 **수렴</strong>(<strong><em>convergence</em></strong>) 될 지 모름.</li>\n  <li>강한 보장(strongest consistency):  데이터 시스템이 선택적으로 제공. 성능이 나쁘거나 약한 보장을 제공하는 시스템보다 내결함성이 약할 수 있음.</li>\n  <li>트랜잭션 격리 수준과의 차이점;\n    <ul>\n      <li>트랜잭션 격리:  주로 동시에 실행되는 트랜잭션 때문에 발생하는 경쟁 조건을 회피하는 것에 관한 것.</li>\n      <li>분산 일관성:  대개 지연과 결함이 있더라도 복제본의 상태를 코디네이션하는 것</li>\n    </ul>\n  </li>\n</ul>\n\n<p>p322 <strong>Linearizability</strong></p>\n\n<ul>\n  <li>“데이터베이스가 복제본이 하나만 있다면 훨씬 더 단순해 지지 않을까?”  원자적 일관성(<em>atomic consistency</em>), 강한 일관성(<em>strong consistency</em>), 즉각 일관성(<em>immediate consistency</em>), 외부 일관성(<em>external consistency</em>)</li>\n</ul>\n\n<p><img src=\"/images/2024/06/22/09-consistency.png\" alt=\"consistency\" title=\"consistency\" class=\"center-image\" /></p>\n\n<ul>\n  <li>최신성 보장(recency guarantee)</li>\n</ul>\n\n<p>시스템에 선형성을 부여하는 것</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">*cas*(*x*, *v*old, *v*new) ⇒ *r*</code>:\n    <ul>\n      <li>클라이언트가 cas(compare-and-set) 연산 요청</li>\n      <li>레지스터 x의 현재 값이 Vold과 같으면 Vnew로 설정하고 아니면 오류 발환(error)</li>\n    </ul>\n  </li>\n</ul>\n\n<p><img src=\"/images/2024/06/22/09-linearizability.png\" alt=\"linearizability\" title=\"linearizability\" class=\"center-image\" /></p>\n\n<ul>\n  <li>p332 요약; 다이나모 스타일 복제를 하는 리더 없는 시스템은 선형성을 제공하지 않는다고 보는 게 안전.</li>\n</ul>\n\n<p>p333 선형성의 비용</p>\n\n<ul>\n  <li>단일 리더 설정에서 데이터센터 사이에 연결 끊기면 선형성 읽기 불가능.  팔로워로부터 읽을 수는 있지만 데이터가 뒤쳐졌을 수 있다(비선형적).</li>\n</ul>\n\n<p>p335 CAP</p>\n\n<ul>\n  <li>네트워크 결함이 생기면 선형성과 완전한 가용성 사이에서 선택해야 함.  CAP는 <strong>네트워크 분단이 생겼을때 일관성과 가용성 중 하나를 선택</strong>하라는 의미.</li>\n</ul>\n\n<p><strong>p342 비인과적 일련번호 생성기(Noncausal sequence number generators)</strong></p>\n\n<ul>\n  <li>노드별) 각 노드 개별로 독립적인 일련번호 집합 생성. 예) 한 노드는 홀수만, 다른 노드는 짝수만 생성.</li>\n  <li>잘 동작하지만 생성한 일련번호가 인과성에 일관적이지 않음(timestamp 불일치)</li>\n</ul>\n\n<p><strong>p343 Lamport timestamps</strong></p>\n\n<ul>\n  <li>&lt;카운터, 노드&gt; ID 조합.</li>\n  <li>때때로 카운터 값이 같을 수 있지만 타임스탬프에 노드 ID를 포함시켜서 각 타임스탬프 유일.</li>\n  <li>물리적 일 기준 시계와 아무 관련이 없지만 전체 순서화를 제공.</li>\n  <li>버전 벡터와의 차이;\n    <ul>\n      <li>버전 벡터:  두 연산이 동시적인지 또는 어떤 연산이 다른 연산에 인과적으로 의존하는지 구별</li>\n      <li>램포트 타임스탬프:  항상 전체 순서화를 강제화</li>\n    </ul>\n  </li>\n</ul>\n\n<p><strong>Total Order Broadcast</strong></p>\n\n<ul>\n  <li>p346  후속 메시지가 이미 전달됐다면 노드는 그 순서의 앞에 메시지를 소급적으로 끼워넣는 게 허용되지 않는다.  전체 순서 브로드캐스트가 타임스탬프 순서화보다 강하다.</li>\n  <li>p347 선형성과의 차이\n    <ul>\n      <li>전체 순서 브로드캐스트는 비동기식.  메시지는 고정된 순서로 신뢰성 있게 전달되도록 보장.   하지만 언제 메시지가 전달되는지 보장되지 않음.</li>\n      <li>선형성은 최신성 보장. 읽기가 최근에 쓰여진 값을 보는 게 보장.</li>\n    </ul>\n  </li>\n</ul>\n\n<hr />\n\n<h2 id=\"09장-분산-트랜잭션과-합의p349\">09장: 분산 트랜잭션과 합의(p.349~)</h2>\n\n<p>p349 Consensus</p>\n\n<ul>\n  <li>합의:  여러 노드들이 뭔가에 동의하게 만드는 것</li>\n  <li>리더 선출(Leader election):  단일 리더 복제를 사용하는 DB에서 모든 노드는 어떤 노드가 리더인지 동의해야 함.</li>\n  <li>원자적 커밋(Atomic commit): 트랜잭션 원자성을 유지하고 싶다면 모든 노드가 트랜잭션의 결과에 동의해야 함.</li>\n</ul>\n\n<p>p351 원자적 커밋과 2PC(Two-Phase Commit) why?</p>\n\n<ul>\n  <li>트랜잭션의 결과는 항상 커밋 Success 이거나 Abort.  The outcome of a transaction is either a <strong>successful</strong> commit, in which case all of the transaction’s writes are made durable, or an <strong>abort</strong>, in which case all of the transaction’s writes are rolled back (i.e., undone or discarded).</li>\n  <li>트랜잭션에 여러 노드가 관여하게 되면 어떤 노드에서는 커밋이 성공하고 다른 노드에서는 실패해서 원자성 보장을 위반하기 쉽다.  어떤 노드가 트랜잭션을 Commit 하지만 다른 노드는 Abort 한다면 노드들이 서로 일관성이 없어진다.</li>\n  <li>트랜잭션 커밋은 되돌릴 수 없어야 한다.  보상 트랜잭션(compensating transaction)은 취소 가능하지만 분리된 트랜잭션 개념으로 이해해야 한다.</li>\n</ul>\n\n<p>p.352-355  2PC</p>\n\n<ul>\n  <li>2PC는 여러 노드에 걸친 원자적 트랜잭션 Commit 되게 함. 즉 모든 노드가 커밋되거나 모든 노드가 Abort 되도록 보장하는 알고리즘</li>\n</ul>\n\n<p><img src=\"/images/2024/06/22/09-2pc.png\" alt=\"2pc\" title=\"2pc\" class=\"center-image\" /></p>\n\n<ul>\n  <li>코디네이터(coordinator, transaction manager), 데이터베이스 노드들을 트랜잭션 참여자(participant)</li>\n  <li>애플리케이션이 Commit할 준비가 되면 코디네이터가 1단계 시작 → 각 노드에 준비 요청을 보내서 커밋할 수 있는지 물어본다 → 그 후 코디네이터는 참여자들의 응답을 추적\n    <ul>\n      <li>모든 참여자가 커밋할 준비가 됐다면 “yes” 응답 → 코디네이터는 2단계에서 커밋 요청(commit request)하고 실제로 커밋 발생</li>\n      <li>참여자 중 누구라도 “no” 응답 → 코디네이터는 2단게에서 모든 노드에 Abort 요청</li>\n    </ul>\n  </li>\n  <li>코디네이터 장애시,  2PC가 완료될 수 있는 유일한 방법은 코디네이터가 복구되기를 기다리는것 뿐.\n    <ul>\n      <li>코디네이터가 Commit 이나 Abort 요청 보내기 전에 디스크에 있는 트랜잭션 로그에 기록</li>\n      <li>코디네이터가 복구될 때 트랜잭션 로그를 읽어서 모든 의심스러운 트랜잭션들의 상태를 결정</li>\n    </ul>\n  </li>\n</ul>\n\n<p>p358-360  XA transactions</p>\n\n<ul>\n  <li>트랜잭션 코디네이터와 연결되는 인터페이스를 제공하는 API.  트랜잭션 코디네이터는 XA API를 구현.</li>\n  <li>애플리케이션이 네트워크 드라이버나 클라이언트 라이브러리를 사용해 참여자 DB나 메시징 서비스와 통신한다고 가정.</li>\n  <li>애플리케이션 프로세그가 죽거나 애플리케이션이 실행중인 장비가 죽으면 코디네이터도 함께 사라짐</li>\n  <li>코디네이터 장애 복구 문제.  유일한 방법은 관리자가 수동으로 트랜잭션을 커밋하거나 롤백할지 결정하는 것.</li>\n  <li>\n    <p>JDBC 분산 트랜잭션</p>\n\n    <p>JDBC(Java Database Connectivity)의 트랜잭션은 로컬. 이는 단일 연결이 트랜잭션의 모든 작업을 수행하며 연결은 한 번에 하나의 트랜잭션에 대해서만 작업할 수 있음을 의미.</p>\n\n    <p>하나의 트랜잭션에 대한 모든 작업이 완료되거나 실패한 경우, 작업을 영구적으로 만들기 위해 커밋 또는 롤백이 호출되고 새 트랜잭션이 시작된다.  그러나 Java에서는 로컬 트랜잭션을 넘어서는 기능을 제공하는 고급 트랜잭션 지원도 있고, Java 트랜잭션 API에서 제공한다.</p>\n\n    <p>Java Transaction API(JTA)는 복잡한 트랜잭션을 지원.  또한 연결 객체에서 트랜잭션을 분리하는 기능도 제공합니다.  JDBC가 객체 데이터베이스 연결(ODBC) 및 X/Open 호출 수준 인터페이스(CLI) 명세를 모델로 삼았듯이, JTA는 X/Open eXtended Architecture(XA) 명세를 모델로 삼습니다. JTA와 JDBC는 연결 객체에서 트랜잭션을 분리하기 위해 함께 작동. 연결 객체에서 트랜잭션을 분리함으로써 단일 연결이 여러 트랜잭션을 동시에 처리할 수 있다.  반대로 여러 연결이 단일 트랜잭션을 처리할 수도 있음.</p>\n  </li>\n</ul>\n\n<p>p360 분산 트랜잭션의 제약</p>\n\n<ul>\n  <li>XA 트랜잭션은 여러 참여 데이터 시스템이 서로 일관성을 유지하게 하는 실제적이고 중요한 문제를 해결해 주지만, XA 트랜잭션도 중요한 운영상 문제를 가져온다.</li>\n  <li>핵심 구현은 트랜잭션의 코디네이터 자체가 일종의 데이터베이스여야 하고, 따라서 다른 중요한 데이터베이스와 동일하게 신경 써서 접근해야함.\n    <ul>\n      <li>분산 트랜잭션은 <strong>장애를 증폭</strong>시키는 경향이 있으며 이는 내결함성을 지닌 시스템을 구축하려는 목적에 어긋난다.</li>\n    </ul>\n  </li>\n</ul>\n\n<p>p361 내결함성을 지닌 합의(Fault-Tolerant Consensus)</p>\n\n<ul>\n  <li>합의 알고리즘이 만족해야 할 속성\n    <ul>\n      <li>균일한 동의(Uniform agreement):  어떤 두 노드도 다르게 결정하지 않음</li>\n      <li>무결성(Integrity): 어떤 노드도 두 번 결정하지 않음</li>\n      <li>유효성(Validity): 한 노드가 값 v를 결정한다면 v는 어떤 노드에서 제안된 것이다.</li>\n      <li>종료(Termination): 죽지 않은 모든 노드는 결국 어떤 값을 결정한다.</li>\n    </ul>\n  </li>\n  <li>카프카 Zookeeper → Raft consensus\n    <ul>\n      <li><a href=\"https://seongjin.me/raft-consensus-algorithm/\">https://seongjin.me/raft-consensus-algorithm/</a></li>\n      <li><a href=\"https://devocean.sk.com/blog/techBoardDetail.do?ID=165711\">https://devocean.sk.com/blog/techBoardDetail.do?ID=165711</a></li>\n      <li><a href=\"https://devocean.sk.com/blog/techBoardDetail.do?ID=165737\">https://devocean.sk.com/blog/techBoardDetail.do?ID=165737</a></li>\n    </ul>\n  </li>\n</ul>\n\n<p>p365 에포크 번호 붙이기와 정족수</p>\n\n<ul>\n  <li>현재 리더가 죽었다고 생각될 때마다 새 노드를 선출하기 위해 노드 사이에서 투표를 해서 에포크 번호가 높은 리더가 이겨서 리더가 된다.</li>\n  <li>리더가 뭔가를 결정하도록 허용하기 전에 충돌되는 결정을 할 지도 모르는 에포크 번호가 더 높은 리더가 없는지 먼저 확인해야 한다.</li>\n  <li>노드의 정족수로부터 투표를 받아서 리더를 판단합니다. 정족수는 항상은 아니지만 노드의 과반수로 구성된다.</li>\n  <li>2PC는 “yes” 투표가 필요하지만,  내결함성을 지닌 합의 알고리즘은 노드의 과반수로부터만 투표.</li>\n</ul>\n\n<p>p367 멤버십과 코디네이션 서비스</p>\n\n<ul>\n  <li>HBase, 하둡, 노바, 카프카는 모두 <strong>주키퍼</strong>에 의존.</li>\n  <li>주키퍼 기능\n    <ul>\n      <li><strong>선형적 원자적 연산(Linearizable atomic operations):</strong>  원자적 compare-and-set 연산을 사용해 잠금을 구현</li>\n      <li><strong>연산의 전체 순서화(Total ordering of operations):</strong>  펜싱 토큰 등을 사용해 클라이언트 충돌 막기</li>\n      <li><strong>장애 감지(Failure detection):</strong> 세션에서 획득한 잠금은 세션이 타임아웃 됐을 때 자동으로 해제되도록 설정</li>\n      <li><strong>변경 알림(Change notifications):</strong>  클라이언트는 다른 클라이언트가 생성한 잠금과 값을 읽을 수 있을 뿐만 아니라 거기에 변경이 있는지 감시할 수 있다.</li>\n    </ul>\n  </li>\n</ul>\n\n<p>p369 Service discovery,  Membership services</p>\n\n<ul>\n  <li>Service discovery: 특정 서비스에 연결하려면 어떤 IP 주소로 접속해야 하는지 알아내는 용도로도 자주 사용.</li>\n  <li>Membership services:  클러스터에서 어떤 노드가 현재 활성화된 살아 있는 멤버인지 결정.</li>\n</ul>\n\n<hr />\n\n<h2 id=\"11장-스트림-처리p453\">11장: 스트림 처리(~p453)</h2>\n\n<p>p382 레코드 시스템과 파생 데이터 시스템</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>레코드 시스템(Systems of record)</th>\n      <th>파생 데이터 시스템(Derived data systems)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>믿을 수 있는 데이터 버전을 저장</td>\n      <td>다른 시스템에 존재하는 데이터를 가져와 특정 방식으로 변환하고 처리한 결과</td>\n    </tr>\n    <tr>\n      <td>진실의 근원(source of truth). 사용자의 입력과 같은 새로운 데이터가 들어오면 먼저 레코드 시스템에 저장</td>\n      <td>데이터를 잃게 되더라도 원천 데이터로부터 다시 생성 가능</td>\n    </tr>\n    <tr>\n      <td>일반적으로 정규화를 거쳐 정확하게 한 번 표현된다.  레코드 시스템과 다른 시스템 간에 차이가 난다면 레코드 시스템이 옳다.</td>\n      <td>필요한 데이터가 캐시에 있다면 캐시에서 제공하고, 그렇지 않다면 기반 데이터베이스를 거쳐 제공할 수 있다.</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>p436 <strong>이벤트 스트림 전송(Transmitting Event Streams)</strong></p>\n\n<ul>\n  <li><strong>이벤트</strong>\n    <ul>\n      <li>스트림 처리 문맥(stream processing context)에서 레코드는 보통 <strong>이벤트</strong>라고 한다.</li>\n      <li>특정 시점에 일어난 사건에 대한 세부 사항을 포함하는, 작고 독립된 불변 객체라는 점에서 본질적으로 동일하다.</li>\n    </ul>\n  </li>\n  <li>생성자(producer)와 소비자(consumer)\n    <ul>\n      <li>생산자(producer), 발행자(publisher), 발송자(sender)</li>\n      <li>소비자(consumer), 구독자(subscriber), 수신자(recipient)</li>\n      <li>레코드 식별(그룹핑)\n        <ul>\n          <li>파일 시스템: 파일 이름으로 관련 레코드 식별</li>\n          <li>스트림 시스템: 토픽(topic)이나 스트림으로 관련 이벤트 그룹핑</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n\n<h3 id=\"메시징-시스템-p438\">메시징 시스템 p438</h3>\n\n<ul>\n  <li>\n    <p>메시지 시스템에 대한 질문(Publish, Subscribe 모델)</p>\n\n    <p>Q 소비자가 메시지 처리하는 속도보다 생산자가 메시지 전송하는게 빠른 경우(생산자 &gt; 소비자)</p>\n\n    <ul>\n      <li>메시지를 버리기</li>\n      <li>큐에 메시지 버퍼링하기</li>\n      <li>배압(backpressure) 적용:\n        <ul>\n          <li>생산자가 메시지를 더 보내지 못하게 막기</li>\n          <li>유닉스 파이프와 TCP가 사용</li>\n        </ul>\n      </li>\n    </ul>\n\n    <p>Q 노드가 죽거나 일시적으로 오프라인이 된다면?</p>\n\n    <ul>\n      <li>디스크에 기록하거나 복제본 생성을 하거나 둘 모두를 해야 한다.</li>\n      <li>메시지를 잃어도 괜찮다면 같은 하드웨어에서 처리량은 높이고 지연 시간은 낮출 수 있다.</li>\n    </ul>\n  </li>\n  <li>생산자에서 소비자로 메시지 직접 전달:  프로토콜이 네트워크 상에서 패킷 유실을 감지하고 재전송하더라도 직접 메시지 시스템은 일반적으로 생산자와 소비자가 항상 온라인 상태라고 가정</li>\n  <li>\n    <p>p440 메시지 브로커와 데이터베이스 비교</p>\n\n    <table>\n      <thead>\n        <tr>\n          <th>데이터베이스(Databases)</th>\n          <th>메시지 브로커(Message brokers)</th>\n        </tr>\n      </thead>\n      <tbody>\n        <tr>\n          <td>명시적으로 데이터가 삭제될 때까지 데이터 보관</td>\n          <td>대부분 소비자에게 데이터 배달이 성공할 경우 자동으로 메시지 삭제. 대부분 메시지를 빨리 지우기 때문에 작업 집합이 상당히 작다고 가정(큐 크기가 작다). 소비자 처리가 느려서 메시지 브로커가 많은 메시지를 버퍼링해야 한다면, 개별 메시지 처리 시간이 길어지고 전체 처리량이 저하됨</td>\n        </tr>\n        <tr>\n          <td>보조 색인을 지원. 데이터 검색을 위한 다양한 방법 지원</td>\n          <td>특정 패턴과 부합하는 토픽의 부분 집합을 구독하는 방식 지원</td>\n        </tr>\n        <tr>\n          <td>일반적으로 질의 시점의 데이터 스냅숏을 기준으로 질의</td>\n          <td>임의 질의를 지원하지 않음. 데이터가 변하면 클라이언트에게 전달</td>\n        </tr>\n      </tbody>\n    </table>\n  </li>\n  <li>p441 복수 소비자(Multiple consumers)\n    <ul>\n      <li>로드 밸런싱(Load balancing):\n        <ul>\n          <li>각 메시지는 소비자 중 <strong>하나</strong>로 전달</li>\n          <li>메시지를 처리하는 비용이 비싸서 처리를 병렬화하기 위해 소비자를 추가하고 싶을때 유용</li>\n        </ul>\n      </li>\n      <li>팬 아웃(Fan-out):\n        <ul>\n          <li>각 메시지는 <strong>모든</strong> 소비자에게 전달</li>\n          <li>여러 독립적인 소비자가 브로드캐스팅된 동일한 메시지를 서로 간섭 없이 리스닝 가능</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n\n<p><img src=\"/images/2024/06/22/11-fanout.png\" alt=\"fanout\" title=\"fanout\" class=\"center-image\" /></p>\n\n<p>p442 확인 응답과 재전송(Acknowledgments and redelivery)</p>\n\n<ul>\n  <li>생산자: m1 → m2 → m3 → m4 → m5</li>\n  <li>소비자2가 m3 처리중 장애 발생해서 소비자1로 m3 메시지 재전송.</li>\n  <li>소비자 1에서는 m4 → m3 → m5 순으로 메시지 처리</li>\n</ul>\n\n<p><img src=\"/images/2024/06/22/11-consumer.png\" alt=\"consumer\" title=\"consumer\" class=\"center-image\" /></p>\n\n<ul>\n  <li>소비자마다 독립된 큐를 사용하면, 즉 부하 균형 분산 기능을 사용하지 않는다면 이 문제를 피할 수 있다.\n    <ul>\n      <li>카프카 토픽의 파티션 키(해시) - stickness</li>\n    </ul>\n  </li>\n  <li>메시지가 서로 완전히 독립적이라면 메시지 순서가 바뀌는 것은 문제가 되지 않는다.</li>\n</ul>\n\n<h3 id=\"p443-파티셔닝된-로그partitioned-logs\">p443 파티셔닝된 로그(Partitioned Logs)</h3>\n\n<p><strong>로그 기반 메시지 브로커(log- based message brokers)</strong>:  데이터베이스의 지속성 있는 저장 방법과 메시징 시스템의 지연 시간이 짧은 알림 기능을 조합</p>\n\n<ul>\n  <li>\n    <p>로그를 사용한 메시지 저장소</p>\n\n    <p>로그 == 파티션</p>\n\n    <ul>\n      <li>\n        <p><strong>p445 로그 파티셔닝(the log can be partitioned)</strong>: 디스크 하나를 쓸 때보다 처리량을 높이기 위해 확장하는 방법</p>\n\n        <blockquote>\n          <ul>\n            <li>하나의 파티션을 처리하는 작업을 공유하는 소비자 둘로 나누어 로드 밸런싱하는 방식을 사용할 수 있다.  양쪽 소비자 모두 모든 메시지를 읽지만 그 중 하나는 짝수 오프셋 메시지만 처리하고, 다른 소비자는 홀수 오프셋 메시지만 처리하는 방식.</li>\n            <li>스레드 풀 사용을 사용해 메시지 처리 분산하는 방식. 소비자 오프셋 관리가 복잡해진다. 일반적으로 한 파티션은 단일 스레드가 처리하는 것이 적절하고,  병렬성을 높이고 싶다면 파티션 수를 늘리는 게 좋다.</li>\n          </ul>\n        </blockquote>\n      </li>\n      <li>\n        <p>단 다른 파티션 간 메시지의 순서는 보장하지 않는다.</p>\n      </li>\n    </ul>\n  </li>\n</ul>\n\n<p><img src=\"/images/2024/06/22/11-partition.png\" alt=\"partition\" title=\"partition\" class=\"center-image\" /></p>\n\n<ul>\n  <li>\n    <p>p444 로그 방식과 전통적인 메시징 방식의 비교</p>\n\n    <p>로그 기반 접근법은 팬 아웃 메시지 방식 제공:</p>\n\n    <ul>\n      <li>소비자가 서로 영향 없이 독립적으로 로그를 읽을 수 있고 메시지를 읽어도 로그에서 삭제되지 않는다.</li>\n    </ul>\n\n    <p>각 클라이언트는 할당된 파티션의 메시지를 모두 소비:</p>\n\n    <ul>\n      <li>토픽 하나를 소비하는 작업을 공유하는 노드 수는 많아야 해당 토픽의 로그 파티션 수로 제한.  같은 파티션 내 메시지는 같은 노드로 전달되기 때문.</li>\n      <li>특정 메시지 처리가 느리면 파티션 내 후속 메시지 처리가 지연된다(head-of-line blocking).</li>\n    </ul>\n\n    <table>\n      <tbody>\n        <tr>\n          <td>메시지 처리 비용 비싼 경우, 메시지 단위로 병렬화 처리 하고 싶은 경우, 메시지 순서는 그렇게 중요하지 않은 경우</td>\n          <td>JMS / AMQP 방식의 메시지 브로커 적합</td>\n        </tr>\n        <tr>\n          <td>처리량이 많은 경우, 메시지를 처리하는 속도가 빠른 경우, 메시지 순서가 중요한 경우</td>\n          <td>로그 기반 접근법 적합</td>\n        </tr>\n      </tbody>\n    </table>\n  </li>\n  <li>p445 소비자 오프셋(Consumer offsets)\n    <ul>\n      <li>브로커는 주기적으로 소비자 오프셋을 기록</li>\n    </ul>\n  </li>\n  <li>p446 디스크 공간 사용\n    <ul>\n      <li>로그는 크기가 제한된 버퍼로 구현하고,  버퍼가 가득 차면 오래된 메시지 순서대로 버린다.</li>\n      <li>원형 버퍼(circular buffer), 링 버퍼(ring buffer)</li>\n    </ul>\n  </li>\n</ul>\n\n<h3 id=\"데이터베이스와-스트림\">데이터베이스와 스트림</h3>\n\n<ul>\n  <li>p448 데이터베이스에 뭔가를 기록한다는 사실은 캡처해서 저장하고 처리할 수 있는 이벤트다.</li>\n  <li>p448 시스템 동기화 유지하기</li>\n  <li>이중 기록(dual write)의 심각한 문제\n    <ul>\n      <li>문제 1) 타이밍이 좋지 않아 요청이 서로 교차한 경우, 두 시스템은 오류가 발생하지 않았음에도 영원히 서로 불일치 상태로 남음.</li>\n      <li>문제2) 한쪽 쓰기가 성공할 때, 다른쪽 쓰기는 실패할 수 있다.  동시성 문제라기보다는 내결함성 문제로 두 시스템 간 불일치가 발생하는 현상.</li>\n    </ul>\n  </li>\n</ul>\n\n<p><img src=\"/images/2024/06/22/11-dual-write.png\" alt=\"dual-write\" title=\"dual-write\" class=\"center-image\" /></p>\n\n<ul>\n  <li><strong>p450 변경 데이터 캡처(Change Data Capture)</strong>\n    <ul>\n      <li>데이터베이스에 기록하는 모든 데이터의 변화를 관찰해 다른 시스템으로 데이터를 복제할 수 있는 형태로 추출하는 과정</li>\n    </ul>\n  </li>\n</ul>\n\n<p><img src=\"/images/2024/06/22/11-cdc.png\" alt=\"cdc\" title=\"cdc\" class=\"center-image\" /></p>\n\n<ul>\n  <li>파생 데이터 시스템이 레코드 시스템의 정확한 데이터 복제본을 가지게 하기 위해,  레코드 시스템에 발생하는 모든 변경 사항을 파생 데이터 시스템에 반영하는 것을 보장하는 메커니즘</li>\n  <li>\n    <p>멕스웰(maxwell), 디비지움(debezium): binlog를 파싱해 유사한 방식으로 mysql용 변경 데이터 캡처를 구현</p>\n  </li>\n  <li>p452 초기 스냅숏(Initial snapshot):\n    <ul>\n      <li>데이터베이스에서 발생한 모든 변경 로그가 있다면,  로그를 재현해서 데이터베이스의 전체 상태를 재구축 가능</li>\n    </ul>\n  </li>\n  <li>p452 로그 컴팩션(Log compaction):\n    <ul>\n      <li>저장 엔진은 주기적으로 같은 키의 로그 레코드를 찾아 중복을 제거</li>\n      <li>각 키에 대해 가장 최근에 갱신된 내용만 유지</li>\n    </ul>\n  </li>\n  <li>p453 변경 스트림용 API 지원\n    <ul>\n      <li>kafka connect;\n        <ul>\n          <li>변경 이벤트를 스트림하는데 카프카 사용</li>\n          <li>검색 색인과 같은 파생 데이터 시스템을 갱신하는데 사용 가능</li>\n          <li>스트림 처리 시스템에도 이벤트 공급이 가능</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n\n<hr />\n\n<h2 id=\"11장-스트림-처리p45411장-끝\">11장: 스트림 처리(p454~11장 끝)</h2>\n\n<p><strong>이벤트 소싱(event sourcing)</strong></p>\n\n<ul>\n  <li>CDC:\n    <ul>\n      <li>데이터베이스를 변경 가능한 방식으로 사용해 레코드를 자유롭게 갱신하고 삭제</li>\n    </ul>\n  </li>\n  <li>이벤트 소싱:\n    <ul>\n      <li>이벤트 로그에 기록된 불변 이벤트를 기반으로 명시적으로 구축</li>\n      <li>이벤트 저장은 추가만 가능하고 갱신이나 삭제는 권장하지 않거나 금지</li>\n      <li>어떤 상황이 발생한 후에 상황 파악이 쉽기 때문에 디버깅에 도움이 되고 버그 방지</li>\n    </ul>\n  </li>\n</ul>\n\n<p><strong>이벤트 로그에서 현재 상태 파생하기</strong></p>\n\n<ul>\n  <li>시스템에 기록한 데이터를 표현한 이벤트 로그를 가져와 사용자에게 보여주기에 적당한 애플리케이션 상태(시스템에서 데이터를 읽는 방식)로 변환해야 한다.\n    <ul>\n      <li>예시; 현재 장바구니 내용(변경 사항은 관심사 x)</li>\n    </ul>\n  </li>\n  <li>CDC: 레코드의 가장 새로운 버전만 보유.  같은 키의 이전 이벤트는 로그 컴팩션을 통해 버린다</li>\n  <li>이벤트 소싱: 뒤에 발생한 이벤트가 앞선 이벤트를 덮어쓰지 않는다. 마지막 상태 재구축을 위해서는 이벤트의 전체 히스토리가 필요하다.</li>\n</ul>\n\n<p><strong>p455 명령과 이벤트</strong></p>\n\n<ul>\n  <li>명령이 실행 가능한지 확인.</li>\n  <li>무결성이 검증되고 명령이 승인되면 명령은 지속성 있는 불변 이벤트가 된다.</li>\n  <li>명령의 유효성은 이벤트가 되기 전에 동기식으로 검증해야 한다.\n    <ul>\n      <li>좌석 예약 예시.</li>\n    </ul>\n  </li>\n</ul>\n\n<p><strong>p456 상태와 스트림 그리고 불변성</strong></p>\n\n<ul>\n  <li>상태의 본질을 변하는 것이다. 상태가 어떻게 바뀌었든 항상 이런 변화를 일으킨 일련의 이벤트가 있다.</li>\n  <li>변경 가능한 상태와 추가 전용 불변 이벤트 로그는 마치 동전의 양면과 같이 서로 모순이다. 모든 <strong>변경 로그(changelog)</strong>는 시간이 지남에 따라 바뀌는 상태를 나타낸다.</li>\n  <li>변경 로그를 지속성 있게 저장한다면 상태를 간단히 재생성할 수 있는 효과가 있다. 이벤트 로그를 레코드 시스템으로 생각하고 모든 변경 가능 상태를 이벤트 로그로부터 파생된 것으로 생각하면 시스템을 거치는 데이터 흐름에 관해 추론하기 쉽다.</li>\n</ul>\n\n<p><strong>p458 동일한 이벤트 로그로 여러 가지 뷰 만들기</strong></p>\n\n<ul>\n  <li>CQRS(Command Query Responsibility Segregation)</li>\n  <li>읽기 최적화된 뷰(read-optimized views)는 데이터를 비정규화하는 것이 전적으로 합리적이다.</li>\n</ul>\n\n<p><strong>동시성 제어</strong></p>\n\n<ul>\n  <li>이벤트가 아직 읽기 뷰에 반영되지 않았을 가능성</li>\n</ul>\n\n<p><strong>p460 불변성의 한계</strong></p>\n\n<ul>\n  <li>성능적인 이유 외에도. 사생활 침해 규제로 인해 개인 정보 지울 필요가 있다든지, 데이터 보호법에 따라 잘못된 정보를 삭제해야 한다든지, 민감한 정보가 우발적으로 노출되는 것을 방지해야 하는 경우</li>\n</ul>\n\n<h3 id=\"p461-스트림-처리\"><strong>p461 스트림 처리</strong></h3>\n\n<ul>\n  <li>몇 분 동안 실행된 일괄 처리 작업은 실패한 태스크를 처음부터 재시작하는 것으로 충분,  하지만 몇 년 동안 실행 중인 스트림 작업은 장애 발생 이후 처음부터 재시작하는 방법은 비현실적.</li>\n</ul>\n\n<p><strong>p462 복잡한 이벤트 처리</strong></p>\n\n<ul>\n  <li>CEP(Complex event processing): 특정 이벤트 패턴을 검색해야 하는 애플리케이션에 적합</li>\n  <li>스트림 분석(Stream analytics): 연속한 특정 이벤트 패턴을 찾는 것보다 대량의 이벤트를 집계하고 통계적 지표를 뽑는 것을 더 우선.\n    <ul>\n      <li>특정 유형의 이벤트 빈도 측정(시간당 얼마나 자주 발생하는지)</li>\n      <li>특정 기간에 걸친 값의 이동 평균(ROLLING AVERAGE) 계산</li>\n      <li>이전 시간 간격과 현재 통계값의 비교(추세 감지,  지난주 대비 비정상적으로 높거나 낮은 지표에 대해 경고)</li>\n    </ul>\n  </li>\n</ul>\n\n<p><strong>p464 구체화 뷰(materialized views) 유지하기</strong></p>\n\n<ul>\n  <li>이벤트 소싱에서 애플리케이션 상태는 이벤트 로그를 적용함으로써 유지된다.  여기서 애플리케이션 상태는 일종의 구체화 뷰다.</li>\n  <li>구체화 뷰를 만들려면 잠재적으로 임의의 시간 범위에 발생한 모든 이벤트가 필요하다.</li>\n</ul>\n\n<p><strong>p464 스트림 상에서 검색하기</strong></p>\n\n<ul>\n  <li>질의를 먼저 저장.</li>\n</ul>\n\n<p><strong>메시지 전달과 RPC</strong></p>\n\n<p>메시지 전달이 stream 처리, RPC는 액터.  액터는 1:1로 메시지 주고 받는 형태.  스트림 처리는 카프카처럼 데이터 파이프라인 구축.  스트림 처리는 메시지 관심있는 사람 여러명이 관심.  1:1 / 1:n 또는 n:m</p>\n\n<ul>\n  <li>액터:\n    <ul>\n      <li>주로 동시성 관리, 통신 모듈을 분산 실행하는 매커니즘</li>\n      <li>액터간 통신은 주로 단기적이고 일대일</li>\n      <li>임의의 방식으로 통신 가능</li>\n    </ul>\n  </li>\n  <li>스트림 처리:\n    <ul>\n      <li>기본적으로 데이터 관리 기법</li>\n      <li>지속성 있고 다중 구독 가능</li>\n      <li>비순환 파이프라인에 설정됨.</li>\n    </ul>\n  </li>\n</ul>\n\n<p><strong>p468 어떤 시계를 사용할 것인가?</strong></p>\n\n<p>잘못된 장치 시계를 조정하는 한 가지 방법: 세 가지 타임스탬프를 로그로 남기기</p>\n\n<ol>\n  <li>이벤트가 발생한 시간 - 장치 시계</li>\n  <li>이벤트를 서버로 보낸 시간 - 장치 시계</li>\n  <li>서버에서 이벤트를 받은 시간 - 서버 시계</li>\n</ol>\n\n<p>2와 3 차이를 구하면 장치 시계와 서버 시계 간의 오프셋 추정 가능.  계산한 오프셋을 이벤트 타임스탬프에 적용해 이벤트가 실제로 발생한 시간을 추정.</p>\n\n<p><strong>p469 윈도우 유형</strong></p>\n\n<ul>\n  <li>텀블링 윈도우(Tumbling window)</li>\n  <li>홉핑 윈도우(Hopping window)</li>\n  <li>슬라이딩 윈도우(Sliding window)</li>\n  <li>세션 윈도우(Session window)</li>\n</ul>\n\n<h3 id=\"p474-내결함성fault-tolerance\">p474 내결함성(<strong>Fault Tolerance)</strong></h3>\n\n<ul>\n  <li>태스크를 재시작하는 것은 사실 레코드를 여러 번 처리할 수도 있다는 뜻. 출력은 한 번만 처리된 것으로 보이는 효과.\n    <ul>\n      <li><strong>exactly-once semantics(정확히 한 번 시맨틱), effectively-once(결과적으로 한 번)</strong></li>\n    </ul>\n  </li>\n  <li>Microbatching and checkpointing(마이크로 일괄 처리와 체크포인트)</li>\n  <li>Atomic commit revisited(원자적 커밋 재검토)</li>\n  <li>멱등성(Idempotence)</li>\n  <li>실패 후에 상태 재구축하기(Rebuilding state after a failure)</li>\n</ul>",
        "guid": "https://sungjk.github.io/2024/06/22/ddia.html",
        "isoDate": "2024-06-22T00:00:00.000Z"
      }
    ]
  },
  {
    "name": "구교준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김덕기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "고명환",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강성희",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강성훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강병수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김봉현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강형석",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수로",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강미경",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강진우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권민재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권태관",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김도곤",
    "category": "개인",
    "posts": []
  },
  {
    "name": "칡토스의 게임 개발",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김선철",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김놀부",
    "category": "개인",
    "posts": []
  },
  {
    "name": "프리웨어 이야기",
    "category": "개인",
    "posts": [
      {
        "creator": "어떤오후의 프리웨어 이야기",
        "title": "안드로이드 추천 앱, 추천 어플 (24.6.24) 중고등학생 전용 SNS, 디어스, 문서를 PDF로 스캔하기, 반려동물 커뮤니티, 예술 중심 소셜 플랫폼",
        "link": "http://muzbox.tistory.com/2000",
        "pubDate": "Mon, 24 Jun 2024 08:06:28 +0900",
        "author": "어떤오후의 프리웨어 이야기",
        "comments": "http://muzbox.tistory.com/2000#entry2000comment",
        "content": "<p style=\"text-align: left;\" data-ke-size=\"size16\">구글플레이 스토어에 등록된 유용한 앱 5개를&nbsp; 소개합니다. 새로운 기능, 사용자 경험 향상을 위한 앱들을 발견하고, 일상생활을 더욱 편리하게 만들어 줄 최고의 앱들을 찾아보세요.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"안드로이드 추천앱.png\" data-origin-width=\"500\" data-origin-height=\"500\"><span data-url=\"https://blog.kakaocdn.net/dn/Q585t/btsH8AlrCXT/dirdDG0wqmDFEpLUkh6Tk0/img.png\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/Q585t/btsH8AlrCXT/dirdDG0wqmDFEpLUkh6Tk0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FQ585t%2FbtsH8AlrCXT%2FdirdDG0wqmDFEpLUkh6Tk0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"안드로이드 추천 앱, 추천 어플\" data-filename=\"안드로이드 추천앱.png\" data-origin-width=\"500\" data-origin-height=\"500\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;안드로이드 앱스토어인 구글 플레이 스토어에는 하루에도 엄청난 수의 앱과 게임이 신규로 등록됩니다. 이 모든앱들을 사용자가 확인하고 양질의 앱을 선택하는 것이 사실상 불가능 하다는 얘기죠.&nbsp; <br /><br />또한, 최근들어 강화되었다 하지만 여전히 구글 플레이스토어에는 유해한 앱들이 사라지지 않고 이들 앱으로 피해를 보는 사용자도 많습니다. 본 블로그에서는 일주일에 한 번정도 운영자가 직접 유용하고 편리한 앱을 엄선하여 소개합니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: left;\" data-ke-size=\"size16\"><b>'어떤오후의 프리웨어 이야기'에서 추천하는 </b><b><span style=\"color: #ee2323;\">2024년 6월 24일자 '안드로이드 추천 앱'</span>입니다.</b><b></b></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><b>1. 디어스 - 크리에이터와 팬들이 소통할 수 있는 온라인 플랫폼</b></span></h2>\n<p data-ke-size=\"size16\">&nbsp; 디어스는&nbsp;크리에이터와&nbsp;팬들이&nbsp;소통할&nbsp;수&nbsp;있는&nbsp;특별한&nbsp;온라인&nbsp;플랫폼입니다.&nbsp;이&nbsp;앱에서는&nbsp;크리에이터와&nbsp;팬들이&nbsp;'스페이스'라는&nbsp;공간에서&nbsp;자유롭게&nbsp;소통하고&nbsp;콘텐츠를&nbsp;공유할&nbsp;수&nbsp;있습니다.&nbsp;디어스는&nbsp;다양한&nbsp;이벤트와&nbsp;최신&nbsp;콘텐츠를&nbsp;제공하며,&nbsp;모바일&nbsp;티케팅&nbsp;서비스도&nbsp;간편하게&nbsp;이용할&nbsp;수&nbsp;있습니다.&nbsp;또한,&nbsp;팬들은&nbsp;좋아하는&nbsp;크리에이터의&nbsp;일정과&nbsp;활동을&nbsp;캘린더&nbsp;기능을&nbsp;통해&nbsp;한눈에&nbsp;확인할&nbsp;수&nbsp;있습니다.&nbsp;이&nbsp;플랫폼은&nbsp;크리에이터와&nbsp;팬들이&nbsp;하나가&nbsp;되어&nbsp;즐길&nbsp;수&nbsp;있는&nbsp;새로운&nbsp;형태의&nbsp;커뮤니티&nbsp;공간을&nbsp;제공합니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"디어스.jpg\" data-origin-width=\"803\" data-origin-height=\"1421\"><span data-url=\"https://blog.kakaocdn.net/dn/b8Xc5e/btsH8UqbeAg/kn1cwXGWapHRU5FErwwYdK/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/b8Xc5e/btsH8UqbeAg/kn1cwXGWapHRU5FErwwYdK/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb8Xc5e%2FbtsH8UqbeAg%2Fkn1cwXGWapHRU5FErwwYdK%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"디어스 - 크리에이터와 팬들이 소통할 수 있는 온라인 플랫폼\" width=\"740\" height=\"1310\" data-filename=\"디어스.jpg\" data-origin-width=\"803\" data-origin-height=\"1421\"/></span></figure>\n</p>\n<figure id=\"og_1719183541098\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"디어스 - Google Play 앱\" data-og-description=\"크리에이터와 팬, 우리가 되는 공간\" data-og-host=\"play.google.com\" data-og-source-url=\"https://play.google.com/store/apps/details?id=com.binary.theus\" data-og-url=\"https://play.google.com/store/apps/details?id=com.binary.theus&amp;hl=ko\" data-og-image=\"https://scrap.kakaocdn.net/dn/BgdL2/hyWoPOY1Bo/r40yayDdA5lMkdkkba6ECK/img.png?width=512&amp;height=512&amp;face=0_0_512_512,https://scrap.kakaocdn.net/dn/bk0GhV/hyWrNaTKRL/Yyd5ywRc5U8J1Am4vc2oW0/img.png?width=600&amp;height=300&amp;face=0_0_600_300,https://scrap.kakaocdn.net/dn/Wk3fi/hyWrZ3twUJ/FqYVvAK0pVOYvWmOgSE681/img.png?width=240&amp;height=240&amp;face=0_0_240_240\"><a href=\"https://play.google.com/store/apps/details?id=com.binary.theus\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://play.google.com/store/apps/details?id=com.binary.theus\">\n<div class=\"og-image\" style=\"background-image: url('https://scrap.kakaocdn.net/dn/BgdL2/hyWoPOY1Bo/r40yayDdA5lMkdkkba6ECK/img.png?width=512&amp;height=512&amp;face=0_0_512_512,https://scrap.kakaocdn.net/dn/bk0GhV/hyWrNaTKRL/Yyd5ywRc5U8J1Am4vc2oW0/img.png?width=600&amp;height=300&amp;face=0_0_600_300,https://scrap.kakaocdn.net/dn/Wk3fi/hyWrZ3twUJ/FqYVvAK0pVOYvWmOgSE681/img.png?width=240&amp;height=240&amp;face=0_0_240_240');\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">디어스 - Google Play 앱</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">크리에이터와 팬, 우리가 되는 공간</p>\n<p class=\"og-host\" data-ke-size=\"size16\">play.google.com</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><b>2. OMG&nbsp;-&nbsp;중고등학생&nbsp;전용&nbsp;SNS</b></span></h2>\n<p data-ke-size=\"size16\">OMG는&nbsp;중고등학생들을&nbsp;위한&nbsp;익명&nbsp;SNS&nbsp;플랫폼으로,&nbsp;친구들과&nbsp;익명으로&nbsp;속마음을&nbsp;나누고&nbsp;투표할&nbsp;수&nbsp;있는&nbsp;서비스입니다.&nbsp;이&nbsp;앱을&nbsp;통해&nbsp;사용자는&nbsp;자신에&nbsp;대한&nbsp;친구들의&nbsp;생각을&nbsp;알아볼&nbsp;수&nbsp;있으며,&nbsp;직접&nbsp;만든&nbsp;질문에&nbsp;대한&nbsp;응답도&nbsp;받을&nbsp;수&nbsp;있습니다.&nbsp;OMG는&nbsp;또한&nbsp;학교&nbsp;내&nbsp;인기&nbsp;투표와&nbsp;같은&nbsp;재미있는&nbsp;기능을&nbsp;제공하여&nbsp;학교&nbsp;생활을&nbsp;더욱&nbsp;흥미롭게&nbsp;만듭니다.&nbsp;더&nbsp;나아가,&nbsp;전국의&nbsp;또래&nbsp;친구들과&nbsp;소통하고&nbsp;다양한&nbsp;챌린지에&nbsp;참여할&nbsp;수&nbsp;있어,&nbsp;폭넓은&nbsp;교류와&nbsp;재미를&nbsp;경험할&nbsp;수&nbsp;있습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"OMG.jpg\" data-origin-width=\"803\" data-origin-height=\"1421\"><span data-url=\"https://blog.kakaocdn.net/dn/91tIP/btsH9ENGbfM/qF2rHep1gC5fgfGuzVYmV1/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/91tIP/btsH9ENGbfM/qF2rHep1gC5fgfGuzVYmV1/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F91tIP%2FbtsH9ENGbfM%2FqF2rHep1gC5fgfGuzVYmV1%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"OMG - 중고등학생 전용 SNS\" width=\"740\" height=\"1310\" data-filename=\"OMG.jpg\" data-origin-width=\"803\" data-origin-height=\"1421\"/></span></figure>\n</p>\n<figure id=\"og_1719183581909\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"OMG - 중고등학생 전용 SNS - Google Play 앱\" data-og-description=\"내 친구들과 익명으로 더 재밌게 소통하고, 전국의 또래 친구들과도 부담없이 소통해보세요!\" data-og-host=\"play.google.com\" data-og-source-url=\"https://play.google.com/store/apps/details?id=io.trinitystudio.omg\" data-og-url=\"https://play.google.com/store/apps/details?id=io.trinitystudio.omg&amp;hl=ko\" data-og-image=\"https://scrap.kakaocdn.net/dn/bg3FVj/hyWozL8tNl/GNVPrJkzbaHSPpOt0aRPdK/img.png?width=512&amp;height=512&amp;face=0_0_512_512,https://scrap.kakaocdn.net/dn/dLSdRj/hyWoJnHibK/FClzKHe59Vj6Hzh7Ah5wb0/img.png?width=600&amp;height=300&amp;face=0_0_600_300,https://scrap.kakaocdn.net/dn/15eNI/hyWrQenEHd/aRhcLeWpybGYBx1XijKeDK/img.png?width=240&amp;height=240&amp;face=0_0_240_240\"><a href=\"https://play.google.com/store/apps/details?id=io.trinitystudio.omg\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://play.google.com/store/apps/details?id=io.trinitystudio.omg\">\n<div class=\"og-image\" style=\"background-image: url('https://scrap.kakaocdn.net/dn/bg3FVj/hyWozL8tNl/GNVPrJkzbaHSPpOt0aRPdK/img.png?width=512&amp;height=512&amp;face=0_0_512_512,https://scrap.kakaocdn.net/dn/dLSdRj/hyWoJnHibK/FClzKHe59Vj6Hzh7Ah5wb0/img.png?width=600&amp;height=300&amp;face=0_0_600_300,https://scrap.kakaocdn.net/dn/15eNI/hyWrQenEHd/aRhcLeWpybGYBx1XijKeDK/img.png?width=240&amp;height=240&amp;face=0_0_240_240');\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">OMG - 중고등학생 전용 SNS - Google Play 앱</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">내 친구들과 익명으로 더 재밌게 소통하고, 전국의 또래 친구들과도 부담없이 소통해보세요!</p>\n<p class=\"og-host\" data-ke-size=\"size16\">play.google.com</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><b>3. CamScanner&nbsp;-&nbsp;문서를&nbsp;PDF로&nbsp;스캔하기</b></span></h2>\n<p data-ke-size=\"size16\">&nbsp; CamScanner는&nbsp;스마트폰을&nbsp;이용해&nbsp;문서를&nbsp;스캔하고&nbsp;관리할&nbsp;수&nbsp;있는&nbsp;다목적&nbsp;앱입니다.&nbsp;이&nbsp;앱은&nbsp;고품질&nbsp;이미지&nbsp;처리&nbsp;기술을&nbsp;사용하여&nbsp;문서를&nbsp;스캔하고,&nbsp;OCR&nbsp;기능으로&nbsp;텍스트를&nbsp;인식하며,&nbsp;클라우드&nbsp;저장소를&nbsp;통해&nbsp;여러&nbsp;기기&nbsp;간&nbsp;동기화를&nbsp;지원합니다.&nbsp;CamScanner는&nbsp;문서&nbsp;공유,&nbsp;프린트,&nbsp;팩스&nbsp;기능도&nbsp;제공하여&nbsp;비즈니스맨,&nbsp;학생,&nbsp;디자이너&nbsp;등&nbsp;다양한&nbsp;사용자들의&nbsp;needs를&nbsp;충족시킵니다.&nbsp;무료&nbsp;버전과&nbsp;고급&nbsp;계정이&nbsp;있어&nbsp;사용자의&nbsp;필요에&nbsp;따라&nbsp;선택할&nbsp;수&nbsp;있으며,&nbsp;전&nbsp;세계적으로&nbsp;4000만&nbsp;명&nbsp;이상의&nbsp;사용자가&nbsp;이용하고&nbsp;있는&nbsp;인기&nbsp;앱입니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"CamScanner.jpg\" data-origin-width=\"803\" data-origin-height=\"1421\"><span data-url=\"https://blog.kakaocdn.net/dn/bn4wmG/btsH9JuwQsG/wNDPZKZk4Yc76vm7AEh4w0/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/bn4wmG/btsH9JuwQsG/wNDPZKZk4Yc76vm7AEh4w0/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbn4wmG%2FbtsH9JuwQsG%2FwNDPZKZk4Yc76vm7AEh4w0%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"CamScanner - 문서를 PDF로 스캔하기\" width=\"740\" height=\"1310\" data-filename=\"CamScanner.jpg\" data-origin-width=\"803\" data-origin-height=\"1421\"/></span></figure>\n</p>\n<figure id=\"og_1719183614772\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"CamScanner - 문서를 PDF로 스캔하기 - Google Play 앱\" data-og-description=\"쉽고 간편한 1:1 고화질 정밀스캔, PDF/Word/IMG 다양한 편집, 포맷 저장, 이미지 텍스트 인식 지원, 지금 바로 다운로드하세요!\" data-og-host=\"play.google.com\" data-og-source-url=\"https://play.google.com/store/apps/details?id=com.intsig.camscanner\" data-og-url=\"https://play.google.com/store/apps/details?id=com.intsig.camscanner&amp;hl=ko\" data-og-image=\"https://scrap.kakaocdn.net/dn/bKgkNJ/hyWoMxVPZw/Js9JkH4gzJGb4JB94fvI3k/img.png?width=512&amp;height=512&amp;face=0_0_512_512,https://scrap.kakaocdn.net/dn/uWSrg/hyWrQrVeZE/ZpaCwkrF2E8TrJU287BDXK/img.png?width=600&amp;height=300&amp;face=0_0_600_300,https://scrap.kakaocdn.net/dn/vQCtk/hyWoCWqvSe/YoDb87DO3ZyQiXra5tiop0/img.png?width=240&amp;height=240&amp;face=0_0_240_240\"><a href=\"https://play.google.com/store/apps/details?id=com.intsig.camscanner\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://play.google.com/store/apps/details?id=com.intsig.camscanner\">\n<div class=\"og-image\" style=\"background-image: url('https://scrap.kakaocdn.net/dn/bKgkNJ/hyWoMxVPZw/Js9JkH4gzJGb4JB94fvI3k/img.png?width=512&amp;height=512&amp;face=0_0_512_512,https://scrap.kakaocdn.net/dn/uWSrg/hyWrQrVeZE/ZpaCwkrF2E8TrJU287BDXK/img.png?width=600&amp;height=300&amp;face=0_0_600_300,https://scrap.kakaocdn.net/dn/vQCtk/hyWoCWqvSe/YoDb87DO3ZyQiXra5tiop0/img.png?width=240&amp;height=240&amp;face=0_0_240_240');\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">CamScanner - 문서를 PDF로 스캔하기 - Google Play 앱</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">쉽고 간편한 1:1 고화질 정밀스캔, PDF/Word/IMG 다양한 편집, 포맷 저장, 이미지 텍스트 인식 지원, 지금 바로 다운로드하세요!</p>\n<p class=\"og-host\" data-ke-size=\"size16\">play.google.com</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><b><span style=\"color: #006dd7;\">4. 피피&nbsp;-&nbsp;우리동네&nbsp;반려동물&nbsp;커뮤니티</span><br /></b></h2>\n<p data-ke-size=\"size16\">\"피피\"는&nbsp;동네&nbsp;반려동물과&nbsp;반려인들의&nbsp;이야기를&nbsp;공유하는&nbsp;소셜&nbsp;플랫폼입니다.&nbsp;이&nbsp;앱을&nbsp;통해&nbsp;사용자들은&nbsp;자신의&nbsp;반려동물을&nbsp;동네&nbsp;사람들에게&nbsp;소개하고,&nbsp;다른&nbsp;동네의&nbsp;반려동물들도&nbsp;구경할&nbsp;수&nbsp;있습니다.&nbsp;또한,&nbsp;반려인들이&nbsp;추천하는&nbsp;장소&nbsp;정보를&nbsp;공유하고&nbsp;자신만의&nbsp;추억이&nbsp;있는&nbsp;장소를&nbsp;지도에&nbsp;표시할&nbsp;수&nbsp;있습니다.&nbsp;\"피피\"는&nbsp;사용자들의&nbsp;활동에&nbsp;따라&nbsp;클로버를&nbsp;제공하며,&nbsp;이를&nbsp;통해&nbsp;다양한&nbsp;혜택을&nbsp;받을&nbsp;수&nbsp;있고&nbsp;동물&nbsp;친구들에게도&nbsp;도움을&nbsp;줄&nbsp;수&nbsp;있습니다.&nbsp;이&nbsp;앱은&nbsp;지역&nbsp;기반의&nbsp;반려동물&nbsp;커뮤니티를&nbsp;형성하여&nbsp;반려인들&nbsp;간의&nbsp;소통과&nbsp;정보&nbsp;공유를&nbsp;촉진합니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"피피.jpg\" data-origin-width=\"661\" data-origin-height=\"1421\"><span data-url=\"https://blog.kakaocdn.net/dn/T0xf1/btsH9ZYkluB/lJM7X74MqIcIPH0IVQYH60/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/T0xf1/btsH9ZYkluB/lJM7X74MqIcIPH0IVQYH60/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FT0xf1%2FbtsH9ZYkluB%2FlJM7X74MqIcIPH0IVQYH60%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"피피 - 우리동네 반려동물 커뮤니티\" width=\"740\" height=\"1591\" data-filename=\"피피.jpg\" data-origin-width=\"661\" data-origin-height=\"1421\"/></span></figure>\n</p>\n<figure id=\"og_1719183650046\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"피피 - 우리동네 반려동물 커뮤니티 - Google Play 앱\" data-og-description=\"동네 반려생활의 시작, 피피\" data-og-host=\"play.google.com\" data-og-source-url=\"https://play.google.com/store/apps/details?id=com.PPFriends.PP.android\" data-og-url=\"https://play.google.com/store/apps/details?id=com.PPFriends.PP.android&amp;hl=ko\" data-og-image=\"https://scrap.kakaocdn.net/dn/IS0WK/hyWrT91HjZ/LZUr0D6GfKn5m8jA3SXjHK/img.png?width=512&amp;height=512&amp;face=0_0_512_512,https://scrap.kakaocdn.net/dn/nxoBE/hyWoPVKR46/nkL9k10kJt3j5dHpqMDN10/img.png?width=600&amp;height=300&amp;face=0_0_600_300,https://scrap.kakaocdn.net/dn/b5Howf/hyWoEmodWP/bQRb5DOVx04IHgb6ebRxv0/img.png?width=240&amp;height=240&amp;face=0_0_240_240\"><a href=\"https://play.google.com/store/apps/details?id=com.PPFriends.PP.android\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://play.google.com/store/apps/details?id=com.PPFriends.PP.android\">\n<div class=\"og-image\" style=\"background-image: url('https://scrap.kakaocdn.net/dn/IS0WK/hyWrT91HjZ/LZUr0D6GfKn5m8jA3SXjHK/img.png?width=512&amp;height=512&amp;face=0_0_512_512,https://scrap.kakaocdn.net/dn/nxoBE/hyWoPVKR46/nkL9k10kJt3j5dHpqMDN10/img.png?width=600&amp;height=300&amp;face=0_0_600_300,https://scrap.kakaocdn.net/dn/b5Howf/hyWoEmodWP/bQRb5DOVx04IHgb6ebRxv0/img.png?width=240&amp;height=240&amp;face=0_0_240_240');\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">피피 - 우리동네 반려동물 커뮤니티 - Google Play 앱</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">동네 반려생활의 시작, 피피</p>\n<p class=\"og-host\" data-ke-size=\"size16\">play.google.com</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><b>5. 하입앤 HypeN - 아티스트와 팬들을 연결하는 예술 중심 소셜 플랫폼 </b></span></h2>\n<p data-ke-size=\"size16\">HypeN은 아티스트와 팬들을 연결하는 예술 중심 소셜 플랫폼입니다. 이 앱에서 사용자들은 작품에 'HYPE'를 누르고 개인 아트 갤러리를 만들 수 있으며, 작가들의 일상과 작품 활동을 팔로우하고 소통할 수 있습니다. 또한 HypeN은 독점 DROP 이벤트를 통해 작가의 드로잉이나 브랜드 콜라보레이션 상품을 제공하며, 다양한 카테고리의 예술 작품을 검색하고 탐색할 수 있는 기능을 제공합니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"하입앤.jpg\" data-origin-width=\"803\" data-origin-height=\"1421\"><span data-url=\"https://blog.kakaocdn.net/dn/bejqQQ/btsH9Yd3Y5W/EWkFVii9qnKINw8otHS5rK/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/bejqQQ/btsH9Yd3Y5W/EWkFVii9qnKINw8otHS5rK/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbejqQQ%2FbtsH9Yd3Y5W%2FEWkFVii9qnKINw8otHS5rK%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"하입앤 HypeN - 아티스트와 팬들을 연결하는 예술 중심 소셜 플랫폼\" width=\"740\" height=\"1310\" data-filename=\"하입앤.jpg\" data-origin-width=\"803\" data-origin-height=\"1421\"/></span></figure>\n</p>\n<figure id=\"og_1719183685455\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"하입앤 HypeN - Google Play 앱\" data-og-description=\"아트를 더 가깝게\" data-og-host=\"play.google.com\" data-og-source-url=\"https://play.google.com/store/apps/details?id=com.im.hypen\" data-og-url=\"https://play.google.com/store/apps/details?id=com.im.hypen&amp;hl=ko\" data-og-image=\"https://scrap.kakaocdn.net/dn/2Pc55/hyWoBXtKsM/TEfIU7265mglsqoAoRKsLK/img.png?width=512&amp;height=512&amp;face=0_0_512_512,https://scrap.kakaocdn.net/dn/PYaPR/hyWoJ83UQh/YNSdfxSGRNKL9jsEFEAZnK/img.png?width=600&amp;height=300&amp;face=0_0_600_300,https://scrap.kakaocdn.net/dn/R3lmE/hyWr0g0Gmd/lIHiY7ocDvrVUdhLM9Ea30/img.png?width=240&amp;height=240&amp;face=0_0_240_240\"><a href=\"https://play.google.com/store/apps/details?id=com.im.hypen\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://play.google.com/store/apps/details?id=com.im.hypen\">\n<div class=\"og-image\" style=\"background-image: url('https://scrap.kakaocdn.net/dn/2Pc55/hyWoBXtKsM/TEfIU7265mglsqoAoRKsLK/img.png?width=512&amp;height=512&amp;face=0_0_512_512,https://scrap.kakaocdn.net/dn/PYaPR/hyWoJ83UQh/YNSdfxSGRNKL9jsEFEAZnK/img.png?width=600&amp;height=300&amp;face=0_0_600_300,https://scrap.kakaocdn.net/dn/R3lmE/hyWr0g0Gmd/lIHiY7ocDvrVUdhLM9Ea30/img.png?width=240&amp;height=240&amp;face=0_0_240_240');\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">하입앤 HypeN - Google Play 앱</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">아트를 더 가깝게</p>\n<p class=\"og-host\" data-ke-size=\"size16\">play.google.com</p>\n</div>\n</a></figure>",
        "contentSnippet": "구글플레이 스토어에 등록된 유용한 앱 5개를  소개합니다. 새로운 기능, 사용자 경험 향상을 위한 앱들을 발견하고, 일상생활을 더욱 편리하게 만들어 줄 최고의 앱들을 찾아보세요.\n\n\n \n 안드로이드 앱스토어인 구글 플레이 스토어에는 하루에도 엄청난 수의 앱과 게임이 신규로 등록됩니다. 이 모든앱들을 사용자가 확인하고 양질의 앱을 선택하는 것이 사실상 불가능 하다는 얘기죠.  \n또한, 최근들어 강화되었다 하지만 여전히 구글 플레이스토어에는 유해한 앱들이 사라지지 않고 이들 앱으로 피해를 보는 사용자도 많습니다. 본 블로그에서는 일주일에 한 번정도 운영자가 직접 유용하고 편리한 앱을 엄선하여 소개합니다.\n \n'어떤오후의 프리웨어 이야기'에서 추천하는 2024년 6월 24일자 '안드로이드 추천 앱'입니다.\n \n1. 디어스 - 크리에이터와 팬들이 소통할 수 있는 온라인 플랫폼\n  디어스는 크리에이터와 팬들이 소통할 수 있는 특별한 온라인 플랫폼입니다. 이 앱에서는 크리에이터와 팬들이 '스페이스'라는 공간에서 자유롭게 소통하고 콘텐츠를 공유할 수 있습니다. 디어스는 다양한 이벤트와 최신 콘텐츠를 제공하며, 모바일 티케팅 서비스도 간편하게 이용할 수 있습니다. 또한, 팬들은 좋아하는 크리에이터의 일정과 활동을 캘린더 기능을 통해 한눈에 확인할 수 있습니다. 이 플랫폼은 크리에이터와 팬들이 하나가 되어 즐길 수 있는 새로운 형태의 커뮤니티 공간을 제공합니다.\n\n\n\n \n디어스 - Google Play 앱\n크리에이터와 팬, 우리가 되는 공간\nplay.google.com\n\n \n \n \n2. OMG - 중고등학생 전용 SNS\nOMG는 중고등학생들을 위한 익명 SNS 플랫폼으로, 친구들과 익명으로 속마음을 나누고 투표할 수 있는 서비스입니다. 이 앱을 통해 사용자는 자신에 대한 친구들의 생각을 알아볼 수 있으며, 직접 만든 질문에 대한 응답도 받을 수 있습니다. OMG는 또한 학교 내 인기 투표와 같은 재미있는 기능을 제공하여 학교 생활을 더욱 흥미롭게 만듭니다. 더 나아가, 전국의 또래 친구들과 소통하고 다양한 챌린지에 참여할 수 있어, 폭넓은 교류와 재미를 경험할 수 있습니다.\n\n\n\n \nOMG - 중고등학생 전용 SNS - Google Play 앱\n내 친구들과 익명으로 더 재밌게 소통하고, 전국의 또래 친구들과도 부담없이 소통해보세요!\nplay.google.com\n\n \n \n \n3. CamScanner - 문서를 PDF로 스캔하기\n  CamScanner는 스마트폰을 이용해 문서를 스캔하고 관리할 수 있는 다목적 앱입니다. 이 앱은 고품질 이미지 처리 기술을 사용하여 문서를 스캔하고, OCR 기능으로 텍스트를 인식하며, 클라우드 저장소를 통해 여러 기기 간 동기화를 지원합니다. CamScanner는 문서 공유, 프린트, 팩스 기능도 제공하여 비즈니스맨, 학생, 디자이너 등 다양한 사용자들의 needs를 충족시킵니다. 무료 버전과 고급 계정이 있어 사용자의 필요에 따라 선택할 수 있으며, 전 세계적으로 4000만 명 이상의 사용자가 이용하고 있는 인기 앱입니다.\n\n\n\n \nCamScanner - 문서를 PDF로 스캔하기 - Google Play 앱\n쉽고 간편한 1:1 고화질 정밀스캔, PDF/Word/IMG 다양한 편집, 포맷 저장, 이미지 텍스트 인식 지원, 지금 바로 다운로드하세요!\nplay.google.com\n\n \n \n \n4. 피피 - 우리동네 반려동물 커뮤니티\n\n\"피피\"는 동네 반려동물과 반려인들의 이야기를 공유하는 소셜 플랫폼입니다. 이 앱을 통해 사용자들은 자신의 반려동물을 동네 사람들에게 소개하고, 다른 동네의 반려동물들도 구경할 수 있습니다. 또한, 반려인들이 추천하는 장소 정보를 공유하고 자신만의 추억이 있는 장소를 지도에 표시할 수 있습니다. \"피피\"는 사용자들의 활동에 따라 클로버를 제공하며, 이를 통해 다양한 혜택을 받을 수 있고 동물 친구들에게도 도움을 줄 수 있습니다. 이 앱은 지역 기반의 반려동물 커뮤니티를 형성하여 반려인들 간의 소통과 정보 공유를 촉진합니다.\n\n\n\n \n피피 - 우리동네 반려동물 커뮤니티 - Google Play 앱\n동네 반려생활의 시작, 피피\nplay.google.com\n\n \n \n \n5. 하입앤 HypeN - 아티스트와 팬들을 연결하는 예술 중심 소셜 플랫폼 \nHypeN은 아티스트와 팬들을 연결하는 예술 중심 소셜 플랫폼입니다. 이 앱에서 사용자들은 작품에 'HYPE'를 누르고 개인 아트 갤러리를 만들 수 있으며, 작가들의 일상과 작품 활동을 팔로우하고 소통할 수 있습니다. 또한 HypeN은 독점 DROP 이벤트를 통해 작가의 드로잉이나 브랜드 콜라보레이션 상품을 제공하며, 다양한 카테고리의 예술 작품을 검색하고 탐색할 수 있는 기능을 제공합니다.\n\n\n\n \n하입앤 HypeN - Google Play 앱\n아트를 더 가깝게\nplay.google.com",
        "guid": "http://muzbox.tistory.com/2000",
        "categories": [
          "ANDROID &amp; 모바일/추천 무료 앱",
          "디어스",
          "모바일",
          "문서를 pdf로 스캔하기",
          "반려동물 커뮤니티",
          "안드로이드 추천 앱",
          "예술 중심 소셜 플랫폼",
          "중고등학생 전용 sns",
          "추천 어플"
        ],
        "isoDate": "2024-06-23T23:06:28.000Z"
      },
      {
        "creator": "어떤오후의 프리웨어 이야기",
        "title": "바이러스 치료 최강, MZK 다운로드 방법과 자세한 사용법",
        "link": "http://muzbox.tistory.com/1307",
        "pubDate": "Sun, 23 Jun 2024 22:30:21 +0900",
        "author": "어떤오후의 프리웨어 이야기",
        "comments": "http://muzbox.tistory.com/1307#entry1307comment",
        "content": "<p data-ke-size=\"size16\">&nbsp;MZK는 단순한 백신 프로그램이 아닌, 다양한 명령어를 모아둔 스크립트 파일로, 복잡한 문제를 효과적으로 해결합니다. 이 기사에서는 MZK를 이용해 악성코드를 제거하는 방법을 단계별로 설명합니다.&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthContent\" data-filename=\"00.jpg\" data-origin-width=\"500\" data-origin-height=\"500\"><span data-url=\"https://blog.kakaocdn.net/dn/rIvVC/btq1Qn7A1K4/OA40qWmNrm6siBAkanjWf1/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/rIvVC/btq1Qn7A1K4/OA40qWmNrm6siBAkanjWf1/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FrIvVC%2Fbtq1Qn7A1K4%2FOA40qWmNrm6siBAkanjWf1%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"00.jpg\" data-origin-width=\"500\" data-origin-height=\"500\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">악성코드나 바이러스로 고생하신 경험이 있으신 분들이라면 MZK의 강력함을 잘 아실겁니다.</span></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 class=\"hdg-03\" style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 7px; padding: 0px 5px 7px 0px; border-width: 0px 0px 1px; border-bottom: 1px solid #dadada; font-size: 16px; box-sizing: border-box; float: none; clear: both; font-weight: bold; line-height: 1.4; color: #333333; font-style: normal; font-variant: normal; letter-spacing: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; background-color: #ffffff;\" data-ke-size=\"size23\"><span style=\"margin: 0px; padding: 1px 0px 2px 9px; border-width: 0px 0px 0px 5px; border-style: solid; border-left-color: #3366ff; font-size: 16px; box-sizing: border-box; display: block;\"><span style=\"font-family: NanumGothic; font-size: 18pt;\">MZK 란?</span><br /></span></h3>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">MZK는 </span><span style=\"font-size: 12pt;\">Malware Zero Kit의 약자로 V3 처럼 백신 프로그램이라기 보다 악성코드나 바이러스를 효과적으로 처리하기 위한 명령어들을 모아둔 스크립트 파일입니다. 따라서 EXE</span><span style=\"font-size: 12pt;\">형태의 실행파일이 아닌 명령어들을 모아서 실행하는 BAT 파일로 실행됩니다.</span></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">지금은 워낙 유명해져 컴퓨터에 대해 관심인 있는 분중에 </span><span style=\"font-size: 12pt;\">MZK를 모르는 사람이 거의 없지만 초기에 V3로도 알약으로도 해결되지 않는 문제를 MZK를 통해 해결했다는 분들의 경험담이 인터넷에 자주 올라오곤 했습니다.</span></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">MZK의 제거 대상은악성코드(바이러스, 트로이 목마 등), 악성 애드웨어(광고 프로그램), 위협 가능성 및 심각한 불편함 유발이 확인된 불필요하거나 잠재적으로 원하지 않는 프로그램(PUA/PUP)입니다.</span></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">저도 해결되지 않던 악성코드 제거나 이유</span><span style=\"font-size: 12pt;\">모를 PC 오작동 문제를 MZK로 해결한 경험이 수차례 있습니다.</span></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">MZK 실행을 원활하게 실행하기 위해서는 PC를 재 부팅하여 '안전모드 환경 -명령 프롬프트 모드'에서 구동하는것을 권장하나 초보분들에는 조금 어려운 부분이라 판단되는데요.</span></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">MZK를 이용하여 악성코드를 치료하는 방법을 소개합니다.</span></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 class=\"hdg-03\" style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 7px; padding: 0px 5px 7px 0px; border-width: 0px 0px 1px; border-bottom: 1px solid #dadada; font-size: 16px; box-sizing: border-box; float: none; clear: both; font-weight: bold; line-height: 1.4; color: #333333; font-style: normal; font-variant: normal; letter-spacing: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; background-color: #ffffff;\" data-ke-size=\"size23\"><span style=\"margin: 0px; padding: 1px 0px 2px 9px; border-width: 0px 0px 0px 5px; border-style: solid; border-left-color: #3366ff; font-size: 16px; box-sizing: border-box; display: block;\"><span style=\"font-family: NanumGothic; font-size: 18pt;\">따라하기</span><br /></span></h3>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">1. 먼저 '</span><span style=\"font-size: 12pt;\">Malware Zero</span><span style=\"font-size: 12pt;\">' 사이트에 방문하시어 MZK 를 다운받습니다.▼<span style=\"font-size: 12pt; color: #0055ff;\"> - 다운로드 링크는 본문 맨아래 참고</span></span></p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthContent\" data-origin-width=\"700\" data-origin-height=\"427\"><span data-url=\"https://t1.daumcdn.net/cfile/tistory/99450E505CDA8F4802?original\" data-phocus=\"phocus\"><img src=\"https://t1.daumcdn.net/cfile/tistory/99450E505CDA8F4802\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99450E505CDA8F4802\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" width=\"700\" height=\"427\" data-origin-width=\"700\" data-origin-height=\"427\"/></span></figure>\n</p>\n<p style=\"text-align: center; clear: none; float: none;\" data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">2. 다운 받은 압축파일을 해제한후 start.bat 를 실행합니다.</span></p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\"> * 윈도우에서 실행을 해도 되지만 PC 환경에 따라 검사 시간이 오래걸릴 수 있으니 안전모드에서 실행하는 것을 권장합니다.</span></p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\"> 안전모드 부팅후 명령프롬프트 모드에서 실행하는 것을 대비해서 가능하면 그림과 같이 C:\\ 에 압축을 푸시기 바랍니다.</span><span style=\"font-size: 12pt;\">▼</span></p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthContent\" data-origin-width=\"700\" data-origin-height=\"428\"><span data-url=\"https://t1.daumcdn.net/cfile/tistory/998A83455CDA8F5D0A?original\" data-phocus=\"phocus\"><img src=\"https://t1.daumcdn.net/cfile/tistory/998A83455CDA8F5D0A\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F998A83455CDA8F5D0A\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" width=\"700\" height=\"428\" data-origin-width=\"700\" data-origin-height=\"428\"/></span></figure>\n</p>\n<p style=\"text-align: center; clear: none; float: none;\" data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">3. PC를 재부팅합니다. 이때 안전모드 (명령 프롬프트 사용)을 선택합니다.</span><span style=\"font-size: 16px;\">▼</span></p>\n<blockquote data-ke-style=\"style3\">\n<p data-ke-size=\"size16\"><b><span style=\"font-size: 12pt;\">■ Windows Vista, 7</span></b></p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\"> 초기 Windows 부팅 화면이 나오기 바로 전 잠깐의 검은 화면 상태에서 F8 키를 누르면 나오는 고급 부팅 화면에서 선택</span></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><b><span style=\"font-size: 12pt;\">■ Windows 8</span></b></p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\"> 설정(단축: 윈도우 + I 키) &rarr; PC 설정 변경 &rarr; 업데이트 및 복구 &rarr; 복구 &rarr; 고급 시작 옵션 &rarr; 다시 시작 &rarr; 문제 해결 &rarr; 고급 옵션 &rarr; 시작 설정 &rarr; 다시 시작 &rarr; 고급 부팅 화면에서 선택</span></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><b><span style=\"font-size: 12pt;\">■ Windows 10</span></b></p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\"> 설정(단축: 윈도우 + I 키) &rarr; 업데이트 및 복구 &rarr; 복구 &rarr; 고급 시작 옵션 &rarr; 다시 시작 &rarr; 문제 해결 &rarr; 고급 옵션 &rarr; 시작 설정 &rarr; 다시 시작 &rarr; 고급 부팅 화면에서 선택</span></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n</blockquote>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthContent\" data-origin-width=\"640\" data-origin-height=\"365\"><span data-url=\"https://t1.daumcdn.net/cfile/tistory/997C8D4F5B2B789507?original\" data-phocus=\"phocus\"><img src=\"https://t1.daumcdn.net/cfile/tistory/997C8D4F5B2B789507\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F997C8D4F5B2B789507\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" width=\"640\" height=\"365\" data-origin-width=\"640\" data-origin-height=\"365\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">4. 프롬프트 상태에서 그림과 같이 명령어을 입력하여 MZK 가 설치된경로로 이동한 후 start.bat 를 실행합니다.</span><span style=\"font-size: 16px;\">▼</span></p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthContent\" data-origin-width=\"385\" data-origin-height=\"243\"><span data-url=\"https://t1.daumcdn.net/cfile/tistory/99FE4A4D5CDA906802?original\" data-phocus=\"phocus\"><img src=\"https://t1.daumcdn.net/cfile/tistory/99FE4A4D5CDA906802\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99FE4A4D5CDA906802\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" width=\"385\" height=\"243\" data-origin-width=\"385\" data-origin-height=\"243\"/></span></figure>\n</p>\n<p style=\"text-align: center; clear: none; float: none;\" data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: center; clear: none; float: none;\" data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">5. MZK 실행 초기화면입니다. 스크립트 초기화중으로 잠시 기다립니다.</span><span style=\"font-size: 16px;\">▼</span></p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthContent\" data-origin-width=\"640\" data-origin-height=\"365\"><span data-url=\"https://t1.daumcdn.net/cfile/tistory/99ECB2335B2B789618?original\" data-phocus=\"phocus\"><img src=\"https://t1.daumcdn.net/cfile/tistory/99ECB2335B2B789618\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99ECB2335B2B789618\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" width=\"640\" height=\"365\" data-origin-width=\"640\" data-origin-height=\"365\"/></span></figure>\n</p>\n<p style=\"text-align: center; clear: none; float: none;\" data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: center; clear: none; float: none;\" data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: left; clear: none; float: none;\" data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">6. 검사진행전 숙지사항이 공지됩니다. 잘 읽어보시고 'Y'를 클릭</span><span style=\"font-size: 16px;\">▼</span></p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthContent\" data-origin-width=\"640\" data-origin-height=\"365\"><span data-url=\"https://t1.daumcdn.net/cfile/tistory/993A9E4E5B2B78950A?original\" data-phocus=\"phocus\"><img src=\"https://t1.daumcdn.net/cfile/tistory/993A9E4E5B2B78950A\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F993A9E4E5B2B78950A\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" width=\"640\" height=\"365\" data-origin-width=\"640\" data-origin-height=\"365\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">7. 다음 팝업창에서도 'Y' </span><span style=\"font-size: 16px;\">▼</span></p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthContent\" data-origin-width=\"640\" data-origin-height=\"365\"><span data-url=\"https://t1.daumcdn.net/cfile/tistory/995AAA485B2B78960A?original\" data-phocus=\"phocus\"><img src=\"https://t1.daumcdn.net/cfile/tistory/995AAA485B2B78960A\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F995AAA485B2B78960A\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" width=\"640\" height=\"365\" data-origin-width=\"640\" data-origin-height=\"365\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">8. 검사가 시작되었습니다.</span><span style=\"font-size: 16px;\">▼</span></p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthContent\" data-origin-width=\"640\" data-origin-height=\"365\"><span data-url=\"https://t1.daumcdn.net/cfile/tistory/99585F405B2B78953B?original\" data-phocus=\"phocus\"><img src=\"https://t1.daumcdn.net/cfile/tistory/99585F405B2B78953B\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99585F405B2B78953B\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" width=\"640\" height=\"365\" data-origin-width=\"640\" data-origin-height=\"365\"/></span></figure>\n</p>\n<p style=\"text-align: center; clear: none; float: none;\" data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: left; clear: none; float: none;\" data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">9. 검사중에 PC 에서 이상이 발견되면 화면이 적색으로 변경됩니다. 몇가지 문제가 보이는군요.</span><span style=\"font-size: 16px;\">▼</span></p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthContent\" data-origin-width=\"640\" data-origin-height=\"365\"><span data-url=\"https://t1.daumcdn.net/cfile/tistory/99A8A1415B2B789538?original\" data-phocus=\"phocus\"><img src=\"https://t1.daumcdn.net/cfile/tistory/99A8A1415B2B789538\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99A8A1415B2B789538\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" width=\"640\" height=\"365\" data-origin-width=\"640\" data-origin-height=\"365\"/></span></figure>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthContent\" data-origin-width=\"640\" data-origin-height=\"365\"><span data-url=\"https://t1.daumcdn.net/cfile/tistory/9953D6345B2B789536?original\" data-phocus=\"phocus\"><img src=\"https://t1.daumcdn.net/cfile/tistory/9953D6345B2B789536\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F9953D6345B2B789536\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" width=\"640\" height=\"365\" data-origin-width=\"640\" data-origin-height=\"365\"/></span></figure>\n</p>\n<p style=\"text-align: center; clear: none; float: none;\" data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: center; clear: none; float: none;\" data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: left; clear: none; float: none;\" data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">10. 검사가 완료 되었습니다. 진단</span><span style=\"font-size: 12pt;\">내역을 확인하기 위해 '확인'버튼을 클릭합니다.</span><span style=\"font-size: 16px;\">▼</span></p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthContent\" data-origin-width=\"640\" data-origin-height=\"365\"><span data-url=\"https://t1.daumcdn.net/cfile/tistory/9973823B5B2B789505?original\" data-phocus=\"phocus\"><img src=\"https://t1.daumcdn.net/cfile/tistory/9973823B5B2B789505\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F9973823B5B2B789505\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" width=\"640\" height=\"365\" data-origin-width=\"640\" data-origin-height=\"365\"/></span></figure>\n</p>\n<p style=\"text-align: center; clear: none; float: none;\" data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: left; clear: none; float: none;\" data-ke-size=\"size16\"><span style=\"font-size: 12pt;\">11. 그림과 같이 진단 내역을 확인할 수 있습니다.</span><span style=\"font-size: 16px;\">▼</span></p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthContent\" data-origin-width=\"640\" data-origin-height=\"365\"><span data-url=\"https://t1.daumcdn.net/cfile/tistory/995DDC475B2B789505?original\" data-phocus=\"phocus\"><img src=\"https://t1.daumcdn.net/cfile/tistory/995DDC475B2B789505\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F995DDC475B2B789505\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" width=\"640\" height=\"365\" data-origin-width=\"640\" data-origin-height=\"365\"/></span></figure>\n</p>\n<p style=\"text-align: center; clear: none; float: none;\" data-ke-size=\"size16\">&nbsp;</p>\n<h3 class=\"hdg-03\" style=\"box-sizing: border-box; line-height: 1.4; color: #333333; margin: 0px 0px 7px; padding: 0px 5px 7px 0px; border-width: 0px 0px 1px; border-image: initial; letter-spacing: 1px; word-spacing: 3px; float: none; clear: both; border-color: initial initial #dadada #55555b; border-style: initial initial solid solid;\" data-ke-size=\"size23\"><span style=\"box-sizing: border-box; margin: 0px; padding: 1px 0px 2px 9px; border-width: 0px 0px 0px 5px; border-style: solid; border-left-color: #3366ff; display: block;\"><span><span style=\"font-size: 24px;\">MZK 다운받기</span></span></span></h3>\n<p style=\"box-sizing: border-box; margin: 1em 0px; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 12px; padding-top: 0px !important; padding-bottom: 0px !important;\" data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"box-sizing: border-box; margin: 1em 0px; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 12px; padding-top: 0px !important; padding-bottom: 0px !important;\" data-ke-size=\"size16\"><span style=\"box-sizing: border-box; font-size: 12pt;\"><span style=\"box-sizing: border-box; font-weight: bold;\"></span></span><span style=\"box-sizing: border-box; font-weight: bold;\"><span style=\"box-sizing: border-box; font-size: 12pt;\">》</span></span><span style=\"box-sizing: border-box; font-size: 12pt;\"><span style=\"box-sizing: border-box; font-weight: bold;\">License</span> : 프리웨어 (</span><span style=\"box-sizing: border-box; font-size: 12pt;\">개인무료, 기업무료) 기부환영.</span></p>\n<p style=\"box-sizing: border-box; margin: 1em 0px; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 12px; padding-top: 0px !important; padding-bottom: 0px !important;\" data-ke-size=\"size16\"><span style=\"box-sizing: border-box; font-size: 12pt;\"><span style=\"box-sizing: border-box; font-weight: bold;\"> 》 다운로드 바로가기</span></span></p>\n<p style=\"box-sizing: border-box; margin: 1em 0px; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 12px; padding-top: 0px !important; padding-bottom: 0px !important;\" data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"box-sizing: border-box; margin: 1em 0px; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 12px; padding-top: 0px !important; padding-bottom: 0px !important; text-align: center;\" data-ke-size=\"size16\"><span style=\"box-sizing: border-box; font-size: 12pt;\"><span style=\"font-size: 18pt;\"><b><a class=\"tx-link\" href=\"https://malzero.xyz/\" target=\"_blank\" rel=\"noopener\"><span style=\"color: #0900ff;\">https://malzero.xyz/</span></a></b></span></span></p>",
        "contentSnippet": "MZK는 단순한 백신 프로그램이 아닌, 다양한 명령어를 모아둔 스크립트 파일로, 복잡한 문제를 효과적으로 해결합니다. 이 기사에서는 MZK를 이용해 악성코드를 제거하는 방법을 단계별로 설명합니다. \n \n\n\n \n악성코드나 바이러스로 고생하신 경험이 있으신 분들이라면 MZK의 강력함을 잘 아실겁니다.\n \n \nMZK 란?\n\nMZK는 Malware Zero Kit의 약자로 V3 처럼 백신 프로그램이라기 보다 악성코드나 바이러스를 효과적으로 처리하기 위한 명령어들을 모아둔 스크립트 파일입니다. 따라서 EXE형태의 실행파일이 아닌 명령어들을 모아서 실행하는 BAT 파일로 실행됩니다.\n \n지금은 워낙 유명해져 컴퓨터에 대해 관심인 있는 분중에 MZK를 모르는 사람이 거의 없지만 초기에 V3로도 알약으로도 해결되지 않는 문제를 MZK를 통해 해결했다는 분들의 경험담이 인터넷에 자주 올라오곤 했습니다.\n \nMZK의 제거 대상은악성코드(바이러스, 트로이 목마 등), 악성 애드웨어(광고 프로그램), 위협 가능성 및 심각한 불편함 유발이 확인된 불필요하거나 잠재적으로 원하지 않는 프로그램(PUA/PUP)입니다.\n \n저도 해결되지 않던 악성코드 제거나 이유모를 PC 오작동 문제를 MZK로 해결한 경험이 수차례 있습니다.\n \nMZK 실행을 원활하게 실행하기 위해서는 PC를 재 부팅하여 '안전모드 환경 -명령 프롬프트 모드'에서 구동하는것을 권장하나 초보분들에는 조금 어려운 부분이라 판단되는데요.\n \nMZK를 이용하여 악성코드를 치료하는 방법을 소개합니다.\n \n \n따라하기\n\n1. 먼저 'Malware Zero' 사이트에 방문하시어 MZK 를 다운받습니다.▼ - 다운로드 링크는 본문 맨아래 참고\n\n\n \n \n \n2. 다운 받은 압축파일을 해제한후 start.bat 를 실행합니다.\n * 윈도우에서 실행을 해도 되지만 PC 환경에 따라 검사 시간이 오래걸릴 수 있으니 안전모드에서 실행하는 것을 권장합니다.\n 안전모드 부팅후 명령프롬프트 모드에서 실행하는 것을 대비해서 가능하면 그림과 같이 C:\\ 에 압축을 푸시기 바랍니다.▼\n\n\n \n \n \n3. PC를 재부팅합니다. 이때 안전모드 (명령 프롬프트 사용)을 선택합니다.▼\n■ Windows Vista, 7\n 초기 Windows 부팅 화면이 나오기 바로 전 잠깐의 검은 화면 상태에서 F8 키를 누르면 나오는 고급 부팅 화면에서 선택\n \n■ Windows 8\n 설정(단축: 윈도우 + I 키) → PC 설정 변경 → 업데이트 및 복구 → 복구 → 고급 시작 옵션 → 다시 시작 → 문제 해결 → 고급 옵션 → 시작 설정 → 다시 시작 → 고급 부팅 화면에서 선택\n \n■ Windows 10\n 설정(단축: 윈도우 + I 키) → 업데이트 및 복구 → 복구 → 고급 시작 옵션 → 다시 시작 → 문제 해결 → 고급 옵션 → 시작 설정 → 다시 시작 → 고급 부팅 화면에서 선택\n \n \n\n\n \n \n4. 프롬프트 상태에서 그림과 같이 명령어을 입력하여 MZK 가 설치된경로로 이동한 후 start.bat 를 실행합니다.▼\n\n\n \n \n \n5. MZK 실행 초기화면입니다. 스크립트 초기화중으로 잠시 기다립니다.▼\n\n\n \n \n6. 검사진행전 숙지사항이 공지됩니다. 잘 읽어보시고 'Y'를 클릭▼\n\n\n \n \n7. 다음 팝업창에서도 'Y' ▼\n\n\n \n8. 검사가 시작되었습니다.▼\n\n\n \n9. 검사중에 PC 에서 이상이 발견되면 화면이 적색으로 변경됩니다. 몇가지 문제가 보이는군요.▼\n\n\n\n \n \n10. 검사가 완료 되었습니다. 진단내역을 확인하기 위해 '확인'버튼을 클릭합니다.▼\n\n\n \n11. 그림과 같이 진단 내역을 확인할 수 있습니다.▼\n\n\n \nMZK 다운받기\n \n》License : 프리웨어 (개인무료, 기업무료) 기부환영.\n 》 다운로드 바로가기\n \nhttps://malzero.xyz/",
        "guid": "http://muzbox.tistory.com/1307",
        "categories": [
          "추천 프리웨어/시스템관리,보안",
          "광고제거",
          "맬웨어",
          "멀웨어 제거 프로그램 추천",
          "무료 바이러스 백신",
          "무료 악성코드 제거 프로그램",
          "바이러스",
          "악성코드",
          "악성코드 제거 프로그램",
          "유해 프로그램 제거"
        ],
        "isoDate": "2024-06-23T13:30:21.000Z"
      },
      {
        "creator": "어떤오후의 프리웨어 이야기",
        "title": "똑키, 즐겨찾기도 되는 단축키 뷰어",
        "link": "http://muzbox.tistory.com/1999",
        "pubDate": "Thu, 20 Jun 2024 16:20:13 +0900",
        "author": "어떤오후의 프리웨어 이야기",
        "comments": "http://muzbox.tistory.com/1999#entry1999comment",
        "content": "<p data-ke-size=\"size18\">&nbsp;</p>\n<p data-ke-size=\"size18\">&nbsp; 다양한 프로그램의 단축키를 쉽게 검색하고 즐겨찾기에 추가할 수 있는 똑키를 소개합니다. 문서 작업, 디자인, 게임 등에서 작업 효율을 높여주는 필수 프로그램이라 생각합니다.</p>\n<p data-ke-size=\"size18\">&nbsp;</p>\n<table style=\"border-collapse: collapse; width: 95.6979%; height: 248px;\" border=\"1\" data-ke-align=\"alignLeft\">\n<tbody>\n<tr style=\"height: 62px;\"><!-- 첫 번째 열에 배경색을 노란색으로 설정 -->\n<td style=\"width: 13.1835%; height: 62px; text-align: center; background-color: #555555;\"><span style=\"color: #ffffff;\"><b>분류</b></span></td>\n<td style=\"width: 22.4483%; height: 62px; text-align: center;\">문서작업/단축키</td>\n<td style=\"width: 64.3682%; height: 248px; text-align: center;\" rowspan=\"4\"><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"똑키.png\" data-origin-width=\"336\" data-origin-height=\"448\"><span data-url=\"https://blog.kakaocdn.net/dn/ctLVp0/btsH7iXsRFA/kr7xldQkI4gye9FABqt380/img.png\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/ctLVp0/btsH7iXsRFA/kr7xldQkI4gye9FABqt380/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FctLVp0%2FbtsH7iXsRFA%2Fkr7xldQkI4gye9FABqt380%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" width=\"238\" height=\"317\" data-filename=\"똑키.png\" data-origin-width=\"336\" data-origin-height=\"448\"/></span></figure>\n</td>\n</tr>\n<tr style=\"height: 62px;\"><!-- 첫 번째 열에 배경색을 노란색으로 설정 -->\n<td style=\"width: 13.1835%; height: 62px; text-align: center; background-color: #555555;\"><span style=\"color: #ffffff;\"><b>사용범위</b></span></td>\n<td style=\"width: 22.4483%; height: 62px; text-align: center;\">무료(개인)</td>\n</tr>\n<tr style=\"height: 62px;\"><!-- 첫 번째 열에 배경색을 노란색으로 설정 -->\n<td style=\"width: 13.1835%; height: 62px; text-align: center; background-color: #555555;\"><span style=\"color: #ffffff;\"><b>사용환경</b></span></td>\n<td style=\"width: 22.4483%; height: 62px; text-align: center;\">Windows</td>\n</tr>\n<tr style=\"height: 62px;\"><!-- 첫 번째 열에 배경색을 노란색으로 설정 -->\n<td style=\"width: 13.1835%; height: 62px; text-align: center; background-color: #555555;\"><span style=\"color: #ffffff;\"><b>제작사</b></span></td>\n<td style=\"width: 22.4483%; height: 62px; text-align: center;\"><a href=\"https://trendsoftware.co.kr/\" target=\"_blank\" rel=\"noopener\">트렌드소프트웨어</a></td>\n</tr>\n</tbody>\n</table>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><b> 프로그램 소개</b></h2>\n<p data-ke-size=\"size18\">&nbsp;현대인의 일상에서 컴퓨터는 필수적인 도구입니다. 업무를 처리하거나, 디자인 작업을 하거나, 게임을 즐기기 위해 우리는 컴퓨터 앞에 많은 시간을 할애합니다. 하지만 컴퓨터 작업 속도를 높이는 가장 간단하고 효과적인 방법은 바로 단축키를 활용하는 것인데요. 단축키를 잘 활용하면 불필요한 마우스 클릭을 줄이고, 작업 시간을 크게 단축할 수 있기때문이죠.<br /><br />이번 포스팅에서는 다양한 프로그램의 단축키를 쉽고 빠르게 검색하고 즐겨찾기에 추가할 수 있는 프로그램, <span style=\"color: #ee2323;\"><b>똑키</b></span>에 대해 소개합니다.</p>\n<p data-ke-size=\"size18\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"똑키 단축키 뷰어.png\" data-origin-width=\"500\" data-origin-height=\"500\"><span data-url=\"https://blog.kakaocdn.net/dn/U7oev/btsH5vEe98q/OVCgmgcK1aEigz8z0elR3k/img.png\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/U7oev/btsH5vEe98q/OVCgmgcK1aEigz8z0elR3k/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FU7oev%2FbtsH5vEe98q%2FOVCgmgcK1aEigz8z0elR3k%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"똑키 단축키 뷰어.png\" data-origin-width=\"500\" data-origin-height=\"500\"/></span></figure>\n</p>\n<p data-ke-size=\"size18\">&nbsp;</p>\n<p data-ke-size=\"size18\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><b> 주요 특징</b></h2>\n<p data-ke-size=\"size18\">&nbsp;<span style=\"color: #ee2323;\"><b>똑키</b></span>는 다양한 프로그램의 단축키를 쉽고 빠르게 검색할 수 있는 프로그램입니다. 이 프로그램은 사용자가 찾고자 하는 단축키를 간편하게 검색할 수 있도록 도와주며, 자주 사용하는 단축키를 즐겨찾기에 추가하여 언제든지 빠르게 접근할 수 있게 합니다. 문서 작업, 디자인 및 영상 편집, PC 게임, 개발자 명령어, 메신저 등 여러 분야의 단축키를 지원하여 사용자들의 다양한 요구를 충족시킵니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"똑키.jpg\" data-origin-width=\"720\" data-origin-height=\"441\"><span data-url=\"https://blog.kakaocdn.net/dn/dxW0ij/btsH7iJXxfZ/o0HRGRQRG2S92slCRjgmC1/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/dxW0ij/btsH7iJXxfZ/o0HRGRQRG2S92slCRjgmC1/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdxW0ij%2FbtsH7iJXxfZ%2Fo0HRGRQRG2S92slCRjgmC1%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"똑키.jpg\" data-origin-width=\"720\" data-origin-height=\"441\"/></span></figure>\n</p>\n<p data-ke-size=\"size18\">&nbsp;</p>\n<p data-ke-size=\"size18\"><span style=\"color: #ef5369;\"><b>1. 간편한 검색 기능</b></span></p>\n<p data-ke-size=\"size18\">똑키는 직관적인 검색 인터페이스를 제공하여 사용자가 원하는 단축키를 손쉽게 검색할 수 있습니다. 검색창에 키워드를 입력하면 관련 단축키 목록이 바로 나타나며, 필요한 정보를 빠르게 찾을 수 있습니다.</p>\n<p data-ke-size=\"size18\"><br /><span style=\"color: #ef5369;\"><b>2. 즐겨찾기 기능</b></span> </p>\n<p data-ke-size=\"size18\">자주&nbsp;사용하는&nbsp;단축키를&nbsp;즐겨찾기에&nbsp;추가하여&nbsp;언제든지&nbsp;빠르게&nbsp;접근할&nbsp;수&nbsp;있습니다.&nbsp;즐겨찾기에&nbsp;저장된&nbsp;단축키는&nbsp;상단&nbsp;메뉴에서&nbsp;쉽게&nbsp;확인할&nbsp;수&nbsp;있어&nbsp;다시&nbsp;찾아볼&nbsp;필요&nbsp;없이&nbsp;즉시&nbsp;사용할&nbsp;수&nbsp;있습니다. <br /><br /><span style=\"color: #ef5369;\"><b>3. 다양한 프로그램 지원</b></span><br />똑키는&nbsp;문서&nbsp;작업&nbsp;프로그램(예:&nbsp;Excel,&nbsp;Word),&nbsp;디자인&nbsp;및&nbsp;영상&nbsp;편집&nbsp;프로그램(예:&nbsp;Photoshop,&nbsp;Premiere),&nbsp;PC&nbsp;게임,&nbsp;개발자&nbsp;명령어,&nbsp;메신저&nbsp;등&nbsp;다양한&nbsp;프로그램의&nbsp;단축키를&nbsp;지원합니다.&nbsp;이를&nbsp;통해&nbsp;사용자는&nbsp;여러&nbsp;프로그램에서&nbsp;단축키를&nbsp;익히고&nbsp;활용할&nbsp;수&nbsp;있습니다. <br /><br /><span style=\"color: #ef5369;\"><b>4. 유용한 단축키 추천</b></span><br />똑키는&nbsp;사용자들에게&nbsp;작업&nbsp;효율을&nbsp;높여주는&nbsp;유용한&nbsp;단축키를&nbsp;추천합니다.&nbsp;불필요한&nbsp;마우스&nbsp;클릭을&nbsp;줄이고,&nbsp;작업&nbsp;속도를&nbsp;두&nbsp;배로&nbsp;올려주는&nbsp;단축키를&nbsp;제공하여&nbsp;사용자들의&nbsp;생산성을&nbsp;극대화합니다. <br /><br /><span style=\"color: #ef5369;\"><b>5. 실무 엑셀 함수 모음</b></span> <br />특히, 엑셀 작업을 자주 하는 사용자들을 위해 실무에서 자주 사용하는 엑셀 함수 모음을 제공합니다. 엑린이들도 쉽게 사용할 수 있도록 잘 정리되어 있어 업무 효율을 크게 높일 수 있습니다.</p>\n<p data-ke-size=\"size18\">&nbsp;</p>\n<p data-ke-size=\"size18\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><b>  프로그램 장단점</b></h2>\n<h4 data-ke-size=\"size20\"><b> 장점</b></h4>\n<blockquote data-ke-style=\"style3\"><b>1. 작업 효율성 향상:</b> 다양한 프로그램의 단축키를 쉽게 배우고 사용할 수 있어, 마우스 클릭을 줄이고 생산성을 높일 수 있습니다. <br /><b>2. 직관적인 인터페이스:</b> 초보자도 쉽게 사용할 수 있는 간단한 인터페이스를 제공합니다. <br /><b>3. 즐겨찾기 기능:</b> 자주 사용하는 단축키를 즐겨찾기에 추가하여 빠르게 접근할 수 있습니다. <br /><b>4. 다양한 프로그램 지원:</b> 여러 종류의 프로그램에서 단축키를 배우고 사용할 수 있도록 지원합니다. <br /><b>5. 유용한 단축키 추천:</b> 필요한 단축키를 추천하여 작업 효율을 높여줍니다.</blockquote>\n<h4 data-ke-size=\"size20\"><b> 단점</b></h4>\n<blockquote data-ke-style=\"style3\"><b>1. 일부 프로그램의 단축키 미지원:</b> 모든 프로그램의 단축키를 지원하지 않아, 필요한 경우 사용자가 직접 찾아야 할 수 있습니다. <br /><b>2. 초보자에게 어려움:</b> 단축키를 처음 배우는 사용자에게는 초기 학습이 어려울 수 있습니다. <br /><b>3. 기능 부족:</b> 단축키 외의 추가 기능이 부족하여, 특정 기능이 필요한 사용자는 다른 프로그램을 찾아야 할 수도 있습니다.</blockquote>\n<p data-ke-size=\"size18\">&nbsp;</p>\n<p data-ke-size=\"size18\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><b> &nbsp;간단 사용법</b></h2>\n<p data-ke-size=\"size18\">&nbsp;</p>\n<p data-ke-size=\"size18\"><span style=\"color: #ef5369;\"><b>1.&nbsp;프로그램&nbsp;설치&nbsp;및&nbsp;실행</b></span> <br />&nbsp; &nbsp;- 똑키 공식 웹사이트 또는 아래 링크에서 설치 파일을 다운로드합니다. <br />&nbsp;&nbsp;&nbsp;-&nbsp;다운로드한&nbsp;설치&nbsp;파일을&nbsp;실행하여&nbsp;설치&nbsp;과정을&nbsp;진행합니다. <br />&nbsp;&nbsp;&nbsp;-&nbsp;설치가&nbsp;완료되면&nbsp;바탕화면&nbsp;또는&nbsp;시작&nbsp;메뉴에서&nbsp;똑키&nbsp;아이콘을&nbsp;클릭하여&nbsp;프로그램을&nbsp;실행합니다. <br /><br /><span style=\"color: #ef5369;\"><b>2. 단축키가 필요한 프로그램 검색</b></span> <br />- 프로그램을 실행하면 메인 화면 상단에 검색창이 나타납니다. <br />- 찾고자 하는 단축키와 관련된 프로그램명을 입력합니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"똑키 단축키 검색.jpg\" data-origin-width=\"801\" data-origin-height=\"535\"><span data-url=\"https://blog.kakaocdn.net/dn/Mgxd1/btsH6YycKwN/Mcf1ZYNxksd9uk7PSukiLK/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/Mgxd1/btsH6YycKwN/Mcf1ZYNxksd9uk7PSukiLK/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FMgxd1%2FbtsH6YycKwN%2FMcf1ZYNxksd9uk7PSukiLK%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" width=\"700\" height=\"468\" data-filename=\"똑키 단축키 검색.jpg\" data-origin-width=\"801\" data-origin-height=\"535\"/></span></figure>\n</p>\n<p data-ke-size=\"size18\">&nbsp;</p>\n<p data-ke-size=\"size18\"><span style=\"color: #ef5369;\"><b>3. 프로그램별 단축키 탐색</b></span><br />&nbsp; 각 프로그램별로 기능별 카테고리가 나누어져 있어 필요한 단축키를 쉽게 찾을 수 있습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"검색2.jpg\" data-origin-width=\"336\" data-origin-height=\"528\"><span data-url=\"https://blog.kakaocdn.net/dn/ZRVyG/btsH5eJyj0Y/vPyQmia1hSKligmOzPx031/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/ZRVyG/btsH5eJyj0Y/vPyQmia1hSKligmOzPx031/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FZRVyG%2FbtsH5eJyj0Y%2FvPyQmia1hSKligmOzPx031%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"검색2.jpg\" data-origin-width=\"336\" data-origin-height=\"528\"/></span></figure>\n</p>\n<p data-ke-size=\"size18\"><br /><span style=\"color: #ef5369;\"><b>4. 즐겨찾기 추가</b></span> <br />&nbsp;- 검색 결과에서 자주 사용할 단축키를 선택합니다. <br />&nbsp;- 해당 단축키 오른쪽에 있는 즐겨찾기 노란 아이콘을 클릭하여 즐겨찾기에 추가합니다. <br />&nbsp;- 즐겨찾기에 추가된 단축키는 메인 화면 상단의 즐겨찾기 메뉴에서 확인할 수 있습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"즐겨찾기.jpg\" data-origin-width=\"336\" data-origin-height=\"464\"><span data-url=\"https://blog.kakaocdn.net/dn/b8i8CD/btsH6cxaq0D/IoxXKgaFJhqKStRT3QUuD1/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/b8i8CD/btsH6cxaq0D/IoxXKgaFJhqKStRT3QUuD1/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb8i8CD%2FbtsH6cxaq0D%2FIoxXKgaFJhqKStRT3QUuD1%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"즐겨찾기.jpg\" data-origin-width=\"336\" data-origin-height=\"464\"/></span></figure>\n</p>\n<p data-ke-size=\"size18\">&nbsp;</p>\n<h2 style=\"color: #000000; text-align: start;\" data-ke-size=\"size26\"><b>  라이센스 정책</b></h2>\n<p data-ke-size=\"size18\">똑키는 자유롭게 설치하여 사용이 가능한 프리웨어입니다.</p>\n<p data-ke-size=\"size18\">&nbsp;</p>\n<p data-ke-size=\"size18\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><b> 효과적인 활용 방법&nbsp;&nbsp;</b></h2>\n<p data-ke-size=\"size18\"><span style=\"color: #ef5369;\"><b>1. 자주 사용하는 단축키 익히기</b></span> <br />&nbsp;문서나 이미지, 영상 편집 작업 시 자주 사용하는 단축키를 익히면 작업 속도를 크게 향상시킬 수 있습니다. 예를 들어, 엑셀의 복사(Ctrl + C), 붙여넣기(Ctrl + V), 셀 병합(Alt + H + M + M) 등의 단축키를 익혀두면 반복적인 작업을 빠르게 처리할 수 있습니다. <br /><br /><span style=\"color: #ef5369;\"><b>2. 게임에서의 단축키 활용</b></span> <br />게임에서도 단축키는 중요한 역할을 합니다. 각 게임별로 중요한 단축키를 미리 숙지하여 사용하면 게임 플레이가 한층 더 원활해지고, 반응 속도도 빨라질 수 있습니다. 예를 들어, FPS 게임에서 무기 변경(Q), 리로드(R) 등의 단축키를 익혀두면 게임에서의 전투력을 높일 수 있습니다. <br /><br /><span style=\"color: #ef5369;\"><b>3. 실무 엑셀 함수 모음 활용</b></span> <br />엑셀을 자주 사용하는 업무 환경에서 실무 엑셀 함수 모음을 활용하면 업무 효율을 극대화할 수 있습니다. 기본적인 함수부터 고급 함수까지 다양하게 제공되므로, 필요한 함수를 바로 찾아 사용할 수 있습니다.</p>\n<p data-ke-size=\"size18\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><b>⬇️ 프로그램 다운로드</b></h2>\n\n<p data-ke-size=\"size16\">&nbsp;</p>\n<figure id=\"og_1718867779975\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"트렌드소프트웨어 똑똑단축키 프로그램\" data-og-description=\"트렌드소프트웨어 똑똑단축키, 엑셀, 윈도우, 게임, 프로그램 단축키 정보 프로그램 입니다.\" data-og-host=\"trendsoftware.co.kr\" data-og-source-url=\"https://trendsoftware.co.kr/ddockey\" data-og-url=\"https://trendsoftware.co.kr/ddockey\" data-og-image=\"https://scrap.kakaocdn.net/dn/bxt7HX/hyWoFEJmDt/2Kk5p4G4enCLVxFsBvlN91/img.png?width=1920&amp;height=1901&amp;face=0_0_1920_1901,https://scrap.kakaocdn.net/dn/AjGwo/hyWoKTy7VM/N7v5UFHyZdCIRkj2KbSKj1/img.png?width=1920&amp;height=1765&amp;face=0_0_1920_1765\"><a href=\"https://trendsoftware.co.kr/ddockey\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://trendsoftware.co.kr/ddockey\">\n<div class=\"og-image\" style=\"background-image: url('https://scrap.kakaocdn.net/dn/bxt7HX/hyWoFEJmDt/2Kk5p4G4enCLVxFsBvlN91/img.png?width=1920&amp;height=1901&amp;face=0_0_1920_1901,https://scrap.kakaocdn.net/dn/AjGwo/hyWoKTy7VM/N7v5UFHyZdCIRkj2KbSKj1/img.png?width=1920&amp;height=1765&amp;face=0_0_1920_1765');\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">트렌드소프트웨어 똑똑단축키 프로그램</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">트렌드소프트웨어 똑똑단축키, 엑셀, 윈도우, 게임, 프로그램 단축키 정보 프로그램 입니다.</p>\n<p class=\"og-host\" data-ke-size=\"size16\">trendsoftware.co.kr</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "다양한 프로그램의 단축키를 쉽게 검색하고 즐겨찾기에 추가할 수 있는 똑키를 소개합니다. 문서 작업, 디자인, 게임 등에서 작업 효율을 높여주는 필수 프로그램이라 생각합니다.\n \n\n분류\n문서작업/단축키\n\n\n\n\n사용범위\n무료(개인)\n\n\n사용환경\nWindows\n\n\n제작사\n트렌드소프트웨어\n\n\n\n \n 프로그램 소개\n 현대인의 일상에서 컴퓨터는 필수적인 도구입니다. 업무를 처리하거나, 디자인 작업을 하거나, 게임을 즐기기 위해 우리는 컴퓨터 앞에 많은 시간을 할애합니다. 하지만 컴퓨터 작업 속도를 높이는 가장 간단하고 효과적인 방법은 바로 단축키를 활용하는 것인데요. 단축키를 잘 활용하면 불필요한 마우스 클릭을 줄이고, 작업 시간을 크게 단축할 수 있기때문이죠.\n이번 포스팅에서는 다양한 프로그램의 단축키를 쉽고 빠르게 검색하고 즐겨찾기에 추가할 수 있는 프로그램, 똑키에 대해 소개합니다.\n \n\n\n \n \n 주요 특징\n 똑키는 다양한 프로그램의 단축키를 쉽고 빠르게 검색할 수 있는 프로그램입니다. 이 프로그램은 사용자가 찾고자 하는 단축키를 간편하게 검색할 수 있도록 도와주며, 자주 사용하는 단축키를 즐겨찾기에 추가하여 언제든지 빠르게 접근할 수 있게 합니다. 문서 작업, 디자인 및 영상 편집, PC 게임, 개발자 명령어, 메신저 등 여러 분야의 단축키를 지원하여 사용자들의 다양한 요구를 충족시킵니다.\n\n\n \n1. 간편한 검색 기능\n똑키는 직관적인 검색 인터페이스를 제공하여 사용자가 원하는 단축키를 손쉽게 검색할 수 있습니다. 검색창에 키워드를 입력하면 관련 단축키 목록이 바로 나타나며, 필요한 정보를 빠르게 찾을 수 있습니다.\n2. 즐겨찾기 기능 \n자주 사용하는 단축키를 즐겨찾기에 추가하여 언제든지 빠르게 접근할 수 있습니다. 즐겨찾기에 저장된 단축키는 상단 메뉴에서 쉽게 확인할 수 있어 다시 찾아볼 필요 없이 즉시 사용할 수 있습니다. \n3. 다양한 프로그램 지원\n똑키는 문서 작업 프로그램(예: Excel, Word), 디자인 및 영상 편집 프로그램(예: Photoshop, Premiere), PC 게임, 개발자 명령어, 메신저 등 다양한 프로그램의 단축키를 지원합니다. 이를 통해 사용자는 여러 프로그램에서 단축키를 익히고 활용할 수 있습니다. \n4. 유용한 단축키 추천\n똑키는 사용자들에게 작업 효율을 높여주는 유용한 단축키를 추천합니다. 불필요한 마우스 클릭을 줄이고, 작업 속도를 두 배로 올려주는 단축키를 제공하여 사용자들의 생산성을 극대화합니다. \n5. 실무 엑셀 함수 모음 \n특히, 엑셀 작업을 자주 하는 사용자들을 위해 실무에서 자주 사용하는 엑셀 함수 모음을 제공합니다. 엑린이들도 쉽게 사용할 수 있도록 잘 정리되어 있어 업무 효율을 크게 높일 수 있습니다.\n \n \n  프로그램 장단점\n 장점\n1. 작업 효율성 향상: 다양한 프로그램의 단축키를 쉽게 배우고 사용할 수 있어, 마우스 클릭을 줄이고 생산성을 높일 수 있습니다. \n2. 직관적인 인터페이스: 초보자도 쉽게 사용할 수 있는 간단한 인터페이스를 제공합니다. \n3. 즐겨찾기 기능: 자주 사용하는 단축키를 즐겨찾기에 추가하여 빠르게 접근할 수 있습니다. \n4. 다양한 프로그램 지원: 여러 종류의 프로그램에서 단축키를 배우고 사용할 수 있도록 지원합니다. \n5. 유용한 단축키 추천: 필요한 단축키를 추천하여 작업 효율을 높여줍니다.\n 단점\n1. 일부 프로그램의 단축키 미지원: 모든 프로그램의 단축키를 지원하지 않아, 필요한 경우 사용자가 직접 찾아야 할 수 있습니다. \n2. 초보자에게 어려움: 단축키를 처음 배우는 사용자에게는 초기 학습이 어려울 수 있습니다. \n3. 기능 부족: 단축키 외의 추가 기능이 부족하여, 특정 기능이 필요한 사용자는 다른 프로그램을 찾아야 할 수도 있습니다.\n \n \n  간단 사용법\n \n1. 프로그램 설치 및 실행 \n   - 똑키 공식 웹사이트 또는 아래 링크에서 설치 파일을 다운로드합니다. \n   - 다운로드한 설치 파일을 실행하여 설치 과정을 진행합니다. \n   - 설치가 완료되면 바탕화면 또는 시작 메뉴에서 똑키 아이콘을 클릭하여 프로그램을 실행합니다. \n2. 단축키가 필요한 프로그램 검색 \n- 프로그램을 실행하면 메인 화면 상단에 검색창이 나타납니다. \n- 찾고자 하는 단축키와 관련된 프로그램명을 입력합니다.\n\n\n \n3. 프로그램별 단축키 탐색\n  각 프로그램별로 기능별 카테고리가 나누어져 있어 필요한 단축키를 쉽게 찾을 수 있습니다.\n\n\n\n4. 즐겨찾기 추가 \n - 검색 결과에서 자주 사용할 단축키를 선택합니다. \n - 해당 단축키 오른쪽에 있는 즐겨찾기 노란 아이콘을 클릭하여 즐겨찾기에 추가합니다. \n - 즐겨찾기에 추가된 단축키는 메인 화면 상단의 즐겨찾기 메뉴에서 확인할 수 있습니다.\n\n\n \n  라이센스 정책\n똑키는 자유롭게 설치하여 사용이 가능한 프리웨어입니다.\n \n \n 효과적인 활용 방법  \n1. 자주 사용하는 단축키 익히기 \n 문서나 이미지, 영상 편집 작업 시 자주 사용하는 단축키를 익히면 작업 속도를 크게 향상시킬 수 있습니다. 예를 들어, 엑셀의 복사(Ctrl + C), 붙여넣기(Ctrl + V), 셀 병합(Alt + H + M + M) 등의 단축키를 익혀두면 반복적인 작업을 빠르게 처리할 수 있습니다. \n2. 게임에서의 단축키 활용 \n게임에서도 단축키는 중요한 역할을 합니다. 각 게임별로 중요한 단축키를 미리 숙지하여 사용하면 게임 플레이가 한층 더 원활해지고, 반응 속도도 빨라질 수 있습니다. 예를 들어, FPS 게임에서 무기 변경(Q), 리로드(R) 등의 단축키를 익혀두면 게임에서의 전투력을 높일 수 있습니다. \n3. 실무 엑셀 함수 모음 활용 \n엑셀을 자주 사용하는 업무 환경에서 실무 엑셀 함수 모음을 활용하면 업무 효율을 극대화할 수 있습니다. 기본적인 함수부터 고급 함수까지 다양하게 제공되므로, 필요한 함수를 바로 찾아 사용할 수 있습니다.\n \n⬇️ 프로그램 다운로드\n \n\n \n트렌드소프트웨어 똑똑단축키 프로그램\n트렌드소프트웨어 똑똑단축키, 엑셀, 윈도우, 게임, 프로그램 단축키 정보 프로그램 입니다.\ntrendsoftware.co.kr",
        "guid": "http://muzbox.tistory.com/1999",
        "categories": [
          "추천 프리웨어/문서,업무",
          "단축키 검색 프로그램",
          "디자인 단축키",
          "똑키 단축키",
          "문서 작업 단축키",
          "엑셀 함수 모음",
          "작업 효율 향상"
        ],
        "isoDate": "2024-06-20T07:20:13.000Z"
      }
    ]
  },
  {
    "name": "동우리의 블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "하테나",
    "category": "개인",
    "posts": []
  },
  {
    "name": "늑돌이네 라지온",
    "category": "개인",
    "posts": [
      {
        "creator": "늑돌이",
        "title": "LG디스플레이, 밝고 오래가는 13인치 탠덤 OLED 업계 최초 양산에 성공",
        "link": "http://lazion.com/2513708",
        "pubDate": "Mon, 24 Jun 2024 10:41:03 +0900",
        "author": "늑돌이",
        "comments": "http://lazion.com/2513708#entry2513708comment",
        "content": "<h3 data-ke-size=\"size23\"><b>LG디스플레이</b>가 <b>업계 최초</b>로 수명과 밝기를 높이고 소비전력은 줄인 <b>노트북용 13인치 탠덤(Tandem) OLED 양산</b>에 성공했습니다.</h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><span style=\"color: #006dd7;\"><b>탠덤(Tandem) OLED는 무엇?</b></span></h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"56632.jpg\" data-origin-width=\"826\" data-origin-height=\"620\"><span data-url=\"https://blog.kakaocdn.net/dn/cJXrNs/btsIaG5dm1o/tPVHHyBmbNNjYc0v5dBcsk/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/cJXrNs/btsIaG5dm1o/tPVHHyBmbNNjYc0v5dBcsk/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcJXrNs%2FbtsIaG5dm1o%2FtPVHHyBmbNNjYc0v5dBcsk%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"56632.jpg\" data-origin-width=\"826\" data-origin-height=\"620\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">지난 2019년 LG디스플레이가 처음으로 상용화에 성공한 탠덤(Tandem) OLED는 <b>레드&middot;그린&middot;블루(RGB) 유기발광층을 2개 층으로 쌓는 방식</b>으로 <b>장수명, 고휘도</b>를 구현, 기존 1개 층인 OLED 패널 대비 내구성과 성능이 뛰어납니다.&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">특히 OLED 소자에 가해지는 에너지를 분산시켜 보다 오랫동안 안정적으로 작동할 수 있어 품질 기준이 까다로운 차량용 OLED에 처음 적용되었으며, <b>노트북</b>, 모니터, <b>태블릿</b> 등 화면 사용 시간이 상대적으로 긴 IT 제품에도 잘 어울립니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">참고로 노트북 패널에 탠덤 OLED가 적용된 것은 이번이 처음으로, LG디스플레이는 노트북 사용 환경에 맞춘 탠덤 OLED를 새롭게 개발했습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><span style=\"color: #006dd7;\"><b>LG디스플레이의 노트북용 탠덤 OLED 패널의 특징</b></span></h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">LG디스플레이가 업계 최초로 양산을 시작한<b> 13인치 노트북용 탠덤 OLED 패널</b>의 주요 특징은 다음과 같습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>기존 OLED 패널 대비 <b>수명은 2배, 밝기는 3배</b>까지 향상시킬 수 있고, <b>소비전력은 최대 40% 저감</b> 가능해 일반 노트북뿐 아니라 AI 노트북 등 고성능 IT 기기에도 최적화되어 있습니다.<br /><br /></li>\n<li>부품 설계 및 구조 개선 등을 통해 기존 노트북용 OLED 대비 <b>약 40% 얇아지고, 28% 가벼워져</b> 날렵한 디자인을 구현하고 휴대성을 높일 수 있습니다.<br /><br /></li>\n<li><b>WQXGA+(2880x1800)</b> 고해상도, 디지털영화협회(DCI) 표준 색 영역 <b>DCI-P3를 100% 충족</b>하는 정확한 색 표현력으로 높은 화질을 자랑합니다.<br /><br /></li>\n<li>화소 스스로 빛을 내는 OLED 특유의 무한대의 명암비를 바탕으로 비디오전자공학표준협회(VESA)의 디스플레이 <b>HDR(High Dynamic Range) 트루 블랙 500</b> 기준을 충족합니다.<br /><br /></li>\n<li>터치 센서를 패널 안에 내장, 성능을 높인 고감도 토털 터치 솔루션을 탑재해 정확한 터치감을 구현합니다.</li>\n</ul>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">LG디스플레이가 이미 양산에 들어간 만큼 조만간 13.3인치 탠덤 OLED 패널을 이용한 노트북이나 태블릿 제품을 만나볼 수 있을 것으로 보입니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: right;\" data-ke-size=\"size16\">(출처 : LG디스플레이)</p>",
        "contentSnippet": "LG디스플레이가 업계 최초로 수명과 밝기를 높이고 소비전력은 줄인 노트북용 13인치 탠덤(Tandem) OLED 양산에 성공했습니다.\n \n탠덤(Tandem) OLED는 무엇?\n \n\n\n \n지난 2019년 LG디스플레이가 처음으로 상용화에 성공한 탠덤(Tandem) OLED는 레드·그린·블루(RGB) 유기발광층을 2개 층으로 쌓는 방식으로 장수명, 고휘도를 구현, 기존 1개 층인 OLED 패널 대비 내구성과 성능이 뛰어납니다. \n \n특히 OLED 소자에 가해지는 에너지를 분산시켜 보다 오랫동안 안정적으로 작동할 수 있어 품질 기준이 까다로운 차량용 OLED에 처음 적용되었으며, 노트북, 모니터, 태블릿 등 화면 사용 시간이 상대적으로 긴 IT 제품에도 잘 어울립니다.\n \n참고로 노트북 패널에 탠덤 OLED가 적용된 것은 이번이 처음으로, LG디스플레이는 노트북 사용 환경에 맞춘 탠덤 OLED를 새롭게 개발했습니다.\n \n \nLG디스플레이의 노트북용 탠덤 OLED 패널의 특징\n \nLG디스플레이가 업계 최초로 양산을 시작한 13인치 노트북용 탠덤 OLED 패널의 주요 특징은 다음과 같습니다.\n \n기존 OLED 패널 대비 수명은 2배, 밝기는 3배까지 향상시킬 수 있고, 소비전력은 최대 40% 저감 가능해 일반 노트북뿐 아니라 AI 노트북 등 고성능 IT 기기에도 최적화되어 있습니다.\n\n부품 설계 및 구조 개선 등을 통해 기존 노트북용 OLED 대비 약 40% 얇아지고, 28% 가벼워져 날렵한 디자인을 구현하고 휴대성을 높일 수 있습니다.\n\nWQXGA+(2880x1800) 고해상도, 디지털영화협회(DCI) 표준 색 영역 DCI-P3를 100% 충족하는 정확한 색 표현력으로 높은 화질을 자랑합니다.\n\n화소 스스로 빛을 내는 OLED 특유의 무한대의 명암비를 바탕으로 비디오전자공학표준협회(VESA)의 디스플레이 HDR(High Dynamic Range) 트루 블랙 500 기준을 충족합니다.\n\n터치 센서를 패널 안에 내장, 성능을 높인 고감도 토털 터치 솔루션을 탑재해 정확한 터치감을 구현합니다.\n \nLG디스플레이가 이미 양산에 들어간 만큼 조만간 13.3인치 탠덤 OLED 패널을 이용한 노트북이나 태블릿 제품을 만나볼 수 있을 것으로 보입니다.\n \n(출처 : LG디스플레이)",
        "guid": "http://lazion.com/2513708",
        "categories": [
          "#TV#디스플레이#프로젝터",
          "Display",
          "ipad",
          "Laptop",
          "LG Display",
          "News",
          "OLED",
          "tandem oled"
        ],
        "isoDate": "2024-06-24T01:41:03.000Z"
      },
      {
        "creator": "늑돌이",
        "title": "삼성, 에너지 절약 강화한 스마트싱스 에너지(SmartThings Energy) 서비스 개편",
        "link": "http://lazion.com/2513706",
        "pubDate": "Thu, 20 Jun 2024 11:33:34 +0900",
        "author": "늑돌이",
        "comments": "http://lazion.com/2513706#entry2513706comment",
        "content": "<h3 data-ke-size=\"size23\">삼성전자가 에너지 절약 기능 중심으로 <b>스마트싱스 에너지(SmartThings Energy)</b> 서비스를 오늘(20일) 개편했습니다.</h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"2009358633_20240619182812_6407040536_web.jpg\" data-origin-width=\"2530\" data-origin-height=\"1628\"><span data-url=\"https://blog.kakaocdn.net/dn/0qA3T/btsH5036GkT/Lij2A8qdX4QVJVWBTJMIHk/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/0qA3T/btsH5036GkT/Lij2A8qdX4QVJVWBTJMIHk/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F0qA3T%2FbtsH5036GkT%2FLij2A8qdX4QVJVWBTJMIHk%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"2009358633_20240619182812_6407040536_web.jpg\" data-origin-width=\"2530\" data-origin-height=\"1628\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">삼성 스마트싱스 에너지는 에너지의 사용량을 실시간으로 알려주고 AI 절감 솔루션을 제공하는 홈 에너지 관리 서비스로 세계 97개국 601만 명이 사용하고 있습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><span style=\"color: #006dd7;\"><b>삼성 스마트싱스 에너지 서비스, 달라진 점은?</b></span></h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>AI 절약 모드</b><br />AI 절약 모드로 기기를 미리 설정해 두면, 옵션에 따라 누진 단계에 이르기 전 또는 탄소 배출이 높은 시간 등에 AI 절약 모드를 실행하여 에너지를 절약하고, AI를 통해 사용 환경이나 전기요금 체계, 사용자 편의를 고려한 월말 사용량 예측 등을 제공합니다.<br />다만 누진 단계는 국내에서만 쓸 수 있으며, 스마트싱스와 연결할 스마트 미터기는 별도 구매가 필요합니다. <br /><br /></li>\n<li><b>게임 요소 접목</b><br />에너지 모니터링 및 절약량을 환산한 점수인 에너지 등급, 스마트싱스 에너지 서비스 활동에 참여하여 모으는 활동 배지, 전력 사용량 절감 알림(DR 발령)시 절약한 전기 사용량만큼 혜택을 받는 에너지 절약 미션* 등을 도입했습니다.<br />특히 AI 절약 모드를 통해 에너지를 일일 400Wh 이상 절약한 경우, 에너지 스탬프 최대 1개를 제공하며, 7월 1일부터 에너지 스탬프 1개는 <b>삼성전자 멤버십 포인트 100점</b>으로 전환 가능합니다.<br />삼성전자 멤버십 포인트는 삼성닷컴, 전국 삼성스토어와 이마트&middot;홈플러스 등 오프라인에서도 사용할 수 있습니다.<br /><br /></li>\n</ul>\n<blockquote data-ke-style=\"style3\">*에너지 절약 미션(DR) : 전력 사용량이 많은 시간에 전력거래소나 지자체가 전력 사용량 절감을 권고하고(DR 발령) 이에 맞춰 개별 세대가 전기 사용량을 줄일 경우, 인센티브를 지급하는 제도임</blockquote>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: right;\" data-ke-size=\"size16\">(출처 : <a href=\"https://www.samsung.com/sec/\" target=\"_blank\" rel=\"noopener\">삼성전자</a>)</p>\n<p style=\"text-align: right;\" data-ke-size=\"size16\">&nbsp;</p>\n<h4 style=\"text-align: left;\" data-ke-size=\"size20\">관련 글</h4>\n<figure id=\"og_1718850677813\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"article\" data-og-title=\"삼성 스마트싱스 제품, 지속적인 업데이트 강조하는 스마트 포워드 본격화\" data-og-description=\"삼성전자가 스마트싱스(SmartThings) 기반의 스마트 포워드(Smart Forward) 서비스를 본격화합니다.&nbsp;스마트 포워드는 정기적이고 지속적인 소프트웨어 업데이트를 제공해, 신제품이 아니더라도 최신 \" data-og-host=\"lazion.com\" data-og-source-url=\"https://lazion.com/2513650\" data-og-url=\"https://lazion.com/2513650\" data-og-image=\"https://scrap.kakaocdn.net/dn/pPtOW/hyWoJUzSdq/Kt5mjkUr5xVejdAazXR5Hk/img.jpg?width=800&amp;height=532&amp;face=326_141_403_226,https://scrap.kakaocdn.net/dn/NYWYX/hyWoI9dVQa/R7qwxh07eqKh5YKYHOHKYk/img.jpg?width=800&amp;height=532&amp;face=326_141_403_226,https://scrap.kakaocdn.net/dn/betoMh/hyWoI2szfp/AX0yDuc04G8R9rKLLhwiv1/img.jpg?width=1000&amp;height=666&amp;face=414_175_504_275\"><a href=\"https://lazion.com/2513650\" rel=\"noopener\" data-source-url=\"https://lazion.com/2513650\">\n<div class=\"og-image\" style=\"background-image: url('https://scrap.kakaocdn.net/dn/pPtOW/hyWoJUzSdq/Kt5mjkUr5xVejdAazXR5Hk/img.jpg?width=800&amp;height=532&amp;face=326_141_403_226,https://scrap.kakaocdn.net/dn/NYWYX/hyWoI9dVQa/R7qwxh07eqKh5YKYHOHKYk/img.jpg?width=800&amp;height=532&amp;face=326_141_403_226,https://scrap.kakaocdn.net/dn/betoMh/hyWoI2szfp/AX0yDuc04G8R9rKLLhwiv1/img.jpg?width=1000&amp;height=666&amp;face=414_175_504_275');\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">삼성 스마트싱스 제품, 지속적인 업데이트 강조하는 스마트 포워드 본격화</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">삼성전자가 스마트싱스(SmartThings) 기반의 스마트 포워드(Smart Forward) 서비스를 본격화합니다.&nbsp;스마트 포워드는 정기적이고 지속적인 소프트웨어 업데이트를 제공해, 신제품이 아니더라도 최신</p>\n<p class=\"og-host\" data-ke-size=\"size16\">lazion.com</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "삼성전자가 에너지 절약 기능 중심으로 스마트싱스 에너지(SmartThings Energy) 서비스를 오늘(20일) 개편했습니다.\n \n\n\n \n삼성 스마트싱스 에너지는 에너지의 사용량을 실시간으로 알려주고 AI 절감 솔루션을 제공하는 홈 에너지 관리 서비스로 세계 97개국 601만 명이 사용하고 있습니다.\n \n삼성 스마트싱스 에너지 서비스, 달라진 점은?\n \nAI 절약 모드\nAI 절약 모드로 기기를 미리 설정해 두면, 옵션에 따라 누진 단계에 이르기 전 또는 탄소 배출이 높은 시간 등에 AI 절약 모드를 실행하여 에너지를 절약하고, AI를 통해 사용 환경이나 전기요금 체계, 사용자 편의를 고려한 월말 사용량 예측 등을 제공합니다.\n다만 누진 단계는 국내에서만 쓸 수 있으며, 스마트싱스와 연결할 스마트 미터기는 별도 구매가 필요합니다. \n\n게임 요소 접목\n에너지 모니터링 및 절약량을 환산한 점수인 에너지 등급, 스마트싱스 에너지 서비스 활동에 참여하여 모으는 활동 배지, 전력 사용량 절감 알림(DR 발령)시 절약한 전기 사용량만큼 혜택을 받는 에너지 절약 미션* 등을 도입했습니다.\n특히 AI 절약 모드를 통해 에너지를 일일 400Wh 이상 절약한 경우, 에너지 스탬프 최대 1개를 제공하며, 7월 1일부터 에너지 스탬프 1개는 삼성전자 멤버십 포인트 100점으로 전환 가능합니다.\n삼성전자 멤버십 포인트는 삼성닷컴, 전국 삼성스토어와 이마트·홈플러스 등 오프라인에서도 사용할 수 있습니다.\n\n*에너지 절약 미션(DR) : 전력 사용량이 많은 시간에 전력거래소나 지자체가 전력 사용량 절감을 권고하고(DR 발령) 이에 맞춰 개별 세대가 전기 사용량을 줄일 경우, 인센티브를 지급하는 제도임\n \n(출처 : 삼성전자)\n \n관련 글\n\n \n삼성 스마트싱스 제품, 지속적인 업데이트 강조하는 스마트 포워드 본격화\n삼성전자가 스마트싱스(SmartThings) 기반의 스마트 포워드(Smart Forward) 서비스를 본격화합니다. 스마트 포워드는 정기적이고 지속적인 소프트웨어 업데이트를 제공해, 신제품이 아니더라도 최신\nlazion.com",
        "guid": "http://lazion.com/2513706",
        "categories": [
          "#가전#음식#문화",
          "AI",
          "Ha",
          "IOT",
          "News",
          "Samsung",
          "SEC",
          "Smartthings"
        ],
        "isoDate": "2024-06-20T02:33:34.000Z"
      },
      {
        "creator": "늑돌이",
        "title": "삼성, 중저가 5G폰 갤럭시 A35 5G 한국 출시! RAM은 6GB",
        "link": "http://lazion.com/2513705",
        "pubDate": "Thu, 20 Jun 2024 10:08:12 +0900",
        "author": "늑돌이",
        "comments": "http://lazion.com/2513705#entry2513705comment",
        "content": "<p data-ke-size=\"size16\">삼성전자가 중저가 5G폰 갤럭시 A35 5G를 오는 21일 국내에 출시합니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><b><span style=\"color: #006dd7;\">갤럭시 A35 5G 가격은?</span></b></h3>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"003 - 삼성전자-모바일-갤럭시-갤럭시-A35-5G-출시-2-e1718779850889.jpg\" data-origin-width=\"1000\" data-origin-height=\"667\"><span data-url=\"https://blog.kakaocdn.net/dn/LBBgg/btsH4ATiAy1/cPXtYq6fSaxxB2KoR9rB00/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/LBBgg/btsH4ATiAy1/cPXtYq6fSaxxB2KoR9rB00/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FLBBgg%2FbtsH4ATiAy1%2FcPXtYq6fSaxxB2KoR9rB00%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"003 - 삼성전자-모바일-갤럭시-갤럭시-A35-5G-출시-2-e1718779850889.jpg\" data-origin-width=\"1000\" data-origin-height=\"667\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">국내이동통신 3사와 자급제 모델로 출시되는 <b>갤럭시 A35 5G의 가격은 49만 9,400원</b>입니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><span style=\"color: #006dd7;\"><b>갤럭시 A35 5G 주요 특징</b></span></h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"002 - 삼성전자-모바일-갤럭시-갤럭시-A35-5G-출시-1-e1718779744456.jpg\" data-origin-width=\"1000\" data-origin-height=\"667\"><span data-url=\"https://blog.kakaocdn.net/dn/dgu6Oj/btsH5khZJfF/FJPlVxMxmfHBbeeKQktyaK/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/dgu6Oj/btsH5khZJfF/FJPlVxMxmfHBbeeKQktyaK/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdgu6Oj%2FbtsH5khZJfF%2FFJPlVxMxmfHBbeeKQktyaK%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"002 - 삼성전자-모바일-갤럭시-갤럭시-A35-5G-출시-1-e1718779744456.jpg\" data-origin-width=\"1000\" data-origin-height=\"667\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">라이트 블루, 라이트 바이올렛, 블루 블랙의 3가지 색상으로 출시되는 갤럭시 A35 5G의 주요 특징은 다음과 같습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li>디스플레이<br /><b>168.3mm(6.6형) 대화면 슈퍼 아몰레드 화면은 120Hz 고주사율</b>을 지원하며, 비전부스터로 최대 1,000니트의 밝기를 지원하고 색상 대비를 극대화해 다양한 조도 환경에서도 선명한 화질을 구현합니다. 편안하게 보기(Eye Comfort Shield)은 블루라이트를 줄여줘 눈의 피로를 최소화해줍니다.<br /><br /></li>\n<li><b>RAM은 6GB</b><br /><b>램크루지</b>라는 명성을 여전히 이어갑니다. 해외에는 RAM 8GB 모델도 있지만 우리나라에는 6GB 모델 밖에 안 나옵니다.</li>\n<li>카메라<br />후면에는 5,000만 화소의 광각 카메라를 비롯해, 800만 화소의 초광각 카메라, 500만 화소의 접사 카메라가, 전면에는 1,300만 화소 카메라가 탑재되었습니다. <br />후면 카메라에는 OIS(광학식 손떨림 보정)와 향상된 VDIS(동영상 손떨림 보정) 기능이 탑재돼 흔들림이나 움직임이 많은 상황에서도 또렷하고 매끄러운 촬영이 가능하며, 나이토그래피 기능으로 야간이나 어두운 환경에서 선명한 촬영이 가능합니다.<br /><br /></li>\n<li>배터리/저장소/방수방진<br /><b>5,000mAh의 대용량 배터리</b>를 채용하고, 최대 25W의 충전을 지원(25W 충전기 별매)합니다. <br />스토리지는 128G가 탑재됐으며, 최대 1TB의 마이크로SD 카드를 추가할 수 있습니다.<br />IP67 등급의 방수∙방진이 가능합니다.<br /><br /></li>\n<li><b>삼성월렛</b>과 삼성 녹스<br />삼성월렛을 통해 결제부터 모바일 신분증, 탑승권, 전자증명서, 쿠폰, 멤버십 등을 사용할 수 있으며 데이터를 안전하게 보호하는 삼성 녹스(Knox)도 탑재되었습니다.</li>\n</ul>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><span style=\"color: #006dd7;\"><b>갤럭시 A35 5G 출시 기념 이벤트</b></span></h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"004 - 삼성전자-모바일-갤럭시-갤럭시-A35-5G-출시-3-e1718779867991.jpg\" data-origin-width=\"1000\" data-origin-height=\"667\"><span data-url=\"https://blog.kakaocdn.net/dn/Meomq/btsH6Aw4glc/qjN6rwKB2yYSauGxoQwku0/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/Meomq/btsH6Aw4glc/qjN6rwKB2yYSauGxoQwku0/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FMeomq%2FbtsH6Aw4glc%2FqjN6rwKB2yYSauGxoQwku0%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"004 - 삼성전자-모바일-갤럭시-갤럭시-A35-5G-출시-3-e1718779867991.jpg\" data-origin-width=\"1000\" data-origin-height=\"667\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">삼성전자는 갤럭시 A35 5G의 국내 출시를 기념해 9월 30일까지 구매 및 개통한 고객을 대상으로 윌라 3개월 무료 체험권과 추가 3개월 30% 할인권, 유튜브 프리미엄 2개월 무료 체험권과 마이크로소프트 365 베이직 6개월 체험권을 제공합니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><b><span style=\"color: #006dd7;\">RAM만 8GB였다면...</span></b></h3>\n<p data-ke-size=\"size16\">RAM이 8GB였다면 추천할만한 모델인데, 6GB라니 좀 애매하네요.<br /><br /></p>\n<p data-ke-size=\"size16\">아시다시피 갈수록 스마트폰이 처리하는 일의 종류나 양이 늘어나는 만큼 대부분의 경우 RAM은 많을수록 좋습니다. 전세계 RAM 반도체 생산 1위라는 삼성전자의 이런 짠내나는 스펙 제한을 보면 많이 아쉽네요. 해외판에는 8GB 모델 선택이 가능한데 정작 우리나라에서는 선택 자체가 불가능합니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<table style=\"border-collapse: collapse; width: 100%;\" border=\"1\" width=\"623\" data-ke-align=\"alignLeft\" data-ke-style=\"style12\">\n<tbody>\n<tr>\n<td style=\"width: 15%;\" colspan=\"3\"><span><span style=\"color: #ffffff;\"><b>갤럭시 A35 5G 스펙(제원/사양)<br /></b></span></span></td>\n</tr>\n<tr>\n<td style=\"width: 3.48837%;\"><b>디스플레이</b></td>\n<td style=\"width: 11.5058%;\" colspan=\"2\">6.6인치 FHD+<span><span><span><span>슈퍼 AMOLED 디스플레이 </span></span></span></span><br /><span><span><span><span>최대 120Hz 재생률 </span></span></span></span><br />비전 부스터</td>\n</tr>\n<tr>\n<td style=\"width: 3.48837%;\"><b>크기 및 무게 </b></td>\n<td style=\"width: 11.5058%;\" colspan=\"2\">161.7 x 78.0 x 8.2mm, 209g</td>\n</tr>\n<tr>\n<td style=\"width: 3.48837%;\"><b>카메라 </b></td>\n<td style=\"width: 11.5058%;\" colspan=\"2\">후면 : 8MP 초광각 카메라<span> F2.2, </span>50MP 메인 카메라<span>&middot; F1.8, AF/OIS, </span>5MP 매크로 카메라<span> F2.4 <br />전면 : 13MP 전면 카메라</span><span> F2.2</span></td>\n</tr>\n<tr>\n<td style=\"width: 3.48837%;\"><b>RAM<br /></b></td>\n<td style=\"width: 11.5058%;\" colspan=\"2\"><span><span><span><span>6GB<br /></span></span></span></span></td>\n</tr>\n<tr>\n<td style=\"width: 3.48837%;\"><b>스토리지 </b></td>\n<td colspan=\"2\"><span><span><span><span>128GB / 마이크로SD 카드<span><span><span><span>(별매)</span></span></span></span> 최대 1TB 추가 가능</span></span></span></span></td>\n</tr>\n<tr>\n<td style=\"width: 3.48837%;\"><b>배터리 </b></td>\n<td style=\"width: 11.5058%;\" colspan=\"2\">5,000mAh</td>\n</tr>\n<tr>\n<td style=\"width: 3.48837%;\"><b>OS</b></td>\n<td style=\"width: 11.5058%;\" colspan=\"2\">안드로이드 14 <br />원 UI 6.1</td>\n</tr>\n<tr>\n<td style=\"width: 3.48837%;\"><b>보안 </b></td>\n<td style=\"width: 11.5058%;\" colspan=\"2\">삼성 녹스</td>\n</tr>\n</tbody>\n</table>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: right;\" data-ke-size=\"size16\">(출처 : <a href=\"https://www.samsung.com/sec/\" target=\"_blank\" rel=\"noopener\">삼성전자</a>)</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h4 data-ke-size=\"size20\">관련 글</h4>\n<figure id=\"og_1718845888180\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"article\" data-og-title=\"삼성 중급형 갤럭시 A55와 갤럭시 A35 발표, S24보다 더 중요?\" data-og-description=\"삼성전자 갤럭시 시리즈의 플래그십 모델은 갤럭시 S 시리즈입니다만, 실제로 많이 팔리는 모델은 더 저렴한 제품이죠. 그런 의미에서 갤럭시 A 시리즈는 삼성전자 스마트폰 판매에서 없어서는 \" data-og-host=\"lazion.com\" data-og-source-url=\"https://lazion.com/2513603\" data-og-url=\"https://lazion.com/2513603\" data-og-image=\"https://scrap.kakaocdn.net/dn/dQew0F/hyWoEFKGx3/mc24V4NDcrqdmsKhHty2u0/img.jpg?width=800&amp;height=533&amp;face=0_0_800_533,https://scrap.kakaocdn.net/dn/WMBFG/hyWoNvVzxN/aqAkF8eKKKy3Nmu86KV8O1/img.jpg?width=800&amp;height=533&amp;face=0_0_800_533,https://scrap.kakaocdn.net/dn/CsfAp/hyWoMqe4kL/Pq7vLPO0meycuWXjSyshTK/img.jpg?width=4500&amp;height=3000&amp;face=0_0_4500_3000\"><a href=\"https://lazion.com/2513603\" rel=\"noopener\" data-source-url=\"https://lazion.com/2513603\">\n<div class=\"og-image\" style=\"background-image: url('https://scrap.kakaocdn.net/dn/dQew0F/hyWoEFKGx3/mc24V4NDcrqdmsKhHty2u0/img.jpg?width=800&amp;height=533&amp;face=0_0_800_533,https://scrap.kakaocdn.net/dn/WMBFG/hyWoNvVzxN/aqAkF8eKKKy3Nmu86KV8O1/img.jpg?width=800&amp;height=533&amp;face=0_0_800_533,https://scrap.kakaocdn.net/dn/CsfAp/hyWoMqe4kL/Pq7vLPO0meycuWXjSyshTK/img.jpg?width=4500&amp;height=3000&amp;face=0_0_4500_3000');\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">삼성 중급형 갤럭시 A55와 갤럭시 A35 발표, S24보다 더 중요?</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">삼성전자 갤럭시 시리즈의 플래그십 모델은 갤럭시 S 시리즈입니다만, 실제로 많이 팔리는 모델은 더 저렴한 제품이죠. 그런 의미에서 갤럭시 A 시리즈는 삼성전자 스마트폰 판매에서 없어서는</p>\n<p class=\"og-host\" data-ke-size=\"size16\">lazion.com</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "삼성전자가 중저가 5G폰 갤럭시 A35 5G를 오는 21일 국내에 출시합니다.\n \n갤럭시 A35 5G 가격은?\n\n\n \n국내이동통신 3사와 자급제 모델로 출시되는 갤럭시 A35 5G의 가격은 49만 9,400원입니다.\n \n \n갤럭시 A35 5G 주요 특징\n \n\n\n \n라이트 블루, 라이트 바이올렛, 블루 블랙의 3가지 색상으로 출시되는 갤럭시 A35 5G의 주요 특징은 다음과 같습니다.\n \n디스플레이\n168.3mm(6.6형) 대화면 슈퍼 아몰레드 화면은 120Hz 고주사율을 지원하며, 비전부스터로 최대 1,000니트의 밝기를 지원하고 색상 대비를 극대화해 다양한 조도 환경에서도 선명한 화질을 구현합니다. 편안하게 보기(Eye Comfort Shield)은 블루라이트를 줄여줘 눈의 피로를 최소화해줍니다.\n\nRAM은 6GB\n램크루지라는 명성을 여전히 이어갑니다. 해외에는 RAM 8GB 모델도 있지만 우리나라에는 6GB 모델 밖에 안 나옵니다.\n카메라\n후면에는 5,000만 화소의 광각 카메라를 비롯해, 800만 화소의 초광각 카메라, 500만 화소의 접사 카메라가, 전면에는 1,300만 화소 카메라가 탑재되었습니다. \n후면 카메라에는 OIS(광학식 손떨림 보정)와 향상된 VDIS(동영상 손떨림 보정) 기능이 탑재돼 흔들림이나 움직임이 많은 상황에서도 또렷하고 매끄러운 촬영이 가능하며, 나이토그래피 기능으로 야간이나 어두운 환경에서 선명한 촬영이 가능합니다.\n\n배터리/저장소/방수방진\n5,000mAh의 대용량 배터리를 채용하고, 최대 25W의 충전을 지원(25W 충전기 별매)합니다. \n스토리지는 128G가 탑재됐으며, 최대 1TB의 마이크로SD 카드를 추가할 수 있습니다.\nIP67 등급의 방수∙방진이 가능합니다.\n\n삼성월렛과 삼성 녹스\n삼성월렛을 통해 결제부터 모바일 신분증, 탑승권, 전자증명서, 쿠폰, 멤버십 등을 사용할 수 있으며 데이터를 안전하게 보호하는 삼성 녹스(Knox)도 탑재되었습니다.\n \n갤럭시 A35 5G 출시 기념 이벤트\n \n\n\n \n삼성전자는 갤럭시 A35 5G의 국내 출시를 기념해 9월 30일까지 구매 및 개통한 고객을 대상으로 윌라 3개월 무료 체험권과 추가 3개월 30% 할인권, 유튜브 프리미엄 2개월 무료 체험권과 마이크로소프트 365 베이직 6개월 체험권을 제공합니다.\n \nRAM만 8GB였다면...\nRAM이 8GB였다면 추천할만한 모델인데, 6GB라니 좀 애매하네요.\n\n아시다시피 갈수록 스마트폰이 처리하는 일의 종류나 양이 늘어나는 만큼 대부분의 경우 RAM은 많을수록 좋습니다. 전세계 RAM 반도체 생산 1위라는 삼성전자의 이런 짠내나는 스펙 제한을 보면 많이 아쉽네요. 해외판에는 8GB 모델 선택이 가능한데 정작 우리나라에서는 선택 자체가 불가능합니다.\n \n갤럭시 A35 5G 스펙(제원/사양)\n\n\n\n디스플레이\n6.6인치 FHD+슈퍼 AMOLED 디스플레이 \n최대 120Hz 재생률 \n비전 부스터\n\n\n크기 및 무게 \n161.7 x 78.0 x 8.2mm, 209g\n\n\n카메라 \n후면 : 8MP 초광각 카메라 F2.2, 50MP 메인 카메라· F1.8, AF/OIS, 5MP 매크로 카메라 F2.4 \n전면 : 13MP 전면 카메라 F2.2\n\n\nRAM\n\n6GB\n\n\n\n스토리지 \n128GB / 마이크로SD 카드(별매) 최대 1TB 추가 가능\n\n\n배터리 \n5,000mAh\n\n\nOS\n안드로이드 14 \n원 UI 6.1\n\n\n보안 \n삼성 녹스\n\n\n\n \n(출처 : 삼성전자)\n \n \n관련 글\n\n \n삼성 중급형 갤럭시 A55와 갤럭시 A35 발표, S24보다 더 중요?\n삼성전자 갤럭시 시리즈의 플래그십 모델은 갤럭시 S 시리즈입니다만, 실제로 많이 팔리는 모델은 더 저렴한 제품이죠. 그런 의미에서 갤럭시 A 시리즈는 삼성전자 스마트폰 판매에서 없어서는\nlazion.com",
        "guid": "http://lazion.com/2513705",
        "categories": [
          "#더작은모바일/#스마트폰#PDA#PMP",
          "Galaxy",
          "Galaxy A",
          "Galaxy A35 5G",
          "News",
          "Samsung",
          "SEC",
          "smartphone",
          "램크루지"
        ],
        "isoDate": "2024-06-20T01:08:12.000Z"
      },
      {
        "creator": "늑돌이",
        "title": "RISC-V 노트북? 딥컴퓨팅, 8코어&middot;우분투의 DC-ROMA RISC-V Laptop II 발표",
        "link": "http://lazion.com/2513704",
        "pubDate": "Wed, 19 Jun 2024 16:07:15 +0900",
        "author": "늑돌이",
        "comments": "http://lazion.com/2513704#entry2513704comment",
        "content": "<h3 data-ke-size=\"size23\">세계 최초로 RISC-V 아키텍처를 이용한 노트북을 내놓았던 <b>딥컴퓨팅(DeepComputing)</b>에서 그 후속으로 <b>DC-ROMA RISC-V Laptop II</b>를 내놓았습니다.</h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"DC-ROMAII_UBUNTU-DESKTOP0612-2048x1546-vert.jpg\" data-origin-width=\"1600\" data-origin-height=\"1208\"><span data-url=\"https://blog.kakaocdn.net/dn/FEho3/btsH42uua4A/KuhNYLLCSkhKOAOsId6BB0/img.jpg\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/FEho3/btsH42uua4A/KuhNYLLCSkhKOAOsId6BB0/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FFEho3%2FbtsH42uua4A%2FKuhNYLLCSkhKOAOsId6BB0%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"DC-ROMAII_UBUNTU-DESKTOP0612-2048x1546-vert.jpg\" data-origin-width=\"1600\" data-origin-height=\"1208\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><span style=\"color: #006dd7;\"><b>DC-ROMA RISC-V Laptop II의 특징</b></span></h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">오픈소스 아키텍처 기술로 유명한 RISC-V는 낮은 성능으로 가능한 부문 위주로 시장에 진출해 왔습니다만, 이 제품은 그냥 PC로 나왔습니다. 다만 일반인용 노트북 PC라기 보다는 개발용으로 나온 제품입니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>CPU는 SpacemiT K1</b><br /><br /><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"DC-ROMA-RISC-V-Laptop-II_chip.png\" data-origin-width=\"484\" data-origin-height=\"293\"><span data-url=\"https://blog.kakaocdn.net/dn/9VO79/btsH4VPIGdg/p8MeNcAUWSBAU7apIvBD11/img.png\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/9VO79/btsH4VPIGdg/p8MeNcAUWSBAU7apIvBD11/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F9VO79%2FbtsH4VPIGdg%2Fp8MeNcAUWSBAU7apIvBD11%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"DC-ROMA-RISC-V-Laptop-II_chip.png\" data-origin-width=\"484\" data-origin-height=\"293\"/></span></figure>\n<br />DC-ROMA RISC-V Laptop II는 최대 2.0GHz로 실행되는 SpacemiT의 SoC K1을 최초로 탑재하고 나왔습니다. <br />K1은 1.5GHz에서 실행되는 전작의 4코어 SoC에 비해 성능과 에너지 효율성을 두 배로 향상시켰으며, 256비트 길이의 RISC-V 고성능 컴퓨팅 RVA 22 프로파일 RVV 1.0을 지원하고 IME 그룹 설계 원칙에 기반한 맞춤형 매트릭스 작동 명령으로 강력한 AI 퓨전 컴퓨팅 엔진(최대 2TOPS@INT)을 갖춘 세계 최초의 SoC입니다. <br /><br /></li>\n<li><b>개발용 노트북PC</b><br /><br /><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"DC-ROMA-RISC-V-Laptop-II_09.png\" data-origin-width=\"547\" data-origin-height=\"506\"><span data-url=\"https://blog.kakaocdn.net/dn/bK6KpR/btsH42nKwqN/98dITjuQNJPueQQ1boGnM1/img.png\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/bK6KpR/btsH42nKwqN/98dITjuQNJPueQQ1boGnM1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbK6KpR%2FbtsH42nKwqN%2F98dITjuQNJPueQQ1boGnM1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"DC-ROMA-RISC-V-Laptop-II_09.png\" data-origin-width=\"547\" data-origin-height=\"506\"/></span></figure>\n<br />화면은 14.1인치 풀HD 해상도의 IPS 패널이며, 올메탈 케이스로 전작 대비 나아진 내구성과 함께 열 방출 성능도 좋아졌습니다. RAM은 8/16GB 선택 가능하며 저장소는 1TB까지 제공합니다. 무게는 1.36kg에 배터리는 최대 8시간까지 사용가능합니다.<br /><br /><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"DC-ROMA-RISC-V-Laptop-II_255.png\" data-origin-width=\"1153\" data-origin-height=\"797\"><span data-url=\"https://blog.kakaocdn.net/dn/xf5Xw/btsH5Y5KAzM/kd6naOcrO2RllfnS9YGzq1/img.png\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/xf5Xw/btsH5Y5KAzM/kd6naOcrO2RllfnS9YGzq1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fxf5Xw%2FbtsH5Y5KAzM%2Fkd6naOcrO2RllfnS9YGzq1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"DC-ROMA-RISC-V-Laptop-II_255.png\" data-origin-width=\"1153\" data-origin-height=\"797\"/></span></figure>\n<br />일반적인 PC와는 다르게 개발을 주 목적으로 만들어졌기 때문에 개발용 8핀 인터페이스가 존재합니다.<br /><br /></li>\n<li><b>OS는 우분투 리눅스</b><br />리눅스 분야에서 가장 대중적인 Canonical사의 우분투(Ubuntu) 리눅스가 DC-ROMA RISC-V Laptop II로 이식되었습니다.</li>\n</ul>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><span style=\"color: #006dd7;\"><b>DC-ROMA RISC-V Laptop II의 출시 일자와 가격</b></span></h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"DC-ROMA-RISC-V-Laptop-II_21.png\" data-origin-width=\"590\" data-origin-height=\"463\"><span data-url=\"https://blog.kakaocdn.net/dn/Bm7vp/btsH37Q59bO/YxJ0sLa0oT2qkWSUA7LVy1/img.png\" data-phocus=\"phocus\"><img src=\"https://blog.kakaocdn.net/dn/Bm7vp/btsH37Q59bO/YxJ0sLa0oT2qkWSUA7LVy1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FBm7vp%2FbtsH37Q59bO%2FYxJ0sLa0oT2qkWSUA7LVy1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"DC-ROMA-RISC-V-Laptop-II_21.png\" data-origin-width=\"590\" data-origin-height=\"463\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">DC-ROMA RISC-V Laptop II는 6월 24일부터 28일까지 열리는 RISC-V Summit Europe 2024에서 처음 선보일 예정입니다. 예약판매는 6월 18일부터입니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><b> DC-ROMA RISC-V Laptop II의 가격은 스펙에 따라 55만1천원부터</b> 시작합니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">더 자세한 내용은 <a href=\"https://deepcomputing.io/product/dc-roma-risc-v-laptop-ii/\" target=\"_blank\" rel=\"noopener\">이곳을 확인해 보시기 바랍니다.</a></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: right;\" data-ke-size=\"size16\">(자료 출처 : DeepComputing)</p>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "세계 최초로 RISC-V 아키텍처를 이용한 노트북을 내놓았던 딥컴퓨팅(DeepComputing)에서 그 후속으로 DC-ROMA RISC-V Laptop II를 내놓았습니다.\n \n\n\n \nDC-ROMA RISC-V Laptop II의 특징\n \n오픈소스 아키텍처 기술로 유명한 RISC-V는 낮은 성능으로 가능한 부문 위주로 시장에 진출해 왔습니다만, 이 제품은 그냥 PC로 나왔습니다. 다만 일반인용 노트북 PC라기 보다는 개발용으로 나온 제품입니다.\n \nCPU는 SpacemiT K1\n\nDC-ROMA RISC-V Laptop II는 최대 2.0GHz로 실행되는 SpacemiT의 SoC K1을 최초로 탑재하고 나왔습니다. \nK1은 1.5GHz에서 실행되는 전작의 4코어 SoC에 비해 성능과 에너지 효율성을 두 배로 향상시켰으며, 256비트 길이의 RISC-V 고성능 컴퓨팅 RVA 22 프로파일 RVV 1.0을 지원하고 IME 그룹 설계 원칙에 기반한 맞춤형 매트릭스 작동 명령으로 강력한 AI 퓨전 컴퓨팅 엔진(최대 2TOPS@INT)을 갖춘 세계 최초의 SoC입니다. \n\n개발용 노트북PC\n\n화면은 14.1인치 풀HD 해상도의 IPS 패널이며, 올메탈 케이스로 전작 대비 나아진 내구성과 함께 열 방출 성능도 좋아졌습니다. RAM은 8/16GB 선택 가능하며 저장소는 1TB까지 제공합니다. 무게는 1.36kg에 배터리는 최대 8시간까지 사용가능합니다.\n\n일반적인 PC와는 다르게 개발을 주 목적으로 만들어졌기 때문에 개발용 8핀 인터페이스가 존재합니다.\n\nOS는 우분투 리눅스\n리눅스 분야에서 가장 대중적인 Canonical사의 우분투(Ubuntu) 리눅스가 DC-ROMA RISC-V Laptop II로 이식되었습니다.\n \nDC-ROMA RISC-V Laptop II의 출시 일자와 가격\n \n\n\n \nDC-ROMA RISC-V Laptop II는 6월 24일부터 28일까지 열리는 RISC-V Summit Europe 2024에서 처음 선보일 예정입니다. 예약판매는 6월 18일부터입니다.\n \n DC-ROMA RISC-V Laptop II의 가격은 스펙에 따라 55만1천원부터 시작합니다.\n \n더 자세한 내용은 이곳을 확인해 보시기 바랍니다.\n \n(자료 출처 : DeepComputing)",
        "guid": "http://lazion.com/2513704",
        "categories": [
          "#작은PC/#노트북PC",
          "dc-roma risc-v laptop",
          "deepcomputing",
          "Laptop",
          "News",
          "PC",
          "Risc-V"
        ],
        "isoDate": "2024-06-19T07:07:15.000Z"
      }
    ]
  },
  {
    "name": "루리웹 - 루리웹 리뷰 게시판",
    "category": "게임",
    "posts": []
  },
  {
    "name": "Reasontobe",
    "category": "개인",
    "posts": []
  },
  {
    "name": "자유로운 생활",
    "category": "개인",
    "posts": []
  },
  {
    "name": "에스티마의 인터넷이야기 EstimaStory.com",
    "category": "개인",
    "posts": []
  },
  {
    "name": "나긋한 개발 - 데비안 리눅스와 프로그램 언어",
    "category": "개인",
    "posts": []
  },
  {
    "name": "일상을 여행처럼...",
    "category": "개인",
    "posts": []
  },
  {
    "name": "khris'log",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Just hack'em",
    "category": "개인",
    "posts": []
  },
  {
    "name": "C++ Truths",
    "category": "개인",
    "posts": []
  },
  {
    "name": "jacking75",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Joel on Software",
    "category": "개인",
    "posts": []
  },
  {
    "name": "벤자민로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "악보쓰는 프로그래머",
    "category": "개인",
    "posts": []
  },
  {
    "name": "쭌안아빠",
    "category": "개인",
    "posts": []
  },
  {
    "name": "A Gangster World",
    "category": "개인",
    "posts": []
  },
  {
    "name": "요우의 내맘대로 블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "개발자스럽다",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "Against All Odds.",
    "category": "개인",
    "posts": []
  },
  {
    "name": "움직이는 게임서버",
    "category": "개인",
    "posts": []
  },
  {
    "name": "이상욱",
    "category": "개인",
    "posts": []
  },
  {
    "name": "임철재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "어쩐지 오늘은",
    "category": "개인",
    "posts": []
  },
  {
    "name": "oddpoet’s étude",
    "category": "개인",
    "posts": []
  },
  {
    "name": "0x00 - NULL",
    "category": "개인",
    "posts": []
  },
  {
    "name": "퇴근 후 서버다운",
    "category": "개인",
    "posts": []
  },
  {
    "name": "coolspeed",
    "category": "개인",
    "posts": []
  },
  {
    "name": "오늘도 끄적끄적",
    "category": "개인",
    "posts": []
  },
  {
    "name": "dx11 Vanica's Lifelog - 夢が夢で終わらないように",
    "category": "개인",
    "posts": []
  },
  {
    "name": "초코사랑",
    "category": "개인",
    "posts": []
  },
  {
    "name": "ZeroCho Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Hybrid's Notes",
    "category": "개인",
    "posts": []
  },
  {
    "name": "imays게임엔진개발자",
    "category": "개인",
    "posts": []
  },
  {
    "name": "RSS feed for hurinmon Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "기억보단 기록을",
    "category": "개인",
    "posts": [
      {
        "creator": "향로 (기억보단 기록을)",
        "title": "결혼생활도 커리어처럼",
        "link": "http://jojoldu.tistory.com/790",
        "pubDate": "Mon, 24 Jun 2024 08:40:19 +0900",
        "author": "향로 (기억보단 기록을)",
        "comments": "http://jojoldu.tistory.com/790#entry790comment",
        "content": "<p data-ke-size=\"size16\">다른 사람들이 쉽게 하거나 관심을 가지는 일상적인 것들에 대해 크게 관심이 없는 사람이라서 나 스스로가 어딘가 나사 하나가 빠져있다는 생각을 종종 한다.<br />그래서 커리어 외에 다른 것에 대해 다른 사람들의 조언을 들을때면 크게 와닿지 못하고, 갸우뚱 할 때가 자주 있었다.</p>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"6\" data-ke-size=\"size16\">결혼을 준비하면서도 앞으로의 내 결혼 생활을 어떻게 해야하나 여러 책이나 조언들을 들어도 추상적이여서 나 같이 뭔가가 부족한 사람에게는 머릿 속으로 잘 그려지지 않았다.</p>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"8\" data-ke-size=\"size16\">어떤때면 어떻게 해라 라는 조언이 들릴때면 그럼 이럴땐 어떡하지? 저럴땐 어떡하지? 등의 생각이 자연스럽게 들어서 머릿속만 더 복잡해졌다.</p>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"10\" data-ke-size=\"size16\">예전에 빈지노 &amp; 미초바 부부의 연애/결혼 생활 영상을 보면서 \"이 커플은 어떻게 이렇게 오래 만나고도 이쁘게 사랑할 수 있지?\" 라며 되게 닮고싶다는 생각을 했었다.</p>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"12\" data-ke-size=\"size16\">그러다<span>&nbsp;</span><a style=\"color: #0088cc;\" href=\"https://www.youtube.com/shorts/62tub-_q8Yo\">빈지노님의 짧은 인터뷰</a><span>&nbsp;</span>를 봤는데 \"<b>결혼 생활도 커리어처럼</b>\" 이라는 말이 너무 직관적이고 명확했다.</p>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"14\" data-ke-size=\"size16\">나한테는 구체적인 어떤 방법론들 보다도 이게 훨씬 더 선명한 방법으로 다가왔다.</p>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"16\" data-ke-size=\"size16\">감정적 순간에서도 최선의 선택을 하기 위한 노력이나<br />함께 정한 규칙을 누구보다 잘 지키기 위한 노력,<br />내가 하고 싶은 말 보다는 상대가 듣고 싶은 말을 하기 위한 노력,<br />지금 보다는 더 나은 상황을 만들기 위한 고민 등등</p>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"21\" data-ke-size=\"size16\">\"<b>커리어를 위해 내가 해왔던 노력들을 그대로 결혼생활에 이어가면 되는구나</b>\" 싶어서 나 같이 어딘가 나사 하나가 빠진 사람에게도 선명하게 느껴지는 와닿은 조언이였다.</p>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"23\" data-ke-size=\"size16\">이렇게 생각 해도 잘 할 수 있을지 어떨지는 모르겠지만,<br />커리어처럼 결혼생활도 잘 해볼 수 있겠다, 잘 해봐야겠다는 생각이 가득해졌다.</p>",
        "contentSnippet": "다른 사람들이 쉽게 하거나 관심을 가지는 일상적인 것들에 대해 크게 관심이 없는 사람이라서 나 스스로가 어딘가 나사 하나가 빠져있다는 생각을 종종 한다.\n그래서 커리어 외에 다른 것에 대해 다른 사람들의 조언을 들을때면 크게 와닿지 못하고, 갸우뚱 할 때가 자주 있었다.\n결혼을 준비하면서도 앞으로의 내 결혼 생활을 어떻게 해야하나 여러 책이나 조언들을 들어도 추상적이여서 나 같이 뭔가가 부족한 사람에게는 머릿 속으로 잘 그려지지 않았다.\n어떤때면 어떻게 해라 라는 조언이 들릴때면 그럼 이럴땐 어떡하지? 저럴땐 어떡하지? 등의 생각이 자연스럽게 들어서 머릿속만 더 복잡해졌다.\n예전에 빈지노 & 미초바 부부의 연애/결혼 생활 영상을 보면서 \"이 커플은 어떻게 이렇게 오래 만나고도 이쁘게 사랑할 수 있지?\" 라며 되게 닮고싶다는 생각을 했었다.\n그러다 빈지노님의 짧은 인터뷰 를 봤는데 \"결혼 생활도 커리어처럼\" 이라는 말이 너무 직관적이고 명확했다.\n나한테는 구체적인 어떤 방법론들 보다도 이게 훨씬 더 선명한 방법으로 다가왔다.\n감정적 순간에서도 최선의 선택을 하기 위한 노력이나\n함께 정한 규칙을 누구보다 잘 지키기 위한 노력,\n내가 하고 싶은 말 보다는 상대가 듣고 싶은 말을 하기 위한 노력,\n지금 보다는 더 나은 상황을 만들기 위한 고민 등등\n\"커리어를 위해 내가 해왔던 노력들을 그대로 결혼생활에 이어가면 되는구나\" 싶어서 나 같이 어딘가 나사 하나가 빠진 사람에게도 선명하게 느껴지는 와닿은 조언이였다.\n이렇게 생각 해도 잘 할 수 있을지 어떨지는 모르겠지만,\n커리어처럼 결혼생활도 잘 해볼 수 있겠다, 잘 해봐야겠다는 생각이 가득해졌다.",
        "guid": "http://jojoldu.tistory.com/790",
        "categories": [
          "생각정리",
          "결혼",
          "일상"
        ],
        "isoDate": "2024-06-23T23:40:19.000Z"
      },
      {
        "creator": "향로 (기억보단 기록을)",
        "title": "[Mac] Java 21 &amp; Gradle 8 설치하기",
        "link": "http://jojoldu.tistory.com/789",
        "pubDate": "Sun, 23 Jun 2024 12:06:06 +0900",
        "author": "향로 (기억보단 기록을)",
        "comments": "http://jojoldu.tistory.com/789#entry789comment",
        "content": "<p data-ke-size=\"size16\">Java 21이 2023년 9월에 출시 된지 1년이 되어가고 있고,<span>&nbsp;</span><a style=\"color: #0088cc;\" href=\"https://www.infoworld.com/article/3689880/jdk-21-the-new-features-in-java-21.html\">여러 신규 기능</a>이 패치 되었기도 하여서 개인 노트북에 JDK 21 설치를 하기로 했다.</p>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"5\" data-ke-size=\"size16\"><a style=\"color: #0088cc;\" href=\"https://whichjdk.com/ko/\">whichjdk.com</a>를 보면 크게 2가지 버전의 JDK를 추천한다.</p>\n<ul style=\"list-style-type: disc; background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"7\" data-ke-list-type=\"disc\">\n<li data-source-line=\"7\">Amazon Corretto\n<ul style=\"list-style-type: circle;\" data-source-line=\"8\" data-ke-list-type=\"circle\">\n<li data-source-line=\"8\">AWS의 Amazon Linux 2에서 Java 애플리케이션을 직접 실행하는 경우 최적화된 버전</li>\n</ul>\n</li>\n<li data-source-line=\"9\">Adoptium Eclipse Temurin\n<ul style=\"list-style-type: circle;\" data-source-line=\"10\" data-ke-list-type=\"circle\">\n<li data-source-line=\"10\">오픈 소스 소프트웨어에 대한 리소스와 전문 거버넌스 모델을 제공하는 Eclipse 재단 산하의 최상위 프로젝트</li>\n<li data-source-line=\"11\">Red Hat, IBM, Microsoft, Azul, iJUG 등 Java 기술에 전략적 관심을 갖고 있는 주요 기업 및 조직으로 구성</li>\n<li data-source-line=\"12\">이전의 AdoptOpenJDK 프로젝트는 Eclipse Adoptium으로 이전됨</li>\n</ul>\n</li>\n</ul>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"14\" data-ke-size=\"size16\">현재 여러 팀에서도 대부분 이 중 하나로 선택해서 사용중이다.</p>\n<ul style=\"list-style-type: disc; background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"16\" data-ke-list-type=\"disc\">\n<li data-source-line=\"16\"><a style=\"color: #0088cc;\" href=\"https://www.facebook.com/jojoldu/posts/pfbid0woHLzFSoaYwom5HNEfSYFtDsythkiQdkqakKpxJLC92w1dBn5dQyDpZPVJgk9uUMl?notif_id=1718758810997688&amp;notif_t=feedback_reaction_generic&amp;ref=notif\">(페이스북) 다들 개발팀 로컬 PC는 JDK 어느것들을 사용하시나요?</a></li>\n<li data-source-line=\"17\"><a style=\"color: #0088cc;\" href=\"https://twitter.com/jojoldu/status/1803228699225104577\">(트위터) 다들 개발팀 로컬 PC는 JDK 어느것들을 사용하시나요?</a></li>\n</ul>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"19\" data-ke-size=\"size16\">온프레미스 기반의 회사에서는 당연하게도 Temurin을 사용중이고, AWS를 사용중인 팀에서도 요즘 같이 컨테이너 환경에서는 특정 클라우드 벤더사의 JDK만 써야하는 제약이 있지 않다보니 Temurin을 사용하는 경우가 많다.</p>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"21\" data-ke-size=\"size16\">이 글에서도 속한 회사의 인프라 환경에 크게 제약을 받지 않는 오픈소스인 Temurin 21 로 설치를 진행한다.</p>\n<h3 id=\"adoptium-eclipse-temurin-21-설치\" style=\"background-color: #ffffff; color: #000000; text-align: start;\" data-source-line=\"23\" data-ke-size=\"size23\">Adoptium Eclipse Temurin 21 설치</h3>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"25\" data-ke-size=\"size16\">(설치 안되어있다면)<span>&nbsp;</span>cask<span>&nbsp;</span>를 설치하고</p>\n<pre class=\"mipsasm\" style=\"background-color: #f5f5f5; color: #333333; text-align: left;\" data-source-line=\"27\" data-info=\"bash {data-source-line=&quot;27&quot;}\" data-role=\"codeBlock\"><code>brew install cask\n</code></pre>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"31\" data-ke-size=\"size16\"><b>Temurin 21 설치</b>한다.</p>\n<pre class=\"angelscript\" style=\"background-color: #f5f5f5; color: #333333; text-align: left;\" data-source-line=\"33\" data-info=\"bash {data-source-line=&quot;33&quot;}\" data-role=\"codeBlock\"><code>brew install --cask temurin@21\n</code></pre>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"37\" data-ke-size=\"size16\">설치가 완료되면 잘 설치되어있는지 확인해본다.</p>\n<pre class=\"angelscript\" style=\"background-color: #f5f5f5; color: #333333; text-align: left;\" data-source-line=\"39\" data-info=\"bash {data-source-line=&quot;39&quot;}\" data-role=\"codeBlock\"><code>$ java --version\nopenjdk 21.0.3 2024-04-16 LTS\nOpenJDK Runtime Environment Temurin-21.0.3+9 (build 21.0.3+9-LTS)\nOpenJDK 64-Bit Server VM Temurin-21.0.3+9 (build 21.0.3+9-LTS, mixed mode)\n</code></pre>\n<h3 id=\"jenv-설치\" style=\"background-color: #ffffff; color: #000000; text-align: start;\" data-source-line=\"46\" data-ke-size=\"size23\">jenv 설치</h3>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"48\" data-ke-size=\"size16\">정상혁님의<span>&nbsp;</span><a style=\"color: #0088cc;\" href=\"https://blog.benelog.net/installing-jdk\">\"여러 개의 JDK를 설치하고 선택해서 사용하기\"</a>을 보면 여러 JDK 버전 관리 도구들에 대한 소개가 나온다.</p>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"50\" data-ke-size=\"size16\">SDKMAN 의 경우 JDK 설치까지 편하게 사용 가능하지만, 버전 변경에 대해 불편한 점이 많다.<br />반면<span>&nbsp;</span><b>jenv는 JDK 설치는 수동으로 진행해야하지만, 그 이후 버전 관리에 대해서는 다양하고 편리하게</b><span>&nbsp;</span>사용할 수 있어 여기서는<span>&nbsp;</span><a style=\"color: #0088cc;\" href=\"https://www.jenv.be/\">jenv</a><span>&nbsp;</span>로 진행한다.</p>\n<pre class=\"mipsasm\" style=\"background-color: #f5f5f5; color: #333333; text-align: left;\" data-source-line=\"53\" data-info=\"bash {data-source-line=&quot;53&quot;}\" data-role=\"codeBlock\"><code>$ brew install jenv\n</code></pre>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"57\" data-ke-size=\"size16\">설치된 jenv 를 등록하기 위해<span>&nbsp;</span>~/.bashrc<span>&nbsp;</span>또는<span>&nbsp;</span>~/.bash_profile<span>&nbsp;</span>혹은<span>&nbsp;</span>~/.zsh에 아래 내용을 추가한다.</p>\n<pre class=\"routeros\" style=\"background-color: #f5f5f5; color: #333333; text-align: left;\" data-source-line=\"59\" data-info=\"bash {data-source-line=&quot;59&quot;}\" data-role=\"codeBlock\"><code>export PATH=\"$HOME/.jenv/bin:$PATH\"\neval \"$(jenv init -)\"\n</code></pre>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"64\" data-ke-size=\"size16\">아래와 같이 직접 명령어를 수행해서 추가해도 된다.</p>\n<pre class=\"jboss-cli\" style=\"background-color: #f5f5f5; color: #333333; text-align: left;\" data-source-line=\"66\" data-info=\"bash {data-source-line=&quot;66&quot;}\" data-role=\"codeBlock\"><code>$ echo 'export PATH=\"$HOME/.jenv/bin:$PATH\"' &gt;&gt; ~/.zshrc\n$ echo 'eval \"$(jenv init -)\"' &gt;&gt; ~/.zshrc\n</code></pre>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"71\" data-ke-size=\"size16\">아래 명령어들도 차례로 수행한다.</p>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"73\" data-ke-size=\"size16\"><b>Enable the export plugin</b></p>\n<pre class=\"shell\" style=\"background-color: #f5f5f5; color: #333333; text-align: left;\" data-source-line=\"75\" data-info=\"bash {data-source-line=&quot;75&quot;}\" data-role=\"codeBlock\"><code>$ eval \"$(jenv init -)\"\n$ jenv enable-plugin export\n</code></pre>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"80\" data-ke-size=\"size16\"><b>Restart your shell</b></p>\n<pre class=\"shell\" style=\"background-color: #f5f5f5; color: #333333; text-align: left;\" data-source-line=\"82\" data-info=\"bash {data-source-line=&quot;82&quot;}\" data-role=\"codeBlock\"><code>$ exec $SHELL -l\n</code></pre>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"86\" data-ke-size=\"size16\">jenv 설정이 완료되었다면, 위에서 설치한 temurin21 JDK를 jenv에 등록한다.</p>\n<pre class=\"awk\" style=\"background-color: #f5f5f5; color: #333333; text-align: left;\" data-source-line=\"88\" data-info=\"bash {data-source-line=&quot;88&quot;}\" data-role=\"codeBlock\"><code>$ jenv add /Library/Java/JavaVirtualMachines/temurin-21.jdk/Contents/Home\n</code></pre>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"92\" data-ke-size=\"size16\">jenv에 잘 등록되었는지 확인해본다.</p>\n<pre class=\"routeros\" style=\"background-color: #f5f5f5; color: #333333; text-align: left;\" data-source-line=\"94\" data-info=\"bash {data-source-line=&quot;94&quot;}\" data-role=\"codeBlock\"><code>$ jenv versions\n* system (set by /Users/jojoldu/.jenv/version)\n  21\n  21.0\n  21.0.3\n  temurin64-21.0.3\n</code></pre>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"103\" data-ke-size=\"size16\">JAVA_HOME에도 jenv 로 설정된 버전을 인식할 수 있도록 아래 명령어로 글로벌 JDK 버전을 변경한다.</p>\n<pre class=\"angelscript\" style=\"background-color: #f5f5f5; color: #333333; text-align: left;\" data-source-line=\"105\" data-info=\"bash {data-source-line=&quot;105&quot;}\" data-role=\"codeBlock\"><code>$ jenv global 21.0.3\n</code></pre>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"109\" data-ke-size=\"size16\">(이걸 하지 않으면 Gradle 등에서 JAVA_HOME 인식을 하지 못한다.)</p>\n<h2 id=\"gradle-8-설치\" style=\"background-color: #ffffff; color: #000000; text-align: start;\" data-source-line=\"111\" data-ke-size=\"size26\">Gradle 8 설치</h2>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"113\" data-ke-size=\"size16\">sdkman은 JDK 외에도 여러 JVM 진영의 도구들을 설치, 관리하기 편리한 도구이다.<br />그래서 Gradle 은<span>&nbsp;</span><a style=\"color: #0088cc;\" href=\"https://sdkman.io/install\">sdkman</a>을 통해 진행한다.</p>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"116\" data-ke-size=\"size16\">먼저 sdkman 을 설치한다.</p>\n<pre class=\"shell\" style=\"background-color: #f5f5f5; color: #333333; text-align: left;\" data-source-line=\"118\" data-info=\"bash {data-source-line=&quot;118&quot;}\" data-role=\"codeBlock\"><code>$ curl -s \"https://get.sdkman.io\" | bash\n$ source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n</code></pre>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"123\" data-ke-size=\"size16\">설치된 sdkman 을 통해 Gradle 최신 버전을 설치한다.</p>\n<pre class=\"angelscript\" style=\"background-color: #f5f5f5; color: #333333; text-align: left;\" data-source-line=\"125\" data-info=\"bash {data-source-line=&quot;125&quot;}\" data-role=\"codeBlock\"><code>$ sdk install gradle 8.8\n</code></pre>\n<p style=\"background-color: #ffffff; color: #333333; text-align: start;\" data-source-line=\"129\" data-ke-size=\"size16\">잘 설치되었는지 아래 명령어로 gradle 버전을 확인해본다.</p>\n<pre class=\"yaml\" style=\"background-color: #f5f5f5; color: #333333; text-align: left;\" data-source-line=\"131\" data-info=\"bash {data-source-line=&quot;131&quot;}\" data-role=\"codeBlock\"><code>$ gradle -v\n------------------------------------------------------------\nGradle 8.8\n------------------------------------------------------------\n\nBuild time:   2024-05-31 21:46:56 UTC\nRevision:     4bd1b3d3fc3f31db5a26eecb416a165b8cc36082\n\nKotlin:       1.9.22\nGroovy:       3.0.21\nAnt:          Apache Ant(TM) version 1.10.13 compiled on January 4 2023\nJVM:          21.0.3 (Eclipse Adoptium 21.0.3+9-LTS)\nOS:           Mac OS X 14.1.2 aarch64</code></pre>",
        "contentSnippet": "Java 21이 2023년 9월에 출시 된지 1년이 되어가고 있고, 여러 신규 기능이 패치 되었기도 하여서 개인 노트북에 JDK 21 설치를 하기로 했다.\nwhichjdk.com를 보면 크게 2가지 버전의 JDK를 추천한다.\nAmazon Corretto\n\nAWS의 Amazon Linux 2에서 Java 애플리케이션을 직접 실행하는 경우 최적화된 버전\nAdoptium Eclipse Temurin\n\n오픈 소스 소프트웨어에 대한 리소스와 전문 거버넌스 모델을 제공하는 Eclipse 재단 산하의 최상위 프로젝트\nRed Hat, IBM, Microsoft, Azul, iJUG 등 Java 기술에 전략적 관심을 갖고 있는 주요 기업 및 조직으로 구성\n이전의 AdoptOpenJDK 프로젝트는 Eclipse Adoptium으로 이전됨\n현재 여러 팀에서도 대부분 이 중 하나로 선택해서 사용중이다.\n(페이스북) 다들 개발팀 로컬 PC는 JDK 어느것들을 사용하시나요?\n(트위터) 다들 개발팀 로컬 PC는 JDK 어느것들을 사용하시나요?\n온프레미스 기반의 회사에서는 당연하게도 Temurin을 사용중이고, AWS를 사용중인 팀에서도 요즘 같이 컨테이너 환경에서는 특정 클라우드 벤더사의 JDK만 써야하는 제약이 있지 않다보니 Temurin을 사용하는 경우가 많다.\n이 글에서도 속한 회사의 인프라 환경에 크게 제약을 받지 않는 오픈소스인 Temurin 21 로 설치를 진행한다.\nAdoptium Eclipse Temurin 21 설치\n(설치 안되어있다면) cask 를 설치하고\nbrew install cask\n\nTemurin 21 설치한다.\nbrew install --cask temurin@21\n\n설치가 완료되면 잘 설치되어있는지 확인해본다.\n$ java --version\nopenjdk 21.0.3 2024-04-16 LTS\nOpenJDK Runtime Environment Temurin-21.0.3+9 (build 21.0.3+9-LTS)\nOpenJDK 64-Bit Server VM Temurin-21.0.3+9 (build 21.0.3+9-LTS, mixed mode)\n\njenv 설치\n정상혁님의 \"여러 개의 JDK를 설치하고 선택해서 사용하기\"을 보면 여러 JDK 버전 관리 도구들에 대한 소개가 나온다.\nSDKMAN 의 경우 JDK 설치까지 편하게 사용 가능하지만, 버전 변경에 대해 불편한 점이 많다.\n반면 jenv는 JDK 설치는 수동으로 진행해야하지만, 그 이후 버전 관리에 대해서는 다양하고 편리하게 사용할 수 있어 여기서는 jenv 로 진행한다.\n$ brew install jenv\n\n설치된 jenv 를 등록하기 위해 ~/.bashrc 또는 ~/.bash_profile 혹은 ~/.zsh에 아래 내용을 추가한다.\nexport PATH=\"$HOME/.jenv/bin:$PATH\"\neval \"$(jenv init -)\"\n\n아래와 같이 직접 명령어를 수행해서 추가해도 된다.\n$ echo 'export PATH=\"$HOME/.jenv/bin:$PATH\"' >> ~/.zshrc\n$ echo 'eval \"$(jenv init -)\"' >> ~/.zshrc\n\n아래 명령어들도 차례로 수행한다.\nEnable the export plugin\n$ eval \"$(jenv init -)\"\n$ jenv enable-plugin export\n\nRestart your shell\n$ exec $SHELL -l\n\njenv 설정이 완료되었다면, 위에서 설치한 temurin21 JDK를 jenv에 등록한다.\n$ jenv add /Library/Java/JavaVirtualMachines/temurin-21.jdk/Contents/Home\n\njenv에 잘 등록되었는지 확인해본다.\n$ jenv versions\n* system (set by /Users/jojoldu/.jenv/version)\n  21\n  21.0\n  21.0.3\n  temurin64-21.0.3\n\nJAVA_HOME에도 jenv 로 설정된 버전을 인식할 수 있도록 아래 명령어로 글로벌 JDK 버전을 변경한다.\n$ jenv global 21.0.3\n\n(이걸 하지 않으면 Gradle 등에서 JAVA_HOME 인식을 하지 못한다.)\nGradle 8 설치\nsdkman은 JDK 외에도 여러 JVM 진영의 도구들을 설치, 관리하기 편리한 도구이다.\n그래서 Gradle 은 sdkman을 통해 진행한다.\n먼저 sdkman 을 설치한다.\n$ curl -s \"https://get.sdkman.io\" | bash\n$ source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n\n설치된 sdkman 을 통해 Gradle 최신 버전을 설치한다.\n$ sdk install gradle 8.8\n\n잘 설치되었는지 아래 명령어로 gradle 버전을 확인해본다.\n$ gradle -v\n------------------------------------------------------------\nGradle 8.8\n------------------------------------------------------------\n\nBuild time:   2024-05-31 21:46:56 UTC\nRevision:     4bd1b3d3fc3f31db5a26eecb416a165b8cc36082\n\nKotlin:       1.9.22\nGroovy:       3.0.21\nAnt:          Apache Ant(TM) version 1.10.13 compiled on January 4 2023\nJVM:          21.0.3 (Eclipse Adoptium 21.0.3+9-LTS)\nOS:           Mac OS X 14.1.2 aarch64",
        "guid": "http://jojoldu.tistory.com/789",
        "categories": [
          "개발환경",
          "adoptium eclipse temurin",
          "amazon corretto",
          "gradle",
          "java 21",
          "jdk 21",
          "개발환경"
        ],
        "isoDate": "2024-06-23T03:06:06.000Z"
      }
    ]
  },
  {
    "name": "WestwoodForever's Dev Log",
    "category": "개인",
    "posts": []
  },
  {
    "name": "허니몬(Honeymon)의 자바guru",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Wolf Loves Fox :: 일상",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Game Programmer Life",
    "category": "개인",
    "posts": []
  },
  {
    "name": "IT 프리랜서 이야기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "yuchi's dev",
    "category": "개인",
    "posts": []
  },
  {
    "name": "만화로 나누는 자유/오픈소스 소프트웨어 이야기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "신현석(Hyeonseok Shin)",
    "category": "개인",
    "posts": []
  },
  {
    "name": "한상곤 - Sigmadream",
    "category": "개인",
    "posts": [
      {
        "creator": "Sangkon Han",
        "title": "내 맘대로 위클리 뉴스 - 2024년 24주(2024.06.16 - 2024.06.22)",
        "link": "https://www.sangkon.com/sigmadream_weekly_2024_24-2/",
        "pubDate": "Sat, 22 Jun 2024 19:16:00 GMT",
        "content:encodedSnippet": "Python\nHow to Move From pandas to Polars\n\n요즘 각광받는 Polars와 기존에 많이 사용하는 Pandas에 대한 기사 입니다.\nJavaScript\nBuild a drag and drop to-do list\n칸반 스타일의 할 일을 간략하게 실습해 볼 수 있는 튜토리얼 기사입니다.\nReact 19 – New Hooks Explained with Examples\nReact 19에 도입되는 4가지 새로운 Hooks에 대해서 간략하게 소개하는 기사 입니다.\nOOP\nThe Pitfalls of Comparing BigDecimals in Java\n\nBigDecimal을 사용하실 때 참고하세요.\nEtc\nA Beautiful and Timely Python Multi-page Streamlit Application\nStarting and Stopping uvicorn in the Background\nBUILDING A BLUESKY ITALIAN ART BOT\nParsing Python ASTs 20x Faster with Rust\nBuilding Generative AI apps with .NET 8\nUsing PostgreSQL with .NET and Entra ID",
        "dc:creator": "Sangkon Han",
        "content": "<h2 id=\"python\">Python</h2>\n<ul>\n<li><a href=\"https://blog.jetbrains.com/pycharm/2024/06/how-to-move-from-pandas-to-polars/?ref=sangkon.com\">How to Move From pandas to Polars</a>\n<ul>\n<li>&#xC694;&#xC998; &#xAC01;&#xAD11;&#xBC1B;&#xB294; <code>Polars</code>&#xC640; &#xAE30;&#xC874;&#xC5D0; &#xB9CE;&#xC774; &#xC0AC;&#xC6A9;&#xD558;&#xB294; <code>Pandas</code>&#xC5D0; &#xB300;&#xD55C; &#xAE30;&#xC0AC; &#xC785;&#xB2C8;&#xB2E4;.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"javascript\">JavaScript</h2>\n<ul>\n<li>\n<p><a href=\"https://reactpractice.dev/exercise/build-a-drag-and-drop-to-do-list/?ref=sangkon.com\">Build a drag and drop to-do list</a></p>\n<ul>\n<li>&#xCE78;&#xBC18; &#xC2A4;&#xD0C0;&#xC77C;&#xC758; &#xD560; &#xC77C;</li></ul></li></ul>",
        "contentSnippet": "Python\nHow to Move From pandas to Polars\n\n요즘 각광받는 Polars와 기존에 많이 사용하는 Pandas에 대한 기사 입니다.\nJavaScript\nBuild a drag and drop to-do list\n칸반 스타일의 할 일",
        "guid": "6678748f4d836e346732ccc5",
        "categories": [
          "주간 뉴스"
        ],
        "isoDate": "2024-06-22T19:16:00.000Z"
      }
    ]
  },
  {
    "name": "개발자 울이 노트",
    "category": "개인",
    "posts": []
  },
  {
    "name": "즐거운 개발자 :: 네이버  블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "황제펭귄의 게임개발이야기 [여기는 한국]",
    "category": "개인",
    "posts": []
  },
  {
    "name": "LINE ENGINEERING",
    "category": "기업",
    "posts": [
      {
        "title": "LINE 클라이언트 개발자들이 만드는 '코드 리뷰 문화'",
        "link": "https://techblog.lycorp.co.jp/ko/code-review-culture-of-line-client-developers",
        "pubDate": "Mon, 24 Jun 2024 02:00:00 GMT",
        "content": "LINE 개발 조직에서는 성숙한 개발 문화를 만들기 위해 다양한 시도를 하고 있습니다. 클라이언트 앱 품질을 향상시키기 위해 개발 프로세스를 개선하고 있는 LY Mobile Dev...",
        "contentSnippet": "LINE 개발 조직에서는 성숙한 개발 문화를 만들기 위해 다양한 시도를 하고 있습니다. 클라이언트 앱 품질을 향상시키기 위해 개발 프로세스를 개선하고 있는 LY Mobile Dev...",
        "guid": "https://techblog.lycorp.co.jp/ko/code-review-culture-of-line-client-developers",
        "isoDate": "2024-06-24T02:00:00.000Z"
      },
      {
        "title": "오픈챗 해시태그 예측을 위한 다중 레이블 분류 모델 개발하기",
        "link": "https://techblog.lycorp.co.jp/ko/multi-label-classification-model-for-openchat-hashtag-prediction",
        "pubDate": "Wed, 19 Jun 2024 02:00:00 GMT",
        "content": "들어가며\n안녕하세요. AI Services Lab 팀의 ML 엔지니어 박희웅입니다. 저희 팀에서는 오픈챗과 관련된 다양한 AI/ML 모델을 개발해 서빙하고 있는데요. 앞서 오프라인...",
        "contentSnippet": "들어가며\n안녕하세요. AI Services Lab 팀의 ML 엔지니어 박희웅입니다. 저희 팀에서는 오픈챗과 관련된 다양한 AI/ML 모델을 개발해 서빙하고 있는데요. 앞서 오프라인...",
        "guid": "https://techblog.lycorp.co.jp/ko/multi-label-classification-model-for-openchat-hashtag-prediction",
        "isoDate": "2024-06-19T02:00:00.000Z"
      }
    ]
  },
  {
    "name": "뱅크샐러드 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "우아한형제들 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "TOAST Meetup",
    "category": "기업",
    "posts": []
  },
  {
    "name": "ZUM 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "SK Planet",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Spoqa tech blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "팀 왈도 번역팀",
    "category": "게임",
    "posts": []
  },
  {
    "name": "이한",
    "category": "개인",
    "posts": []
  },
  {
    "name": "근원님",
    "category": "개인",
    "posts": []
  },
  {
    "name": "황의윤",
    "category": "개인",
    "posts": []
  },
  {
    "name": "호돌맨",
    "category": "개인",
    "posts": [
      {
        "creator": "호돌맨",
        "title": "광명찾는 Intellij vmoption 설정 값",
        "link": "https://hodolman.com/59",
        "pubDate": "Mon, 24 Jun 2024 10:17:29 +0900",
        "author": "호돌맨",
        "comments": "https://hodolman.com/59#entry59comment",
        "content": "<h2>Intellij vmoption</h2>\n<ul>\n<li>Zulu JDK 17버젼 사용중.</li>\n<li>Intellij 프로젝트 3~5개에서 자바 애플리케이션 6개정도 띄워놓는 게 일반적</li>\n<li>이 전에는 컴퓨타가 너무 버벅여서 작업하기가 너무 힘들었음</li>\n<li>아래 값으로 개발환경 광명찾음</li>\n<li>단 한번도 Intellij가 버벅이지 않음</li>\n</ul>\n<pre><code>-ea\n-server\n-Xms1024m\n-Xmx5120m\n-Xss256k\n-XX:+UnlockExperimentalVMOptions\n-XX:-UseSerialGC\n-XX:-UseParallelGC\n-XX:-UseG1GC\n-XX:+UseZGC\n-XX:+IgnoreUnrecognizedVMOptions\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:-OmitStackTraceInFastThrow\n-XX:ReservedCodeCacheSize=512m\n-XX:SoftRefLRUPolicyMSPerMB=50\n-XX:+UseStringCache\n-XX:+UseStringDeduplication\n-XX:+AggressiveOpts\n-XX:+AlwaysPreTouch\n-XX:+OptimizeStringConcat\n-XX:+UseFastAccessorMethods\n-Dide.no.platform.update=true\n-Djava.net.preferIPv4Stack=true\n-Djdk.attach.allowAttachSelf=true\n-Djdk.module.illegalAccess.silent=true\n-Dkotlinx.coroutines.debug=off\n-Dsun.io.useCanonCaches=false\n-Dsun.java2d.d3d=false\n-Dsun.java2d.metal=true\n-Dsun.java2d.opengl=false\n-Dsun.tools.attach.tmp.only=true\n-Dsun.awt.mac.a11y.enabled=false\n--add-exports=java.desktop/com.apple.eawt.event=ALL-UNNAMED\n--add-exports=java.desktop/com.apple.eawt=ALL-UNNAMED\n--add-exports=java.desktop/com.apple.laf=ALL-UNNAMED\n--add-exports=java.desktop/sun.awt.image=ALL-UNNAMED\n--add-exports=java.desktop/sun.font=ALL-UNNAMED\n--add-opens=java.base/java.io=ALL-UNNAMED\n--add-opens=java.base/java.lang.reflect=ALL-UNNAMED\n--add-opens=java.base/java.lang=ALL-UNNAMED\n--add-opens=java.base/java.net=ALL-UNNAMED\n--add-opens=java.base/java.nio.charset=ALL-UNNAMED\n--add-opens=java.base/java.text=ALL-UNNAMED\n--add-opens=java.base/java.time=ALL-UNNAMED\n--add-opens=java.base/java.util.concurrent=ALL-UNNAMED\n--add-opens=java.base/java.util=ALL-UNNAMED\n--add-opens=java.base/jdk.internal.org.objectweb.asm.tree=ALL-UNNAMED\n--add-opens=java.base/jdk.internal.org.objectweb.asm=ALL-UNNAMED\n--add-opens=java.base/jdk.internal.vm=ALL-UNNAMED\n--add-opens=java.base/sun.nio.ch=ALL-UNNAMED\n--add-opens=java.desktop/com.apple.eawt.event=ALL-UNNAMED\n--add-opens=java.desktop/com.apple.eawt=ALL-UNNAMED\n--add-opens=java.desktop/com.apple.laf=ALL-UNNAMED\n--add-opens=java.desktop/java.awt.dnd.peer=ALL-UNNAMED\n--add-opens=java.desktop/java.awt.event=ALL-UNNAMED\n--add-opens=java.desktop/java.awt.image=ALL-UNNAMED\n--add-opens=java.desktop/java.awt.peer=ALL-UNNAMED\n--add-opens=java.desktop/java.awt=ALL-UNNAMED\n--add-opens=java.desktop/javax.swing.plaf.basic=ALL-UNNAMED\n--add-opens=java.desktop/javax.swing.text.html=ALL-UNNAMED\n--add-opens=java.desktop/javax.swing=ALL-UNNAMED\n--add-opens=java.desktop/sun.awt.datatransfer=ALL-UNNAMED\n--add-opens=java.desktop/sun.awt.image=ALL-UNNAMED\n--add-opens=java.desktop/sun.awt.windows=ALL-UNNAMED\n--add-opens=java.desktop/sun.awt.X11=ALL-UNNAMED\n--add-opens=java.desktop/sun.awt=ALL-UNNAMED\n--add-opens=java.desktop/sun.font=ALL-UNNAMED\n--add-opens=java.desktop/sun.java2d=ALL-UNNAMED\n--add-opens=java.desktop/sun.lwawt.macosx=ALL-UNNAMED\n--add-opens=java.desktop/sun.lwawt=ALL-UNNAMED\n--add-opens=java.desktop/sun.swing=ALL-UNNAMED\n--add-opens=jdk.attach/sun.tools.attach=ALL-UNNAMED\n—add-opens=jdk.internal.jvmstat/sun.jvmstat.monitor=ALL-UNNAMED\n—add-opens=jdk.jdi/com.sun.tools.jdi=ALL-UNNAMED\n-Dide.managed.by.toolbox=/Applications/JetBrains Toolbox.app/Contents/MacOS/jetbrains-toolbox\n-Dtoolbox.notification.token=96384ae9-2756-497b-8309-23240f1e3002\n-Dtoolbox.notification.portFile=/Users/hodolman/Library/Caches/JetBrains/Toolbox/ports/1732101961.port</code></pre>",
        "contentSnippet": "Intellij vmoption\nZulu JDK 17버젼 사용중.\nIntellij 프로젝트 3~5개에서 자바 애플리케이션 6개정도 띄워놓는 게 일반적\n이 전에는 컴퓨타가 너무 버벅여서 작업하기가 너무 힘들었음\n아래 값으로 개발환경 광명찾음\n단 한번도 Intellij가 버벅이지 않음\n-ea\n-server\n-Xms1024m\n-Xmx5120m\n-Xss256k\n-XX:+UnlockExperimentalVMOptions\n-XX:-UseSerialGC\n-XX:-UseParallelGC\n-XX:-UseG1GC\n-XX:+UseZGC\n-XX:+IgnoreUnrecognizedVMOptions\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:-OmitStackTraceInFastThrow\n-XX:ReservedCodeCacheSize=512m\n-XX:SoftRefLRUPolicyMSPerMB=50\n-XX:+UseStringCache\n-XX:+UseStringDeduplication\n-XX:+AggressiveOpts\n-XX:+AlwaysPreTouch\n-XX:+OptimizeStringConcat\n-XX:+UseFastAccessorMethods\n-Dide.no.platform.update=true\n-Djava.net.preferIPv4Stack=true\n-Djdk.attach.allowAttachSelf=true\n-Djdk.module.illegalAccess.silent=true\n-Dkotlinx.coroutines.debug=off\n-Dsun.io.useCanonCaches=false\n-Dsun.java2d.d3d=false\n-Dsun.java2d.metal=true\n-Dsun.java2d.opengl=false\n-Dsun.tools.attach.tmp.only=true\n-Dsun.awt.mac.a11y.enabled=false\n--add-exports=java.desktop/com.apple.eawt.event=ALL-UNNAMED\n--add-exports=java.desktop/com.apple.eawt=ALL-UNNAMED\n--add-exports=java.desktop/com.apple.laf=ALL-UNNAMED\n--add-exports=java.desktop/sun.awt.image=ALL-UNNAMED\n--add-exports=java.desktop/sun.font=ALL-UNNAMED\n--add-opens=java.base/java.io=ALL-UNNAMED\n--add-opens=java.base/java.lang.reflect=ALL-UNNAMED\n--add-opens=java.base/java.lang=ALL-UNNAMED\n--add-opens=java.base/java.net=ALL-UNNAMED\n--add-opens=java.base/java.nio.charset=ALL-UNNAMED\n--add-opens=java.base/java.text=ALL-UNNAMED\n--add-opens=java.base/java.time=ALL-UNNAMED\n--add-opens=java.base/java.util.concurrent=ALL-UNNAMED\n--add-opens=java.base/java.util=ALL-UNNAMED\n--add-opens=java.base/jdk.internal.org.objectweb.asm.tree=ALL-UNNAMED\n--add-opens=java.base/jdk.internal.org.objectweb.asm=ALL-UNNAMED\n--add-opens=java.base/jdk.internal.vm=ALL-UNNAMED\n--add-opens=java.base/sun.nio.ch=ALL-UNNAMED\n--add-opens=java.desktop/com.apple.eawt.event=ALL-UNNAMED\n--add-opens=java.desktop/com.apple.eawt=ALL-UNNAMED\n--add-opens=java.desktop/com.apple.laf=ALL-UNNAMED\n--add-opens=java.desktop/java.awt.dnd.peer=ALL-UNNAMED\n--add-opens=java.desktop/java.awt.event=ALL-UNNAMED\n--add-opens=java.desktop/java.awt.image=ALL-UNNAMED\n--add-opens=java.desktop/java.awt.peer=ALL-UNNAMED\n--add-opens=java.desktop/java.awt=ALL-UNNAMED\n--add-opens=java.desktop/javax.swing.plaf.basic=ALL-UNNAMED\n--add-opens=java.desktop/javax.swing.text.html=ALL-UNNAMED\n--add-opens=java.desktop/javax.swing=ALL-UNNAMED\n--add-opens=java.desktop/sun.awt.datatransfer=ALL-UNNAMED\n--add-opens=java.desktop/sun.awt.image=ALL-UNNAMED\n--add-opens=java.desktop/sun.awt.windows=ALL-UNNAMED\n--add-opens=java.desktop/sun.awt.X11=ALL-UNNAMED\n--add-opens=java.desktop/sun.awt=ALL-UNNAMED\n--add-opens=java.desktop/sun.font=ALL-UNNAMED\n--add-opens=java.desktop/sun.java2d=ALL-UNNAMED\n--add-opens=java.desktop/sun.lwawt.macosx=ALL-UNNAMED\n--add-opens=java.desktop/sun.lwawt=ALL-UNNAMED\n--add-opens=java.desktop/sun.swing=ALL-UNNAMED\n--add-opens=jdk.attach/sun.tools.attach=ALL-UNNAMED\n—add-opens=jdk.internal.jvmstat/sun.jvmstat.monitor=ALL-UNNAMED\n—add-opens=jdk.jdi/com.sun.tools.jdi=ALL-UNNAMED\n-Dide.managed.by.toolbox=/Applications/JetBrains Toolbox.app/Contents/MacOS/jetbrains-toolbox\n-Dtoolbox.notification.token=96384ae9-2756-497b-8309-23240f1e3002\n-Dtoolbox.notification.portFile=/Users/hodolman/Library/Caches/JetBrains/Toolbox/ports/1732101961.port",
        "guid": "https://hodolman.com/59",
        "isoDate": "2024-06-24T01:17:29.000Z"
      }
    ]
  },
  {
    "name": "박우빈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "문다영",
    "category": "개인",
    "posts": []
  },
  {
    "name": "유수민",
    "category": "개인",
    "posts": []
  },
  {
    "name": "안건주",
    "category": "개인",
    "posts": []
  },
  {
    "name": "손현호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "STARTUP BIBLE",
    "category": "개인",
    "posts": [
      {
        "creator": "Kihong Bae",
        "title": "인생에서 가장 도움이 되는 수업",
        "link": "https://www.thestartupbible.com/2024/06/public-speaking-and-writing-are-the-most-undervalued-classes.html",
        "pubDate": "Sun, 23 Jun 2024 21:24:00 +0000",
        "content:encodedSnippet": "자주는 아니지만, 나도 가끔 세미나와 강연 요청을 받는다. 바쁘기도 하고, 내가 전문적으로 강연을 하는 사람은 아니라서 대부분 다 사양하고 거절하지만, 없는 시간을 만들어서라도 기꺼이 하는 게 딱 하나 있는데, 바로 학생들을 대상으로 하는 강연이나 세미나이다. 두 가지 이유가 있다. 하나는 학생들이 미래의 창업가들이고, 이들이 스트롱의 미래 고객이기 때문에, 잠재 고객을 만나서 이들에게 스트롱벤처스에 대한 홍보를 하기 위해서이다. 두 번째 이유는 어쩌면 업무적이기도 하지만, 개인적인 이유도 있는데, 바로 학생들과 이야기하면서 나도 큰 배움을 얻기 때문이다.\n내 나이의 절반도 안 되는 학생들에게 내가 뭘 배울 수 있겠느냐는 생각할 수도 있지만, 그렇기 때문에 오히려 나는 더 배울 수 있는 것들이 많다고 생각한다. 학생들 대부분 아직 경험이 없는데, 경험이 없기 때문에 경험이 많은 사람들이 마음속에 갖고 있는 편견이 없고, 편견이 없기 때문에 어떻게 보면 너무 뻔하지만, 자세히 생각해 보면 굉장히 의미 있는 상상과 질문들을 많이 한다. 바로 이런 대화를 하면서 나는 생각도 많이 하고 꽤 많은 배움을 얻을 수 있다. 그리고, 젊은 학생들과 이야기하다 보면 그냥 왠지 나도 더 젊어지는 것 같고, 더 긍정적인 마인드를 얻게 되는 것 같아서 좋다.\n학생들과 이야기하다 보면, 이 질문을 자주 받는다. “기홍님은 지금 생각해 봤을 때, 학교에서 배웠던 내용 중, 일하면서 가장 도움이 많이 됐던 수업은 어떤 거였나요?”\n이 질문을 받으면 나는 생각도 하지 않고 ‘Public Speaking(말하기)’과 ‘Writing(쓰기)’ 수업이라고 자신 있게 말해준다. 내가 한국에서 학교 다닐 땐 이런 수업이 존재하지 않아서, 나는 미국에서 유학할 때 말하기와 쓰기 수업을 들었는데, 요샌 한국에서도 이런 수업이 제공되는 거로 알고 있다. 전공을 불문하고 모든 대학생들에게 이 두 수업을 되도록 졸업하기 전에, 시간 날 때마다 수강하는 걸 강력하게 추천한다.\n나는 오랫동안 글을 쓰긴 했다. 초등학교 때부터 지금까지 공부를 위한 글쓰기나 취미를 위한 글쓰기는 꾸준히 해왔고, 나름 나만의 스타일로 글을 쓰면서 내 생각을 정리하는 동시에, 이 생각을 남에게 명확하게 전달하는 데에는 어느 정도 성공하고 있다고 생각한다. 하지만, 아직도 부족해서 매일 일정 시간을 할애해서 글을 쓰고 있다. 글을 매일 쓰면 글쓰기 실력은 누구나 향상할 수 있다. 하지만, 더 잘 쓰고 싶으면 쓰는 동시에 많이 읽어야 한다. 참고로, 나는 매년 50권의 책을 읽으려고 노력한다. 글쓰기는 이렇게 평생 연습을 하고 있기 때문에 쓰기 수업은 미국에서 유학할 때 한 번 들었다. 이 수업의 핵심은, 글을 잘 쓰고 싶다면, 많이, 그리고 자주 써야 하고, 이만큼 많이, 그리고 자주 읽어야 한다는 것이다.\n말하기는 내가 항상 자신이 없었던 분야였다. 그런데, 대학원에 가니까 나보다 실력도 없고 멍청한 학생들이 남들 앞에서 말을 논리적으로 잘하면서 내 밥그릇과 기회를 빼앗아 가는 걸 직접 경험하면서, 나도 말을 좀 잘해야겠다는 결심을 했고, 대학원에서 ‘Public Speaking’이라는 수업을 2학기나 들었다. 이 수업은 3학점짜리 수업이니까 총 6학점을 들은 것이다. 실은, 나와 1대 1로 연습/훈련을 했던 코치는 학부생이었는데, 노벨 물리학상을 받은 스탠포드 교수보다 나에겐 훨씬 더 뛰어나고 인상 깊었던 선생님이었다. 수업마다 특정 주제에 대해서 각자 3~5분 동안 발표를 하고, 이걸 동영상으로 촬영한 후 코치와 함께 자세하게 분석해서 발표 실력을 지속적으로 개선해 나가는 과정을 반복하게 된다. 나는 첫 학기에 B-를 받았지만 – 참고로 B 이하는 매우 형편없는 점수이다 – 그 다음 학기는 B+를 받았다. 성적은 조금 향상했지만 내 발표 실력과 청중 앞에 섰을 때의 자신감은 10배 정도 상승했다.\n나는 꽤 많은 사람들을 만나는데, 상당히 자주 깜짝깜짝 놀랄 때가 있다. 이렇게 공부도 많이 하고, 일도 오래 한 사람들이, 본인의 생각을 남들에게 말이나 글로 전달하는 걸 보면, 초등학생 수준으로도 미달일 때가 많기 때문이다.\n지금 이 글을 읽는 학생들이 있다면, 이분들에겐 더욱더 강력하게 ‘말하기’와 ‘쓰기’ 수업을 추천한다. 직장인들이라면, 이분들에게도 강력하게 추천한다. 절대 후회하지 않을 것이다.",
        "dc:creator": "Kihong Bae",
        "comments": "https://www.thestartupbible.com/2024/06/public-speaking-and-writing-are-the-most-undervalued-classes.html#respond",
        "content": "자주는 아니지만, 나도 가끔 세미나와 강연 요청을 받는다. 바쁘기도 하고, 내가 전문적으로 강연을 하는 사람은 아니라서 대부분 다 사양하고 거절하지만, 없는 시간을 만들어서라도 기꺼이 하는 게 딱 하나 있는데, 바로 학생들을 대상으로 하는 강연이나 세미나이다. 두 가지 이유가 있다. 하나는 학생들이 미래의 창업가들이고, 이들이 스트롱의 미래 고객이기 때문에, 잠재 고객을 만나서 이들에게 스트롱벤처스에 대한 홍보를(...)",
        "contentSnippet": "자주는 아니지만, 나도 가끔 세미나와 강연 요청을 받는다. 바쁘기도 하고, 내가 전문적으로 강연을 하는 사람은 아니라서 대부분 다 사양하고 거절하지만, 없는 시간을 만들어서라도 기꺼이 하는 게 딱 하나 있는데, 바로 학생들을 대상으로 하는 강연이나 세미나이다. 두 가지 이유가 있다. 하나는 학생들이 미래의 창업가들이고, 이들이 스트롱의 미래 고객이기 때문에, 잠재 고객을 만나서 이들에게 스트롱벤처스에 대한 홍보를(...)",
        "guid": "https://www.thestartupbible.com/?p=9134",
        "categories": [
          "Uncategorized",
          "books",
          "education",
          "FoundersAtWork",
          "스타트업 바이블 QA"
        ],
        "isoDate": "2024-06-23T21:24:00.000Z"
      },
      {
        "creator": "Kihong Bae",
        "title": "컴백",
        "link": "https://www.thestartupbible.com/2024/06/the-great-comeback.html",
        "pubDate": "Wed, 19 Jun 2024 21:31:00 +0000",
        "content:encodedSnippet": "얼마 전에 4대 메이저 테니스 대회 중 하나인 프랑스 오픈이 끝났다. 2주 동안 밤늦게까지 거의 매일 테니스를 봐서 행복했지만, 내가 좋아하는 선수들은 이제 대부분 은퇴했거나 늙어서 초반에 탈락했다. 이 중 우승 가능성이 아직도 높았던 조코비치 선수가 우승하길 바랬는데, 준준결승전에 부상으로 인해서 기권패 했다.\n나는 이번에 조코비치 선수의 경기를 다 봤는데, 모든 경기마다 안타까움과 경외감의 두 가지 감정이 교체했다. 거의 완벽함을 자랑하던 선수가 나이 들면서 젊은 선수들에게 밀리는 걸 볼 때마다 역시 아무리 체력이 좋고 몸 관리를 잘해도 세월을 이길 수 없다는 안타까움과 아쉬움이 있었지만, 반대로, 이제 거의 40세가 된 이 선수가 20대 초반 선수들과 체력적으로 대등한 경기를 하는 걸 보면 정말 대단하다는 경외심이 생겼다.\n특히, 이번 프랑스 오픈 시합에서 인상 깊었던 점은 이 노장의 컴백 능력이었다. 남자 테니스는 5세트 중 3세트를 먼저 이겨야 하는데, 5세트를 모두 플레이하면, 그리고 정말 치열한 경기를 펼치면 5시간 이상 걸린다. 사업도 그렇지만, 운동 경기도 분위기와 흐름이라는 게 있어서, 이 분위기와 흐름이 내 쪽으로 오지 않으면 상승모드를 유지하는 게 정말 어렵다. 테니스의 경우 세트 스코어가 2대 1이면, 네 번째 세트에서 경기는 3대 1로 거의 종료된다. 즉, 2세트를 뒤지고 있으면, 그 이후에 다시 흐름을 뺏어오는 게 거의 불가능하다.\n그런데 조코비치는 이런 통계에 포함되길 거부하는 선수다. 나는 그동안 이 선수가 세트 스코어 2대 0으로 뒤지고 있는 상황에서 완벽하게 컴백해서 결국엔 다섯 번째 세트까지 가서 3대 2로 이기는 걸 너무 많이 봤다. 실은, 너무 많이 봐서 이 선수에겐 이게 당연한 것 같이 느껴지지만, 현실적으론 거의 불가능한 컴백을 조코비치는 밥 먹듯이 하고 있다는 점이 정말 놀랍다. 그리고 아무리 나이를 먹어도 이런 불가능한 컴백을 이번 프랑스 오픈에서도 여러 번 보여줬다. 3 라운드와 4라운드 모두 세트스코어 2대 1로 지고 있던 상황에서 결국엔 다섯 번째 세트까지 경기를 끌고 가서, 4시간 반이 넘는 시합을 하면서 두 번이나 3대 2로 역전승했다.\n아무리 뛰어나고 위대한 선수들도 이런 컴백을 하기는 쉽지 않은데, 어떻게 조코비치는 반복적으로 이렇게 컴백 할 수 있을까? 결국엔 정신력, 체력, 자기관리, 그리고 끝까지 포기하지 않는 끈기 때문이라고 생각한다. 운동선수들은 – 특히, 팀이 아닌 개인에게 퍼포먼스의 100%를 의지해야 하는 테니스와 같은 – 몸이 돈이기 때문에 정말 체력을 종교와도 같이 관리하는데 조코비치는 이런 운동선수 중에서도 심할 정도로 관리를 잘 한다. 몇 년 전에 메이저 대회 우승한 후에 초콜릿을 딱 한 입 먹은 후에 우승을 자축한 일화가 유명하지만, 이런 관리 스타일은 이 선수의 일상생활이다. 이런 자기 관리에서 오는 체력과 정신력은 다른 선수들이 흉내조차 낼 수 없을 정도다.\n창업가들은 하루하루가 이렇게 뒤지고 있는 경기에서 컴백해야 하는 전쟁이다. 다른 경쟁 스타트업과의 경기에서 항상 지기 때문에 컴백해야 한다. 대기업과의 경기에서 이미 진 상태로 시작하기 때문에 컴백해야 한다. 자신의 제품과의 경기에서 지기 때문에 컴백해야 한다. 고객과의 경기에서도 항상 지기 때문에 컴백해야 한다. 회사에 사람이 많아지면, 직원들에게도 치이면서 지기 때문에 항상 컴백해야 한다. 도대체 이기는 경기는 하나도 없기 때문에, 모든 면에서 매일, 매시간, 매 순간 컴백해야 한다.\n이렇게 계속 컴백하기 위해서는 조코비치같이 창업가들도 몸과 마음을 잘 단련하고, 절제하고, 관리해야 한다. 운동도 매일 해야 하고, 음식도 절제해야 하고, 술도 절제해야 하고, 항상 최상의 컨디션으로 일하고, 뒤지는 경기에서도 항상 컴백할 수 있게 항상 스스로를 관리해야 한다. 이게 안 되면 오랫동안 지속되는 사업을 만들 수가 없다.\n*참고로, 조코비치가 이번에 준준결승에서 기권한 이유는 늙어서 체력이 약해서라기 보단, 그 전 경기가 주최 측의 잘못된 결정으로 너무 늦게 밤 11시에 시작해서 새벽 3시가 넘어서 끝났기 때문이다. 이렇게 누적된 피로로 그다음 날 다시 경기하는 건 말도 안 된다고 생각했기 때문에 기권했다.",
        "dc:creator": "Kihong Bae",
        "comments": "https://www.thestartupbible.com/2024/06/the-great-comeback.html#comments",
        "content": "얼마 전에 4대 메이저 테니스 대회 중 하나인 프랑스 오픈이 끝났다. 2주 동안 밤늦게까지 거의 매일 테니스를 봐서 행복했지만, 내가 좋아하는 선수들은 이제 대부분 은퇴했거나 늙어서 초반에 탈락했다. 이 중 우승 가능성이 아직도 높았던 조코비치 선수가 우승하길 바랬는데, 준준결승전에 부상으로 인해서 기권패 했다. 나는 이번에 조코비치 선수의 경기를 다 봤는데, 모든 경기마다 안타까움과 경외감의 두(...)",
        "contentSnippet": "얼마 전에 4대 메이저 테니스 대회 중 하나인 프랑스 오픈이 끝났다. 2주 동안 밤늦게까지 거의 매일 테니스를 봐서 행복했지만, 내가 좋아하는 선수들은 이제 대부분 은퇴했거나 늙어서 초반에 탈락했다. 이 중 우승 가능성이 아직도 높았던 조코비치 선수가 우승하길 바랬는데, 준준결승전에 부상으로 인해서 기권패 했다. 나는 이번에 조코비치 선수의 경기를 다 봤는데, 모든 경기마다 안타까움과 경외감의 두(...)",
        "guid": "https://www.thestartupbible.com/?p=9131",
        "categories": [
          "Uncategorized",
          "discipline",
          "FoundersAtWork",
          "sports"
        ],
        "isoDate": "2024-06-19T21:31:00.000Z"
      }
    ]
  },
  {
    "name": "Build a Great Product",
    "category": "개인",
    "posts": []
  },
  {
    "name": "지금 써보러 갑니다",
    "category": "개인",
    "posts": []
  },
  {
    "name": "매거진 입맛",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "요즘 IT",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "쿠팡 엔지니어링",
    "category": "기업",
    "posts": []
  },
  {
    "name": "지마켓 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "리멤버 (드라마 앤 컴퍼니)",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Kimchi hill",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "Hudi.blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "토스",
    "category": "기업",
    "posts": [
      {
        "title": "무조건 선택지가 많을수록 더 좋은 것일까?",
        "link": "https://blog.toss.im/article/everyday-economics-16-too-many-options",
        "pubDate": "Tue, 25 Jun 2024 11:24:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-nv7vyi{margin:24px 0 8px;padding:16px 40px 32px;border-radius:16px;background-color:var(--adaptiveGrey100);}.css-123co55{font-size:19px;letter-spacing:0em;line-height:1.6;margin:24px 0 0;font-weight:400;color:var(--adaptiveGrey900);background-color:transparent;}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}.css-q3ktjb{white-space:pre-wrap;font-weight:bold;}💡 이 글에서 알 수 있는 것들\n.css-uswsmm{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;margin:24px 0 8px;padding:0;list-style:none;}.css-uswsmm ul,.css-uswsmm ol{margin:16px 0 0;}.css-uswsmm>li{margin-bottom:16px;padding-left:24px;}.css-uswsmm>li:last-of-type{margin-bottom:0;}.css-uswsmm>li>span{position:relative;}.css-uswsmm>li>span>:first-child::before{content:'•';font-weight:500;color:var(--adaptiveGrey800);position:absolute;left:-24px;}\n.css-1hwiibq{font-size:17px;line-height:1.6;word-break:keep-all;letter-spacing:0em;font-weight:400;color:var(--adaptiveGrey800);}\n.css-1kxrhf3{white-space:pre-wrap;}너무 많은 선택지가 주어질 때, 오히려 스트레스를 받는 이유\n넷플릭스, 유튜브 같은 스트리밍 서비스 홍수 속에서 오히려 볼 게 없다고 느끼는 이유\n수많은 선택지보다 엄선된 선택지 몇 개가 더 끌리는 이유\n\n.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n.css-12p6bv8{white-space:pre-wrap;color:#15c47e;font-weight:bold;}교수 K (이하 K): 팬데믹이 한창일 때, 우리는 선택지가 줄어드는 경험을 했습니다. 전세계적으로 공급망 교란이 일어났기 때문이죠. 원하는 자동차 모델을 1년 넘게 기다려야 살 수 있었고, 그나마도 일부 옵션이 제외된 모델을 울며 겨자먹기로 구입해야 했습니다. '선택의 자유'가 줄어들면서, 사람들의 불편함도 증가하게 되었고요.\n.css-94on8q{white-space:pre-wrap;color:#c770e4;font-weight:bold;}에디터 G (이하 G): 맞아요. 팬데믹이 끝난 후 시간이 꽤 흘러서 지금은 좀 나아졌지만… 그 시기에는 어쩔 수 없이 선택해야 할 정도로 선택지가 적었던 것 같아요.\nK: 에디터 님도 선택지가 적은 환경보다 선택지가 많은 환경이 더 낫다고 생각하시듯, 우리는 보통 선택지가 많을수록 더 좋다고 생각합니다. 아무래도 더 많은 옵션이 있으면, 내게 딱 맞는 최적의 선택을 할 가능성이 높아지기 때문이죠.\n.css-16cuouw{white-space:pre-wrap;color:var(--adaptiveGrey800);background-color:#3fd59936;}하지만, 선택의 다양성이 항상 좋은 것만은 아니예요. 너무 많은 선택지가 주어질 때 사람들은 오히려 더 큰 스트레스를 받고, 만족도가 낮아질 수 있거든요.\nG: 생각해보니 그런 것 같기도 해요. 점심 시간에 더 선호하게 되는 식당은 메뉴가 너무 다양한 곳보다 1~2개에 주력하는 곳이거든요.\nK: 맞아요. 정말 그런지 확인해보기 위해 컬럼비아 대학교의 쉬나 아이엔가(Sheena Iyengar) 교수와 스탠포드대학교의 마크 래퍼(Mark Lepper) 교수는 세 가지 실험을 실시합니다.\n.css-120saye{white-space:pre-wrap;font-style:italic;font-weight:bold;}1. 소비자들은 선택지가 적을 때 지갑을 열까, \n선택지가 많을 때 지갑을 열까?\n\n첫 번째 실험은 슈퍼마켓에서 6가지(제한된 선택지)의 잼을 시식할 수 있는 부스와 24가지(광범위한 선택지) 종류의 잼을 시식할 수 있는 부스를 운영하며, 소비자들의 행동을 비교하는 것입니다.\n관찰 결과, 24가지의 잼을 시식할 수 있는 부스에는 60%의 고객이 관심을 가진 반면, 6가지의 잼을 시식할 수 있는 부스에는 40%의 고객만이 관심을 보였습니다.\nG: 오호 그렇군요. 절반 넘는 사람들이 제한된 선택지보다 광범위한 선택지에 더 매력을 느꼈나 보네요.\nK: 그렇죠, 하지만 재미있게도 고객들의 최종 구매 결정에서는 반대의 결과가 나옵니다. 6가지 잼이 있는 부스에서는 시식한 고객들의 30%가 실제로 잼을 구매했지만, 24가지 잼이 있는 부스에서 시식한 고객들 중에서는 오직 3%만이 잼의 구매로 이어졌습니다.\nG: 헉! 그렇게나 차이가 많이 난다구요?\nK: 네, 재미있죠. 처음엔 다양한 종류의 잼이 사람들의 관심을 끄는 것에 성공했지만, 실제 제품 구입으로 이어지는 데에는 잼의 종류가 적은 것이 더 효과적이었던 거죠.\n2. 학생들은 선택지가 적을 때 더 높은 학업 성취도를 보일까, \n선택지가 많을 때 더 높은 학업 성취도를 보일까?\n\nK: 두 번째 실험은 같은 수업을 듣는 대학생들 대상으로 진행됐어요. 2장 분량의 에세이 과제를 제출하면, 추가 점수를 받을 수 있는 기회를 주는 것이었습니다.\n이 때 학생들을 두 그룹으로 나누게 되는데요. A 그룹은 6개의 주제(제한된 선택지) 가운데 하나를 에세이 주제로 정하고, B 그룹은 30개의 주제(광범위한 선택지) 중에서 하나를 골라야 했습니다. 결과는 어떻게 되었을까요?\nG: 음… 다양한 주제 중에서 고를 수 있었던 B 그룹 대학생들의 에세이가 더 좋았을 것 같아요. 주제가 다양한 만큼 생각의 폭도 넓어질 수 있으니까요.\nK: 좋은 추측이에요. 결과를 알려드릴게요. 먼저, 두 그룹의 과제 제출 비율을 비교해보죠. A 그룹(제한된 선택지)에 배정된 학생들 중 74%가 과제를 제출했던 반면, B 그룹(광범위한 선택지)에 배정된 학생들 중에서는 60%만이 과제를 제출했습니다.\nG: 오호, 첫 번째 실험과 비슷한 결과네요. A 실험에서도 제한된 선택지를 마주한 사람들의 결제율이 더 높았으니까요.\nK: 맞아요. 제출된 에세이의 퀄리티에서도 두 그룹 간 차이가 보였답니다. 에디터 님은 B 그룹(광범위한 선택지)에 배정된 학생들 에세이가 더 좋았을거라 예상하셨지만, A 그룹(제한된 선택지)에 배정된 학생들의 평균 점수가 B 그룹(광범위한 선택지)에 배정된 학생들의 평균 점수보다 더 높게 나왔답니다.\nG: 오, 제한된 선택지에 배정된 학생들의 과제 제출률도 높았고, 평균 점수도 더 높게 나오다니! 두 번째 실험에서는, 오히려 선택지가 제한된 상황에서 선택하는 것이 더 선호될 뿐 아니라 더 나은 성과도 만들어낸 다는 것을 보여준 거네요.\nK: 맞아요. 첫 번째 실험과는 살짝 다르게, 모든 면에서 제한된 선택지가 더 낫다는 결과가 나왔어요.\n3. 사람들은 선택지가 적을 때 더 만족감을 느낄까?\n선택지가 많을 때 더 만족감을 느낄까?\n\nK: 세 번째 실험에는 초콜릿이 등장합니다. 이번에도 참가자들을 두 그룹으로 나눴어요. A 그룹에서는 6개의 초콜릿(제한된 선택지) 가운데 한 가지를 골라 맛볼 수 있게 한 반면, B 그룹에서는 30개의 초콜릿(광범위한 선택지) 가운데 한 가지를 고를 수 있게 했습니다.\n세 번째 실험에서도 비슷한 결과가 나왔어요. 두 그룹의 초콜릿 맛에 대한 만족도를 비교해 본 결과, A 그룹(제한된 선택지)의 참가자들이 B 그룹(광범위한 선택지)의 참가자들보다 초콜릿 맛에 대해 더 만족한 것으로 나타났습니다.\n여기에 흥미로운 사실이 하나 더 발견되는데요. B 그룹의 참가자들은 30개 초콜릿 중 하나를 고르는 과정 자체는 즐겼지만, “30개 가운데 가장 맛있는 1개를 고르는 것”에 대한 부담감을 느꼈습니다. ‘더 맛있는 것을 골랐어야 했는데…’ 라며 자신이 내린 선택에 후회를 하거나, 잘못된 선택에 대해 불만족을 느끼는 모습도 나타났고요.\nG: 선택지가 많은 상황에서 고른 1개가 최선이 아닐까봐, 잘못된 선택일까봐, 오히려 불만족을 느끼는 거군요.\nK: 맞습니다. 이 세 번의 실험 결과는 ‘더 많은 선택지가 항상 유익하다’는 기존의 믿음을 뒤엎었습니다. 우리는 선택지가 별로 없는 상황에서는 별다른 대안이 없기 때문에, 오히려 최종 선택에 쉽게 만족할 수 있는거죠.\n반면 선택지가 많아지면 그 가운데 가장 완벽한 선택이 있을 것이라 믿고, 그것을 찾아내려 합니다. 만약 이때 성급하게 결정을 내린다면, 더 좋은 것이 있었을지도 모른다 후회하겠죠. 설령 좋은 대안들을 추려냈다고 하더라도, 그중 무엇이 가장 나은지 골라야 하고요. 그리고 시간이 지나면 ‘아, 그때 다른 것을 선택했어야 하는데' 하며 후회하게 되는 것이죠.\n이것을 ‘선택의 과부하(choice overload)’라고 하는데요. 너무 광범위한 선택지는 오히려 사람들의 결정을 힘들게 하고, 다양한 선택지 가운데 최선을 선택해야만 한다는 기대를 만들어냅니다. 결과적으로는 더 큰 후회를 초래하고 만족도를 낮추게 되는 것이죠.\nG: 저도 비슷한 경험이 있어요. 책 종류가 엄청 많은 대형 서점에서 고르고 고른 책보다, 책 종류는 적지만 책방 주인의 취향에 맞게 큐레이션 되어있는 동네 작은 책방에서 고른 책이 더 재미있을 때가 있었거든요.\nK: 그렇죠. 이 실험 연구는 경제적 관점에서도 흥미로운 시사점을 제공하는데요. 너무 많은 선택지는 소비자들의 구매 의욕을 떨어뜨릴 수 있다는 겁니다. 결정에 대한 피로가 따르기 때문이에요. 기업 입장에서는 너무 많은 선택지보다, 오히려 적절한 수의 선택지를 선보이는 것이 더 나을 수 있습니다. 결정해야 하는 피로도를 줄여주니 소비자의 만족도는 높아지고 더 나은 구매 경험을 제공할 수 있는 것이죠.\n예를 들어, 어떤 레스토랑의 메뉴판에 100개의 요리가 적혀 있다 가정해보죠. 아마 고객들은 어떤 음식을 주문해야 할지 혼란스러워 하고 최종 선택에 만족하지 못할 가능성이 높습니다. 이 레스토랑은 우선 음식 개수를 줄여야 하고요. 가장 인기 있는 10개 정도의 요리 중심으로 메뉴를 구성하거나, 계절별 특별 메뉴 혹은 오늘의 추천 메뉴를 제공하여 고객의 선택을 단순화할 수 있겠죠.\n우리에게 친숙한 스트리밍 서비스도 비슷합니다. 수천 개의 영화와 TV 프로그램을 제공하는 스트리밍 서비스를 구독하면서, ‘어떤 걸 봐야하나…’ 고르는 데만 상당한 시간을 쓴 경험 한 번쯤은 있을 것입니다.\nG: 저요! 메인 화면에서 이리 돌리고 저리 돌리면서 뭐 볼지 고민하다가 결국 꺼버리게 돼요.\nK: 너무 많은 콘텐츠는 무엇을 볼지 결정하는 데 어려움을 겪게 합니다. 그리고 힘들게 고른 콘텐츠가 기대했던 것보다 별로일 때 불만족을 느낄 가능성이 높고, 결국엔 에디터 님처럼 아무것도 보지 않게 될 수도 있고요.\n그래서 개인화된 추천 알고리즘이 사용되는 건데요. 고객이 선호할 만한 콘텐츠를 얼마나 잘 추천하는지, 인기 콘텐츠와 평가 좋은 콘텐츠를 얼마나 잘 강조해서 보여주는지에 따라 스트리밍 서비스를 이용하는 고객들의 만족도가 크게 달라질 수 있습니다.\nG: 혹시 ‘선택의 과부하’가 정부 정책과도 연결되는 부분이 있을까요?\nK: 물론입니다. 스웨덴은 2000년에 새로운 퇴직연금제도를 시행했는데요. 처음 시작할 땐 자유롭게 자신들이 원하는 펀드들로 연금 포트폴리오 구성을 선택할 수 있도록 했습니다. 제도 시행 초기에 가입자들이 선택할 수 있는 펀드의 개수는 무려 456개에 달했어요.\n물론 펀드를 직접 고르고 싶지 않은 사람들을 위한 기본 설정(default) 펀드도 있었지만, 스웨덴 정부는 적극적인 광고 캠페인을 통해 사람들이 기본설정 펀드보다는 주체적으로 자신만의 포트폴리오를 선택하도록 유도했습니다.\n그 결과, 전체 가입자의 3분의 2가 포트폴리오를 직접 선택하는 것으로 결정했고, 나머지 3분의 1은 기본 설정 펀드를 선택했어요. 자, 결과는 어땠을까요?\nG: 지금까지 살펴본 바로는… 포트폴리오를 직접 선택한 사람들의 성과가 더 안 좋을 것 같아요. 나중에 본인 선택을 후회하거나요.\nK: 맞아요. 포트폴리오를 직접 선택하기로 결정한 사람들은 수많은 펀드 중 ‘예상 수익은 비교적 낮은데 위험은 큰 펀드’를 선택하는 경향이 높은 것으로 나타났습니다. 이들이 직접 선택한 포트폴리오의 수익률은 기본설정 펀드에 비해 낮았고, 그 격차는 점점 벌어졌어요.\n이후 스웨덴 퇴직연금 가입자들이 선택할 수 있는 펀드의 개수는 900개 정도로 늘어났는데요. 이렇게 많은 펀드 중 뭐가 좋을지 직접 선택해야 한다고 상상만 해도 머리가 어지럽지 않으세요?\nG: 네… 지금 우리나라에 있는 연금 상품들도 언제 공부하고 선택하나 싶은데, 어질어질하네요.\nK: 이 사례도 많은 선택지가 반드시 바람직한 결과를 보장하지는 않다는 것을 말해줍니다.\n수백 개에 달하는 펀드 중 직접 선택하도록 하는 것보다는, 수수료가 상대적으로 낮고 위험이 적절하게 분산된 기본설정 펀드를 선택하는 쪽으로 ‘넛지(nudge)’하는 것이 차라리 나은 선택일 수 있었던 것이죠. 정부가 정책을 만들 때, 과도하게 많은 선택지가 주는 부정적 효과를 함께 고려해야만 하는 이유입니다.\n오늘은 선택지가 많은 상황과 적은 상황에 대한 실험 결과들과 실생활 사례들을 살펴봤어요. 선택지가 많다고 무조건 좋은 것은 아니고, 너무 많은 선택은 오히려 우리를 불행하게 만들 수 있다는 사실. 이해하셨지요?\n여러분은 일상 생활에서 선택지가 늘어나서 행복했던 경험이 많나요, 아니면 오히려 더 힘들었던 경험이 많나요? 쇼핑을 하거나 식당에서 메뉴를 고를 때, 오늘 소개해드린 연구를 한 번쯤 떠올려 보시면 좋을 것 같네요.\n\n.css-13d8cj1{font-size:17px;line-height:1.6;word-break:keep-all;letter-spacing:0em;margin:24px 0 8px;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:var(--adaptiveGrey700);}\n.css-1dzrkjz{width:16px;margin-right:8px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}\n.svg-icon-wrapper{position:relative;display:inline-block;width:24px;height:24px;}.svg-icon-wrapper >.svg-icon:empty+.svg-icon-fallback{visibility:visible;z-index:inherit;}.svg-icon{color:var(--adaptiveGrey900);display:inline-block;width:24px;height:24px;display:block;width:100%;height:100%;}.svg-icon svg,.svg-icon img{display:block;width:100%;height:100%;}.svg-icon--hide{display:none;}.svg-icon-fallback{position:absolute;left:0;right:0;top:0;z-index:z-index(hidden);visibility:hidden;display:block;width:100%;height:100%;}.svg-icon-fallback--show{visibility:visible;z-index:inherit;}\n참고자료\n.css-1ifza5r{border:0;margin-top:0;margin-bottom:0;height:1px;opacity:1;background:var(--tHairlineBackground);margin:0;}\nEdit 금혜원 Graphic 조수희 이동건",
        "content": "사실 사람들은 너무 많은 선택지가 주어질 때 오히려 스트레스를 받는대요. 왜일까요?",
        "contentSnippet": "사실 사람들은 너무 많은 선택지가 주어질 때 오히려 스트레스를 받는대요. 왜일까요?",
        "guid": "https://blog.toss.im/article/everyday-economics-16-too-many-options",
        "isoDate": "2024-06-25T11:24:00.000Z"
      },
      {
        "title": "환전 없이 토스로 해외 결제 하는 법",
        "link": "https://blog.toss.im/article/tosspay-offline-pay",
        "pubDate": "Mon, 24 Jun 2024 06:15:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-of5acw{margin:24px 0 8px;text-align:center;}.css-oni540{position:relative;width:100%;height:0;padding-bottom:56.25%;}\n.css-122y91a{position:absolute;top:0;left:0;width:100%;height:100%;}\n.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}\n.css-1kxrhf3{white-space:pre-wrap;}일본 여행 가면 동전으로 계산하는 것도 어렵고, 환전한 돈 부족할까 봐 신경쓰였을 텐데요. 토스페이 해외결제 서비스를 이용하면 일본에서도 한국에서처럼 결제할 수 있어요.\n.css-1feg9au{font-size:24px;letter-spacing:0em;line-height:1.6;font-weight:bold;color:var(--adaptiveGrey900);margin:24px 0 4px;}\n토스 해외 결제 사용 방법 \n1. 홈 화면 하단에 ‘토스페이’를 선택한 뒤 오른쪽 상단의 ‘현장결제’를 눌러주세요. \n.css-1pgssrp{max-width:100%;border-radius:16px;}.css-wgpbp3{display:block;margin-top:6px;}.css-18442ym{font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}.css-jfs1hr{white-space:pre-wrap;font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}토스 해외 결제 하는 법\n2. 해외 결제를 위한 동의를 진행하면 해외 결제 준비가 모두 끝나요. \n\n3. 결제하려는 나라를 눌러 선택하면 해외에서도 바로 결제할 수 있는 결제 바코드와 QR코드가 만들어지는데요. 편의점, 식당 등 알리페이+ 로고가 있는 모든 곳에서 바로 결제할 수 있어요.\n\n‘토스페이 해외결제’ 서비스를 이용하면 QR스캔으로 바로 결제할 수 있고, 원화로 결제되기 때문에 별도의 환전 절차도 필요 없어요.  현재 말레이시아, 일본, 싱가포르, 중국 등 총 42개 나라에서 토스페이로 결제할 수 있어요.",
        "content": "42개 나라의 알리페이+ 가맹점에서 사용할 수 있어요.",
        "contentSnippet": "42개 나라의 알리페이+ 가맹점에서 사용할 수 있어요.",
        "guid": "https://blog.toss.im/article/tosspay-offline-pay",
        "isoDate": "2024-06-24T06:15:00.000Z"
      },
      {
        "title": "고령 운전자 안전운전 교육부터 치매 안심 상담까지 ",
        "link": "https://blog.toss.im/article/money-policies-14",
        "pubDate": "Fri, 21 Jun 2024 01:47:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}.css-1kxrhf3{white-space:pre-wrap;}고령 시대를 맞이한 한국, 어르신을 위한 다양한 정책적 지원을 마련하고 있어요. 크게 일자리 정책과 돌봄 정책으로 나눠 소개합니다.\n.css-1feg9au{font-size:24px;letter-spacing:0em;line-height:1.6;font-weight:bold;color:var(--adaptiveGrey900);margin:24px 0 4px;}\n.css-q3ktjb{white-space:pre-wrap;font-weight:bold;}활기찬 노후 생활을 위한 일자리 정책\n⓵ 신중년 경력형 일자리\n50세 이상 70세 미만, 경력과 자격이 있지만 현재는 미취업 상태인 신중년을 위한 재취업 지원 제도예요. \n.css-16cuouw{white-space:pre-wrap;color:var(--adaptiveGrey800);background-color:#3fd59936;}최저임금 이상의 임금(주휴 수당, 연차수당 포함)과 4대 보험 가입을 지원해요. 단 자치단체별로 일자리 사업의 세부 계획과 근로 시간, 임금 수준 등이 다를 수 있어요. 고용노동부 (1350, moel.go.kr)에 문의해주세요.\n⓶ 노인 일자리 및 사회활동 지원\n어르신들이 경력과 능력에 맞는 일자리 혹은 사회 활동에 참여할 수 있도록 지원해요. \n(1) 노노케어, 취약계층 지원봉사 등 노인이 지역 사회에 기여하고 성취감을 얻을 수 있는 ‘공익 활동' \n(2) 취약계층 돌봄, 안전 등 공공 서비스를 제공하는 ‘사회 서비스형' 일자리 \n(3) 민간 기업이나 시장형사업단 등에 취업을 알선하거나 인턴 기회를 제공하는 ‘민간형’ 으로 구분돼요. \n\n주소지 행정복지센터나 노인복지관, 시니어클럽, 대한노인회 지회 등에서 신청할 수 있어요.\n⓷ 고령 운전자 교통 안전 교육\n일자리와 직접 관련은 없지만, 65세 이상 운전자가 운전에 필요한 인지 능력을 가지고 있는지 검사하고 안전 교육을 받을 수 있는 프로그램이에요. \n65세 이상 운전자는 모두 참여할 수 있고, 75세 이상 운전자 중 면허 갱신 대상자는 의무적으로 안전교육을 받아야 해요. 도로교통공단 통합민원실(1577-1120) 또는 경찰청 콜센터(182)를 통해 자세한 내용을 확인해보세요.\n건강한 노후를 위한 돌봄 정책\n⓸ 노인 장기요양 시설 및 재가 서비스\n65세 이상이거나 노인성 질병을 앓아 장기요양등급 판정을 받았다면 맞춤형 돌봄 서비스를 받을 수 있어요. \n노인요양시설에서 지내는 경우 장기요양 급여비용의 20%는 본인이 부담하고, 가정에 머물면서 방문 요양, 목욕, 간호 지원 등을 받는 수급자는 15%를 본인이 부담해요. 기초생활수급자나 저소득층은 본인부담금을 경감 또는 면제 받을 수 있어요. 국민건강보험공단을 통해 신청할 수 있습니다.\n⓹ 치매 안심센터 & 치매상담 콜센터\n전국 256개 치매 안심센터에서는 예방, 상담, 조기 진단, 교육까지 치매에 대한 두려움을 해소하고, 보호와 관리에 관한 정보를 얻을 수 있어요. \n치매 상태에 따른 인지 및 신체 활동 프로그램을 운영하고, 치료 관리비, 기저귀, 물티슈 등을 제공해요. 치매노인이 성년후견제도를 이용할 수 있도록 지원하기도 해요. \n또 치매상담콜센터(1899-9988)에서는 전문상담사로부터 전화 상담을 받을 수 있어요.\n.css-1ifza5r{border:0;margin-top:0;margin-bottom:0;height:1px;opacity:1;background:var(--tHairlineBackground);margin:0;}\nEdit 정경화 Graphic 조수희",
        "content": "100세 시대를 준비하는 실버 정책",
        "contentSnippet": "100세 시대를 준비하는 실버 정책",
        "guid": "https://blog.toss.im/article/money-policies-14",
        "isoDate": "2024-06-21T01:47:00.000Z"
      },
      {
        "title": "모디노믹스, 경제 대국 인도로 향할 수 있을까",
        "link": "https://blog.toss.im/article/economic-terms-18-modinomics",
        "pubDate": "Thu, 20 Jun 2024 02:00:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-of5acw{margin:24px 0 8px;text-align:center;}.css-8atqhb{width:100%;}.css-nv7vyi{margin:24px 0 8px;padding:16px 40px 32px;border-radius:16px;background-color:var(--adaptiveGrey100);}.css-123co55{font-size:19px;letter-spacing:0em;line-height:1.6;margin:24px 0 0;font-weight:400;color:var(--adaptiveGrey900);background-color:transparent;}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}.css-q3ktjb{white-space:pre-wrap;font-weight:bold;}💡 이 글이 필요한 순간\n.css-uswsmm{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;margin:24px 0 8px;padding:0;list-style:none;}.css-uswsmm ul,.css-uswsmm ol{margin:16px 0 0;}.css-uswsmm>li{margin-bottom:16px;padding-left:24px;}.css-uswsmm>li:last-of-type{margin-bottom:0;}.css-uswsmm>li>span{position:relative;}.css-uswsmm>li>span>:first-child::before{content:'•';font-weight:500;color:var(--adaptiveGrey800);position:absolute;left:-24px;}\n.css-1hwiibq{font-size:17px;line-height:1.6;word-break:keep-all;letter-spacing:0em;font-weight:400;color:var(--adaptiveGrey800);}\n.css-1kxrhf3{white-space:pre-wrap;}급성장중인 인도 경제 현황에 대해 좀더 자세히 알고 싶을 때\n모디노믹스가 우리나라에 어떤 영향을 주는지 궁금할 때\n\n.css-1c1qox8{font-size:30px;letter-spacing:0em;line-height:1.55;font-weight:bold;color:var(--adaptiveGrey900);margin:40px 0 4px;}\n.css-p4abj2{display:contents;line-height:1.55;}🔖 이번 주 경제 용어\n모디노믹스\n.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n이번 주 경제 용어는 글로벌 경제를 파악하기 위해 필요한 정보예요.\n\n.css-1pgssrp{max-width:100%;border-radius:16px;}\n인도 총리 나렌드라 모디가 추진하는 경제정책이에요.\n\n\n인도 경제는 최근 몇 년간 눈에 띄는 성장을 이루며 글로벌 시장에서 주목받고 있습니다. 14억 인구의 강력한 내수 시장과 젊은 노동 인구, 그리고 강화된 제조 및 서비스 산업 덕분에 인도는 2022년 1분기에 세계 GDP 순위에서 영국을 제치고 5위를 차지했어요.\n전문가들은 인도가 몇 년 내로 일본을 추월하여 세계 4위 경제 대국이 될 것으로 보고 있으며, 이미 2023년에는 인구 수에서 중국을 추월하여 세계 1위가 되었습니다. 이러한 인구 구조적 장점을 바탕으로 인도는 '세계의 공장'으로서의 위치를 확고히 할 것으로 예상됩니다.\n.css-16cuouw{white-space:pre-wrap;color:var(--adaptiveGrey800);background-color:#3fd59936;}인도는 나렌드라 모디 총리의 리더십 하에 인도는 글로벌 제조업 허브로 변모하고자 하는 야심찬 계획을 추진해 왔는데요. 테슬라, 애플, 현대자동차와 같은 세계적인 기업들을 유치하기 위한 노력이 계속되고 있습니다. 이러한 기업들은 인도의 방대한 인구와 성장하는 소비 시장을 고려하여 대규모 투자를 검토하고 있어요.\n이 외에도 모디 총리는 인도의 인프라를 개선하기 위해 '스마트 시티(Smart City)' 정책을 추진했고, 농산물 유통 체계를 개선하고 농업 기술 개발을 지원하는 등 농업 개혁을 실시했습니다. 인도의 디지털 경제를 육성하기 위해 '디지털 인디아(Digital India)' 정책도 추진하고 있고요.\n이러한 정책들은 모두 '모디노믹스 (Modinomics)' 라는 용어로 집약됩니다. 나렌드라 모디(Narendra Modi)이름과 '경제학(Economics)'을 결합한 말로 인도 경제의 새로운 방향을 상징하고 있습니다.\n\n\n.css-2yhypk{white-space:pre-wrap;cursor:pointer;color:var(--adaptiveGrey600);font-style:italic;-webkit-text-decoration:underline!important;text-decoration:underline!important;}인도 총선, 여권 연합 승리 확정… 모디 총리 3연임 가닥\n(조선비즈 2024.06.05)\n나렌드라 모디 인도 총리가 4일(현지시각) 여당 연합인 국민민주연합(NDA)의 총선 승리를 선언하며 자축했다.\n이날 로이터통신에 따르면 모디 총리는 “국민들의 열망을 이루기 위해 우리는 새로운 에너지, 새로운 열정, 새로운 결의로 전진할 것임을 확신한다”고 말했다. 이어 “모든 노동자들의 헌신과 지칠 줄 모르는 노력에 대해 진심으로 감사와 축하를 표한다”고 했다.\n모디 총리는 소셜미디어 엑스(X)에도 “국민들이 3회 연속으로 여권 연합인 NDA를 믿어 줬다”며 “이는 인도 역사상 가장 역사적인 위업”이라고 평가했다. 인도 선거관리위원회에 따르면 NDA는 총 543개 의석 중 최소 291석을 확보해 과반을 차지했다.\n모디 총리의 총리직 3연임도 가시화됐다. 인도국민당(BJP) 대변인은 “NDA는 3기 정부를 구성하게 된다. 모디 총리가 3번째로 취임한다”고 밝혔다. 인도 정치사에서 3연임을 달성한 총리는 자와할랄 네루 초대 총리 이래 최초다.\n당초 현지에서는 여당 연합이 과반 의석을 차지하더라도 모디 총리의 소속 정당 BJP의 성적이 저조하면 총리직에 대한 입지가 흔들릴 것이라는 관측이 나왔다. 그러나 NDA는 BJP가 단독 과반을 얻지 못할 것이라는 전망 속에서도 모디 총리에 대한 지지를 표명하며 가능성을 일축했다.(중략)\n\n\n인도는 의원내각제 국가로 상하원 선거를 진행합니다. 인도 하원 의원은 임기가 5년이며 국민의 직접 선거를 통해 선출돼요. 하원 의석수는 총 543석이고, 올해 4월 19일부터 6월 1일까지 진행된 인도 총선이 바로 하원 의원을 뽑는 선거였습니다. 하원 의석 다수를 차지한 정당이 집권하며, 집권당 당수가 총리로 선출되는 방식이에요.\n6월 4일에 발표된 인도 총선 결과는 많은 이들에게 놀라움을 주었습니다. 여당 연합인 국민민주연합(NDA)가 승리했으나, 나렌드라 모디 총리가 이끄는 인도국민당(BJP)은 240석 획득에 그쳐 2014년 집권 이후 처음으로 단독 과반 확보에 실패했기 때문입니다.\n이는 모디 총리에게 세 번째 임기 동안 모디노믹스를 강력하게 추진하는 데 어려움을 줄 수 있습니다. 선거 결과에 영향을 준 요인으로는 ➀무슬림 커뮤니티의 반발 격화, ➁경제 성장에 따른 부의 격차 확대, ➂치솟은 물가 등이 꼽힙니다.\n다만 현 총리인 모디 총리가 압도적인 표차로 이긴 게 아니라서, 인도 증시에 일시적인 불안정성을 가져왔습니다. 하지만 장기적인 관점에서 볼 때 투자의 좋은 기회가 될 수도 있습니다. 모디 총리의 경제 정책인 '모디노믹스'는 여전히 제조업 부흥을 목표로 하고 있고, 이번 선거 결과가 그 방향성에는 크게 영향을 주지 않았다는 평가를 받고 있거든요.\n또한, 모디 정부는 인프라 투자를 포함한 여러 경제 활성화 조치를 예고하고 있는데요. 이는 국민의 소비력을 높이고 시장을 활성화하는 데 기여할 것으로 기대되고 있답니다. 이러한 조치들은 인도 증시에 장기적인 투자 기회를 제공할 수 있음을 시사하고 있어요.\n\n\n레이거노믹스(Reaganomics): 미국의 제 40대 대통령인 로널드 레이건이 추진한 경제 정책. .css-iynyr0{white-space:pre-wrap;cursor:pointer;color:var(--adaptiveGrey600);-webkit-text-decoration:underline!important;text-decoration:underline!important;}세출의 삭감, 소득세의 대폭 감세, 기업에 대한 정부 규제의 완화, 안정적인 금융 정책 등이 핵심이라 볼 수 있어요.\n대처리즘(Thatcherism): 영국의 전 총리 마거릿 대처가 추진한 경제 정책. 자유시장 경제를 중시하고 정부의 개입을 최소화하는 것이 특징이에요.\n아베노믹스(Abenomics): 일본의 전 총리 아베 신조가 추진한 경제 정책. 20년 가까이 이어져 온 디플레이션과 엔고(円高) 탈출을 위해, 대규모 양적 완화와 재정지출 확대, 구조 개혁 등을 시행했어요.\n.css-1ifza5r{border:0;margin-top:0;margin-bottom:0;height:1px;opacity:1;background:var(--tHairlineBackground);margin:0;}\nEdit 금혜원 Graphic 조수희 이동건",
        "content": "인도 경제는 최근 몇 년간 눈에 띄는 성장을 이루며 글로벌 시장에서 주목받고 있어요.",
        "contentSnippet": "인도 경제는 최근 몇 년간 눈에 띄는 성장을 이루며 글로벌 시장에서 주목받고 있어요.",
        "guid": "https://blog.toss.im/article/economic-terms-18-modinomics",
        "isoDate": "2024-06-20T02:00:00.000Z"
      }
    ]
  },
  {
    "name": "모나미",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김진홍",
    "category": "개인",
    "posts": []
  }
]