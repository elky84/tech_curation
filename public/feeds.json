[
  {
    "name": "ㅍㅍㅅㅅ",
    "category": "큐레이팅",
    "posts": []
  },
  {
    "name": "C++ Team Blog",
    "category": "기업",
    "posts": [
      {
        "creator": "Mryam Girmay",
        "title": "Prevent Critical Bugs with MSVC Code Analysis",
        "link": "https://devblogs.microsoft.com/cppblog/prevent-critical-bugs-with-msvc-code-analysis/",
        "pubDate": "Thu, 22 Aug 2024 18:12:24 +0000",
        "content:encodedSnippet": "Imagine this: You’re deep into a complex C++ project, and everything seems to be running smoothly. But then, out of nowhere, a critical bug surfaces— one that requires a bit more foresight. We’ve all been there, right? This is where code analysis steps in as your silent guardian. \nCode analysis is a great tool for catching those elusive bugs and ensuring your code adheres to the best programming practices. It identifies defects that are difficult to discover through testing by searching for specific code patterns known to cause problems.  \n The analysis results are displayed in the Visual Studio Error List window and as squiggles in the editor. This feature checks for problematic code patterns, such as buffer overruns caused by converting an element count into a byte count and null pointer dereferences, even if the code  looks correct. In this blog, we will focus on MSVC Code Analysis, which is one of the different types of code analysis available in Visual Studio for C++. \nWhere MSVC Code Analysis Shines \nIn 2014, the tech world was shaken by the discovery of the Heartbleed bug in OpenSSL. This critical vulnerability, caused by a missing bounds check, allowed attackers to exploit the TLS heartbeat extension and read sensitive data from server memory, including private keys, usernames, and passwords. The fallout was massive, affecting millions of users and causing widespread panic. \nNow, picture yourself as a C++ developer working on a high-stakes project. You know that even a small mistake can lead to significant security vulnerabilities, just like Heartbleed. This is where MSVC Code Analysis becomes your best ally. \nMSVC Code Analysis is a static analysis tool that checks your code for errors, potential improvements, and adherence to coding best practices when using the Microsoft Visual C++ (MSVC) compiler. For example, failing to initialize a pointer (e.g., int* uninitializedPtr;) in your project can result in unpredictable behavior, crashes, and security vulnerabilities. Consider the following scenario: You declare a pointer and initialize it to nullptr (int* imageData = nullptr;). Later, you attempt to allocate memory for the pointer based on uninitialized width and height variables (imageData = new int[width * height];). This can lead to undefined behavior because width and height are not initialized before use. If the pointer is used before being properly assigned, it can lead to accessing uninitialized memory, which Rule C6001 identifies, helping you catch these issues before they become critical problems. The following sample generates ‘Using uninitialized memory’ warning: \n#include <iostream> \r\n\r\n#include <stdexcept> \r\n\r\n class ImageProcessor { \r\n\r\npublic: \r\n\r\n    void processImage() { \r\n\r\n        int width, height; \r\n\r\n        int* imageData = nullptr; \r\n\r\n        try { \r\n\r\n            // Attempt to allocate memory based on width and height \r\n\r\n            imageData = new int[width * height]; // Uninitialized width and height \r\n\r\n            // Process the image data (this will cause undefined behavior) \r\n\r\n            for (int i = 0; i < width * height; ++i) { \r\n\r\n                imageData[i] = i; // Potentially accessing uninitialized memory \r\n\r\n            } \r\n\r\n            // Simulate further processing \r\n\r\n            std::cout << \"Image processed successfully.\" << std::endl; \r\n\r\n        } \r\n\r\n        catch (const std::bad_alloc& e) { \r\n\r\n            std::cerr << \"Memory allocation failed: \" << e.what() << std::endl; \r\n\r\n        } \r\n\r\n        // Clean up allocated memory \r\n\r\n        delete[] imageData; \r\n\r\n    } \r\n\r\n}; \nNow, let’s use this example to understand the different ways to invoke code analysis in Visual Studio.\nBackground Code Analysis  \nThis integral feature of Visual Studio functions as a real-time code analysis tool. This tool is particularly beneficial for you because it: \nProvides immediate feedback on potential issues, aiding in early problem resolution. \nFocuses on the files that are currently open in the editor, streamlining analysis during active development.  \nBackground code analysis will automatically run after you open or save the file. The warning for uninitialized memory will be enabled by default, displaying a green squiggle in the editor and appearing in the error list, as demonstrated in the example below. \n\nThis feature is enabled by default; however, you can double check the setting by navigating to Tools > Options > Text Editor > C/C++ > Advanced > Code Analysis > Disable Background Code Analysis: False.\nManually Running Code Analysis \nIn addition to background code analysis, you can also manually run code analysis as needed. You can start by clearing all warnings in the current file you’re working on, then invoke Code Analysis for the current project. Periodically, run Code Analysis for the entire solution to maintain overall code quality. You can also manually run code analysis for individual files. There are several ways to manually run a code analysis. Following any of these steps will display the ‘Using uninitialized memory’ warning in the error list window for our example. \n1. Menu-bar: \nOpen the project.\nFrom the menu-bar, select Build -> Run Code Analysis on [Solution | Project Name | File] \nFrom the menu-bar, select Analyze -> Run Code Analysis -> [On Solution | Run Code Analysis on ‘project name’ | Run Code Analysis on File]\n\n2. Keyboard Shortcut: \nFor a single file: Ctrl + Shift + Alt + F7 \nLearn more about the different ways to run code analysis manually in Run code analysis. \nEnable Code Analysis on Build \nThis setup ensures that code analysis runs automatically every time you build your project or compile a single file. Think of code analysis as an inspector who checks your project. While this inspector might take more time to examine everything, catching potential issues early provides peace of mind. This additional time you invest   is necessary to ensure your code is secure. To enable it: \n Open Project Properties in Visual Studio.\nNavigate to Configuration Properties > Code Analysis > General. \nSelect the Yes option for Enable Code Analysis on Build. Note that this option is disabled by default, whereas Microsoft Code Analysis is enabled by default.\n\nTo view all default selected rules, including the rule for detecting uninitialized memory, navigate to Configuration Properties > Code Analysis > Microsoft, and then click Configure.   \n\nAfter enabling code analysis on build, building the example code will cause Visual Studio to generate a warning for the lines imageData = new int[width * height];, indicating that width and height are uninitialized. Background code analysis focuses on the files you’re actively working on, while build-time analysis ensures all project files are checked, catching any missed issues that aren’t in the current file. This warning will appear in the Error List window. \n\nKey Events in Microsoft C++ Code Analysis help you quickly identify and fix defects by providing detailed information in selected warnings from the Error List. They trace code flow to pinpoint root causes, making it easier to understand issues like variable initialization or branching. For example, double-clicking the C6001 ‘Using uninitialized memory: width’ warning in the Error List opens a new window showing the Key Events. For further insights, please refer to the Microsoft C++ Code Analysis Warnings with Key Events blog. \n\nCode Analysis Rules and Rulesets  \nRulesets in Visual Studio for C++ are collections of code analysis rules that ensure code quality and adherence to standards. For example, enabling the MSVC rule C26440, ‘Function can be declared ‘noexcept’,’ suggests marking functions with ‘noexcept’ if they do not throw exceptions. This can improve both performance and reliability. \nTo create a new custom rule set with the “Function can be declared ‘noexcept’” rule added, follow these steps: \nOpen Project Properties in Visual Studio \nNavigate to Configuration Properties > Code Analysis > Microsoft. \nIn the Active rules section, click “Configure”  \nSelect the check box for the rule that you want to include in the ruleset. The Action will automatically change from ‘None’ to ‘Warning’. You can change the severity of this rule based on your needs, with options like Error, Info, Hidden, None, or <Inherit>.  \nSave the rule set with a new file name. The custom rule set is now automatically assigned to the project. \n\nTo learn more about using rule sets in depth, refer to the Use Rule Sets to Specify the C++ Rules to Run article.  \nAdditional Tools and Techniques\nVisual Studio offers several features that can enhance code quality and prevent issues like the Heartbleed bug. Here are some tools you might find useful: \nClang-Tidy Code Analysis \nClang-Tidy, a tool used with the LLVM/clang-cl compiler, is designed to modernize your code, ensure adherence to standards, perform static analysis, and automatically format your code. When using an MSVC toolset, you can configure Clang-Tidy to complement or replace the conventional Code Analysis process. This helps catch different types of issues and improves overall code quality. You can find more details in Using Clang-Tidy in Visual Studio article. \nSuppress Specific Warnings \nSuppressing specific warnings in C++ involves configuring your project settings to ignore certain compiler warnings for a single line, section of code, file, or entire project. This can be done at the project level or for individual files using Visual Studio’s project properties or ‘#pragma warning’ directives. By suppressing less relevant warnings, you can focus on the most critical issues, making the build output cleaner and easier to read, which simplifies identifying and addressing significant problems. To dive deeper into this topic, check out the Suppress compiler warnings article. \nIt’s important to note that Code Analysis tools may occasionally generate false positives. If you encounter a false positive, please report it through the Visual Studio Developer Community channel with detailed repro code and information. This helps us to improve the accuracy of Code Analysis tools and ensures a smoother development experience. \nLearn More \nTo learn more about securing your C++ programs, visit the Build Reliable and Secure C++ programs blog. For the latest updates to the MSVC backend, check out the MSVC Backend Updates in Visual Studio 2022 version 17.10 blog.  \nYour feedback is invaluable in helping us enhance the MSVC Code Analysis experience. Please share your suggestions in the comments below or through the Developer Community. You can also reach us via email at visualcpp@microsoft.com or via X at @VisualC. \nThe post Prevent Critical Bugs with MSVC Code Analysis appeared first on C++ Team Blog.",
        "dc:creator": "Mryam Girmay",
        "comments": "https://devblogs.microsoft.com/cppblog/prevent-critical-bugs-with-msvc-code-analysis/#comments",
        "content": "<p>Imagine this: You’re deep into a complex C++ project, and everything seems to be running smoothly. But then, out of nowhere, a critical bug surfaces— one that requires a bit more foresight. We’ve all been there, right? This is where code analysis steps in as your silent guardian. </p>\n<p>The post <a href=\"https://devblogs.microsoft.com/cppblog/prevent-critical-bugs-with-msvc-code-analysis/\">Prevent Critical Bugs with MSVC Code Analysis</a> appeared first on <a href=\"https://devblogs.microsoft.com/cppblog\">C++ Team Blog</a>.</p>\n",
        "contentSnippet": "Imagine this: You’re deep into a complex C++ project, and everything seems to be running smoothly. But then, out of nowhere, a critical bug surfaces— one that requires a bit more foresight. We’ve all been there, right? This is where code analysis steps in as your silent guardian. \nThe post Prevent Critical Bugs with MSVC Code Analysis appeared first on C++ Team Blog.",
        "guid": "https://devblogs.microsoft.com/cppblog/?p=34525",
        "categories": [
          "C++"
        ],
        "isoDate": "2024-08-22T18:12:24.000Z"
      }
    ]
  },
  {
    "name": "Facebook Engineering",
    "category": "기업",
    "posts": [
      {
        "creator": "",
        "title": "Meta is getting ready for post-quantum cryptography",
        "link": "https://engineering.fb.com/2024/08/28/security/post-quantum-cryptography-meta/",
        "pubDate": "Wed, 28 Aug 2024 16:19:35 +0000",
        "content:encodedSnippet": "The Quantum Apocalypse is coming. The advent of quantum computers has raised real questions about the future of data privacy over the internet.  Someday, advances in quantum computing will make it possible to decrypt sensitive data that was encrypted using today’s complex cryptography systems.\nIn the latest episode of the Meta Tech Podcast you’ll meet Sheran and Rafael, two engineers leading Meta’s post-quantum readiness work. They sit down with Pascal Hartig (@passy) to discuss the threat of quantum computing and how Meta is working to keep today’s users safe from the quantum attacks of tomorrow.\nAnd for more on post-quantum readiness at Meta be sure to read their blog, Post-quantum readiness for TLS at Meta.\nDownload or listen to the podcast episode below:\n\nSpotify\nApple Podcasts\nPocketCasts\nOvercast\nThe Meta Tech Podcast is a podcast, brought to you by Meta, where we highlight the work Meta’s engineers are doing at every level – from low-level frameworks to end-user features.\nSend us feedback on Instagram, Threads, or X.\nAnd if you’re interested in learning more about career opportunities at Meta visit the Meta Careers page.\nThe post Meta is getting ready for post-quantum cryptography appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>The Quantum Apocalypse is coming. The advent of quantum computers has raised real questions about the future of data privacy over the internet.  Someday, advances in quantum computing will make it possible to decrypt sensitive data that was encrypted using today&#8217;s complex cryptography systems. In the latest episode of the Meta Tech Podcast you’ll meet Sheran [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2024/08/28/security/post-quantum-cryptography-meta/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2024/08/28/security/post-quantum-cryptography-meta/\">Meta is getting ready for post-quantum cryptography</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "The Quantum Apocalypse is coming. The advent of quantum computers has raised real questions about the future of data privacy over the internet.  Someday, advances in quantum computing will make it possible to decrypt sensitive data that was encrypted using today’s complex cryptography systems. In the latest episode of the Meta Tech Podcast you’ll meet Sheran [...]\nRead More...\nThe post Meta is getting ready for post-quantum cryptography appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=21662",
        "categories": [
          "Culture",
          "Security",
          "Meta Tech Podcast"
        ],
        "isoDate": "2024-08-28T16:19:35.000Z"
      },
      {
        "creator": "",
        "title": "How Meta enforces purpose limitation via Privacy Aware Infrastructure at scale",
        "link": "https://engineering.fb.com/2024/08/27/security/privacy-aware-infrastructure-purpose-limitation-meta/",
        "pubDate": "Tue, 27 Aug 2024 16:00:50 +0000",
        "content:encodedSnippet": "At Meta, we’ve been diligently working to incorporate privacy into different systems of our software stack over the past few years. Today, we’re excited to share some cutting-edge technologies that are part of our Privacy Aware Infrastructure (PAI) initiative. These innovations mark a major milestone in our ongoing commitment to honoring user privacy. \nPAI offers efficient and reliable first-class privacy constructs embedded in Meta infrastructure to address complex privacy issues. For example, we built Policy Zones that apply across our infrastructure to address restrictions on data, such as using it only for allowed purposes, providing strong guarantees for limiting the purposes of its processing.\nAs we expanded PAI across Meta, increasing its maturity, we gained valuable insights. Our understanding of the technology evolved, revealing the need for a larger investment than initially planned to create a cohesive ecosystem of libraries, tool suites, integrations, and more. These investments have been crucial in enforcing complex purpose limitation scenarios while ensuring scalability, reliability, and a streamlined developer experience.\nPurpose limitation, a core data protection principle, is about ensuring data is only processed for explicitly stated purposes. A crucial aspect of purpose limitation is managing data as it flows across systems and services. Commonly, purpose limitation can rely on “point checking” controls at the point of data processing. This approach involves using simple if statements in code (“code assets”) or access control mechanisms for datasets (“data assets”) in data systems. However, this approach can be fragile as it requires frequent and exhaustive code audits to ensure the continuous validity of these controls, especially as the codebase evolves. Additionally, access control mechanisms manage permissions for different datasets to reflect various purposes using mechanisms like access control lists (ACLs), which requires the physical separation of data into distinct assets to ensure each maintains a single purpose. When Meta started to address more and larger-scope purpose limitation requirements that crossed dozens of our systems, these point checking controls did not scale.\nAt Meta, millions of data assets are crucial for powering our product ecosystem, optimizing machine learning models for personalized experiences, and ensuring our products are high quality and meet user expectations. Identifying which code branches and data assets require protection is challenging due to complex propagation requirements and permissions models that need constant revision. For example, when a data consumer reads from one data asset (“source”) and stores the output in another (“sink”), point checking controls would require complex orchestration to ensure propagation from sources to sinks, which can become operationally unviable.\n\nTo address this problem, point checking controls can be enhanced by leveraging data flow signals. Data flows can be tracked from the same origin, where relevant data is collected, using various techniques such as static code analysis, logging, and post-query processing. This creates a graph, known as “data lineage,” that tracks the relationships between source and sink data assets. By utilizing data lineage, permissions can be applied to relevant data assets based on these source-to-sink relationships. The combination of point checking and data lineage, while viable at a small scale, leads to significant operational overhead as point checking still requires auditing many individual assets. \nBuilding on these insights, in our latest iteration, we found that the information flow control (IFC) model offers a more durable and sustainable approach by controlling not only data access but also how data is processed and transferred in real-time, rather than relying on point checking or out-of-band audits. Thus, we developed Policy Zones as our IFC-based technology and integrated it across major Meta systems to enhance our purpose limitation capabilities at scale. This effort was later expanded into the Privacy Aware Infrastructure (PAI) initiative, a transformative investment that integrates first-class privacy support into Meta’s infrastructure systems.\nWe believe PAI is the right investment to protect people’s privacy at scale and can effectively enforce purpose limitation requirements.\nWhy invest in Policy Zones?\nThrough our experience deploying purpose limitation solutions over the years, we identified several key themes:\nNeeds\nProblem\nSolution\n\n\nProgrammatic Control: We needed to rely more on programmatic controls instead of point checking human audits to control data flows, and do so in real-time\nTraditional point checking controls, combined with data lineage checks, can detect data transfers within a specific time frame but not in real-time. Addressing these risks requires implementing resource-intensive human audits at access points.\nIn contrast, PAI is designed to check data flows in real-time during code execution, blocking problematic data flows from occurring, facilitated by UX tooling, thus making it more scalable.\n\n\nGranular Flow Control: We needed to maximize the reuse of existing data and business logic on complex infra\nAccess control is easy to roll out when data is separated physically, but poses significant costs, complexity, and limitations when dealing with Meta’s complex infrastructure, where data for different purposes is often processed by shared code.\nPAI solves this by providing precise decision making at the granular level of individual requests, function calls, or data elements, achieving logical data separation at a relatively low compute cost even on complex infrastructures where it’s needed.\n\n\nAdaptable and Extensible Control: We needed to handle ever-evolving requirements, even multiple for the same data assets\nWe are facing a rapidly changing world for privacy. Data use restrictions can vary over time depending on evolving privacy and product requirements. A single data asset or different parts of it might be subject to multiple privacy requirements. While “point checking” can address this to some extent, it struggles to control downstream data flows, even combined with data lineage.\nPAI is designed to check multiple requirements involved in data flows and is highly flexible to adapt to changing requirements.\n\n\n\n \nHow Policy Zones works\nLet’s dive into what Policy Zones is and how we can leverage it to meet purpose limitation requirements. Policy Zones provides a comprehensive mechanism for encapsulating, evaluating, and propagating privacy constraints for data both “in transit” and “at rest,” including transitions between different systems. It conducts runtime evaluation of constraints, context propagation, and is deeply integrated with numerous data and code frameworks (e.g., HHVM, Presto, and Spark), representing a step change in how we approach information flow control.\nTo make the explanation more relatable and bring some levity to a serious topic, we’ll use a simple example: Let’s say a new requirement comes up, where banana data can only be used for the purposes of making smoothies and fruit baskets, but not for making banana bread. For simplicity, this example and the illustration below only demonstrate the first row of the above table. \n\nHow would developers leverage Policy Zones to implement such a requirement?\nFirst, to demarcate relevant data assets, they assign a metadata label (“data annotation,” e.g., BANANA_DATA) to data assets at different granularities. This annotation is associated with the purpose limitation requirement as a set of data flow rules that enable systems to understand the allowed purposes for the data.\n\nWhen annotated data is processed, Policy Zones kicks in and checks whether the data processing is allowed and data can flow downstream. Policy Zones has been built into different Meta systems, including:\nFunction-based systems that load, process, and propagate data through stacks of function calls in different programming languages. Examples include web frontend, middle-tier, and backend services.\nBatch-processing systems that process data rows in batch (mainly via SQL). Examples include real-time and data warehouse systems that power Meta’s AI and analytics workloads.\nLet’s dive deeper into how Policy Zones works for the function-based systems, while the same logic applies to the batch-processing systems as well.\nIn function-based systems, data is passed through parameters, variables, or return values in a stack of function calls. \nLet’s walk through an example: \nA web request, “BananaRequest,” loads annotated data from BananaDB, causing a data flow violation because the intent of the caller is unknown.\nTo remediate the data flow violation, we annotate BananaRequest with the BANANA_DATA label, creating a zone (“Banana Zone”) for the request. \nBehind the scenes at runtime, Policy Zones programmatically checks all data flows against the flow rules based on the context, flagging new data flow violations from BananaRequest to logB and logC. \nWe annotate logB as banana and remove the logging of banana data into logC to cut off the disallowed data flow. \nWith all data flow violations remediated, the zone can be moved from logging mode to enforcement. If a developer adds a write to a sink outside of the zone, it will be blocked automatically.\n\nIn a more complex scenario, a function, “makeBananaSmoothie()” from a web request, “BreakfastRequest” calls another function, “makeBanana().” Besides the previous data flow violations, we need to remediate another data flow violation: makeBanana() returns banana data to makeBananaSmoothie(). This means we can create a “Banana Zone” from the function makeBananaSmoothie() that includes all functions that it calls directly or indirectly.\n\nIn batch-processing systems, data is processed in batches for rows from tables that are annotated as containing relevant data. When a job runs a query (usually SQL-based) to process the data, a zone is created and Policy Zones flags any data flow violations. Remediation options are provided, similar to those for function-based systems. Once all violations have been remediated, the zone can be moved from logging mode to enforcement mode to prevent future data flow violations. Data annotation can be done at various levels of granularity, including table, column, row, or potentially even cell.\nWhen data flows across different systems (e.g., from frontend, to data warehouse, then to AI), Policy Zones ensures that relevant data is annotated correctly and thus continues to be protected according to the requirements. For some systems that don’t have Policy Zones integrated yet, the point checking control is still used to protect the data.\nHow we applied PAI to existing systems at scale\nThe above gives you a glimpse into how the technology is used to roll out a simple use case. However, adopting Policy Zones is a non-trivial task for complex requirements across tens or hundreds of systems. The requirement owner usually collaborates with other engineers who are code and data asset owners across Meta to implement different aspects of that requirement. In some cases, this may involve hundreds or thousands of engineers to complete the implementation and audits. To address this challenge, PAI offers Policy Zone Manager (PZM), a suite of UX tools that helps requirement owners to efficiently enforce privacy requirements using PAI.\nLet’s take a look at how PZM makes it easy for people to satisfy their purpose limitation needs in existing systems, using the above banana requirement as an example. At a high level, the requirement owner carries out the following workflow, facilitated by PZM:\nIdentify relevant assets: This is to identify which source assets need to be purpose limited for the given requirement.\nDiscover relevant data flows: This is to discover the downstream data flows from the source assets in order to integrate Policy Zones at scale.\nRemediate data flow violations: This is to allow people to choose which option to take to remediate data flow violations.\nContinuously enforce and monitor data flows: This is to turn on Policy Zones enforcement and monitor it to prevent new data flow violations. \n\nTo hear more about this process, check out our presentation at the PEPR conference in June 2024.\n\nStep 1 – Identify relevant assets\nFor a given requirement, we check the relevant product entry points (e.g., mobile apps, web requests, and databases) to pinpoint data assets that are collected. These assets may take the form of request parameters, database entries, or event log entries. We use data structures to represent (“schematize”) these data assets and fields, capturing relevant data at various granularities. In the running example, a table in the banana database might contain entirely banana data, a single banana column, or a mix of banana and other fruit data.\nIn addition to manual code inspection, we heavily rely on various techniques such as our scalable ML-based classifier to automatically identify data assets.\nStep 2 – Discover relevant data flows\nFrom a given annotated source, the requirement owner can identify its downstream data flows and sinks (see diagram below). The owner can then decide how to handle these data flows. However, this process can be time consuming when there are many data flows that are one or multiple hops away from the same origin. This often occurs when implementing a new requirement over existing data flows. \n\nAlthough data lineage presents significant operational overhead for point checking mechanisms, it can efficiently identify where to integrate Policy Zones into the codebase. Therefore, we have integrated data lineage into PZM, allowing requirement owners to discover multiple downstream assets from a given source simultaneously. Once the requirement has been fully implemented, we can rely solely on Policy Zones to enforce the requirements.\nStep 3 – Remediate data flow violations\nBy default, the data flow from a source asset to a sink must meet all of the requirements of the source. If not, it’s considered a data flow violation and needs remediation, enforced by Policy Zones programmatically at runtime. There are three main cases to remediate data flow violations (using the running example to help concretize the general cases):\nCase 1: Safe flow – relevant data is used for allowed purpose(s): Assign the banana annotation to the sink asset.\nCase 2: Unsafe flow – relevant data is used for disallowed purpose(s): Block data access and code execution to prevent further processing of banana data.\nCase 3: Reclassified flow – relevant data is not used or propagated: Annotate the data flow as reclassified as being permitted. Banana data from the source is not used or propagated to the sink.\n\nStep 4 – Continuously enforce and monitor data flows\nPAI is integrated into our major data systems to check data flows and catch violations at runtime. During the initial rollout of a new requirement, Policy Zones can be configured to allow remediations of flow violations in “logging mode.” Once Policy Zones enforcement is enabled, any data flow with unremediated violations is denied. This also prevents new data flow violations, even if code changes or new code is added.\nPAI continuously monitors the enforcement of requirements to ensure that it operates correctly. PZM provides a set of verifiers to check the accuracy of asset annotations and control configurations.\nLessons learned from adoption at scale across Meta\nAs PAI has been adopted by a multitude of purpose limitation requirements across Meta, we’ve learned several key lessons over the past few years:\nFocus on solving one specific end-to-end use case first\nInitially, we developed Policy Zones for batch-processing systems with some basic use cases. However, we realized that our designs for function-based systems were quite abstract and the adoption for a large-scale use case resulted in significant challenges, consequently, requiring considerable effort to map patterns to customer needs. Furthermore, refining the APIs and building missing operational support made it work effectively end-to-end across multiple systems. Only after addressing these challenges were we able to make it more generic and proceed with integrating Policy Zones across extensive platforms.\nStreamline integration complexity\nIntegrating PAI into major Meta systems coherently was a complex, lengthy, and challenging process. We encountered significant difficulties in integrating PAI with Meta’s diverse systems broadly. It took us years to overcome these challenges. For example, initially, product teams expended considerable effort to schematize data assets across different data systems. Then we developed reliable, computationally efficient, and widely applicable PAI libraries in various programming languages (Hack, C++, Python, etc.) that enabled a smoother integration with a broad range of Meta’s systems.\nInvest in computational and developer efficiency early on \nWe also undertook multiple iterations to simplify PAI and improve its computational efficiency. Our initial annotation APIs were overly complex, resulting in high cognitive overhead for engineers. Furthermore, the computational overhead of data flow checking was prohibitively high in Meta’s high-throughput systems. Through several rounds of refinement, we simplified policy lattice representation and evaluation, built language-level features to natively propagate Policy Zones context, and canonicalized policy annotation structures, achieving 10x improvements in computational efficiency.\nSimplified and independent annotations are a must to scale to a wide range of requirements\nInitially, we employed a monolithic annotation API to model intricate data flow rules and annotate relevant code and data. However, as data from multiple requirements were combined, propagating these annotations from sources to sinks became increasingly complex, resulting in data annotation conflicts that were difficult to resolve. To address this challenge, we implemented simplified data annotations to decouple data from requirements and separate data flow rules for different requirements. This significantly streamlined the annotation process, ultimately improving developer experiences.\nBuild tools; they are required \nWe have made significant efforts to ensure the use of PAI is easy and efficient, ultimately improving the developer experience. Initially, we focused on the correctness of the technology first before investing in tooling. Adopting Policy Zones required a lot of manual effort, and it was challenging for engineers to understand how to properly annotate their assets, which led to additional cleanup work later. To address this issue, we developed the PZM tool family, which includes built-in automated rules and classifiers. These tools guide teams through standard workflows, ensuring safe and efficient rollout of purpose limitation requirements and reducing engineering efforts by orders of magnitude.\nDurable privacy protection for everyone\nMeta is committed to protecting user privacy. The PAI initiative is a crucial step in safeguarding data and preserving privacy efficiently and reliably. It provides a robust foundation for Meta to sustainably tackle privacy challenges, meet high reliability standards, and address future privacy issues more efficiently than traditional solutions. While we’ve laid a strong groundwork, our journey is just beginning. We aim to build upon this foundation by expanding our capabilities and controls to accommodate a wider range of privacy requirements, enhancing the developer experience, and exploring new frontiers.\nWe hope our work sparks innovation and fosters collaboration across the industry in the field of privacy.\nAcknowledgements\nThe authors would like to acknowledge the contributions of many current and former Meta employees who have played a crucial role in productionizing and adopting PAI over the years. In particular, we would like to extend special thanks to (in alphabetical order) Adrian Zgorzalek, Alex Gorelik, Amritha Raghunath, Anuja Jaiswal, Brian Sniffen, Brian Romanko, Brian Spanton, David Detlefs, David Mortenson, David Taieb, Gabriela Jacques da Silva, Ian Carmichael, Iuliu Rus, Jafar Husain, Jerry Pan, Jiang Wu, Joel Krebs, Jun Fang, Komal Mangtani, Marc Celani, Mark Konetchy, Michael Levin, Perry Stoll, Peter Prelich, Pieter Viljoen, Prashant Dhamdhere, Rajesh Nishtala, Rajkishan Gunasekaran, Rishab Mangla, Sergey Doroshenko, Seth Silverman, Sriguru Chakravarthi, Tarek Sheasha, Thomas Georgiou, Uday Ramesh Savagaonkar, Vitalii Tsybulnyk, Vlad Fedorov, Wolfram Schulte, and Yi Huang. We would also like to express our gratitude to all reviewers of this post, including (in alphabetical order) Aleksandar Ilic, Benjamin Renard, Emil Vazquez, Emile Litvak, Harrison Fisk, Jason Hendrickson, Jessica Retka, Nimish Shah, Sabrina B Ross, and Sam Blatchford. We would like to especially thank Emily DiPietro for championing the idea, leading the editorial effort, and pulling all required support together to make this blog post happen.\nThe post How Meta enforces purpose limitation via Privacy Aware Infrastructure at scale appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>At Meta, we&#8217;ve been diligently working to incorporate privacy into different systems of our software stack over the past few years. Today, we&#8217;re excited to share some cutting-edge technologies that are part of our Privacy Aware Infrastructure (PAI) initiative. These innovations mark a major milestone in our ongoing commitment to honoring user privacy.  PAI offers [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2024/08/27/security/privacy-aware-infrastructure-purpose-limitation-meta/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2024/08/27/security/privacy-aware-infrastructure-purpose-limitation-meta/\">How Meta enforces purpose limitation via Privacy Aware Infrastructure at scale</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "At Meta, we’ve been diligently working to incorporate privacy into different systems of our software stack over the past few years. Today, we’re excited to share some cutting-edge technologies that are part of our Privacy Aware Infrastructure (PAI) initiative. These innovations mark a major milestone in our ongoing commitment to honoring user privacy.  PAI offers [...]\nRead More...\nThe post How Meta enforces purpose limitation via Privacy Aware Infrastructure at scale appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=21618",
        "categories": [
          "Security"
        ],
        "isoDate": "2024-08-27T16:00:50.000Z"
      },
      {
        "creator": "",
        "title": "RETINAS: Real-Time Infrastructure Accounting for Sustainability",
        "link": "https://engineering.fb.com/2024/08/26/data-infrastructure/retinas-real-time-infrastructure-accounting-for-sustainability/",
        "pubDate": "Mon, 26 Aug 2024 16:00:40 +0000",
        "content:encodedSnippet": "We are introducing a new metric— real-time server fleet utilization effectiveness —as part of the RETINAS initiative to help reduce emissions and achieve net zero emissions across our value chain in 2030.\nThis new metric allows us to measure server resource usage (e.g., compute, storage) and efficiency in our large-scale data center server fleet in near real-time.\nWe are sharing our learnings in adopting depreciation methods for accumulated carbon assets for internal fleet measurements, and encourage further industry improvement and development on these concepts. This is not intended to replace global emissions accounting standards for purposes of external reporting.\nSince 2020, Meta has maintained net zero emissions in our operations and matched 100% of our electricity use with renewable energy. However, we know our work doesn’t stop there, and we recognize our responsibility to decarbonize our footprint beyond our data centers and offices, including emissions from the server components our suppliers manufacture to our employees’ commutes. To align with the Paris Agreement, we have set a goal to reach net zero emissions across our value chain in 2030.\nMeta’s Net Zero Program has three foundational pillars: understanding our emissions, reducing our emissions, and removing remaining emissions. To understand our emissions, improving the granularity, accuracy, and near real-time measurement of our greenhouse gas data goes beyond carbon accounting. The right data will help us apply actionable metrics to advance decarbonization across our business operations and with our suppliers.\nWith this in mind, we have created the Real Time Infrastructure Accounting for Sustainability (RETINAS) initiative, which seeks to study and understand the impact of server reliability, performance, and operational optimization on Meta’s Scope 3 emissions.\nThis initiative has led to the development of a new internal metric— real-time server fleet utilization effectiveness —that enables us to take action to reduce the emissions associated with the embodied carbon of our data center servers and components.  Embodied carbon contributes to Meta’s upstream Scope 3 emissions, and includes the emissions associated with the full lifecycle of the manufacturing, assembly, and transportation of servers and materials in our physical infrastructure.\nOptimizing the utilization of our server fleet is important to reducing these emissions. Real-time server fleet utilization effectiveness provides a framework toward effective measurement and integration of embodied carbon into ubiquitous infrastructure metrics to drive informed decisions to manage our server fleet resource usage (e.g., compute and storage) and their impacts on Meta’s Scope 3 emissions. \nHow we measure greenhouse gas emissions at Meta\nSince 2011, Meta has reported our Scope 1 and 2 emissions. In 2017, we began reporting select Scope 3 emissions categories. Since 2019 we have reported annually on all relevant emissions defined by the Greenhouse Gas Protocol. We obtain limited assurance conducted by a third party for select environmental metrics. In our accounting, data center servers and their components are a significant driver of our Scope 3 emissions footprint, and we have taken numerous steps to deepen our understanding of those emissions in order to surface reduction opportunities. \nAn important reduction strategy we are focused on is the circularity of our servers and components. The more effectively and efficiently servers are utilized, the more sustainable the server fleet. We can extend the lifespans of servers, components, and network infrastructure with improvements to server reliability, efforts to reuse components based on their reliability expectations, and various performance optimizations and operational improvements (e.g., firmware/server upgrades and repairs).\nWhile implementing these circularity strategies, we observed limitations in current carbon accounting practices to understand and weigh Scope 3 emissions trade-offs in our server fleet against traditional power, performance, and total cost of ownership (TCO) metrics, such as performance per dollar, performance per watt, and performance per dollar per watt, in real-time. \nCurrent carbon accounting and reporting practices for Scope 3 emissions are static. For data center servers and components, in particular, this means that the entirety of the embodied emissions from the upstream supply chain, manufacturing, and logistics is attributed in the year of purchase. Benefits from circularity are not realized in our Scope 3 footprint until future purchases of new servers or components are deferred. This does not provide actionable information to our operational teams in real-time on how varying the usage or the expected life of the acquired servers can impact Meta’s Scope 3 emissions. \nWe see a need to develop internal metrics to monitor and incentivize greater efficiency, utilization, and extension of the expected life of servers, which will influence current and future server fleet management. \nIntroducing real-time server fleet utilization effectiveness\nThe RETINAS initiative, launched by Meta’s Infrastructure Engineering team, seeks to study and understand how server reliability, performance, and operational optimization impact Meta’s Scope 3 emissions. To understand this holistically, we introduced a standardized, fleet-wide metric for any given resource (e.g., a server or rack) that measures the utilization of embodied carbon:\n\n \nWhere: \n\n \n\n \nThis metric borrows depreciation concepts from finance and accounting practices and applies them to aspects of server reliability, efficiency, and useful life. The concept of depreciation is used to showcase the expected useful life of acquired assets. This concept also allows for tracking of acquisition and disposition of server resources at fleet scale and is reported on an ongoing basis. \nUtilization metrics like power usage effectiveness (PUE) and hardware usage effectiveness (HUE) measure the effective IT usage from a power perspective at the data center and server level, respectively.  Combining depreciated Scope 3 emissions with these utilization metrics allows us to standardize these measurements along with other fleet health measurements for a defined period of time. \nWe illustrate the usage of this metric with a set of servers and various circularity strategies. \nExample (current static state) \nLet’s consider an example set of servers purchased in 2023 which have associated embodied emissions attributed to the buyer with 1000 tons of CO2e. Here is how this would be represented using current, static carbon accounting methods: \n\nThere is no representation for the useful life of the example set of servers. If we change the server set’s useful life (UL) from four years to five years, the metric doesn’t move. \nExample (with proposed dynamic accounting)\nFor the same example of servers purchased in 2023 with 1000 units of CO2e Scope 3 emissions, we use the concept of depreciation over a period of useful life of four years (example time horizon):\nDepreciation in action:\n\nIf the server set’s useful life is modified from four years to five years, this would be visible as part of the depreciation metric and showcase the longevity of resource usage. \nEffective change in depreciation with extension (from 4y UL to 5y UL): \n\nWithin large-scale infrastructure, there are different layers of availability within the hardware and software stack, such as hardware, firmware, the kernel, the operating system, and the application.  At each layer, there are metrics associated with efficiency based on available capacity, resources, and their effective use. To represent the use of a depreciation-based metric, we examine the efficiency of a service at the application stack. The representative graph below showcases an example set of variations in the utilization effectiveness stemming from application improvements over a larger time scale.\n\nUtilization effectiveness is defined as:  Total resource available / Resource utilized \n\nCombining the depreciation of embodied emissions resources per unit time to the utilization effectiveness for a given unit of time (say, every year), we can arrive at a more real-time measurement of server fleet utilization effectiveness of embodied carbon. (Note: The measurements for utilization effectiveness in the chart are representative values).\nOur goal is to consistently minimize the real-time server fleet utilization effectiveness. Utilization effectiveness ideally is decreasing asymptotically towards 1, when resources available are 100% utilized.  Depreciation of Scope 3 emissions over a longer period of time due to a longer useful life will also minimize this metric. Combined, this metric allows for ranking of different efforts one must pursue within the server fleet and compare and contrast efficiency improvements, reliability efforts like extensions or initial component selection, and associated embodied carbon impacts. \nBelow is the comparison of real-time server fleet utilization effectiveness and the way the metric behaves under server life extension and efficiency improvements with the above considered scenarios.\n\nCharacteristics of the metric:\nThe metric above can enable relative comparison of circularity strategies on the server fleet. It can be sliced horizontally into any given timescale (from seconds to years) for understanding a resource’s (e.g., servers or racks) embodied emissions attribution giving fine-grained real-time insights for the server fleet. The metric can also be vertically sliced to obtain utilization effectiveness at different layers of the stack, from entire servers, to containers, to production workloads, to app residencies for short durations – combining that with the associated resource available for the chosen abstraction. \nTo illustrate how this metric can be used, using the same set example as above: \nIncreasing server useful life from five years to seven years lowers the ​metric by 28% due to slower depreciation. \nEnabling reuse of a component, pursuing an application efficiency improvement, or choosing server parts that have lower emissions will contribute towards the metric and enable cross-stack tradeoff.\nTo close, we can observe in this example that this single metric ties together different fleet operations towards a single goal of reducing embodied emissions, delivering  insights for decision making at any given time horizon. By integrating depreciation and utilization effectiveness to embodied carbon, our operational and server fleet management teams can leverage this metric to make data-driven decisions that address an important portion of Meta’s Scope 3 footprint.  \nThe post RETINAS: Real-Time Infrastructure Accounting for Sustainability appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>We are introducing a new metric— real-time server fleet utilization effectiveness —as part of the RETINAS initiative to help reduce emissions and achieve net zero emissions across our value chain in 2030. This new metric allows us to measure server resource usage (e.g., compute, storage) and efficiency in our large-scale data center server fleet in [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2024/08/26/data-infrastructure/retinas-real-time-infrastructure-accounting-for-sustainability/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2024/08/26/data-infrastructure/retinas-real-time-infrastructure-accounting-for-sustainability/\">RETINAS: Real-Time Infrastructure Accounting for Sustainability</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "We are introducing a new metric— real-time server fleet utilization effectiveness —as part of the RETINAS initiative to help reduce emissions and achieve net zero emissions across our value chain in 2030. This new metric allows us to measure server resource usage (e.g., compute, storage) and efficiency in our large-scale data center server fleet in [...]\nRead More...\nThe post RETINAS: Real-Time Infrastructure Accounting for Sustainability appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=21592",
        "categories": [
          "Data Center Engineering",
          "Data Infrastructure"
        ],
        "isoDate": "2024-08-26T16:00:40.000Z"
      },
      {
        "creator": "",
        "title": "How PyTorch powers AI training and inference",
        "link": "https://engineering.fb.com/2024/08/23/ml-applications/pytorch-ai-training-inference/",
        "pubDate": "Fri, 23 Aug 2024 16:00:54 +0000",
        "content:encodedSnippet": "Learn about new PyTorch advancements for LLMs and how PyTorch is enhancing every aspect of the LLM lifecycle.\nIn this talk from AI Infra @ Scale 2024, software engineers Wanchao Liang and Evan Smothers are joined by Meta research scientist Kimish Patel to discuss our newest features and tools that enable large-scale training, memory efficient fine-tuning, and on-device LLM capabilities.\nFirst, they cover the importance of memory-efficient fine-tuning and a few common architectural and algorithmic techniques to enable fine-tuning on consumer-grade hardware. Then they discuss the challenges of deploying large models for on-device deployment and how techniques such as quantization make these deployments possible.\n\nThe post How PyTorch powers AI training and inference appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>Learn about new PyTorch advancements for LLMs and how PyTorch is enhancing every aspect of the LLM lifecycle. In this talk from AI Infra @ Scale 2024, software engineers Wanchao Liang and Evan Smothers are joined by Meta research scientist Kimish Patel to discuss our newest features and tools that enable large-scale training, memory efficient [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2024/08/23/ml-applications/pytorch-ai-training-inference/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2024/08/23/ml-applications/pytorch-ai-training-inference/\">How PyTorch powers AI training and inference</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "Learn about new PyTorch advancements for LLMs and how PyTorch is enhancing every aspect of the LLM lifecycle. In this talk from AI Infra @ Scale 2024, software engineers Wanchao Liang and Evan Smothers are joined by Meta research scientist Kimish Patel to discuss our newest features and tools that enable large-scale training, memory efficient [...]\nRead More...\nThe post How PyTorch powers AI training and inference appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=21568",
        "categories": [
          "AI Research",
          "ML Applications",
          "Open Source",
          "AI Infra @ Scale"
        ],
        "isoDate": "2024-08-23T16:00:54.000Z"
      },
      {
        "creator": "",
        "title": "Inside the hardware and co-design of MTIA",
        "link": "https://engineering.fb.com/2024/08/22/ml-applications/meta-mtia-hardware-co-design/",
        "pubDate": "Thu, 22 Aug 2024 16:00:34 +0000",
        "content:encodedSnippet": "In this talk from AI Infra @ Scale 2024, Joel Colburn, a software engineer at Meta, technical lead Junqiang Lan, and software engineer Jack Montgomery discuss the second generation of MTIA, Meta’s in-house training and inference accelerator.\nThey cover the co-design process behind building the second generation of Meta’s first-ever custom silicon for AI workloads, including the PyTorch software ecosystem, and the model architectures for Meta’s key applications. They demonstrate how MTIA achieves the performance, efficiency, and developer experience to successfully launch models into production. They also highlight several co-design examples where special silicon features are utilized to accelerate Meta’s models.\n\nThe post Inside the hardware and co-design of MTIA appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>In this talk from AI Infra @ Scale 2024, Joel Colburn, a software engineer at Meta, technical lead Junqiang Lan, and software engineer Jack Montgomery discuss the second generation of MTIA, Meta’s in-house training and inference accelerator. They cover the co-design process behind building the second generation of Meta’s first-ever custom silicon for AI workloads, [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2024/08/22/ml-applications/meta-mtia-hardware-co-design/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2024/08/22/ml-applications/meta-mtia-hardware-co-design/\">Inside the hardware and co-design of MTIA</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "In this talk from AI Infra @ Scale 2024, Joel Colburn, a software engineer at Meta, technical lead Junqiang Lan, and software engineer Jack Montgomery discuss the second generation of MTIA, Meta’s in-house training and inference accelerator. They cover the co-design process behind building the second generation of Meta’s first-ever custom silicon for AI workloads, [...]\nRead More...\nThe post Inside the hardware and co-design of MTIA appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=21571",
        "categories": [
          "AI Research",
          "ML Applications",
          "AI Infra @ Scale"
        ],
        "isoDate": "2024-08-22T16:00:34.000Z"
      }
    ]
  },
  {
    "name": "eBay Tech Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Twitter Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Netflix TechBlog",
    "category": "기업",
    "posts": [
      {
        "creator": "Netflix Technology Blog",
        "title": "Improve Your Next Experiment by Learning Better Proxy Metrics From Past Experiments",
        "link": "https://netflixtechblog.com/improve-your-next-experiment-by-learning-better-proxy-metrics-from-past-experiments-64c786c2a3ac?source=rss----2615bd06b42e---4",
        "pubDate": "Mon, 26 Aug 2024 15:46:24 GMT",
        "content:encodedSnippet": "By Aurélien Bibaut, Winston Chou, Simon Ejdemyr, and Nathan Kallus\n\nWe are excited to share our work on how to learn good proxy metrics from historical experiments at KDD 2024. This work addresses a fundamental question for technology companies and academic researchers alike: how do we establish that a treatment that improves short-term (statistically sensitive) outcomes also improves long-term (statistically insensitive) outcomes? Or, faced with multiple short-term outcomes, how do we optimally trade them off for long-term benefit?\nFor example, in an A/B test, you may observe that a product change improves the click-through rate. However, the test does not provide enough signal to measure a change in long-term retention, leaving you in the dark as to whether this treatment makes users more satisfied with your service. The click-through rate is a proxy metric (S, for surrogate, in our paper) while retention is a downstream business outcome or north star metric (Y). We may even have several proxy metrics, such as other types of clicks or the length of engagement after click. Taken together, these form a vector of proxy metrics.\nThe goal of our work is to understand the true relationship between the proxy metric(s) and the north star metric — so that we can assess a proxy’s ability to stand in for the north star metric, learn how to combine multiple metrics into a single best one, and better explore and compare different proxies.\nSeveral intuitive approaches to understanding this relationship have surprising pitfalls:\n\nLooking only at user-level correlations between the proxy S and north star Y. Continuing the example from above, you may find that users with a higher click-through rate also tend to have a higher retention. But this does not mean that a product change that improves the click-through rate will also improve retention (in fact, promoting clickbait may have the opposite effect). This is because, as any introductory causal inference class will tell you, there are many confounders between S and Y — many of which you can never reliably observe and control for.\nLooking naively at treatment effect correlations between S and Y. Suppose you are lucky enough to have many historical A/B tests. Further imagine the ordinary least squares (OLS) regression line through a scatter plot of Y on S in which each point represents the (S,Y)-treatment effect from a previous test. Even if you find that this line has a positive slope, you unfortunately cannot conclude that product changes that improve S will also improve Y. The reason for this is correlated measurement error — if S and Y are positively correlated in the population, then treatment arms that happen to have more users with high S will also have more users with high Y.\n\nBetween these naive approaches, we find that the second one is the easier trap to fall into. This is because the dangers of the first approach are well-known, whereas covariances between estimated treatment effects can appear misleadingly causal. In reality, these covariances can be severely biased compared to what we actually care about: covariances between true treatment effects. In the extreme — such as when the negative effects of clickbait are substantial but clickiness and retention are highly correlated at the user level — the true relationship between S and Y can be negative even if the OLS slope is positive. Only more data per experiment could diminish this bias — using more experiments as data points will only yield more precise estimates of the badly biased slope. At first glance, this would appear to imperil any hope of using existing experiments to detect the relationship.\nThis figure shows a hypothetical treatment effect covariance matrix between S and Y (white line; negative correlation), a unit-level sampling covariance matrix creating correlated measurement errors between these metrics (black line; positive correlation), and the covariance matrix of estimated treatment effects which is a weighted combination of the first two (orange line; no correlation).\nTo overcome this bias, we propose better ways to leverage historical experiments, inspired by techniques from the literature on weak instrumental variables. More specifically, we show that three estimators are consistent for the true proxy/north-star relationship under different constraints (the paper provides more details and should be helpful for practitioners interested in choosing the best estimator for their setting):\n\nA Total Covariance (TC) estimator allows us to estimate the OLS slope from a scatter plot of true treatment effects by subtracting the scaled measurement error covariance from the covariance of estimated treatment effects. Under the assumption that the correlated measurement error is the same across experiments (homogeneous covariances), the bias of this estimator is inversely proportional to the total number of units across all experiments, as opposed to the number of members per experiment.\nJackknife Instrumental Variables Estimation (JIVE) converges to the same OLS slope as the TC estimator but does not require the assumption of homogeneous covariances. JIVE eliminates correlated measurement error by removing each observation’s data from the computation of its instrumented surrogate values.\nA Limited Information Maximum Likelihood (LIML) estimator is statistically efficient as long as there are no direct effects between the treatment and Y (that is, S fully mediates all treatment effects on Y). We find that LIML is highly sensitive to this assumption and recommend TC or JIVE for most applications.\n\nOur methods yield linear structural models of treatment effects that are easy to interpret. As such, they are well-suited to the decentralized and rapidly-evolving practice of experimentation at Netflix, which runs thousands of experiments per year on many diverse parts of the business. Each area of experimentation is staffed by independent Data Science and Engineering teams. While every team ultimately cares about the same north star metrics (e.g., long-term revenue), it is highly impractical for most teams to measure these in short-term A/B tests. Therefore, each has also developed proxies that are more sensitive and directly relevant to their work (e.g., user engagement or latency). To complicate matters more, teams are constantly innovating on these secondary metrics to find the right balance of sensitivity and long-term impact.\nIn this decentralized environment, linear models of treatment effects are a highly useful tool for coordinating efforts around proxy metrics and aligning them towards the north star:\n\nManaging metric tradeoffs. Because experiments in one area can affect metrics in another area, there is a need to measure all secondary metrics in all tests, but also to understand the relative impact of these metrics on the north star. This is so we can inform decision-making when one metric trades off against another metric.\nInforming metrics innovation. To minimize wasted effort on metric development, it is also important to understand how metrics correlate with the north star “net of” existing metrics.\nEnabling teams to work independently. Lastly, teams need simple tools in order to iterate on their own metrics. Teams may come up with dozens of variations of secondary metrics, and slow, complicated tools for evaluating these variations are unlikely to be adopted. Conversely, our models are easy and fast to fit, and are actively used to develop proxy metrics at Netflix.\n\nWe are thrilled about the research and implementation of these methods at Netflix — while also continuing to strive for great and always better, per our culture. For example, we still have some way to go to develop a more flexible data architecture to streamline the application of these methods within Netflix. Interested in helping us? See our open job postings!\nFor feedback on this blog post and for supporting and making this work better, we thank Apoorva Lal, Martin Tingley, Patric Glynn, Richard McDowell, Travis Brooks, and Ayal Chen-Zion.\n\nImprove Your Next Experiment by Learning Better Proxy Metrics From Past Experiments was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Netflix Technology Blog",
        "guid": "https://medium.com/p/64c786c2a3ac",
        "categories": [
          "data-science",
          "experimentation",
          "machine-learning",
          "a-b-testing",
          "statistics"
        ],
        "isoDate": "2024-08-26T15:46:24.000Z"
      }
    ]
  },
  {
    "name": "JetBrains: Developer Tools for Professionals and Teams – Company Blog | JetBrains",
    "category": "기업",
    "posts": [
      {
        "creator": "Anna Ruban",
        "title": "Join Us for JetBrains GameDev Days 2024",
        "link": "https://blog.jetbrains.com/dotnet/2024/08/28/join-us-for-jetbrains-gamedev-days-2024/",
        "pubDate": "Wed, 28 Aug 2024 08:50:24 +0000",
        "content:encodedSnippet": "We invite you to JetBrains GameDev Days 2024, a hybrid event we’re hosting on October 9-10, 2024.\nCOUNT ME IN!\n                                                    \nOnce a year, we invite community speakers to cover topics they are passionate about. We’ve lined up experts from different fields to deliver talks, lead discussions, and share helpful tips on creating exceptional games. Topics will include CI/CD processes, Unity’s DOTS, C# scripting, game development with Godot, and more.\nIn 2024, we’re again going hybrid. The event will kick off with a few in-person sessions on October 9, followed by the main online event on October 10. More details will be shared soon, so be sure to stay tuned!\n\n\n\n\nAgenda\nTurbocharge Your Unreal Engine Game Development With JetBrains Rider and TeamCity by Daniel Gallo and Matt Ellis\nDeveloper Experience in Mobile GameDev: How We Rebuilt a Plane in Flight by Ilya Bogin\nGodot Game Development Leveraging Kotlin by Pierre-Thomas Meisels, Tristan Grespinet, and Cedric Hippmann\nCSharpify Your Engine: A Guide to Embedding C# by Andreia Gaita\nGame Engines With C# and Rider by Alex Drum\nWhy DOTS Is Unity’s Silver Bullet by Johnny Thompson\nBuilding a Godot Addon by Johannes Ebner\nHow We Built the Backend Architecture for HAWKED: Principles, Technologies, and Tools by Vyacheslav Morov\nThere are more talks to come, so stay tuned for additional announcements.\nYou can choose whether to attend just a few individual sessions or watch every one of them. We hope you enjoy the talks, and we encourage you to ask questions!\nWe’ll stream the presentations live on YouTube, and all the sessions will remain available after the event is over so you can catch up on any you missed. You can also watch session recordings from the previous year.\nCheck out the full agenda here. We look forward to seeing you in October!",
        "dc:creator": "Anna Ruban",
        "content": "We invite you to JetBrains GameDev Days 2024, a hybrid event we&#8217;re hosting on October 9-10, 2024. Once a year, we invite community speakers to cover topics they are passionate about. We&#8217;ve lined up experts from different fields to deliver talks, lead discussions, and share helpful tips on creating exceptional games. Topics will include CI/CD [&#8230;]",
        "contentSnippet": "We invite you to JetBrains GameDev Days 2024, a hybrid event we’re hosting on October 9-10, 2024. Once a year, we invite community speakers to cover topics they are passionate about. We’ve lined up experts from different fields to deliver talks, lead discussions, and share helpful tips on creating exceptional games. Topics will include CI/CD […]",
        "guid": "https://blog.jetbrains.com/?post_type=dotnet&p=505899",
        "categories": [
          "net-tools",
          "events",
          "teamcity-2",
          "conferences",
          "gamedev",
          "gamedev-days-2024",
          "rider",
          "teamcity"
        ],
        "isoDate": "2024-08-28T08:50:24.000Z"
      },
      {
        "creator": "Alex Plate",
        "title": "Workspace Model",
        "link": "https://blog.jetbrains.com/platform/2024/08/workspace-model/",
        "pubDate": "Wed, 28 Aug 2024 07:58:52 +0000",
        "content:encodedSnippet": "The Workspace Model, which serves as the internal storage for project models, has exited its experimental phase. Plugin developers can now safely access and modify the workspace model, benefiting from a centralized API, bulk updates, coroutines, and a persistent data structure.\nThis update is particularly relevant for developers working on build tool integration plugins, such as Maven, Gradle, BSP, Bazel, and others. Additionally, it will be useful for any plugin developers who interact with the JPS project model, known for classes like Module, Library, Facet, ModuleManager, IdeModifiableModelsProvider, and others.\nNew Implementation of the JPS project model\nSome time ago, we announced the reimplementation of project models. The IntelliJ platform still supports old interfaces like Module and ModuleManager to maintain compatibility with existing plugins. However, the new Workspace Model API is now also available for public use. Plugin developers are encouraged to update their code from the old API to the new API. More information on code migration can be found here.\nThe Workspace Model is designed to store any entities, including those defined by plugins. However, defining custom entities is still an experimental part of the API and will be released later. Currently, working with the Workspace Model is limited to using entities provided by the platform.\nBenefits of the Workspace Model\nThe Workspace Model is designed as a persistent data structure, enabling it to handle various multithreading use cases. A snapshot of the storage is immutable and can be processed without a read lock. Batch operations like replaceBySource and applyChangesFrom are designed to support scenarios involving significant updates to the workspace model.\nCompared to the previous implementation of the project models, the Workspace Model keeps everything in one storage, there is a single entry point for all updates, and there is no need to create multiple modifiable models as before. Also, as the old project model API now delegates to the Workspace Model, accessing the new API directly may improve the performance, especially for massive operations or big projects.\nThe flexibility of the Workspace Model allows the entire IntelliJ Platform to change the way we work with the project model. Rather than using specific managers, the subsystem can define extension points that can be contributed by other subsystems. For example, the new WorkspaceFileIndex operates on entities provided by different parts of the IDE instead of directly using the ModuleManager.\nThe event flow allows you to observe all changes occurring within the storage. Every update can be used to incrementally affect subsystems that depend on Workspace Model data. This feature has been utilized to remove some internal implementations of AdditionalLibraryRootsProvider and DirectoryIndexExcludePolicy, which were recalculated from scratch on each change of the project models, leading to improved performance of JetBrains IDEs.\nFurther evolution\nThe new Workspace model is already used in several places. The Rider IDE was the first client of the Workspace Model, and it still actively uses it. The Maven import was rewritten to the new API, giving a boost in performance. Different parts of GoLand, WebStorm, and the Kotlin plugin also started to use the new approach, gaining different benefits.\nThe next steps for the Workspace Model will focus on allowing plugin developers to define and use their own entities. This will make it possible to describe the project model of a build tool with specific entities that accurately reflect the structure of the project.\nCurrently, updates to the Workspace Model occur under a write lock. Future updates will eliminate this requirement, allowing updates to happen without acquiring any locks.\nThe IntelliJ platform still mostly uses the old API for working with the project models, however, all new code will be written using the new Workspace Model API. The subsystems that will benefit from the performance of the new API will also be migrated.",
        "dc:creator": "Alex Plate",
        "content": "The Workspace Model, which serves as the internal storage for project models, has exited its experimental phase. Plugin developers can now safely access and modify the workspace model, benefiting from a centralized API, bulk updates, coroutines, and a persistent data structure. This update is particularly relevant for developers working on build tool integration plugins, such [&#8230;]",
        "contentSnippet": "The Workspace Model, which serves as the internal storage for project models, has exited its experimental phase. Plugin developers can now safely access and modify the workspace model, benefiting from a centralized API, bulk updates, coroutines, and a persistent data structure. This update is particularly relevant for developers working on build tool integration plugins, such […]",
        "guid": "https://blog.jetbrains.com/?post_type=platform&p=506070",
        "categories": [
          "intellij",
          "idea"
        ],
        "isoDate": "2024-08-28T07:58:52.000Z"
      },
      {
        "creator": "Olga Bedrina",
        "title": "TeamCity Pipelines Pulse, Issue #6: Introducing Self- Hosted Agents",
        "link": "https://blog.jetbrains.com/teamcity/2024/08/teamcity-pipelines-pulse-issue6-self-hosted-agents/",
        "pubDate": "Tue, 27 Aug 2024 14:22:34 +0000",
        "content:encodedSnippet": "Want total control over your CI/CD pipeline?\nWe’re introducing self-hosted agents in TeamCity Pipelines! Define your own build environments, manage your resources, and scale effortlessly. No more waiting in line for shared agents.\n\n\n\n\nYou can now choose whether to run jobs on JetBrains-hosted agents or your own. You can also add various requirements for the agents, such as a specific OS, CPU count, CPU arch, or RAM amount, or even go with custom requirements.\n\n\n\n\nJust like with JetBrains-hosted agents, you can open the terminal and connect it directly to the agent during a job to view logs, check installed software, or debug issues – all from the UI.\n\n\n\n\nLearn more about installing self-hosted agents in our docs.\nNew edit mode toggle for pipelines and jobs\nPreviously, it wasn’t obvious how to navigate from the pipeline editor to currently running jobs.\nWe decided to rethink the whole process and introduced the edit mode toggle. Now, you can easily switch between edit mode and run mode. Accessing pipeline and job settings has also become more convenient.\n\n\n\n\nBug fixes and improvements\nWe’ve added a bunch of bug fixes and improvements to TeamCity Pipelines. Here are the highlights.\nIn TeamCity Pipelines, you can run tests in parallel to speed up execution. The Open Terminal button is once again available during parallel tests.\n\n\n\n\nThe Create pipeline button is now completely hidden from users without proper permissions.\n\n\n\n\nWhen TeamCity Pipelines flags errors because settings are wrong, it will now allow you to save your changes after fixing the problems.\n\n\n\n\nTeamCity Pipelines once again correctly auto-detects runners like Maven, Gradle, or npm and suggests optimizations.\n\n\n\n\nAdding a repository to a pipeline no longer triggers a Repository should not be empty error for valid URLs.\n\n\n\n\nWhen duplicating an unsaved job, its dependencies are no longer ignored.\nDid you know?\nIn TeamCity Pipelines, you can define job dependencies conveniently and visually using the drag-and-drop editor. YAML is also available for those who prefer it.\n\n\n\n\nThat’s it for today! As always, feel free to share your feedback in the comments. If you’d like to give TeamCity Pipelines a try, the product is free during Beta.\nLearn more about TeamCity Pipelines\nYours truly,\nThe TeamCity Pipelines team",
        "dc:creator": "Olga Bedrina",
        "content": "Want total control over your CI/CD pipeline? We&#8217;re introducing self-hosted agents in TeamCity Pipelines! Define your own build environments, manage your resources, and scale effortlessly. No more waiting in line for shared agents. You can now choose whether to run jobs on JetBrains-hosted agents or your own. You can also add various requirements for the [&#8230;]",
        "contentSnippet": "Want total control over your CI/CD pipeline? We’re introducing self-hosted agents in TeamCity Pipelines! Define your own build environments, manage your resources, and scale effortlessly. No more waiting in line for shared agents. You can now choose whether to run jobs on JetBrains-hosted agents or your own. You can also add various requirements for the […]",
        "guid": "https://blog.jetbrains.com/?post_type=teamcity&p=505821",
        "categories": [
          "news",
          "releases",
          "release",
          "teamcity-pipelines",
          "teamcity-pipelines-pulse"
        ],
        "isoDate": "2024-08-27T14:22:34.000Z"
      },
      {
        "creator": "Anna Rovinskaia",
        "title": "New Livestream: Introduction to Workspaces in IntelliJ IDEA",
        "link": "https://blog.jetbrains.com/idea/2024/08/new-livestream-java-22-and-intellij-ideanew-livestream/",
        "pubDate": "Tue, 27 Aug 2024 06:59:09 +0000",
        "content:encodedSnippet": "Join us for a new IntelliJ IDEA Livestream episode, where we will explore how to take advantage of the new workspace functionality in your IntelliJ IDEA projects with Andrey Belyaev.\nDate: September 12, 2024\nTime: 3:00 pm – 4:00 pm UTC\nREGISTER FOR THE LIVESTREAM\n\n\n\n\nSession abstract\nThis session will provide a comprehensive overview of the new feature in IntelliJ IDEA: workspaces. Andrey will explain what workspaces are, how they function, and how they can be integrated into your current projects. We will demonstrate some practical examples to get you started: creating a workspace, adding projects, and running them within it.\nAsking questions\nAndrey will try to answer all of your questions during the session. If we run out of time, we’ll publish the answers to any remaining questions in a follow-up blog post.\nYour speaker and host\nSpeaker\nAndrey Belyaev\n\nAndrey Belyaev is a software developer on the IntelliJ IDEA Ultimate team. He works on various plugins and is now primarily focused on cloud and deployment support in IntelliJ IDEA.\n\nHost\nMala Gupta\n\nA Java Champion and JUG leader, Mala has authored multiple books with Manning, Packt, and O’Reilly Publications. She has more than two decades of experience in the software industry and is a regular speaker at industry conferences around the world. She is an active supporter of Java certification as a path to career advancement.\n\nHappy developing!",
        "dc:creator": "Anna Rovinskaia",
        "content": "Join us for a new IntelliJ IDEA Livestream episode, where we will explore how to take advantage of the new workspace functionality in your IntelliJ IDEA projects with Andrey Belyaev. Date: September 12, 2024 Time: 3:00 pm – 4:00 pm UTC REGISTER FOR THE LIVESTREAM Session abstract This session will provide a comprehensive overview of [&#8230;]",
        "contentSnippet": "Join us for a new IntelliJ IDEA Livestream episode, where we will explore how to take advantage of the new workspace functionality in your IntelliJ IDEA projects with Andrey Belyaev. Date: September 12, 2024 Time: 3:00 pm – 4:00 pm UTC REGISTER FOR THE LIVESTREAM Session abstract This session will provide a comprehensive overview of […]",
        "guid": "https://blog.jetbrains.com/?post_type=idea&p=505033",
        "categories": [
          "livestreams",
          "intellij-idea",
          "intellijidealivestream",
          "livestream",
          "webinars"
        ],
        "isoDate": "2024-08-27T06:59:09.000Z"
      },
      {
        "creator": "Aleksandra Aganezova",
        "title": "JetBrains JavaScript Day 2024 Registration Is Now Open",
        "link": "https://blog.jetbrains.com/webstorm/2024/08/jetbrains-javascript-day-2024-registration-is-now-open/",
        "pubDate": "Mon, 26 Aug 2024 15:47:01 +0000",
        "content:encodedSnippet": "Hey everyone! JetBrains JavaScript Day is back for year four, and this edition promises to be our best one yet!\n\n\n\n\n\nLet’s face it, keeping up with the rapid changes in the JavaScript ecosystem is no easy task – it can be hard for us, too. That’s exactly why we created JetBrains JavaScript Day! This year, as always, we brought together top experts to share their insights and discuss the latest trends of modern JavaScript and TypeScript development. Make sure to join us live to ask your questions and be a part of the conversation as it happens.\nWhen: October 24 at 9:00 am EDT (check your timezone)\nWhere: Online\nCost: Free\nRegister!\n                                                    \nHere is this year’s lineup of inspiring speakers and talks: \nVite and the Future of JavaScript Tooling, by Evan You\nTypeScript Generics: Practical Fun!, by Josh Goldberg\nThe Silent Open Source Crisis: When Maintainers Walk Away, by Bekah Hawrot Weigel\nYou Don’t Need JavaScript for That, by Kevin Powell \nEverything You Need to Know About React 19, by Shruti Kapoor\nDriving Your JavaScript Library Ecosystem With Nx, by Chau Tran\nEvolving Angular for the Long Run, by Jeremy Elbourn\nTypeScript and Your Codebase: They Deserve Each Other!, by Danny Thompson\nWe’ve also added an Ask Me Anything (AMA) session with the WebStorm team to our agenda for all of you who want to meet the people behind the JavaScript support for JetBrains IDEs.\nGrab your tickets today, and we’ll see you there!\nYour JetBrains team",
        "dc:creator": "Aleksandra Aganezova",
        "content": "Hey everyone! JetBrains JavaScript Day is back for year four, and this edition promises to be our best one yet! Let’s face it, keeping up with the rapid changes in the JavaScript ecosystem is no easy task – it can be hard for us, too. That’s exactly why we created JetBrains JavaScript Day! This year, [&#8230;]",
        "contentSnippet": "Hey everyone! JetBrains JavaScript Day is back for year four, and this edition promises to be our best one yet! Let’s face it, keeping up with the rapid changes in the JavaScript ecosystem is no easy task – it can be hard for us, too. That’s exactly why we created JetBrains JavaScript Day! This year, […]",
        "guid": "https://blog.jetbrains.com/?post_type=webstorm&p=505594",
        "categories": [
          "news",
          "javascript-day"
        ],
        "isoDate": "2024-08-26T15:47:01.000Z"
      },
      {
        "creator": "Andrei Kislitsyn",
        "title": "Track and Analyze GitHub Star Growth With  Kandy and Kotlin DataFrame",
        "link": "https://blog.jetbrains.com/kotlin/2024/08/track-and-analyze-github-star-growth-with-kandy-and-kotlin-dataframe/",
        "pubDate": "Mon, 26 Aug 2024 14:37:54 +0000",
        "content:encodedSnippet": "Kotlin DataFrame and Kandy are two powerful tools for data analysis in Kotlin. Kotlin DataFrame simplifies data manipulation and processing, while Kandy allows you to create visualizations directly within your Kotlin projects.\nIn this post, we’ll show you how these tools can be used together within Kotlin Notebook to analyze the star history of GitHub repositories. This isn’t just a simple exercise for demonstration purposes – it’s a tutorial that can help you learn how to analyze your own repositories, understand their popularity trends, and visualize your data effectively. All examples from this post are available as a Kotlin Notebook on GitHub or a Notebook on Datalore, a data science platform by JetBrains.\nAnalyze your GitHub star history\nUnderstanding the star history of a GitHub repository can provide insights into its popularity and growth over time. By analyzing this data, you can see how different events and activities impact the interest in your project. Our goal is to equip you with the knowledge and tools to perform this analysis on your own repositories.\nObtain repository stargazers data from GitHub\nFirst, we need to gather data about the users who have starred a given repository. To achieve this, we’ll use the GitHub GraphQL API, which requires a GitHub access token. Here’s a simple function to request data about repo stars, including the starring time and user login:\nimport io.ktor.client.request.*\nimport io.ktor.http.*\n\n/**\n*  We need to specify the repository owner and name, as well as the access token.\n* There can be up to 100 results on one response page.\n* For this example, we'll take only the first 3 results.\n* `endCursor` points to the end of the previous page (`null` for the first one). \n*/\nfun fetchStarHistoryPage(owner: String, name: String, token: String, first: Int = 100, endCursor: String? = null): NotebookHttpResponse {\n   // GraphQL query\n   val query = \"\"\"\n       query {\n         repository(owner: \"$owner\", name: \"$name\") {\n           stargazers(first: $first, after: $endCursor) {\n             edges {\n               starredAt\n               node {\n                 login\n               }\n             }\n             pageInfo {\n               endCursor\n               hasNextPage\n             }\n           }\n         }\n       }\n   \"\"\".trimIndent()\n   // `http` is the default Ktor `HttpClient` for Notebook;\n   // it has the same methods but without `suspend` modifiers, \n   // allowing you to make HTTP requests quickly and easily. \n   // Make a \"post\" request to the API with this query\n   return http.post(\"https://api.github.com/graphql\") {\n       // Set authorization header with token\n       bearerAuth(token)\n       // Set content type header\n       contentType(ContentType.Application.Json)\n       // Set query as body\n       setBody(buildJsonObject { put(\"query\", query) })\n   }\n}\nA convenient and easy way to set an environment variable is through the Kotlin Notebook settings:\n\n\n\n\nNext, specify the repository owner and name, and ensure your GitHub token is securely stored:\nval ownerKotlin = \"Kotlin\"\nval repoKandy = \"kandy\"\n// Keep your token safe as an environment variable or a system property!\n// For example, you can place it in environment variables in Kotlin Notebook settings.\nval token = System.getenv(\"GITHUB_TOKEN\")\nTo start, let’s query a single page with a few users to examine the data. \nval rawResponse = fetchStarHistoryPage(ownerKotlin, repoKandy, token, first = 3)\nrawResponse\n\n\n\n\nThe response from the API looks like this:\nHttpResponse[https://api.github.com/graphql, 200 OK]\nNext, we’ll deserialize the JSON response to a Kotlin data class using the .deserializeJson() extension provided by our Kotlin Notebook Ktor integration. This makes it easier to work with the response body data in Kotlin.\nval starHistorySimplePage = rawResponse.deserializeJson()\n// Take the JSON string for further work with DataFrame\nval responseAsJson = starHistorySimplePage.jsonString\nstarHistorySimplePage\nThe result is a structured object representing the data, which looks like this:\n{\n \"data\": {\n   \"repository\": {\n     \"stargazers\": {\n       \"edges\": [\n         {\n           \"starredAt\": \"2022-07-13T22:46:16Z\",\n           \"node\": {\n             \"login\": \"manojselvam\"\n           }\n  ...\n}\nAfter executing the cell above, starHistorySimplePage is converted to a data class, allowing us to easily access those of its properties that correspond to JSON fields. This seamless integration with IntelliJ IDEA autocompletion makes working with the response straightforward.\n\n\n\n\nFor example, we can extract all the starring times from the page:\nstarHistorySimplePage.data.repository.stargazers.edges.map { it.starredAt }\nOutput:\n[2022-07-13T22:46:16Z, 2022-11-05T14:21:10Z, 2022-11-05T18:42:37Z]\nNext, let’s parse the page data into a DataFrame. \nval starHistoryPageDF = DataFrame.readJsonStr(responseAsJson)\nstarHistoryPageDF\n\n\n\n\nWe need two columns: one showing the user logins and the other their starring times. We can retrieve these columns as follows:\nstarHistoryPageDF.data.repository.stargazers.edges\n   .single() // the `edges` column contains a single DataFrame with current page stargazers\n   .flatten() // `login` is a subcolumn of `node`, after `flatten()` it is a simple column\n\n\n\n\nAdditionally, we need page meta-information, including whether there is a next page and the current page end cursor.\nwith(starHistoryPageDF.data.repository.stargazers.pageInfo) {\n   // Both are columns with a single value\n   println(\"end cursor: ${endCursor.single()}\")\n   println(\"has next page: ${hasNextPage.single()}\")\n}\nThis code outputs the following:\nend cursor: Y3Vyc29yOnYyOpIAzhXiSlk=\nhas next page: true\nNow, let’s create a function that iteratively processes all pages with stargazers and returns a DataFrame with complete information:\n// Casts DataFrame to the type of a given DataFrame so we can use\n// extension columns that have already been generated.\n// Temporary workaround, will be available in future DataFrame releases\n// (https://github.com/Kotlin/dataframe/pull/747)\ninline fun <reified T> AnyFrame.castTo(df: DataFrame<T>): DataFrame<T> {\n   return cast<T>(verify = true)\n}\nimport io.ktor.client.statement.*\n\n// Provide repo owner, name, and access token\nfun fetchStarHistory(owner: String, name: String, token: String): AnyFrame {\n   var hasNextPage: Boolean = true\n   var endCursor: String? = null\n   var buffer: DataFrame<*> = DataFrame.Empty\n   while (hasNextPage) {\n       val response = fetchStarHistoryPage(owner, name, token, 100, endCursor)\n       // Cast type of DataFrame to the type of `starHistoryPageDF`,\n       // so we can use its already-generated extensions\n       val responseDF = DataFrame.readJsonStr(response.bodyAsText()).castTo(starHistoryPageDF)\n       val stargazers = responseDF.data.repository.stargazers\n       buffer = buffer.concat(stargazers.edges.first().flatten())\n       val pageInfo = stargazers.pageInfo\n       endCursor = \"\\\"${pageInfo.endCursor.single()}\\\"\"\n       hasNextPage = pageInfo.hasNextPage.single()\n   }\n   return buffer\n}\nUsing this function, we can now retrieve all the Kandy stargazers:\nval kandyStargazers = fetchStarHistory(ownerKotlin, repoKandy, token)\nkandyStargazers\n\n\n\n\nLook at the DataFrame summary using the .describe() method, which shows meta-information and accumulated statistics about DataFrame columns:\nkandyStargazers.describe()\n\n\n\n\nAll login values are unique, indicating that the dataset is correct. Additionally, there are no null values, so no further processing is needed.\nCreate a DataFrame for cumulative star count analysis\nWe now have two key pieces of information: user logins and the times they award stars. Our next step is to perform an initial analysis.\nWe’ll create a visualization showing the cumulative number of stars received over time, illustrating how user interest in our library grows and changes.\nThis approach will help us understand the dynamics of user engagement and the popularity of our library.\nHere’s how to transform this data:\nConvert the starredAt column to LocalDateTime.\nSort the DataFrame by  starredAt, in ascending order.\nAdd a starsCount column to track the total number of stars over time.\nPut the processing code into a function so that it can be reused later on.\nfun AnyFrame.processStargazers(): AnyFrame {\n   return castTo(kandyStargazers)\n       // Convert `starredAt` column to `LocalDateTime`\n       .convert { starredAt }.toLocalDateTime()\n       // Sort rows by `starredAt`\n       .sortBy { starredAt }\n       // Add `starsCount` column with total stars count at each row.\n       // The star count is simply the row index increased by 1\n       .add(\"starsCount\") { index() + 1 }\n}\nval kandyStarHistory = kandyStargazers.processStargazers()\nkandyStarHistory\n\n\n\n\nVisualize star history: plot with Kandy\nWith the data processed, we can now visualize the star history using Kandy. Here’s a simple line plot to show how the number of stars has changed over time. \nkandyStarHistory.plot {\n   line {\n       // The starring time corresponds to the `x` axis\n       x(starredAt) {\n           axis {\n               // Set the name for the `x` axis\n               name = \"date\"\n               // Set the format for axis breaks\n               breaks(format = \"%b, %Y\")\n           }\n       }\n       // The stars count corresponds to the `y` axis\n       y(starsCount) {\n           // Set the name for the `y` axis\n           axis.name = \"GitHub stars\"\n       }\n   }\n    layout {\n       title = \"Kandy GitHub star history\"\n       size = 800 to 500\n   }\n}\n\n\n\n\nThe plot displays the cumulative growth of stars, reflecting how interest in the Kandy library has evolved. Key points of significant increase can often be associated with major announcements or events related to the library.\nTo better understand how user interest in our library evolves over time, we’ll animate this chart using the Kotlin Jupyter API. This dynamic visualization will help us see how engagement patterns shift and grow, providing deeper insights than a static chart could offer.\nWe’ll start by creating a function that builds a star history chart for the first n star(s). \nfun kandyStarHistoryPlot(n: Int) = kandyStarHistory.plot {\n   line {\n       x(starredAt.take(n)) {\n           axis {\n               name = \"date\"\n               breaks(format = \"%b, %Y\")\n           }\n       }\n       y(starsCount.take(n)) {\n           axis.name = \"GitHub stars\"\n       }\n   }\n   layout {\n       title = \"Kandy GitHub star history\"\n       size = 800 to 500\n   }\n}\nThen, we’ll use the ANIMATE() function to update the cell output for a given set of frames. Each frame will be a star history plot, starting with one star and incrementing by one star each frame until we reach the maximum number of stars.\nANIMATE(50.milliseconds, kandyStarHistory.rowsCount()) { frameID ->\n   // frame with `frameID` contsins plot with `frameID + 1` stars\n   kandyStarHistoryPlot(frameID + 1)\n}\n\n\n\n\nAnalyze key events\nWe’ll look at how different events influenced the growth of stars. We’ll add mark lines with the most important events related to Kandy, such as the Kotlin Notebook video, the Kandy introductory post, the Plotting Financial Data in Kotlin with Kandy post, and KotlinConf 2024. Such analysis helps to identify what drives interest and engagement with the project.\nWe’ll look at events starting from October 2023, which was when we initiated our marketing activities:\nval starHistoryFiltered = kandyStarHistory.filter { starredAt >= LocalDateTime(2023, 10, 1, 0, 0, 0, 0) }\nThen we’ll add mark lines with the events:\nval ktnbYTVideodate = LocalDate(2023, 10, 25)\nval kandyIntroductoryPostDate = LocalDate(2023, 12, 14)\nval kandyFinancialPostDate = LocalDate(2024, 4, 9)\nval kotlinConf24Date = LocalDate(2024, 5, 22)\n\nval kandyEvents = listOf(\n   \"Kotlin Notebook\\nYouTube video\",\n   \"Kandy Introduction\\nKotlin Blog post\",\n   \"Financial Plotting\\nMedium post\",\n   \"KotlinConf 2024\"\n)\nval kandyEventsDates = listOf(ktnbYTVideodate, kandyIntroductoryPostDate, kandyFinancialPostDate, kotlinConf24Date)\n\n\n\n\nTo make the plot more visually engaging, we’ll create a custom color palette for these event markers.\nval eventColors = listOf(\n   Color.hex(\"#1f77b4\"),\n   Color.hex(\"#ff7f0e\"),\n   Color.hex(\"#d62728\"),\n   Color.hex(\"#2ca02c\"),\n)\nFinally, we’ll generate the plot with vertical lines representing these events, allowing us to see how each significant event influenced the star history.\nstarHistoryFiltered.plot {\n   // add vertical marklines with event dates\n   vLine {\n       color(kandyEvents, \"event\") { scale = categorical(eventColors, kandyEvents) }\n       xIntercept(kandyEventsDates)\n       width = 1.5\n       alpha = 0.9\n   }\n   line {\n       x(starredAt) {axis.name = \"date\" }\n       y(starsCount) { axis.name = \"GitHub stars\" }\n   }\n   layout {\n       title = \"Kandy GitHub star history & key events\"\n       size = 800 to 500\n       style {\n           legend.position = LegendPosition.Bottom\n       }\n   }\n}\n\n\n\n\nThis plot shows the number of stars Kandy received each month, with different colors representing key events that influenced these numbers. For example, the introductory post and other significant updates coincide with noticeable increases in stars, highlighting the influence of these activities on community engagement.\nAnalyze monthly star growth\nTo analyze the monthly growth of stars, we will create a bar chart to visually display the changes in the number of stars received each month. This visualization will help us identify key growth periods and evaluate the effectiveness of our marketing strategies.\nFirst, let’s define simple extension functions to convert the LocalDate/LocalDateTime to a month and four-figure year format.\nfun LocalDate.toMonthOfYear(): String = \"$month, $year\"\nfun LocalDateTime.toMonthOfYear(): String = \"$month, $year\"\nNow, we’ll add the “month” column to our DataFrame:\nval starHistoryWithMonth = starHistoryFiltered.add(\"month\") {\n   starredAt.toMonthOfYear()\n}\nstarHistoryWithMonth\n\n\n\n\nNext, we’ll group the DataFrame by the “month” column and count the number of stars in each group.\nval starsCountMonthly = starHistoryWithMonth.groupBy { month }.count()\nstarsCountMonthly\n\n\n\n\nNext, we’ll add information about key events to the DataFrame. We’ll include the events in the corresponding months and set the value to null if there were no events.\nFirst, create a DataFrame with events and their corresponding months:\nval eventsDF = dataFrameOf(\"event\" to kandyEvents, \"month\" to kandyEventsDates.map {\n   it.toMonthOfYear()\n})\nThen, perform a left join with our main DataFrame at the month column:\nval starsMonthlyWithEvent = starsCountMonthly.leftJoin(eventsDF) { month }\nstarsMonthlyWithEvent\n\n\n\n\nNow, we can create a bar plot to visualize the distribution of new stars by month, along with the key events.\nstarsMonthlyWithEvent.plot {\n   bars {\n       x(month)\n       y(count)\n       alpha = 0.8\n       fillColor(event) { scale = categorical(eventColors, kandyEvents) }\n   }\n   // add horizontal markline with median of monthly count\n   hLine {\n       val medianMonthly = count.median()\n       yIntercept.constant(medianMonthly)\n       type = LineType.DASHED\n       color = Color.hex(\"#4b0082\")\n       width = 2.0\n   }\n   layout {\n       title = \"Kandy GitHub star history (monthly count)\"\n       size = 800 to 500\n       style {\n           legend.position = LegendPosition.Bottom\n           xAxis.text { angle = 30.0 }\n       }\n   }\n}\n\n\n\n\nThis plot shows the monthly distribution of stars, with bars representing the number of stars each month. The colors of the bars indicate key events, providing a clear visualization of how these events impacted the star counts. The dashed horizontal line represents the median star count per month.\nUnlike the overall star history chart, which shows cumulative growth, the monthly statistics plot helps you pinpoint the exact timing and impact of key events. By creating similar plots for your own projects, you can better understand the effectiveness of your promotional efforts, identify seasonal patterns, and plan future activities more effectively.\nUnderstand your audience\nUnderstanding the top programming languages of your stargazers can provide insights into your audience. With this in mind, we’ll use the GitHub REST API to find out the most popular languages among Kandy stargazers and visualize this data as a pie chart.\nLet’s write a function that requests user repositories:\nimport io.ktor.http.*\n\nfun getUserRepos(login: String): AnyFrame {\n   return DataFrame.readJsonStr(http.get(\"https://api.github.com/users/$login/repos\") {\n       // Set authorization header with token\n       bearerAuth(token)\n       // Add GitHub API custom \"accept\" header\n       header(HttpHeaders.Accept, \"application/vnd.github.v3+json\")\n   }.deserializeJson().jsonString)\n}\nNext, we’ll test this function on our sample repositories:\nval myRepos = getUserRepos(\"Kotlin\")\nmyRepos\n\n\n\n\nEach column in this DataFrame corresponds to a repository and contains different information about that repository. We are interested in the language column. We can count the most frequent language using the .valueCounts() method, where the first entry represents the most popular language:\nval myLanguagesCounts = myRepos.language.valueCounts(dropNA = false) // Don't drop nulls\nmyLanguagesCounts\n\n\n\n\nBecause the rows are sorted by count by default, identifying the most popular language is straightforward – it’s the first one.\nmyLanguagesCounts.language.first()\nKotlin\nTo generalize this process, we’ll write an extension function for a DataFrame obtained from the user’s repositories. This extension function will retrieve the most popular language (returning null if the account is private, has no repositories, or lacks sufficient information).\nfun AnyFrame.getTopLanguage(): String? {\n   //  Handle non-default response bodies (private account, no repositories, etc.)\n   if (!containsColumn(\"language\")) return null\n   return castTo(myRepos).language\n       .valueCounts(dropNA = false)\n       .castTo(myLanguagesCounts)\n       .language.let { languages ->\n           val first = languages.firstOrNull()\n           //  Try to pick the second value if the first one is null\n           if (first == null && languages.size() >= 2) {\n               languages[1]\n           } else first\n       }\n}\nNow, let’s retrieve the most popular languages for all stargazers. Note that this process might take some time to execute:\nval stargazersLanguages = kandyStarHistory.select {\n   login and login.map { login -> getUserRepos(login).getTopLanguage() }.named(\"language\")\n}\nstargazersLanguages\n\n\n\n\nNext, we’ll count the occurrences of each language:\nval languageCounts = stargazersLanguages.language.valueCounts() // Drops null by default\nlanguageCounts\n\n\n\n\nFinally, let’s plot these counts as a pie chart. We’ll take the seven most popular languages and group the remaining ones into an “other” category:\nlanguageCounts.let {\n   val takeFirst = 7\n   it.take(takeFirst).concat(\n       dataFrameOf(\"language\" to listOf(\"other\"), \"count\" to listOf(it.drop(takeFirst).sum {count}))\n   )\n}.plot {\n   pie {\n       slice(\"count\")\n       fillColor(\"language\")\n       size = 25.0\n       hole = 0.3\n   }\n   layout {\n       title = \"Kandy stargazers' most popular languages\"\n       style(Style.Void)\n   }\n}\n\n\n\n\nThe pie chart shows that Kotlin is the most popular language among Kandy stargazers, confirming our primary audience as Kotlin developers. The presence of Java suggests potential for further engagement with related ecosystems. The inclusion of less-common languages highlights the diversity of our user base, which is important for understanding different use cases and potential feature requests.\nThese insights can help tailor your project’s documentation, tutorials, and marketing efforts to better serve and expand your audience.\nCompare star growth: Kandy vs. Kotlin DataFrame\nComparing star data across different projects can provide valuable insights into their popularity and user engagement. Here, we’ll look at the growth of stars for Kandy alongside Kotlin DataFrame. These two projects, launched within a year of each other, target the same audience of Kotlin developers.\nTo ensure a fair comparison, we’ll use the introduction post date as the starting point for both libraries and examine the six months that followed. This way, we can see how each project grew over the same timeframe, giving us a clearer picture of their growth patterns.\nval repoDataframe = \"dataframe\"\n// Use the already written methods to get star history for DataFrame\nval dataFrameStarHistory = fetchStarHistory(ownerKotlin, repoDataframe, token).processStargazers()\n\n\n\n\nDefining the introductory post date for DataFrame:\nval dataFrameIntroductoryPostDate = LocalDate(2022, 6, 30)\nNext, we’ll define a function to process the star history for the six months following the introduction post:\n// Function that will slightly transform the dataframe with star history for a given library: \n// 1) Take a period of six months after the introduction post date; \n// 2) Add a column \"daysAfterPost\" with the number of days after the post date; \n// 3) Take the maximum number of stars for the day; \n// 4) Add a column \"library\" corresponding to the name of the library.\nfun AnyFrame.proccessAfterPostPeriod(introductionPostDate: LocalDate, library: String): AnyFrame {\n   // Six-month period after `introductionPostDate`\n   val period = (introductionPostDate - DatePeriod(days = 1))..(introductionPostDate + DatePeriod(months = 6))\n   return castTo(kandyStarHistory)\n       // Only take stars placed during that period\n       .filter { starredAt.date in period }\n       // Add daysAfterPost column with number of days after post\n       .add(\"daysAfterPost\") {\n           introductionPostDate.daysUntil(starredAt.date)\n       }\n       // Group by number of days and take the max value of `starsCount` for each group\n       .groupBy(\"daysAfterPost\").max { starsCount }\n       // Add a column with library name\n       .add(\"library\") { library }\n}\nFinally, we’ll combine the star histories for Kandy and DataFrame into a single DataFrame for comparison:\n// Count six-month history for both libraries and concatenate them into one DataFrame\nval kandyAndDataFrameStarHistory = kandyStarHistory\n   .proccessAfterPostPeriod(kandyIntroductoryPostDate, \"Kandy\")\n   .concat(\n       dataFrameStarHistory.proccessAfterPostPeriod(dataFrameIntroductoryPostDate, \"DataFrame\")\n   )\nkandyAndDataFrameStarHistory\n\n\n\n\nNext, we’ll visualize the comparison:\nkandyAndDataFrameStarHistory.plot {\n   line {\n       x(daysAfterPost) {\n           axis {\n               name = \"days after post\"\n           }\n       }\n       y(starsCount) {\n           axis.name = \"GitHub stars\"\n       }\n       color(library)\n   }\n   layout {\n       title = \"Kandy vs. DataFrame GitHub stars history\\nwithin 6 months after the introductory post\"\n       size = 800 to 500\n   }\n}\n\n\n\n\nFrom the initial observation, we can see that before the introduction post, both Kandy and Kotlin DataFrame had similar star counts. However, immediately after the post, Kandy showed a significantly higher growth rate, achieving nearly twice as many stars as DataFrame within the first six months.\nThis difference suggests several things. Firstly, it shows the growing interest in Kotlin for data projects. The period of time that elapsed from the initial DataFrame post and the Kandy post was about a year and a half. While DataFrame helped establish a community of Kotlin data enthusiasts, Kandy attracted a new audience interested in visualization.\nAdditionally, Kandy had more intense promotional activities within the six months following its first post, which likely contributed to its rapid growth.\nShared stargazers\nIt’s also interesting to see how many users have starred both Kandy and DataFrame. We hypothesize that there will be a significant overlap, since both libraries serve the same community of Kotlin developers. Here’s how we can analyze this and get the relevant data:\n// inner join star history dataframes of repositories by login,\n// getting a dataframe with all common stargazers, taking its size to get a number of them\nval commonStargazers = kandyStarHistory.innerJoin(dataFrameStarHistory) { login }.rowsCount()\nval kandyTotalStargazers = kandyStarHistory.rowsCount()\nval kandyOnlyStargazers = kandyTotalStargazers - commonStargazers\nval dataFrameTotalStargazers = dataFrameStarHistory.rowsCount()\nval dataFrameOnlyStargazers = dataFrameTotalStargazers - commonStargazers\nPlot this data as a pie chart:\nplot {\n   pie {\n       slice(listOf(commonStargazers, kandyOnlyStargazers, dataFrameOnlyStargazers))\n       fillColor(listOf(\"Common\", \"Kandy only\", \"DataFrame only\")) {\n           scale = categorical(\n               \"Common\" to Color.hex(\"#4A90E2\"),\n               \"Kandy only\" to Color.hex(\"#F5A623\"),\n               \"DataFrame only\" to Color.hex(\"#7ED321\"),\n           )\n           legend.name = \"\"\n       }\n       size = 25.0\n   }\n   layout {\n       title = \"Kandy & DataFrame stargazers ratio\"\n       style(Style.Void)\n   }\n}\n\n\n\n\nThe analysis shows that the majority of stargazers are unique to DataFrame, with fewer users starring both DataFrame and Kandy. Specifically, the share of DataFrame stargazers who also starred Kandy is quite small. This is probably because many users use DataFrame for data tasks that don’t involve visualization, making Kandy less relevant to them.\nInterestingly, only about a quarter of Kandy stargazers have also starred DataFrame. This suggests that Kandy has attracted a new audience mainly interested in plotting, rather than data processing. This reveals a great opportunity to promote how both libraries can work together.\nUsing Kandy for visualization and DataFrame for data processing allows users to benefit from the strengths of both libraries. This combination, as we’ve shown in this post, can help create powerful and comprehensive data analysis solutions. By highlighting this synergy, we can encourage more users to explore how these tools can complement each other and enhance their data projects.\nConclusion\nIn this post, we explored how to use Kotlin DataFrame and Kandy to dive into the star history of GitHub repositories. But it wasn’t just about looking at the numbers – it was about uncovering the stories those numbers tell.\nOne big takeaway is how quickly Kandy gained traction after its launch, highlighting a growing interest in visualization tools within the Kotlin community. Yet, we also found that many Kandy users haven’t tried DataFrame, and vice versa. This shows there’s an opportunity to help developers see how these tools can complement each other.\nWe also noticed that certain events, like blog posts and conferences, had a noticeable impact on star counts. This kind of insight can help you time your own announcements to get the most attention.\nWhat’s next?\nNow it’s your turn! Apply these techniques to your own repositories, analyze their star history, and create your own visualizations within Kotlin Notebook. All examples from this post are available as a Kotlin Notebook on GitHub or a Notebook on Datalore.\nWe’d love to see your results and hear your feedback. Join us in the #datascience channel on Kotlin Slack, or reach out via GitHub issues for Kandy or Kotlin DataFrame.\nIf you find our repositories useful, we’d really appreciate it if you starred them. Your support helps us improve and develop these tools further.\nWhat else to read and watch\nFor more information, check out the following resources:\nKotlin for Data Analysis Overview\nGet started with Kotlin Notebook\nA Step-by-Step Guide to Performing Data Analysis With Kotlin DataFrame\nData Analytics With Kotlin Notebooks, DataFrame, and Kandy",
        "dc:creator": "Andrei Kislitsyn",
        "content": "Kotlin DataFrame and Kandy are two powerful tools for data analysis in Kotlin. Kotlin DataFrame simplifies data manipulation and processing, while Kandy allows you to create visualizations directly within your Kotlin projects. In this post, we’ll show you how these tools can be used together within Kotlin Notebook to analyze the star history of GitHub [&#8230;]",
        "contentSnippet": "Kotlin DataFrame and Kandy are two powerful tools for data analysis in Kotlin. Kotlin DataFrame simplifies data manipulation and processing, while Kandy allows you to create visualizations directly within your Kotlin projects. In this post, we’ll show you how these tools can be used together within Kotlin Notebook to analyze the star history of GitHub […]",
        "guid": "https://blog.jetbrains.com/?post_type=kotlin&p=504121",
        "categories": [
          "ecosystem",
          "data-analysis",
          "dataframe",
          "kandy",
          "notebooks"
        ],
        "isoDate": "2024-08-26T14:37:54.000Z"
      },
      {
        "creator": "Alena Guzharina",
        "title": "A Complete Guide to Credit Risk Analysis With Python and Datalore AI",
        "link": "https://blog.jetbrains.com/datalore/2024/08/26/a-complete-guide-to-credit-risk-analysis-with-python-and-datalore-ai/",
        "pubDate": "Mon, 26 Aug 2024 14:34:49 +0000",
        "content:encodedSnippet": "This is a guest blog post by Ryan O’Connell, CFA, FRM.\nUnderstanding the relationships between various economic indicators is crucial for navigating the financial landscape. These relationships significantly impact the overall state of the economy, affecting businesses, investors, and individuals alike. \n“In this tutorial, we’ll focus on the complex interplay between federal funds rates, 10-year Treasury yields, and corporate bond yields – key indicators that shape investment strategies, economic forecasts, and policy decisions.”\n\n            \nRyan O’Connell\n                                                                CFA, FRM\n                                    \nWe’ll analyze data spanning over two decades of financial history, leveraging the power of Python, the Federal Reserve Economic Data (FRED) API, and Datalore’s AI-assisted coding capabilities. Python is particularly well-suited for this analysis due to its ability to efficiently pull and process large amounts of financial data through APIs. This approach allows for easy automation and replication of the analysis, offering a significant advantage over traditional spreadsheet methods.\nOur goal is to uncover patterns, anomalies, and insights that illuminate the predictive power of yield curve changes on credit spreads. By using Python, we can create a workflow that’s not only powerful but also easily updatable for future analyses in just a few clicks.\n\n\n\n\nIn this tutorial, you’ll learn how to:\nSet up your Python environment in Datalore for financial data analysis.\nUse Datalore’s AI Assistant to generate code for data retrieval and preprocessing.\nCreate visualizations to analyze yield curve dynamics and credit spreads.\nImplement statistical analyses to explore the relationship between yield curves and credit risk.\nLeverage AI to help interpret results and generate insights.\nWhether you’re a credit risk analyst looking to leverage more of Python within your stack or a curious newcomer to the world of credit risk, this tutorial will provide you with the tools and techniques to conduct your own in-depth analysis using Python and Datalore AI assistance.\nLet’s dive in and start coding!\n      \n      Open Datalore Code\n\n\n\n\n\nDisclaimer: This article is for informational and educational purposes only and is not intended to serve as personal financial advice.\nThe Yield Curve: A Financial Crystal Ball?\nBefore we dive into the data, let’s establish some context. The yield curve, particularly the spread between long-term and short-term interest rates, has long been considered a powerful predictive tool in finance. But why?\nEconomic expectations: The yield curve reflects market expectations about future economic conditions. A normal, upward-sloping curve suggests optimism, while an inverted curve often signals pessimism.\nMonetary policy: It provides insights into the market’s view of future monetary policy decisions by central banks.\nCredit risk: The shape of the yield curve can influence lending behaviors and, consequently, credit risk in the economy.\n\n\n\n\nOur analysis focuses on three key components:\nThe federal funds rate: the short-term interest rate at which banks lend to each other overnight.\nThe 10-year Treasury yield: a benchmark for long-term interest rates in the US.\nCorporate bond yields: representing the cost of borrowing for corporations.\n\n\n\n\nBy examining the relationships between these factors, we aim to uncover insights that could help predict changes in credit spreads and, by extension, credit risk in the market.\nSetting up your environment and retrieving data\nBefore we dive into the analysis, you have two options to get started:\nStart a new Datalore notebook to follow along with this tutorial from scratch.\nIf you have a Datalore account, you can click Edit copy on this Datalore report. This option allows you to see the full analysis and modify it as you go.\nWhichever option you choose, we’ll need to set up our Python environment in Datalore and retrieve the necessary data. We’ll be using the Federal Reserve Economic Data (FRED) API to fetch historical data on federal funds rates, 10-year Treasury yields, and corporate bond yields.\nSetting up your FRED API key\nTo use the FRED API, you’ll need to obtain an API key. Here’s how to set it up in Datalore:\nGo to the FRED API Key documentation and click on Request API Key.\nRegister for an account or log in if you already have one.\nOnce you have your API key, go to Environment in your Datalore workspace.\nClick on Environment Variables.\nSelect + New Variable.\nSet up the variable as follows:\n\nVariable title: FRED_API_KEY\nKey: FRED_API_KEY\nValue: Your actual FRED API key\n\n\n\n\nInstalling required packages\nIn Datalore, you can easily install required packages. For this tutorial, we’ll need the fredapi package. To install it:\nGo to the Environment tab in Datalore.\nClick on Package Management.\nSearch for ”fredapi” and click Install.\n\n\n\n\nDatalore’s Environment Manager\n\n\n\nRetrieving fed funds rate data with AI assistance\nNow, let’s use Datalore’s AI assistance to help us write code to retrieve and plot 20 years of federal funds rate data. \nDatalore AI\n\n\n\nHere’s how you can prompt the AI:\n\nAI Prompt\nWrite Python code to retrieve the last 20 years of federal funds rate data using the FRED API. Please include the following steps:\n1. Import necessary libraries (fredapi and pandas)\n2. Set up the FRED API connection using the FRED_API_KEY environment variable\n3. Retrieve the federal funds rate data (series ID: FEDFUNDS) for the last 20 years\n4. Convert the data to a pandas DataFrame\n5. Display the first few rows of the DataFrame\n6. Plot the federal funds rate over time using matplotlib\nNote: Include comments explaining each step of the process.\n\n\n\n\n\nHere’s the code the AI might generate:\n# Import necessary libraries\nimport pandas as pd\nfrom fredapi import Fred\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nimport os\n\n# Set up the FRED API connection using the environment variable\nfred = Fred(api_key=os.environ['FRED_API_KEY'])\n\n# Calculate the date 20 years ago from today\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=20*365)\n\n# Retrieve the federal funds rate data (series ID: FEDFUNDS) for the last 20 years\nfed_funds_data = fred.get_series('FEDFUNDS', start_date, end_date)\n\n# Convert the data to a pandas DataFrame\nfed_funds_df = pd.DataFrame(fed_funds_data, columns=['Rate'])\nfed_funds_df.index.name = 'Date'\n\n# Display the first few rows of the DataFrame\nprint(fed_funds_df.head())\n\n# Plot the federal funds rate over time\nplt.figure(figsize=(12, 6))\nplt.plot(fed_funds_df.index, fed_funds_df['Rate'])\nplt.title('Federal Funds Rate (Last 20 Years)')\nplt.xlabel('Date')\nplt.ylabel('Rate (%)')\nplt.grid(True)\nplt.show()\n      \n      Open Datalore Code\n\n\n\n\n\nThe graph that this code plots can be seen below.\nFederal funds rate: navigating two decades of monetary policy\nNow that we’ve retrieved and plotted the data, let’s analyze the key trends and events visible in our graph:\n\n\n\n\nKey observations from our initial data exploration of the federal funds rate:\nCyclical nature: The federal funds rate exhibits clear cyclical patterns over the 20-year period, reflecting the economic cycles and the Federal Reserve’s monetary policy responses.\nPre-2008 financial crisis: From 2004 to 2006, we observe a steady increase in the rate, peaking at around 5.25% in 2006–2007. This period of rising rates was likely in response to a growing economy and inflationary pressures.\n2008 financial crisis: The rate drops dramatically starting in 2007 and accelerates in 2008, coinciding with the Global Financial Crisis. The Fed rapidly cut rates to near zero in an attempt to stimulate the economy.\nExtended low-rate environment: From 2009 to 2015, we see an unprecedented period of near-zero interest rates, often referred to as the “zero lower bound” era. This reflects the Fed’s aggressive monetary policy to support economic recovery post-crisis.\nGradual normalization: Starting in late 2015, there’s a gradual increase in rates as the Fed began to normalize monetary policy in response to improving economic conditions.\nCOVID-19 pandemic impact: In early 2020, we observe another sharp drop to near-zero rates, mirroring the Fed’s emergency response to the economic shock of the COVID-19 pandemic.\nRecent rapid increase: From 2022 to 2023, we see a steep and rapid increase in rates, reaching levels not seen since before the 2008 crisis. This reflects the Fed’s aggressive tightening to combat high inflation.\n\n\n\n\nThese observations set the stage for our deeper analysis of how federal funds rate changes interact with other key indicators and influence credit risk. Next, we’ll retrieve Treasury yields and corporate bond rates.\nRetrieving 10-year Treasury rate data with AI assistance\nIn this section, we’ll focus on retrieving historical data for 10-year US Treasury yields, a key indicator in financial markets. The 10-year Treasury yield is often used as a benchmark for other interest rates and is crucial for understanding broader economic trends.\nWe’ll use the FRED API to fetch 20 years of daily data, giving us a comprehensive view of long-term interest rate movements.\nLet’s start by prompting the AI to write our data retrieval and visualization code. Here’s the prompt we’ll use:\n\nAI Prompt\nWrite Python code to retrieve the last 20 years of 10-year Treasury yields data using the FRED API. Please include the following steps:\n1. Import necessary libraries (if not already imported)\n2. Use the existing FRED API connection (assuming it’s already set up)\n3. Retrieve the 10-year Treasury yields data (series ID: DGS10) for the last 20 years\n4. Convert the data to a pandas DataFrame\n5. Display the first few rows of the DataFrame\n6. Plot the 10-year Treasury yields over time using matplotlib\nNote: Include comments explaining each step of the process.\n\n\n\n\n\nHere’s the code the AI might generate:\n# Import necessary libraries (if not already imported)\nimport pandas as pd\nfrom fredapi import Fred\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nimport os\n\n# Use the existing FRED API connection\nfred = Fred(api_key=os.environ['FRED_API_KEY'])\n\n# Calculate the date 20 years ago from today\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=20*365)\n\n# Retrieve the 10-year Treasury yields data (series ID: DGS10) for the last 20 years\ntreasury_10y_data = fred.get_series('DGS10', start_date, end_date)\n\n# Convert the data to a pandas DataFrame\ntreasury_10y_df = pd.DataFrame(treasury_10y_data, columns=['Yield'])\ntreasury_10y_df.index.name = 'Date'\n\n# Display the first few rows of the DataFrame\nprint(treasury_10y_df.head())\n\n# Plot the 10-year Treasury yields over time\nplt.figure(figsize=(12, 6))\nplt.plot(treasury_10y_df.index, treasury_10y_df['Yield'])\nplt.title('10-Year Treasury Yields (Last 20 Years)')\nplt.xlabel('Date')\nplt.ylabel('Yield (%)')\nplt.grid(True)\nplt.show()\n      \n      Open Datalore Code\n\n\n\n\n\nNow, let’s take a look at the plot of 20 years of long-term Treasury data.\n10-year Treasury yields: two decades of fluctuations\n\n\n\n\nNow that we’ve retrieved and plotted the data, let’s analyze the key trends and events visible in our graph:\n2008 financial crisis impact: There’s a sharp decline in yields starting in late 2007 and accelerating through 2008, mirroring the flight to safety during the financial crisis, though less dramatic than the federal funds rate drop.\nPost-crisis volatility and recovery: After the initial drop, yields showed significant volatility between 2009 and 2012, followed by a gradual increase until 2018, reflecting the uncertain economic recovery and the effects of changing monetary policies.\nCOVID-19 pandemic shock: In early 2020, we observe a dramatic plunge in yields to historic lows below 1%, coinciding with the economic shock of the COVID-19 pandemic and aggressive Fed intervention.\nRecent rapid increase: From 2021 onwards, there’s a sharp upward trend in yields, reaching levels not seen since before the 2008 crisis, aligning with rising inflation concerns and the Fed’s tightening monetary policy.\n\n\n\n\nThese observations provide crucial context for understanding how both short-term and longer-term interest rates have behaved in response to economic conditions and policy changes over the past two decades. As we proceed, we’ll explore how these rate movements interact to shape the overall yield curve and influence credit risk in the market.\nRetrieving corporate bond yields with AI assistance\nHaving examined the 10-year Treasury yields, we now turn to corporate bond yields, which provide insights into borrowing costs for corporations and serve as a key component in our credit risk analysis. We’ll use the ICE BofA US Corporate Index Effective Yield as our benchmark, offering a broad measure of US dollar-denominated investment-grade corporate debt performance.\nLet’s use Datalore’s AI assistance to generate code that retrieves and visualizes 20 years of daily data from the FRED API. Here’s the prompt:\n\nAI Prompt\nWrite Python code to retrieve the last 20 years of corporate bond yield data using the FRED API. Please include the following steps:\n1. Use the existing FRED API connection\n2. Retrieve the ICE BofA US Corporate Index Effective Yield data (series ID: BAMLC0A0CMEY) for the last 20 years\n3. Convert the data to a pandas DataFrame\n4. Handle any missing values appropriately\n5. Display the first few rows of the DataFrame\n6. Plot the corporate bond yields over time using matplotlib\nNote: Include comments explaining each step of the process.\n\n\n\n\n\nMy Datalore AI Assistant produced this code:\n# Use the existing FRED API connection\n# Assuming fred is already defined and the API key is set up as an environment variable\n\n# Calculate the date 20 years ago from today\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=20*365)\n\n# Retrieve the ICE BofA US Corporate Index Effective Yield data (series ID: BAMLC0A0CMEY) for the last 20 years\ncorporate_yield_data = fred.get_series('BAMLC0A0CMEY', start_date, end_date)\n\n# Convert the data to a pandas DataFrame\ncorporate_yield_df = pd.DataFrame(corporate_yield_data, columns=['Yield'])\ncorporate_yield_df.index.name = 'Date'\n\n# Handle missing values by forward filling (using the last known value)\ncorporate_yield_df = corporate_yield_df.fillna(method='ffill')\n\n# Display the first few rows of the DataFrame\nprint(corporate_yield_df.head())\n\n# Plot the corporate bond yields over time\nplt.figure(figsize=(12, 6))\nplt.plot(corporate_yield_df.index, corporate_yield_df['Yield'])\nplt.title('ICE BofA US Corporate Index Effective Yield (Last 20 Years)')\nplt.xlabel('Date')\nplt.ylabel('Yield (%)')\nplt.grid(True)\nplt.show()\n      \n      Open Datalore Code\n\n\n\n\n\nIn the next section, we will analyze the results of this code block.\nCorporate bond yields: two decades of risk and reward\n\n\n\n\nNow that we’ve retrieved and plotted the data, let’s analyze the key trends and events in corporate bond yields over the past 20 years:\n2008 financial crisis spike: The most dramatic feature of the graph is the sharp spike in yields during the 2008 financial crisis, reaching a peak of over 9%. This reflects the extreme risk aversion and liquidity concerns in the corporate bond market during this period.\nPre-crisis build-up: Leading up to the 2008 crisis, we observe a gradual increase in yields from 2004 to 2007, possibly indicating growing concerns about credit risk in the years preceding the financial crisis.\nPost-crisis decline and stabilization: Following the 2008 peak, yields declined rapidly and then stabilized, fluctuating between 3% and 4% for much of the period from 2010 to 2019. This reflects the impact of the Fed’s accommodative monetary policy and a general improvement in economic conditions.\nCOVID-19 pandemic impact: In early 2020, we see another significant, though less extreme, spike in yields coinciding with the onset of the COVID-19 pandemic. This spike was quickly reversed, likely due to swift central bank intervention.\nRecent upward trend: From 2021 onwards, we observe an upward trend in yields, reaching levels not seen since before the 2008 crisis. This aligns with rising inflation concerns and tightening monetary policy, reflecting increased borrowing costs for corporations.\n\n\n\n\nThese observations provide valuable insights into how corporate borrowing costs and perceived credit risk have evolved over the past two decades. In the next section, we’ll explore how these corporate bond yields interact with the federal funds rate and Treasury yields to shape the overall credit risk landscape.\nFed Funds Rate vs. 10-year Treasury yield and spread analysis\nAfter examining the federal funds rate and 10-year Treasury yields individually, we now turn our attention to the relationship between these two crucial indicators. The spread between these rates, often referred to as the yield curve slope, provides valuable insights into economic expectations and potential credit risk.\nThe yield curve spread is calculated as follows:\nYield Curve Spread = 10-Year Treasury Yield - Federal Funds Rate\nA positive spread indicates a “normal” yield curve, generally associated with economic growth expectations. Conversely, a negative spread signals an inverted yield curve, which is often seen as a predictor of economic downturns.\nLet’s use AI assistance to create a comprehensive line chart visualization that tracks these key elements. Here’s the prompt for the AI:\n\nAI Prompt\nPlot monthly Fed Funds Rate, Corporate Bond Yields, and their spread:\n1. Use ‘fed_funds_data’ for Fed Funds Rate\n2. Retrieve Corporate Bond Yield data (BAMLC0A0CMEY) from FRED\n3. Resample both to monthly frequency and align to common date range\n4. Calculate spread (Corporate Bond Yield minus Fed Funds Rate)\n5. Remove NaN values\n6. Plot all three lines with different colors\n7. Include legend, title, axis labels, and red dashed line at y=0\n8. Show summary statistics\n\n\n\n\n\nYour AI assistant should produce code that looks something like this:\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Resample both series to monthly frequency and align them\nfed_funds_monthly = fed_funds_data.resample('M').last()\ntreasury_10y_monthly = treasury_10y_data.resample('M').last()\n\n# Create a DataFrame with both series, using the common date range\nstart_date = max(fed_funds_monthly.index.min(), treasury_10y_monthly.index.min())\nend_date = min(fed_funds_monthly.index.max(), treasury_10y_monthly.index.max())\n\ndf = pd.DataFrame({\n    'Fed Funds Rate': fed_funds_monthly[start_date:end_date],\n    '10-Year Treasury Yield': treasury_10y_monthly[start_date:end_date]\n})\n\n# Calculate the spread\ndf['Spread'] = df['10-Year Treasury Yield'] - df['Fed Funds Rate']\n\n# Remove any rows with NaN values\ndf = df.dropna()\n\n# Plot the data\nplt.figure(figsize=(12, 6))\nplt.plot(df.index, df['Fed Funds Rate'], label='Fed Funds Rate', color='blue')\nplt.plot(df.index, df['10-Year Treasury Yield'], label='10-Year Treasury Yield', color='orange')\nplt.plot(df.index, df['Spread'], label='Spread', color='green')\n\nplt.title('Fed Funds Rate vs 10-Year Treasury Yield and Spread')\nplt.xlabel('Date')\nplt.ylabel('Rate (%)')\nplt.legend()\nplt.grid(True)\n\n# Add horizontal line at y=0\nplt.axhline(y=0, color='r', linestyle='--')\n\nplt.show()\n\n# Print some summary statistics\nprint(\"\\nSummary Statistics:\")\nprint(df.describe())\n      \n      Open Datalore Code\n\n\n\n\n\nIn the next section, we will dive into the plotted results of this code to draw some conclusions about the economy over time.\nInterpreting yield curve dynamics\n\n\n\n\nNow that we’ve visualized the data, let’s analyze the key trends and events in yield curve dynamics:\nPre-2008 inversion: Leading up to the 2008 financial crisis, we observe a period of yield curve inversion (negative spread), with the fed funds rate exceeding the 10-year Treasury yield. This inversion is often seen as a predictor of economic recessions. In this particular case, the yield curve inversion was an accurate leading indicator.\nPost-crisis widening: Following the 2008 crisis, we see a significant widening of the spread as the Fed Funds Rate dropped dramatically while the 10-year Treasury Yield declined more gradually. This wide spread reflects the Fed’s aggressive monetary easing and attempts to stimulate long-term borrowing and investment.\nExtended period of positive spread: From 2009 to 2019, the spread remained consistently positive, with long-term rates higher than short-term rates. This “normal” yield curve is generally associated with expectations of economic growth.\nRecent inversions: We observe two notable periods of inversion in recent years – a brief inversion in 2019 and a more pronounced one starting in 2022. These inversions coincide with economic uncertainties, including trade tensions and inflation concerns.\nPandemic impact and recovery: The spread widened sharply at the onset of the COVID-19 pandemic in 2020 as the Fed cut rates to near zero. Subsequently, we see a rapid narrowing and eventual inversion as the Fed aggressively raised rates to combat inflation. As of our analysis in 2024, we are currently experiencing an inverted yield curve, with short-term rates higher than long-term rates. Historically, such inversions have often preceded economic recessions, suggesting the possibility of an economic downturn in the coming years. However, it’s important to note that while yield curve inversions have been reliable predictors in the past, economic conditions can vary, and past performance doesn’t guarantee future outcomes.\n\n\n\n\nThese yield curve dynamics provide crucial context for understanding market expectations and potential economic turning points. The current inversion underscores the importance of monitoring these indicators closely in the coming months and years.\n10-year Treasury yield vs. corporate bond yields analysis\nIn this section, we’ll explore a crucial relationship in fixed-income markets: the comparison between 10-year Treasury yields and corporate bond yields. This analysis is fundamental to understanding credit risk in the economy.\nThe spread between these yields, known as the credit spread, is calculated as:\nCredit Spread = Corporate Bond Yield - 10-Year Treasury Yield\nThis spread represents the additional return investors demand for taking on the risk of lending to corporations rather than the US government. A widening spread typically indicates increased perceived risk, while a narrowing spread suggests improving conditions or increased risk appetite.\nLet’s use AI assistance to create a visualization that includes:\nThe 10-year Treasury yield.\nThe corporate bond yield (using the ICE BofA US Corporate Index Effective Yield as a proxy).\nThe spread between these rates.\n\n\n\n\nHere’s the prompt for the AI:\n\nAI Prompt\nPlot monthly 10-Year Treasury Yield, Corporate Bond Yields, and their spread:\n1. Use ‘treasury_10y_data’ for 10-Year Treasury Yield\n2. Retrieve Corporate Bond Yield data (BAMLC0A0CMEY) from FRED\n3. Resample both to monthly frequency and align to common date range\n4. Calculate spread (Corporate Bond Yield minus 10-Year Treasury Yield)\n5. Remove NaN values\n6. Plot all three lines with different colors\n7. Include legend, title, axis labels, and red dashed line at y=0\n8. Show summary statistics\n\n\n\n\n\nThe code output should look like this:\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom fredapi import Fred\nimport os\nfrom datetime import datetime, timedelta\n\n# Use the existing FRED API connection\nfred = Fred(api_key=os.environ['FRED_API_KEY'])\n\n# Calculate the date range\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=20*365)\n\n# Retrieve the corporate bond yield data\ncorporate_yield_data = fred.get_series('BAMLC0A0CMEY', start_date, end_date)\n\n# Resample both series to monthly frequency\ntreasury_10y_monthly = treasury_10y_data.resample('M').last()\ncorporate_yield_monthly = corporate_yield_data.resample('M').last()\n\n# Create a DataFrame with both series, using the common date range\nstart_date = max(treasury_10y_monthly.index.min(), corporate_yield_monthly.index.min())\nend_date = min(treasury_10y_monthly.index.max(), corporate_yield_monthly.index.max())\n\ndf = pd.DataFrame({\n    '10-Year Treasury Yield': treasury_10y_monthly[start_date:end_date],\n    'Corporate Bond Yield': corporate_yield_monthly[start_date:end_date]\n})\n\n# Calculate the spread\ndf['Spread'] = df['Corporate Bond Yield'] - df['10-Year Treasury Yield']\n\n# Remove any rows with NaN values\ndf = df.dropna()\n\n# Plot the data\nplt.figure(figsize=(12, 6))\nplt.plot(df.index, df['10-Year Treasury Yield'], label='10-Year Treasury Yield', color='blue')\nplt.plot(df.index, df['Corporate Bond Yield'], label='Corporate Bond Yield', color='orange')\nplt.plot(df.index, df['Spread'], label='Spread', color='green')\n\nplt.title('10-Year Treasury Yield vs Corporate Bond Yield and Spread')\nplt.xlabel('Date')\nplt.ylabel('Rate (%)')\nplt.legend()\nplt.grid(True)\n\n# Add horizontal line at y=0\nplt.axhline(y=0, color='r', linestyle='--')\n\nplt.show()\n\n# Print some summary statistics\nprint(\"\\nSummary Statistics:\")\nprint(df.describe())\n      \n      Open Datalore Code\n\n\n\n\n\nNow, let’s dive into the results of this analysis in the next section.\nInterpreting credit spread dynamics\n\n\n\n\nNow that we’ve visualized the data, let’s analyze the key trends in credit spread dynamics:\n2008 financial crisis impact: The most striking feature of the graph is the dramatic widening of the spread during the 2008 financial crisis. This spike reflects a severe increase in perceived credit risk as investors fled to the safety of government bonds, demanding much higher yields for corporate debt.\nPre-crisis build-up: In the years leading up to the 2008 crisis, we observe a gradual narrowing of the spread, suggesting increasing investor comfort with corporate credit risk. This trend reversed sharply with the onset of the crisis.\nPost-crisis normalization: Following the crisis, the spread gradually narrowed but largely remained elevated compared to pre-crisis levels for several years, indicating a more cautious approach to credit risk in the aftermath of the financial turmoil.\nCOVID-19 pandemic spike: In early 2020, we see another significant, though less extreme, widening of the spread coinciding with the onset of the COVID-19 pandemic. This reflects the sudden increase in perceived corporate credit risk due to economic uncertainties.\nRecent trends: Since the pandemic spike, the spread has narrowed but remains above pre-pandemic levels, suggesting that while credit risk perceptions have improved, investors still maintain a degree of caution in the current economic environment.\n\n\n\n\nThese credit spread dynamics provide crucial insights for investors and policymakers. A widening spread often signals increasing economic stress and may precede broader market downturns. Conversely, a narrowing spread typically indicates improving conditions and increased confidence in corporate debt.\nHowever, it’s important to consider credit spreads alongside other economic indicators for a comprehensive view of market conditions and credit risk. In the next section, we’ll explore how these insights can be applied to predictive analysis and risk management strategies.\nCongratulations! You’ve made it this far into our tutorial. Want more? Read on to explore the relationship between yield curve changes and credit spreads.\n[Advanced] Yield curve changes and credit spreads\nIn this section, we’ll explore the predictive relationship between yield curve changes and credit spreads. This analysis is crucial for understanding how changes in interest rates might signal future shifts in credit risk.\nWe’ll focus on two key questions:\nDo changes in the yield curve precede changes in credit spreads?\nHave yield curve inversions historically led to widening credit spreads?\n\n\n\n\nLet’s use AI assistance to create a comprehensive Python script that analyzes these relationships:\n\nAI Prompt\nCreate a Python script to analyze yield curve slopes and credit spread changes:\n1. Use FRED API for 40 years of:\n– Federal funds rate\n– 10-year Treasury yield\n– Corporate bond yield\n2. Calculate:\n– Yield curve slope\n– Credit spread\n– Identify inversions\n3. Analyze periods: 1, 7, 14, 30, 60, 90, 180, 270, 365, 547, 730, 1095 days\nFor each:\n– Compute forward credit spread changes\n– Calculate correlations and mean changes (normal/inverted periods)\n– Perform t-tests (normal vs inverted)\n4. Store results in DataFrame\n5. Plot:\n– Correlation vs forward period\n– Mean changes vs forward period\n6. Print results\n\n\n\n\n\nTo check the resulting code, go to Datalore:\n      \n      Open Datalore Code\n\n\n\n\n\nLet’s delve deeper into the relationship between yield curve slopes and future changes in credit spreads. Our goal is to understand how yield curve inversions might predict future spikes in credit risk and, by extension, potential economic downturns.\nMethodology:\nWe analyzed the correlation between the yield curve slope (10-year Treasury yield minus fed funds rate) and forward changes in credit spreads (corporate bond yield minus 10-year Treasury yield) for various future time periods, ranging from 1 day to 3 years (1,095 days).\nAnalysis of correlation between yield curve slope and forward credit spread change\nKey observations:\nShort-term correlations (1–90 days): We observe weak negative correlations, suggesting limited predictive power in the immediate future.\nMedium-term correlations (180–547 days): There’s a stronger negative relationship, with the peak negative correlation occurring around the 547-day mark. This indicates that yield curve inversions tend to precede credit spread widening most reliably in this time frame.\nLong-term correlation (1,095 days): Interestingly, the correlation turns positive, suggesting a potential reversal or cyclical nature of the relationship over very long periods.\n\n\n\n\nImplications:\nThe strongest predictive power of yield curve inversions for credit spread widening appears to be in the 1–2 year range.\nThis aligns with the common observation that recessions often follow yield curve inversions by 12–24 months.\nThe positive correlation at 1,095 days warrants further investigation and could relate to long-term economic cycles.\n\n\n\n\nAnalysis of mean forward credit spread change\n\n\n\n\nThis graph illustrates the average change in credit spreads over various future time periods, comparing normal yield curve periods (blue line) to inverted yield curve periods (orange line).\nKey observations:\nNormal yield curve periods (blue line):\n\nThe line remains relatively flat and close to zero across all time periods.\nThere’s a slight dip into negative territory for medium-term periods (180–547 days), suggesting a tendency for mild credit improvement during normal economic conditions.\nLong-term periods (730–1,095 days) show a small positive change, indicating a slight widening of credit spreads over extended time frames.\nInverted yield curve periods (orange line):\n\nShort-term periods (1–90 days) show small positive changes, indicating slight immediate credit deterioration following inversions.\nMedium-term periods (180–547 days) exhibit large positive changes, peaking around the 547-day mark. This suggests significant credit spread widening in the 1–2 year period following yield curve inversions.\nLong-term periods (730–1,095 days) show a dramatic reversal, with the 1,095-day point dropping into negative territory. This indicates a potential improvement in credit conditions or a cyclical effect over very long time frames.\nComparison between normal and inverted periods:\n\nThe divergence between the two lines is most pronounced in the medium-term range (180–547 days), highlighting the predictive power of yield curve inversions for credit spread changes in this timeframe.\nThe convergence and eventual crossover of the lines in the long term (1,095 days) is particularly interesting, suggesting a potential “mean reversion” effect over extended periods.\n\n\n\n\nImplications:\nTiming of credit risk: The peak in the inverted line around 547 days suggests that the greatest credit risk following a yield curve inversion typically manifests between 1.5 to 2 years after the inversion occurs.\nMagnitude of impact: The large gap between normal and inverted lines in the medium term indicates that yield curve inversions precede significantly larger credit spread widenings compared to normal periods.\nRisk management strategies: Financial institutions and investors might consider implementing more conservative credit policies or increasing hedges against credit risk in the 1–2 year period following yield curve inversions.\nEconomic cycle insights: The reversal seen at 1,095 days could indicate the typical length of a credit cycle or the delayed effects of policy interventions following economic stress periods.\nPredictive power: This graph reinforces the idea that yield curve inversions can serve as early warning indicators for increased credit risk, with a lead time of approximately 1–2 years.\n\n\n\n\nWhile these patterns offer guidance, they should not be treated as infallible predictors. Economic conditions can vary, and past patterns may not always repeat in the same way. Nonetheless, this analysis offers a framework for understanding the relationship between yield curve dynamics and future credit risk, providing a valuable tool for economic forecasting and risk management.\n[Advanced] Credit spread analysis with economic events: a 90-day forward look\nIn this final section, we’ll explore the dynamic relationship between credit spreads and major economic events, focusing on a 90-day forward-looking perspective. This analysis provides crucial insights into how credit risk perceptions evolve over relatively short periods, offering a window into market sentiment and economic conditions.\nWe’ll use AI assistance to create a comprehensive Python script that combines data manipulation, statistical analysis, and advanced visualization techniques. Our analysis will:\nCalculate 90-day forward changes in credit spreads.\nIdentify and visualize outliers in credit spread changes.\nMap these changes to significant economic events from 2001 to 2020.\nCompare credit spread behavior during normal and inverted yield curve periods.\nConduct statistical tests to validate our observations.\n\n\n\n\nLet’s prompt the AI to help us create this analysis:\n\nAI Prompt\nCreate a Python script to analyze and visualize 90-day forward credit spread changes in relation to major economic events:\n1. Use existing DataFrame ‘df’ with credit spread data\n2. Calculate 90-day forward change in credit spreads: diff(periods=90).shift(-90)\n3. Identify outliers using IQR method\n4. Create a time series plot:\n– X-axis: Date (show every 5 years)\n– Y-axis: 90-day forward change in credit spread\n– Plot main line for spread changes\n– Highlight outliers as red scatter points\n5. Add vertical lines for economic events (2001-2020):\n– Enron Bankruptcy, WorldCom Bankruptcy, Lehman Brothers Bankruptcy\n– European Debt Crisis, 2019 Repo Spike, COVID-19 Pandemic Declaration\n6. Customize plot: large fonts, clear labels, grid, legend\n7. Print summary statistics:\n– Number and percentage of outliers\n– Top 10 positive and negative outliers\n– Mean spread change for normal and inverted yield curve periods\n8. Perform t-test comparing normal and inverted periods\nUse pandas for data manipulation, matplotlib for plotting, seaborn for styling, and scipy.stats for the t-test. Ensure the script is well-commented and follows Python best practices.\n\n\n\n\n\nTo check the generated code, go to Datalore:\n      \n      Open Datalore Code\n\n\n\n\n\nLet’s dive into the results of this code and get a unique look at the history of the United States through the lens of credit spread changes.\n\n\n\n\nNow that we’ve visualized the data and generated our statistics, let’s analyze the key trends and events:\nVolatility clusters: The graph reveals distinct periods of high volatility in credit spread changes, particularly around major economic events. This clustering of volatility suggests that credit risk perceptions can change rapidly and dramatically during times of economic uncertainty.\nAsymmetric outliers: We observe that the most extreme outliers (red dots) tend to be on the positive side, indicating sudden, large increases in credit spreads. This asymmetry suggests that credit risk can spike more sharply than it tends to decrease, reflecting the market’s sensitivity to negative news.\nMajor economic events:\n\nDot-com bubble (2000–2002): We see increased volatility and some outliers, reflecting the tech market crash and subsequent economic uncertainty.\n2008 financial crisis: The most dramatic spike in our dataset occurs around the Lehman Brothers bankruptcy, with extreme positive outliers indicating a severe credit crunch.\nEuropean debt crisis (2011): Another cluster of outliers is visible, showing how international events can impact US credit markets.\nCOVID-19 pandemic (2020): A sharp spike in credit spread changes, rivaling the 2008 crisis in magnitude, demonstrates the immediate and severe impact of the pandemic on credit risk perceptions.\nRecovery patterns: Following each major spike, we observe periods of negative changes in credit spreads, indicating a gradual return to lower credit risk perceptions. However, the recovery patterns vary in duration and stability.\nRecent trends: In the years leading up to 2020, we see relatively stable credit spread changes with fewer outliers, possibly reflecting the prolonged period of low interest rates and economic growth. The COVID-19 pandemic disrupts this stability dramatically.\nFrequency of outliers: Outliers appear to be more frequent and extreme during recognized periods of economic stress, serving as a visual indicator of market turmoil.\n\n\n\n\nLimitations:\nWhile this 90-day forward view provides valuable insights, it doesn’t capture longer-term trends or very short-term fluctuations. The causes behind these changes can be complex and multifaceted, often requiring additional context to fully interpret.\nThis analysis offers a nuanced view of credit risk dynamics over the past two decades. It highlights the market’s sensitivity to major economic events and provides a valuable tool for anticipating potential shifts in credit risk perceptions.\nConclusion: applying your new credit risk analysis skills\nCongratulations on completing this comprehensive tutorial on credit risk analysis using Python and AI-assisted coding in Datalore! Let’s recap what you’ve learned and how you can apply these skills in your own financial analyses:\nKey skills acquired\nData retrieval and preprocessing: You’ve learned how to use the FRED API to fetch economic data and prepare it for analysis using pandas.\nVisualization techniques: You’ve created various plots to visualize federal funds rates, Treasury yields, and corporate bond yields, enhancing your data presentation skills.\nYield curve analysis: You’ve explored how to calculate and interpret yield curve slopes and inversions, crucial indicators for economic forecasting.\nCredit spread calculation: You’ve computed and analyzed credit spreads, gaining insights into market risk perception.\nPredictive analysis: You’ve conducted forward-looking analyses to understand the relationship between yield curve changes and future credit spread movements.\nEvent impact assessment: You’ve examined how major economic events affect credit spreads, developing skills in contextual data interpretation.\nAI-assisted coding: Throughout the tutorial, you’ve leveraged AI to generate code, demonstrating how to use this powerful tool to streamline your workflow.\n\n\n\n\nApplying your skills\nCustom analyses: Use the code templates provided to analyze different time periods or other economic indicators that interest you.\nRisk management: Apply the yield curve inversion and credit spread analysis techniques to assess potential risks in your investment portfolio.\nEconomic forecasting: Utilize the predictive analysis methods to create your own forecasts for credit risk and economic conditions.\nData-driven decision making: Incorporate these data analysis techniques into your financial decision-making processes, whether for personal investments or professional applications.\nContinuous learning: As you’ve seen, economic conditions evolve. Use the skills you’ve learned to stay updated on market trends and continually refine your analysis techniques.\n\n\n\n\nNext steps\nExpand your dataset: Try incorporating additional economic indicators or international data to broaden your analysis.\nEnhance your models: Experiment with more advanced statistical methods or machine learning techniques to improve predictive power.\nAutomate your analyses: Create scripts that automatically update your analyses as new data becomes available with Datalore’s Scheduled runs feature.\nCollaborate and share: Use Datalore’s collaboration features to work with others and share your insights.\n\n\n\n\nWhile these tools and techniques are powerful, they should be used in conjunction with a nuanced understanding of economic principles and current market conditions. Stay curious, keep practicing, and continue to refine your credit risk analysis skills!\nIf you’d like to edit the full code of this tutorial in Datalore, including interactive elements and additional resources, click the button below:\n      \n      Open Datalore Code",
        "dc:creator": "Alena Guzharina",
        "content": "This is a guest blog post by Ryan O’Connell, CFA, FRM. Understanding the relationships between various economic indicators is crucial for navigating the financial landscape. These relationships significantly impact the overall state of the economy, affecting businesses, investors, and individuals alike. We&#8217;ll analyze data spanning over two decades of financial history, leveraging the power of [&#8230;]",
        "contentSnippet": "This is a guest blog post by Ryan O’Connell, CFA, FRM. Understanding the relationships between various economic indicators is crucial for navigating the financial landscape. These relationships significantly impact the overall state of the economy, affecting businesses, investors, and individuals alike. We’ll analyze data spanning over two decades of financial history, leveraging the power of […]",
        "guid": "https://blog.jetbrains.com/?post_type=datalore&p=505317",
        "categories": [
          "datalore-ai",
          "python-for-finance"
        ],
        "isoDate": "2024-08-26T14:34:49.000Z"
      },
      {
        "creator": "Andrey Belyaev",
        "title": "Workspaces in IntelliJ IDEA",
        "link": "https://blog.jetbrains.com/idea/2024/08/workspaces-in-intellij-idea/",
        "pubDate": "Mon, 26 Aug 2024 09:01:26 +0000",
        "content:encodedSnippet": "Better late than never! This idiom is probably the best way to describe the functionality we’ll introduce in this article, specifically, support for workspaces in IntelliJ IDEA.\nIntro\nIn a nutshell, a workspace is a meta-project that allows you to manage multiple projects simultaneously. This feature is useful for various reasons, from coordinating complex development environments to having a couple of unrelated projects on your hands\nNote for the impatient: There will be some theory and history included below, so if you’re eager to get workspaces into your IDE, just go to the How to use workspaces chapter and crack on! Also, join the upcoming webinar for more details. Please note that this feature is currently in preview, so you may encounter some bugs and limitations. \nThere’s been a ticket for workspace support on our issue tracker since 2011. However, the rise of microservices and monorepositories in the mid-2010s seemed to make workspaces obsolete. The monorepo model was totally aligned with IntelliJ IDEA’s focus on the “one project – multiple modules” approach. Yet, demand for workspaces continued to grow, something confirmed by the increasing number of likes and supportive comments from users on YouTrack. In the next section, we’ll explore why workspaces are still relevant in 2024.\nWhy monorepos didn’t put an end to the workspace approach \nMonorepos indeed have plenty of advantages over other approaches to code arrangement, namely multi-repos, namely:\nBetter collaboration. All developers are able to see what’s going on in other parts of the project they’re working on. Every change becomes visible as soon as the code is committed. While implementing a service that communicates with another service written by another team, developers can see the other services’ code to get an understanding of how it works.\nEasier dependency management. In most cases, all projects in a monorepo use the same package management and build tool, making it easier to track dependencies and versions. Also, all application components in a monorepo are usually released together, meaning there is no risk of problems arising with inter-project dependencies. \nCross-project refactoring. Having access to the entire codebase allows the IDE to index code references, meaning it can detect changes and easily perform refactorings correctly.\nImproved code discoverability. New team members can easily navigate the entire codebase and understand the dependencies between different projects and components.\n\n\n\n\n\nIf you check out a monorepo and open it with IntelliJ IDEA, you’ll see that the monorepo structure fits into the “project-modules” paradigm perfectly. The root folder becomes a “project”, and all subfolders become “modules”. So, on this basis, it looked like the model worked fine, and the implementation of the workspaces feature could be postponed.\nHowever, monorepos have some disadvantages:\nMonorepos tend to grow quickly. Loading a massive codebase and gathering metadata about the code can be time-consuming. The vast number of commits to all projects within the monorepo can lead to branching, merging, and rebasing operations taking longer than expected. \nRestricting access to subfolders might become a challenge. For example, with Git, you can do this by using submodules or provider-specific features like GitHub’s Codeowners.\nCI/CD integration can be challenging if different projects within a monorepo have different release cycles.\nIn addition to the purely technical challenges of using monorepos, we can add a behavioral dimension. Usually, developers don’t work on the entire monorepo code. Instead, they tend to focus on an isolated service or a few of them. \nSo, despite offering many advantages, usage of the monorepo approach started to wane due to the following:\nPerformance problems: Cloning, pulling, and merging code became challenging as soon as the codebase began to grow.\nLimited autonomy: In a monorepo, it’s harder to develop and release a selected project independently from other parts of the whole system compared to the multi-repository approach. In addition to this, access control remained a challenge.\nUnknown boundaries: Projects are treated as a single monolith, so code and abstractions can “leak” into other projects, creating tight coupling. Another consequence is a “broken master”. If just one project within the monorepo is broken, the whole monorepo is treated as broken.\nDistracted developer’s focus: While developers usually work on a single small project, with monorepos, they have to deal with a codebase of dozens of projects.\nThe above has contributed to increasing votes on issue IDEA-65293, making it evident that we had to include workspaces as a new feature in our IDEs. \nAs a confirmation, surveys show that microservice developers use multiple repositories for development, and this is a growing trend.\n\n\n\n\nWorkspaces improve developers’ experience when they work with multiple projects and repositories. Please note that workspaces can also be useful for developers who prefer monorepos. Having the ability to check out particular projects from a big monorepo and work on those only is a great benefit.\nIf you’d like to dive deeper into this topic, there are some handy articles like “The issue with monorepos” and “Monorepo is a bad idea” that explain things in greater detail.\nWhat is a workspace?\nIn one of the discussions on the JetBrains support forum, someone posted the following:\n\nProject: This is essentially a description of your final software product.\nWorkspace doesn’t have anything to do with actual projects and/or software products! It is (ideally) your IntelliJ IDEA’s desktop, merely a container for projects you’re currently working with. This is your current view of your developer’s environment. It does not produce results, but controls how results are produced.\n\n\n\n\nThis is the best explanation for the “workspace” concept introduced in this plugin. You can treat a workspace as a collection of the projects that you work on. Every project can use its own technology stack and a build tool and can be run independently.\nUser scenarios for workspaces\nWhen we made the plugin, we thought about a “typical” setup for an application, comprising:\nA set of backend services.\nAn API gateway.\nClient applications, such as a React web app or a mobile app.\nA collection of custom libraries (for example, Spring Boot starters) used across services.\nThe code for these components might be organized in a single monorepo or across multiple repositories. Developers might also need to create small “satellite” applications for testing and debugging components like libraries, without the need to commit these auxiliary apps to the main codebase.\nIt’s important to note that this architectural setup can vary. For instance, in a monolithic application, the backend might be consolidated into a single service, and the API gateway could become optional. In another scenario, if an application lacks a client interface, the focus would be solely on the backend services and the API gateway.\nBased on this, we’ve come up with the following major use cases and corresponding workspace configurations for the plugin:\n1. Fullstack developer\nIn many cases, we need to update both front-end and back-end components, which could be stored in different repositories. The workspaces plugin allows us to check out projects from repositories independently, add them to the same workspace, and work with their code as if they were in one project.\n\n\n\n\n2. Microservices developer\nLet’s assume that we have dozens of microservices in a monorepo, and we need to update only some of them. There’s no longer any need to import the entire repository into IntelliJ IDEA. We can create a workspace and add only the required services to it. Moreover, we can also use different build tools for different projects. If you build one service with Maven and another with Gradle – no problem!\n\n\n\n\n3. Library developer\nThis scenario can be used when we need a simple “Hello, World!” application to test a solution. The perfect example of this is the development of a Spring Boot starter. The test application won’t be committed to the code repository, but we may need it to simplify development. In this case, we can create a simple project right in the workspace in addition to the starter project, run the app, debug the starter, and then just discard the app or reuse it in another workspace.\n\n\n\n\nCurrent implementation\nA workspace is a separate folder that stores references to projects from other folders. Additionally, we can create a project inside a workspace folder if needed. A workspace should not be invasive in terms of project settings. IntelliJ IDEA stores project settings, such as code style, SDK, etc., in a separate file in a project-specific folder (.idea). When we add a project to a workspace, IntelliJ IDEA copies its settings to the workspace folder, and then we can change them locally. This means that one project can be included in several workspaces with different settings. The picture below shows how this works. \n\n\n\n\nSince the functionality of workspaces is still a work in progress, we’ve implemented it as a separate non-bundled plugin to be able to release it faster as we continue to improve it based on the feedback we get from users. We hope that, sooner or later, this feature will become a part of the core IDE functionality.\nHow to use workspaces\nInstall the plugin from JetBrains Marketplace.\nCreate a workspace as a new project type.\n\n\n\n\n\nGive your workspaces folder a name and add all required projects to it.\n\n\n\n\nYou can run projects separately using well-known run configurations in IDEA or Maven/Gradle commands. To run several projects at once, for example, microservices, you’ll probably need a special shell script or Docker compose file. We tried to make the user experience as familiar as possible.\nCurrent version limitations\nLet us share some limitations we are aware of. This isn’t a comprehensive list, but it includes key issues that might seem crucial to you and could prevent you from trying the workspaces today. \nWe do not copy run configurations from the included projects to the workspace. For the time being, you will need to create run configurations manually after project import.\nThere is no constant settings synchronization. For example, if you used JDK 17 in the project and then included it in the workspace – the workspace will use JDK 17. If we upgrade JDK to a newer version in the “main” project settings, the workspace won’t “see” this change.\nProject renaming is not supported.\nResolving dependencies across independent projects is a work in progress.\nYou can find the full list of issues linked to the YouTrack ticket IDEA-65293. \nFurther plans\nWe are currently working on the implementation of a number of tasks to make workspaces support even better, notably:\nProject settings synchronization.\nAbility to store and share workspace in VCS.\nMore build tools to support.\nMaking workspace a first-class citizen in all IDEs.\nSimplify run configuration for running multiple projects at once.\nSmooth debugging across projects.\nFeedback welcome! \n\n\n\n\nThe workspaces feature is far from finished. We’ve only just begun our work as we edge another step toward our goal of creating the ideal IDE. We welcome all feedback you might have, it will help us in our efforts! Try out the new workspaces feature, and let us know your thoughts. Your feedback will help us make IntelliJ IDEA even better. You can rate our efforts on the plugin page or add your thoughts and report issues in YouTrack.\n5-star us if you like the initiative!",
        "dc:creator": "Andrey Belyaev",
        "content": "Better late than never! This idiom is probably the best way to describe the functionality we’ll introduce in this article, specifically, support for workspaces in IntelliJ IDEA. Intro In a nutshell, a workspace is a meta-project that allows you to manage multiple projects simultaneously. This feature is useful for various reasons, from coordinating complex development [&#8230;]",
        "contentSnippet": "Better late than never! This idiom is probably the best way to describe the functionality we’ll introduce in this article, specifically, support for workspaces in IntelliJ IDEA. Intro In a nutshell, a workspace is a meta-project that allows you to manage multiple projects simultaneously. This feature is useful for various reasons, from coordinating complex development […]",
        "guid": "https://blog.jetbrains.com/?post_type=idea&p=505111",
        "categories": [
          "features",
          "tutorials",
          "intellijidea-2",
          "monorepos",
          "monorepositories",
          "workspaces"
        ],
        "isoDate": "2024-08-26T09:01:26.000Z"
      },
      {
        "creator": "Olga Bedrina",
        "title": "The Fail-Fast Strategy: What Is It and How Does TeamCity Support It?",
        "link": "https://blog.jetbrains.com/teamcity/2024/08/fail-fast-strategy-teamcity/",
        "pubDate": "Fri, 23 Aug 2024 09:57:01 +0000",
        "content:encodedSnippet": "This is a guest post from Cameron Pavey, draft.dev.\nSoftware development teams are always looking for ways to move faster and deliver more value in less time. However, one common pitfall for many teams is spending far too much time and effort building something, only to encounter major issues late in the project’s lifecycle.\nThe fail-fast strategy addresses this problem by “shifting left” on potential points of failure and embracing them as part of a virtuous feedback loop. Whether in project management or software development, a failure is often just a signal that something needs to change. The earlier you can detect this signal, the sooner you can adjust for it, thus mitigating the risk of wasted work.\nTo properly implement a fail-fast strategy, you need a tool that supports this way of working. TeamCity is a CI/CD solution that complements the fail-fast strategy and has several features to help you implement it.\nIn this guide, you’ll learn more about the fail fast strategy and how you can leverage TeamCity to adopt this strategy for yourself.\nWhat is the fail-fast strategy?\nIn software development, the fail-fast strategy emphasizes iterative discovery over strict planning. Key principles include:\nRapid feedback loops: Short cycles allow for frequent releases and early feedback, helping teams move quickly and efficiently.\n\n\n\n\nProactive risk management: Early identification of risks allows for timely mitigation or pivoting, reducing wasted effort.\n\n\n\n\nIterative experimentation: Frequent experimentation helps quickly identify dead ends and promising solutions, optimizing the use of resources.\n\n\n\n\nTransparency: Promotes a culture of continuous improvement, where teams share and learn openly.\nThis strategy operates on multiple levels:\nProject management: Focuses on direction, ideation, and finding product-market fit.\nSoftware development: Applies the same principles to concrete practices like continuous integration (CI).\nPairing the fail-fast mentality with a CI service like TeamCity helps shorten feedback cycles and improve outcomes by rapidly identifying and addressing issues.\nMajor advantages of the fail-fast strategy\nCompared to traditional methods like the waterfall strategy, the fail-fast strategy offers significant benefits:\nShorter feedback loop: Submit small, incremental changes for automated checks, catching faults early and fixing them quickly.\nIdentifying pitfalls early on: Incremental builds help identify technical issues early, allowing for timely adjustments and de-risking your approach upfront.\nPotential drawbacks of the fail-fast strategy\nWhile the fail-fast strategy offers many benefits, there are potential drawbacks:\nMindset shift: Teams used to traditional methods may resist the concept of “failure”, impacting morale and causing frustration.\nEmbracing failure: Understanding that failure, discovery, and pivoting are key elements is crucial for the strategy’s success.\nTooling requirements: Effective implementation requires robust tools, particularly a CI/CD tool like TeamCity.\nHow does TeamCity support the fail-fast strategy?\nAs the central goal of the fail-fast strategy is shorter feedback cycles, you need a tool that provides this feedback for you in a timely manner. Continuous integration tools, such as TeamCity, are designed for this purpose. They can run builds, tests, and other scripted workflows in response to code changes.\nReal-time reporting\nLarge software projects can have long, slow-running builds. This makes it especially frustrating when you wait a long time for a build to run, only to discover that it failed. Whether it’s a blocking build issue that stops subsequent steps or an early test failure that makes the rest of the run redundant – the sooner you discover these issues, the better.\nTeamCity mitigates this issue for you by providing real-time reporting. You can view in-progress builds, inspect their current state, and build logs to identify problems as they occur rather than waiting until the end of a CI run.\nBy seeing the live status of running builds, developers can identify and fix problems without waiting for the build to finish. When the build fails, you can see what went wrong, fix it, and run another build. Compared to other CI systems where you need to wait for the build to complete, this workflow offers a shorter feedback cycle that works well with the fail-fast strategy.\n\n\n\n\nThis approach is particularly effective if quick-running build steps provide high-value feedback, like static analysis and other code quality checks. You can view the progress of these live to determine if you need to make any alterations to your code, and once these pass, you can leave the rest of the build to run.\nBuild configurations\nYour build process will likely be nontrivial when dealing with complex software systems. You’ll need to do any number of steps, including:\nBuilding Docker images\nCompiling source code\nDownloading dependencies\nRunning various kinds of tests (with varying degrees of cost)\nAs your build grows over time, it could become too large to reasonably manage. At this point, you may want to find a way to split the build into smaller steps.\nTeamCity solves this through build configurations. Build configurations allow you to split your build into discrete steps. When you do this, each step has a clearly defined responsibility, limiting the potential for complexity to leak between steps as your system grows.\nBuild chains\nAnother helpful feature of TeamCity that can be used to implement the fail-fast strategy is build chains. This feature allows you to declare build configurations as dependent on one another. In practice, this means you can run all your quality gates before the deployment step, allowing you to skip the deployment if there are quality issues that prevent it from being a release candidate.\nExample of a build chain in TeamCity\n\n\n\nFailures in the earlier steps in the chain will stop subsequent build configurations from running. This can save time and resources and help shorten the feedback loop even more by avoiding the effort spent on faulty builds.\nTest reports\nFailing tests are a fact of life for software developers. The key factor that separates frustrating failures from helpful failures is how much information you have when trying to fix them. Ideally, you want to know:\n– What failed: Was it a unit test, an integration test, etc.?\n– When it failed: Is it a new failure? Is this the first time it has happened? Is it a recurring flaky test?\n– Why it failed: Is it a legitimate functional failure or a flaky test?\nThis information helps you narrow down the cause and promptly fix the issue. You could run the test on your development machine and see it fail for yourself, but you’ll likely miss out on a lot of context (for example, information about in which commit this failure first arose). \nTeamCity test reports solve this problem. Every time your CI workflow runs, test data is captured for a wide range of testing frameworks. This data is then presented to you in relation to the CI runs that have experienced test failures, as well as a few other views like Current Problems and Flaky Tests.\nExample of a test report in TeamCity\n\n\n\nThese reports provide immediate insights into what problems your build is facing and the nature of the problems, such as flaky tests, newly introduced issues, or long-standing failures. \nHigh-quality test reports are a must for projects that require heavy use of automated testing at any level, but they’re especially helpful if you have broad coverage from unit tests and integration tests.\nThis way, at a glance, you have a comprehensive snapshot of the state of your code base each time CI runs.\nNotifications\nFail fast is only good if you know about the failure, and detailed build information is only helpful if people know it’s there. Developers would typically prefer to work on things themselves rather than sit and watch something being built by CI.\nThanks to highly configurable notifications, there’s no need to babysit builds. In TeamCity, you can configure rules to determine what you would like to be notified about and where you’d like those notifications to go. From email and browser notifications to Slack and even in-IDE notifications, there are several channels to choose from.\nNotification options in TeamCity\n\n\n\nNotifications are a key requirement in a system that’s intended to help you work more proactively, and the more configuration options you have at your disposal, the more use cases you will be able to satisfy.\nFor a fail-fast workflow, you might want to configure notifications for any failed build to which you’ve contributed. Then, relying on the VCS integration, you can get rapid feedback on your changes directly in your IDE as you make small, atomic changes.\nTeamCity notifications can be configured to only notify you of Builds containing my changes or when The first build error occurs. These settings are great for fine-tuning the notifications you see. Rather than seeing every failing build, you might only want to see failures on builds that include your changes or when the first error happens for a build.\nIf you set up continuous development through TeamCity, you can also enable notifications to inform you whenever a deployment or infrastructure change (through infrastructure as code tools like Terraform or Kubernetes deployments) occurs. In this case, you’d likely want to be notified in case of success as well as failure.\nArtifacts\nIssues often arise in CI that you cannot replicate locally. This can lead to a lot of misdirected time as you try to determine which is different between the CI run and your local application. Using artifacts can help address this problem.\nIn TeamCity, artifacts are typically anything produced by your build, such as binaries, logs, recordings, screenshots, etc. You can treat pretty much anything as an artifact, which gives you great flexibility in how you use this feature. Artifacts are then captured by TeamCity and are available for download through the UI after the build.\nThis can greatly streamline the analysis and debugging process that you go through when trying to rectify a failing build. For example, if you have end-to-end (E2E) tests that only seem to fail in your CI runs, there’s a good chance that your E2E tool has the ability to produce screenshots and screen recordings when failures occur. Being able to capture these as artifacts gives you a trove of data to help with debugging.\nArtifacts can also be used for any other use case where you want to capture the output of a build. Perhaps your project produces binary executables. In that case, you could capture the built binary for each CI run, allowing you to test any build for any commit that runs through your CI workflow.\nWrapping up\nThis guide introduced the fail-fast strategy, including its benefits, such as proactive risk management, transparency, and adaptability. TeamCity supports fail-fast through various powerful features, including real-time reporting, flexible notifications, and detailed test reports.\nWhen utilized properly, the fail-fast strategy can be a powerful tool. It can help you move faster and deliver value without the constraints of slower, more traditional ways of working. However, the process needs to be supported by suitably powerful and flexible tools. If you’re looking for a CI/CD server that fits the bill, consider taking TeamCity for a spin today.",
        "dc:creator": "Olga Bedrina",
        "content": "This is a guest post from Cameron Pavey, draft.dev. Software development teams are always looking for ways to move faster and deliver more value in less time. However, one common pitfall for many teams is spending far too much time and effort building something, only to encounter major issues late in the project&#8217;s lifecycle. The [&#8230;]",
        "contentSnippet": "This is a guest post from Cameron Pavey, draft.dev. Software development teams are always looking for ways to move faster and deliver more value in less time. However, one common pitfall for many teams is spending far too much time and effort building something, only to encounter major issues late in the project’s lifecycle. The […]",
        "guid": "https://blog.jetbrains.com/?post_type=teamcity&p=505010",
        "categories": [
          "news"
        ],
        "isoDate": "2024-08-23T09:57:01.000Z"
      },
      {
        "creator": "Sarah Haggarty",
        "title": "Kotlin 2.0.20 Released",
        "link": "https://blog.jetbrains.com/kotlin/2024/08/kotlin-2-0-20-released/",
        "pubDate": "Thu, 22 Aug 2024 14:51:13 +0000",
        "content:encodedSnippet": "The Kotlin 2.0.20 release is out! This version includes performance improvements and bug fixes for Kotlin 2.0.0, where we announced the Kotlin K2 compiler as Stable. Here are some additional highlights from this release:\nThe data class copy function will have the same visibility as the constructor\nStatic accessors for source sets from the default target hierarchy are now available in multiplatform projects\nConcurrent marking for Kotlin/Native has been made possible in the garbage collector\nThe @ExperimentalWasmDsl annotation in Kotlin/Wasm has a new location\nSupport has been added for Gradle versions 8.6–8.8\nA new option allows sharing JVM artifacts between Gradle projects as class files\nThe Compose compiler has been updated\nSupport for UUIDs has been added to the common Kotlin standard library\n\n\n\n\nFor the complete list of changes, refer to What’s new in Kotlin 2.0.20 or the release notes on GitHub.\nHow to install Kotlin 2.0.20\nStarting from IntelliJ IDEA 2023.3 and Android Studio Iguana (2023.2.1) Canary 15, the Kotlin plugin is distributed as a bundled plugin included in your IDE. This means that you can’t install the plugin from JetBrains Marketplace anymore.\nTo update to the new Kotlin version, change the Kotlin version to 2.0.20 in your build scripts.\nIf you need the command-line compiler, download it from the GitHub release page.\nIf you run into any problems\nFind help on Slack (get an invite).\nReport issues to our issue tracker, YouTrack.\n\n\n\n\n\nStay up to date with the latest Kotlin features! Subscribe to receive Kotlin updates by filling out the form at the bottom of this post. ⬇️\nSpecial thanks to our EAP Champions 🥇👏\nZac Sweers\nRick Clephas\nFlorian Schreiber\nSechaba Mofokeng\nYang\nŁukasz Wasylkowski\nDavid Lopez\nBernd Prünster\nAlexander Nozik\nBenoit ‘BoD’ Lubek\nSterling Albury\nKacper Wojciechowski\nMohamed Rejeb\nJake Wharton\nJohannes Svensson\nArtyom Shendrik\n\n\n\n\nWhat else to read and watch\nWhat’s new in Kotlin 2.0.20 documentation\nK2 compiler migration guide\nK2 Compiler Performance Benchmarks and How to Measure Them on Your Projects\nMeet Renovated Kotlin Support – K2 Mode: What You Need to Know\nJetpack Compose compiler moving to the Kotlin repository\nKotlin EAP Champions",
        "dc:creator": "Sarah Haggarty",
        "content": "The Kotlin 2.0.20 release is out! This version includes performance improvements and bug fixes for Kotlin 2.0.0, where we announced the Kotlin K2 compiler as Stable. Here are some additional highlights from this release: For the complete list of changes, refer to What&#8217;s new in Kotlin 2.0.20 or the release notes on GitHub. How to [&#8230;]",
        "contentSnippet": "The Kotlin 2.0.20 release is out! This version includes performance improvements and bug fixes for Kotlin 2.0.0, where we announced the Kotlin K2 compiler as Stable. Here are some additional highlights from this release: For the complete list of changes, refer to What’s new in Kotlin 2.0.20 or the release notes on GitHub. How to […]",
        "guid": "https://blog.jetbrains.com/?post_type=kotlin&p=503938",
        "categories": [
          "releases"
        ],
        "isoDate": "2024-08-22T14:51:13.000Z"
      },
      {
        "creator": "Alena Guzharina",
        "title": "What’s New in Datalore 2024.4: Limit 500 Toggle, Easier Folder Import, Polars DataFrames Support, and More",
        "link": "https://blog.jetbrains.com/datalore/2024/08/22/what-s-new-in-datalore-2024-4-limit-500-toggle-easier-folder-import-polars-dataframes-support-and-more/",
        "pubDate": "Thu, 22 Aug 2024 10:07:56 +0000",
        "content:encodedSnippet": "Datalore 2024.4 introduces quality-of-life improvements for SQL cells, a revamped folder import workflow, support for Polars DataFrames, and more. Datalore Cloud customers have already received the new update automatically, and Datalore On-Premises customers can upgrade by following these instructions.\nLimit 500 toggle for SQL cells\nTo help you prototype SQL queries faster, we’ve introduced a new Limit 500 feature inside SQL cells in Datalore notebooks. There is now a toggle visible in the SQL cell next to the Browse schema action. This setting will persist after a kernel restart, ensuring a seamless workflow.\n\n\n\n\nRevamped import folder workflow\nIt is now easier to import your previous work into Datalore. We’ve enhanced the folder import experience with a new, streamlined dialog that appears when you click Import | Upload folder. You can now easily select which notebooks and files to import, with the option to import notebooks as files. Hidden files and folders are fully supported and can be imported if selected. \n\n\n\n\nUpdates to the notebook UI\nWe’ve made several updates to the notebook UI to enhance your editing experience. Unfocused code cells now have borders, improving cell visibility and making it easier to target the insert cell buttons. Interactive controls and metric cells now have their options merged with the cell toolbar, no longer occupying the full width of the notebook. Additionally, cell type-specific actions, such as selecting a datasource or target variable, are now positioned to the right of the execution time for easier access. Cells now also feature a Run button, which can be gray or blue depending on whether the cell was changed after execution – offering a clearer indication of a cell’s status.\n\n\n\n\nDrop-down cell improvements\nFor large lists, you can now use a search string within the drop-down to quickly find your desired option. Additionally, multiselect drop-downs now include Select all and Clear all options.\n\n\n\n\nQuick notebook access from the editor\nYou can now quickly search for and open notebooks in a separate tab directly from the Notebooks view. Enjoy easier navigation without extra context switching.\n\n\n\n\nSupport for Polars DataFrames\nDatalore now supports rich table outputs for Polars DataFrames. To get an interactive table output, specify the DataFrame name at the end of the cell without a print statement.\n\n\n\n\nOther updates and bug fixes\nDuplicate file names are no longer permitted inside Datalore’s file system, with existing duplicates renamed with (#N) prefixes to make files easier to find and minimize naming confusion. \nPython `input()` commands are now supported for interactive reports. \nGit repositories with submodules are now cloned correctly. \nTables are no longer cropped when printing to PDF. \nWe’ve resolved the issue where pages would not load when Datalore was set up with an HTTP proxy on a non-standard port. \nWe’ve fixed the issue where refreshing the schema in the Edit database connection dialog would not work for users logged in via OAuth.\nIt is now possible to copy data from the cell outputs to the clipboard via the output’s context menut. This feature is supported for most cell output types. \nThe workspace owner is now indicated in the left-hand sidebar of the home page.\nDatalore Cloud customers have already received these updates automatically. \nFor Datalore On-Premises customers, it is highly important to ensure the database that stores all the Datalore content is backed up before upgrading to the 2024.4 version.\n      \n      Upgrade to 2024.4\n    \n\n\n\n\nKind regards,\nThe Datalore team",
        "dc:creator": "Alena Guzharina",
        "content": "Datalore 2024.4 introduces quality-of-life improvements for SQL cells, a revamped folder import workflow, support for Polars DataFrames, and more. Datalore Cloud customers have already received the new update automatically, and Datalore On-Premises customers can upgrade by following these instructions. Limit 500 toggle for SQL cells To help you prototype SQL queries faster, we&#8217;ve introduced a [&#8230;]",
        "contentSnippet": "Datalore 2024.4 introduces quality-of-life improvements for SQL cells, a revamped folder import workflow, support for Polars DataFrames, and more. Datalore Cloud customers have already received the new update automatically, and Datalore On-Premises customers can upgrade by following these instructions. Limit 500 toggle for SQL cells To help you prototype SQL queries faster, we’ve introduced a […]",
        "guid": "https://blog.jetbrains.com/?post_type=datalore&p=504871",
        "categories": [
          "releases"
        ],
        "isoDate": "2024-08-22T10:07:56.000Z"
      }
    ]
  },
  {
    "name": "Airbnb Engineering & Data Science",
    "category": "기업",
    "posts": [
      {
        "creator": "Leo Wong",
        "title": "Building Postcards for “Airbnb” Scale",
        "link": "https://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec?source=rss----53c7c27702d5---4",
        "pubDate": "Wed, 28 Aug 2024 16:01:39 GMT",
        "content:encodedSnippet": "By: Leo Wong, Henry Johnson\nHow the Airbnb Media team built group travel Postcards for the 2024 Summer Release by leveraging a novel destination matching algorithm while advancing the platform’s image & localized text processing capabilities.\nAirbnb Postcards (see announcement).\nIntroduction\nFor Airbnb’s 2024 Summer Release, the Media Ingestion team at Airbnb took on the exciting challenge of creating a reliable postcard generation system to generate unique, hand-crafted Postcards. Postcards are a beautiful way to invite guests on a trip while keeping friends and family in the loop (see announcement). This feature required a novel solution to match relevant postcards to every possible destination that guests booked on Airbnb. It needed to render performantly not only on all our client platforms (iOS, Android, and Web), but also on different messaging platforms outside the Airbnb app, all while maintaining Airbnb’s high design standards.\nChallenges\n\nLocalized Text Layout: Postcards have strict design guidelines around character count per line, font leading & kerning per language, pixel perfect typography, line break rules, and language-specific styling.\nDesign & Product Flexibility: Text layout, color, fonts, text drop shadows and image transformations need to be flexible for product & design changes.\nDestination Matching: Postcards need to match the destination by including relevant artwork and localized destination names.\nAvailability On and Off Platform: Assets need to be surfaced on and off the platform, which necessitated a pre-generated server-side solution; client-only solutions wouldn’t work since we needed Open Graph compatible links for assets to render properly in iMessage and Instagram, for example.\nPerformance: Postcard presentation shouldn’t interrupt the product UX by taking significant time to render.\nPostcards “in vs. out” of product experience\nSolution\nPostcard Setup\nA Trips Postcard is the combination of (1) an artwork illustration, (2) a postcard template, and (3) a localized destination. A Postcard “template” is a data entity with a reference to the artwork illustration plus some additional metadata to describe how to render it. To render a Postcard in product, we need to have all artwork illustrations, postcard templates, and formatted localized destinations set up before a visitor sees the product flow.\nIn the example below, we have an illustration of a cliff side, a postcard template, and an English destination name of “Galway”. The Postcard template includes parameters to specify how to create the postcard, like text and Belo (Airbnb brand icon) color and positioning. In the example, the text and Belo are rendered in gray color and positioned at the bottom and top left, respectively.\nIllustration + Template Params + Localized Destination = Postcard\nDesign Flexibility — Postcard Templates\nTo accommodate changing design requirements, we built a flexible template data model that empowers our design team to configure various parameters like text positioning and text color while iterating on the designs. The postcard templates include all the metadata required to generate a Postcard and its surrounding presentation elements. It also includes a versioning capability so that we can publish changes to all users with a version bump whenever we have a design revision or visual defect.\nTemplate & Artwork Upload, Management & Preview\nTo make it easy for the creative team to self-serve and debug issues, we built a web-based internal tool for creating and managing templates, previewing postcards, and uploading artwork. This tool made it significantly easier to manage templates, and was especially useful during the team’s peak iteration period where we were constantly fixing bugs and changing designs.\nThe postcard template creation & management formIn the setup workflow, our operations team created Postcard templates, uploaded artwork, and managed them via the template form page.\nLocalized Text Layout\nWe wanted accurate translations of destinations that were properly formatted for each localized postcard. A programmatic solution for localized text layout would require, at the very least, language specific rules (right to left, word wrapping, etc.), knowledge of cultural conventions, accessibility considerations, and text rendering for special characters (diacritics, etc.). This would make business logic complex and brittle.\nWe compromised on this solution with the localization (l10n) team by manually formatting translations for destinations that made up our top booking destinations. These manually formatted translations involved getting our l10n scaled operations team to translate and format (line breaks, layout spacing, etc.) a shortlist of localized destinations, which we helped ingest into our typical i18n platform translated text workflow with some scripting. After ingestion, the Postcard generation system pulls from our i18n platform to get the desired localized formatted text layout for each Postcard.\nThe destinations shortlist was informed by our data science team, who helped gather the top booked destinations by language. This reduced the scale of required postcards to generate from all destinations in the full set of language locale destination combinations on the platform, to a magnitude of scale smaller destination subset. As a result, postcard QA was significantly easier. More importantly, it kept the overall system code and maintenance simple; no need for thousands of lines of language specific business logic!\nThe rest of the postcards that didn’t make the top destinations list followed a simple formula of word count and line breaks per language (e.g. Chinese, Korean, and Japanese had smaller word count limits because of the character size and no line break on spaces because it changes the meaning of the destination).\nEarly design prototype of text layout in a subset of the languages we support at Airbnb. The screenshot is for illustration purposes only.\nPostcard Generation\nDestination Matching\nDecision tree for determining the best postcard template for a given reservation\nTo find the best-matching postcard template for each destination, we have a matching algorithm that matches templates to reservations at booking time using four different criteria:\n\nBy listing — We wanted to be able to support listing-specific artwork. E.g. Our Icons listings show a golden ticket to commemorate the special moment when a guest wins the lottery for staying at an Icons listing.\nBy destination — For popular destinations (matching by city and country), we have curated artwork that showcases both a local artist and the destination. E.g. Trips to Santorini present the iconic Cycladic domes of Santorini as artwork (see diagram below).\nBy taxonomy — For all other artwork, we match destinations based on a set of taxonomy tags. We partnered with the knowledge graph team to apply taxonomy attributes to all of our listings in a few different categories: density (i.e. metropolitan, urban), climate (i.e. tropical, temperate) and geography (i.e. coastal, mountain, river). We ensured the taxonomy was accurate by cross referencing existing internal data and the expertise of our regional representatives teams. They then exposed an API that we called to fetch taxonomy by listing. On the operations side, our production team created taxonomy tagged artwork (e.g. an artwork tagged to be used for a coastal, temperate, metropolitan postcard). When generating postcards, we match the listing to the artwork with the highest number of overlapping tags.\nBy default — If a destination isn’t covered by the above categories, we show a fallback default artwork.\nThe 4 initial Postcard variants: Icons, Destination specific, Taxonomy, and Default.\nFormatted Translations\nWe take the listing of each booking request and fetch the city and country from the listing service and check to see if that destination was in our curated set of formatted destinations loaded into our i18n service. We then take the best fitting artwork and embed the localized destination text on it to generate the final postcard. If we don’t get a translation back, we fall back to serving the postcard without text.\nPerformance — Async Postcard Creation Flow\nPutting a localized destination and a Belo icon onto artwork is a time-consuming operation given the high resolution artwork we used. We knew the image processing flow could take over 8 seconds on average to process an image so we needed to come up with a way to make our postcard API respond quickly. We also wanted to transfer these generated postcards into our primary image storage so we could leverage our existing media serving infrastructure, which introduced an additional 1–2 seconds of latency.\nIn order to still be performant, we went with a partly asynchronous approach where, during the live in product request, we only serve postcards that we’ve already generated and stored internally. If there was a request for a new postcard, we would instead return a fallback postcard and publish an event to a Kafka queue where an async consumer would call the processing service, wait for the asset to be generated and then transfer it into our system to be used for future requests.\nAs shown in the diagram below, we fetched the listing information and taxonomy information in parallel before computing the best matching artwork for the trip. Based on a pattern in how the postcards are stored, we would check in our media service to see if the postcard was generated already before either returning the card if it was found or kicking off the asynchronous flow if it was not found. At that point, our media service’s Kafka consumer would complete the flow by transforming the asset into a postcard and storing it in our system.\nArchitecture Diagram of the Backend Postcard Generation flow\nPre-generation\nWe wanted to generate as many of the postcards as possible before the launch. If the postcard hasn’t been generated when a guest books a group trip, everyone on the booking will see the default, generic postcard. Our data science team helped determine top destinations, and we ran those inputs through our postcard generation pipeline to pre-generate as many postcards as possible and minimize the chance of falling back to a default postcard. Within a week of launching, more than 90% of trips had a custom postcard instead of a default and we inched closer to generating a postcard for all trips in the months after.\nDashboard numbers on Postcard template selection type and postcard usage hit rate. For illustrative purposes only, not real data.\nConclusion\nCreating postcards was a massive effort that required collaboration across multiple engineering, product, design, and data science teams to improve Airbnb’s group travel feature. Our frontline insights team continues to receive positive social media and external feedback on this update that adds delight to joining a group trip.\nThe solution highlights the importance of having the right internal tooling, image and text processing capabilities, and destination matching logic for solving something at Airbnb’s scale.\nPostcards is one of the first major image processing use cases that the Media team built to support a new Airbnb feature. It highlights the power of media capabilities and innovative features we can build with them. If you like the type of work we do at Airbnb, please contact us & check out our careers page!\nAcknowledgments\nThanks to the following engineers who helped to build this feature: Alan Wright, Aditya Punjani, Bill Lovotti, Jessica Chen, Miguel Jimenez\n\nBuilding Postcards for “Airbnb” Scale was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Leo Wong",
        "guid": "https://medium.com/p/dfe0b71b12ec",
        "categories": [
          "front-end-development",
          "image-processing",
          "internationalization",
          "engineering",
          "media-processing"
        ],
        "isoDate": "2024-08-28T16:01:39.000Z"
      }
    ]
  },
  {
    "name": "Instagram Engineering",
    "category": "기업",
    "posts": []
  },
  {
    "name": "PayPal Engineering",
    "category": "기업",
    "posts": []
  },
  {
    "name": "article New updates to Planner comment notifications and settings in Planner Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Visual Studio Blog",
    "category": "기업",
    "posts": [
      {
        "creator": "David Li",
        "title": "C++ Gaming Productivity Update for Visual Studio 2022 version 17.11",
        "link": "https://devblogs.microsoft.com/visualstudio/c-gaming-productivity-update-for-visual-studio-2022-version-17-11/",
        "pubDate": "Wed, 28 Aug 2024 10:00:20 +0000",
        "content:encodedSnippet": "Introduction\nIn Visual Studio 2022 version 17.11, you will find brand new productivity enhancements for your game development workflows. From faster conditional breakpoints to a swath of Unreal Engine enhancements, we have added various productivity boosters based on your feedback. Once again, thank you for sharing Developer Community feedback and answering our surveys. Your valuable feedback continues to drive the direction of the Visual Studio gaming team.\nGeneral C++ Game Development\nSupercharge Your IntelliSense Performance\nWe have received feedback from AAA developers about IntelliSense performance. While we have improved performance over the past releases, we understand that these performance improvements may be less impactful for large C++ projects.\nHere are some helpful tips to further boost the power for Visual Studio’s IntelliSense for developers with powerful hardware:\nEnsure 64-bit IntelliSense is activated.\n\n\nAdjust your memory limits (in MB) to reflect your hardware.\n\n\nFaster Conditional Breakpoints\nIn version 17.11, you will see a significant improvement in performance for conditional break points. Our initial assessment found that execution time is almost four times as fast, reducing execution time from 80 seconds to 21 seconds over 80,000 iterations.\nBuild Insights Quality of Life Improvements\nWe have added several quality-of-life enhancements to Build Insights. Starting in version 17.11, you can now group your results by project, view the relative path and file name in place of the full path, and see a better grouping of results in the Included Files view.\n\nUnreal Engine Integration\nAdding Unreal Engine Classes, Modules, and Plugins\nIn Visual Studio 2022 version 17.11, you can now add Unreal Engine class templates, modules, and plugins. These new productivity features are available for both vcxproj and uproject.\nTo get started, right-click on your project in Solution Explorer and select Add > Unreal Engine Item.\n\nIn the revamped Add New Item dialog, you can select several new choices.\n\nUnreal Engine Classes\nBased on your feedback, we have added additional Unreal Engine class templates. You can now find the common class templates in Visual Studio’s Add New Item dialog just as you would find in the Unreal Engine Editor. The new dialog provides a list of common Unreal Engine class templates that you can add to your project. You can also now add your class to a module of your choice.\n\nUnreal Engine Module\nYou can now also add an Empty Unreal Engine Module using the Add New Item dialog.\n\nUnreal Engine Plugin\nLastly, you can now add an Unreal Engine Plugin to your project through the Add New Item dialog.\n\nUnreal Engine Toolbar\nTo further streamline our integration with Unreal Engine, we have added the Unreal Engine toolbar in Visual Studio 2022 version 17.11. The toolbar contains new productivity features and shortcuts to existing integration.\n\nThe play icon and the dropdown work in tandem to quickly attach to processes while debugging. With the Unreal Editor running, you will see Unreal Engine related processes. Selecting one of the choices will allow you to easily attach to the process to start a debugging session. Additionally, you can type any process name to quickly attach to that process.\n\n\nThe start and stop button will control the Unreal Engine integration server, used for the Unreal Engine Test Adapter. The server runs as a background process that automatically updates your test information. If you have the Unreal Engine Test Adapter option active, the server will automatically start upon opening your UE project.\nThe Rescan Blueprints button will refresh your Unreal Engine Blueprint references.\nThe Unreal Engine Log button is a shortcut to open the Unreal Engine Log window, in case you close it accidentally.\nThe Configure Tools for Unreal Engine button is a shortcut to launch the Unreal Engine Integration Configuration window. The configuration page gives you an overview of the Unreal Engine Integration status and gives you help if certain integrations were not configured properly.\nBlueprints Update\nWe received feedback asking to improve the process of setting up Blueprint references in Unreal Projects. Starting in Visual Studio v17.10, you will no longer need the Unreal Engine Plugin “Visual Studio Integration Tool” to view Blueprint references. Blueprint references will automatically show up if you have the “Visual Studio Tools for Unreal Engine” component installed.\nSend us your feedback\nWe hope you enjoy this update to Visual Studio, and we look forward to hearing what you think. You can share feedback with us via Developer Community, by reporting issues via report a problem and share your suggestions for new features or improvements to existing ones.\nStay connected with the Visual Studio team by following us on Twitter, YouTube, LinkedIn, and on Microsoft Learn.\nThank you for using Visual Studio and happy coding!\nThe post C++ Gaming Productivity Update for Visual Studio 2022 version 17.11 appeared first on Visual Studio Blog.",
        "dc:creator": "David Li",
        "content": "<p>Introduction<br />\nIn Visual Studio 2022 version 17.11, you will find brand new productivity enhancements for your game development workflows. From faster conditional breakpoints to a swath of Unreal Engine enhancements, we have added various productivity boosters based on your feedback. Once again,</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/visualstudio/c-gaming-productivity-update-for-visual-studio-2022-version-17-11/\">C++ Gaming Productivity Update for Visual Studio 2022 version 17.11</a> appeared first on <a href=\"https://devblogs.microsoft.com/visualstudio\">Visual Studio Blog</a>.</p>\n",
        "contentSnippet": "Introduction\nThe post C++ Gaming Productivity Update for Visual Studio 2022 version 17.11 appeared first on Visual Studio Blog.",
        "guid": "https://devblogs.microsoft.com/visualstudio/?p=250354",
        "categories": [
          "Cross-Platform",
          "Gaming",
          "Visual Studio",
          "Game Development",
          "Unreal Engine",
          "Visual Studio 2022"
        ],
        "isoDate": "2024-08-28T10:00:20.000Z"
      },
      {
        "creator": "Mads Kristensen",
        "title": "New IDE features in Visual Studio v17.11",
        "link": "https://devblogs.microsoft.com/visualstudio/new-ide-features-in-visual-studio-v17-11/",
        "pubDate": "Mon, 26 Aug 2024 15:53:30 +0000",
        "content:encodedSnippet": "In Visual Studio 2022 v17.11, you’ll discover several new features that address specific issues developers have reported. These updates don’t belong to a distinct category, but we’re dedicated to sharing them with you in this blog post. You can download the update and view the comprehensive release notes for full details.\nFind the code you’re looking for\nDo you ever feel like you’re seeing too many results in code search? Narrow down your focus with the newly added scoping options in Code Search.\n\nFor the default code search experience and each filter, you can now set the scope to Entire solution, Current project, or Current document and toggle inclusion of external files.\nYou can set different scopes for different filters. For example, the default experience can be set to look through Entire solution and members can be set to look through only current document. Your selections will be preserved past the current session.\nFamiliar keyboard shortcuts\nWhen moving between different IDEs and editors, it can be frustrating to have to relearn keyboard shortcuts. We’ve made some changes to some default keyboard shortcuts to make them more familiar and to preserve your muscle memory.\nToggle line comments\nYou’ve been able to toggle line comments in Visual Studio for a long time, but the default keyboard shortcut was Ctrl+K, Ctrl+/. We’ve now added Ctrl+/ as an alternative shortcut, which is the default in many other IDEs and editors.\n\nOpen Command Palette\nOr Feature Search as it’s called in Visual Studio. The default keyboard shortcut for this feature is now Ctrl+Shift+P, which should be familiar to VS Code users for opening the Command Palette.\n\nNever miss installing a component\nMany teams use *.vsconfig files to standardize their teams’ Visual Studio installations. The *.vsconfig files can be placed in a repo or a project’s solution directory, and Visual Studio will automatically detect if components specified in the *.vsconfig file are missing.\n\nIf any are missing, then a notification such as the one pictured below will appear.\n\nWe’ve made two improvements to this experience in Visual Studio 2022 version 17.11 Preview 1.\nFirst, Visual Studio can now detect if any local or network hosted extensions are missing from the installation, and if so, it’ll prompt you to install them. Previously, with respect to extensions, Visual Studio was only able to recognize if marketplace extensions were missing.\nSecondly, Visual Studio will now re-prompt the notification in certain situations, such as if the *.vsconfig file has changed because new components or extensions get added to it. Previously, the notification would only pop until you acted upon it, at which point it would be suppressed forever.\nImproved user authentication\nVisual Studio now uses the Web Account Manager (WAM) as its main authentication mechanism. This integration not only streamlines the authentication experience for Visual Studio, but it also enhances the security of your credentials.\nHere’s how the new WAM experience looks like:\n\nHow does this impact your experience?\nUsing WAM as the default authentication experience has many benefits, including:\nWindows integration: In addition to reducing the overall number of authentication prompts, you can now select existing Windows accounts instead of repeatedly entering credentials.\nBetter token protection: Refresh tokens are better secured as they are now device bound.\nSupport for the latest security features:\n\nLeverage rich OS capabilities such as Windows Hello & FIDO keys.\nAccess the latest and greatest Microsoft Entra ID capabilities and conditional access policies.\nNew Teams Toolkit templates\nTeams Toolkit now offers an empty Teams template for you to connect with your existing projects or use it as a starting point for new Teams apps.\nStart with this empty template to create a Teams app.\nIf you want to add Teams capability to your existing project, add Empty Teams App to your project and then connect two projects by making simple edits follow https://aka.ms/Config-Teams-app.\n\nTeams Toolkit supports authentication for Search Results from API Message Extensions app.\n\nWe hope you enjoy this update to Visual Studio, and we look forward to hearing what you think. You can share feedback with us via Developer Community, by reporting issues via report a problem and share your suggestions for new features or improvements to existing ones.\nStay connected with the Visual Studio team by following us on Twitter, YouTube, LinkedIn, and on Microsoft Learn.\nThank you for using Visual Studio and happy coding!\nDownload Visual Studio 17.11\n\n \nThe post New IDE features in Visual Studio v17.11 appeared first on Visual Studio Blog.",
        "dc:creator": "Mads Kristensen",
        "content": "<p>In Visual Studio 2022 v17.11, you&#8217;ll discover several new features that address specific issues developers have reported. These updates don&#8217;t belong to a distinct category, but we&#8217;re dedicated to sharing them with you in this blog post. You can download the update and view the comprehensive release notes for full details.</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/visualstudio/new-ide-features-in-visual-studio-v17-11/\">New IDE features in Visual Studio v17.11</a> appeared first on <a href=\"https://devblogs.microsoft.com/visualstudio\">Visual Studio Blog</a>.</p>\n",
        "contentSnippet": "In Visual Studio 2022 v17.11, you’ll discover several new features that address specific issues developers have reported. These updates don’t belong to a distinct category, but we’re dedicated to sharing them with you in this blog post. You can download the update and view the comprehensive release notes for full details.\nThe post New IDE features in Visual Studio v17.11 appeared first on Visual Studio Blog.",
        "guid": "https://devblogs.microsoft.com/visualstudio/?p=250326",
        "categories": [
          "Productivity",
          "Team and Development",
          "Visual Studio",
          "code search",
          "Keyboard Shortcuts",
          "vsconfig"
        ],
        "isoDate": "2024-08-26T15:53:30.000Z"
      },
      {
        "creator": "Rhea Patel",
        "title": "Enhancing Code Comprehension: GitHub Copilot’s Features in Visual Studio 2022 17.11",
        "link": "https://devblogs.microsoft.com/visualstudio/enhancing-code-comprehension-github-copilots-features-in-visual-studio-2022-17-11/",
        "pubDate": "Thu, 22 Aug 2024 10:00:28 +0000",
        "content:encodedSnippet": "GitHub Copilot in Visual Studio 2022 17.11 now offers an improved experience you to refer to their methods, classes, functions, and entire solution directly within the chat. By using the # symbol followed by the name of a method, class, or function, or by referring to your solution with @workspace, you can provide specific context that helps GitHub Copilot better understand their code and the problem at hand.\nRefer to your solution\nGitHub Copilot Chat now allows you to refer to your entire solution. This feature helps provide an even deeper understanding of your solution to get the best results. To refer to your solution, simply use the @workspace and Copilot will answer high level questions about your solution, files and how they work together.\n\nTry asking GitHub Copilot Chat questions like:\n@workspace What project can I run in my solution?\n@workspace In my workspace, where is #methodname referenced?\n@workspace Where does serialization occur in my solution?\n \nReference your methods, classes, functions\nGitHub Copilot Chat now allows you to refer to your methods, classes, and functions inline. By referring to methods, classes, and functions directly within the chat, you can provide specific context that helps GitHub Copilot better comprehend your code and the problem at hand. This feature empowers you to provide richer context to GitHub Copilot, enabling it to deliver more precise responses by gaining a deeper understanding of your codebase.\n\nTry asking GitHub Copilot Chat questions like:\nI have a test method named #methodName. How can I ensure that it’s being executed correctly?\nWhat are some optimizations that could be applied to #methodName?\nHow does #methodName1 use #methodName2 and what issues should I look out for?\n/explain #methodName\n \nSearch the web in Copilot Chat\nGitHub Copilot now includes context from your entire repository & can search the web.\nGitHub Copilot Enterprise subscribers in Visual Studio can now use GitHub Copilot Chat to get answers enriched with context from their entire repository and Bing search results.\n\nGet answers from across your entire codebase\nGitHub Copilot Chat can now answer questions with understanding of your full repository, not just the tabs you have open. Index your repository on GitHub.com, and then ask a question mentioning @github. You can ask questions like @github where is rate limiting implemented?\nSearch with the context of the web\nGitHub Copilot chat can also search Bing to find information outside of its general knowledge or your codebase. When you mention @github, GitHub Copilot will intelligently decide when to use Bing. You can ask questions like @github what is the latest LTS version of .NET?\nBing search is only available if enabled by an administrator – for more details, see Enabling GitHub Copilot Enterprise features or read the docs.\nTry it out today!\nWe hope you enjoy this new feature in GitHub Copilot for Visual Studio! Your feedback is important to us, so please share your thoughts using the thumbs up or down in the Chat, or by visiting the Developer Community. We look forward to hearing from you!\n\nHappy coding!\nWe appreciate the time you’ve spent reporting issues/suggestions and hope you continue to give us feedback when using GitHub Copilot in Visual Studio on what you like and what we can improve. Your feedback is critical to help us make GitHub Copilot  the best tool it can be! You can share feedback with us via Developer Community: report any bugs or issues via report a problem and share your suggestions for new features or improvements to existing ones.\nStay connected with the Visual Studio team by following us on YouTube, Twitter, LinkedIn, Twitch and on Microsoft Learn.\n\nThe post Enhancing Code Comprehension: GitHub Copilot’s Features in Visual Studio 2022 17.11 appeared first on Visual Studio Blog.",
        "dc:creator": "Rhea Patel",
        "content": "<p>GitHub Copilot in Visual Studio 2022 17.11 now offers an improved experience you to refer to their methods, classes, functions, and entire solution directly within the chat. By using the # symbol followed by the name of a method, class, or function,</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/visualstudio/enhancing-code-comprehension-github-copilots-features-in-visual-studio-2022-17-11/\">Enhancing Code Comprehension: GitHub Copilot’s Features in Visual Studio 2022 17.11</a> appeared first on <a href=\"https://devblogs.microsoft.com/visualstudio\">Visual Studio Blog</a>.</p>\n",
        "contentSnippet": "GitHub Copilot in Visual Studio 2022 17.11 now offers an improved experience you to refer to their methods, classes, functions, and entire solution directly within the chat. By using the # symbol followed by the name of a method, class, or function,\nThe post Enhancing Code Comprehension: GitHub Copilot’s Features in Visual Studio 2022 17.11 appeared first on Visual Studio Blog.",
        "guid": "https://devblogs.microsoft.com/visualstudio/?p=249711",
        "categories": [
          "Copilot",
          "Visual Studio",
          "Context",
          "GitHub Copilot"
        ],
        "isoDate": "2024-08-22T10:00:28.000Z"
      }
    ]
  },
  {
    "name": "Instagram Engineering",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Dropbox Tech Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Joshua",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권재명",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김석기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김선영",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권진호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강대명",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권정혁",
    "category": "개인",
    "posts": []
  },
  {
    "name": "줌구",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수보",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김시은",
    "category": "개인",
    "posts": []
  },
  {
    "name": "곽민수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김범진",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민서",
    "category": "개인",
    "posts": []
  },
  {
    "name": "I am not Okay",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권창현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권기호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수민",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강태욱",
    "category": "개인",
    "posts": [
      {
        "title": "인공지능 딥러닝 모델 성능 지표",
        "link": "http://daddynkidsmakers.blogspot.com/2024/08/blog-post.html",
        "pubDate": "2024-08-25T09:46:00.000Z",
        "author": "Daddy Maker",
        "content": "<div style=\"text-align: left;\">이 글은&nbsp;인공지능 딥러닝 모델 성능 지표를 간략히 정리한 것이다. 개발 목적에 따라 다양한 딥러닝 모델을 사용하고, 그에 따라 적절한 성능 지표를 모니터링해 튜닝 등 품질을 개선하는 것이 좋다. 필요할 때 찾아보기 귀찮으니 정리해 놓기로 한다. 이와 관련된 다양한 성능지표를 확인해 본다.</div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEid4gFMYYST6FBuhnZnTi_81U3sox36neNlc6Zl88lIXUyUXN5H8A9jDHJOYBqf0FIxS5oiOWpHUT4Cemd_IIhROVJhMvsmzVh7Mw6puJ__pXHubj9tmQuZDmDjBKGSAxKrqmkvxxZ3yo7ChuqqRYh3GFcDYWzKn_1yhcQLOl2KBeROGvVvhd_8tNnN9lm1\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"480\" data-original-width=\"720\" height=\"213\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEid4gFMYYST6FBuhnZnTi_81U3sox36neNlc6Zl88lIXUyUXN5H8A9jDHJOYBqf0FIxS5oiOWpHUT4Cemd_IIhROVJhMvsmzVh7Mw6puJ__pXHubj9tmQuZDmDjBKGSAxKrqmkvxxZ3yo7ChuqqRYh3GFcDYWzKn_1yhcQLOl2KBeROGvVvhd_8tNnN9lm1\" width=\"320\" /></a></div></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b>개요</b></div><div style=\"text-align: left;\">많은 딥러닝 모델 개발의 경우, 성능 지표를 미리 고민하지 않고, 개발을 하였다가 뒤늦게 관련 기능을 추가하느라 고생하는 경우가 있다. 이런 경우를 대비해, 미리 어떤 지표가 있는 지 확인해 본다.</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b>본론</b></div><div style=\"text-align: left;\">가장 단순한 성능 지표부터 시작해, LLM에서 많이 사용되는 지표도 확인해 보자.&nbsp;</div><div style=\"text-align: left;\">여기서 수식은 n개의 데이터 셋을 가진 입력 변수 x에 대한 참 값 y가 있다고 가정한다. ŷ 은 입력 x 에 대한 f(x)의 결과로 가정한다. f(x)의 참 값은 y이다.&nbsp;</div><div style=\"text-align: left;\"><ul style=\"text-align: left;\"><li>MSE (Mean Squared Error) : 편차의 제곱을 n으로 나눈 편차 평균</li></ul></div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEh69EAXcrN8uA6Nmo9DcVlTgn4EJJ2f4Aqa9iWQenvcLJ2N9UL8l5_pHkB80SsIa5yno-r18Xi6uNeo7U5gm7FXtVpAKxU7_4-2dUSv1akMEiPhnZGDYS8NMa8d0zuimOwLjr0Lt8yILzR55oPyMWdfS7j8xqDmuLxT1N-RoW8V9tRtOCwYu0qM_JK-KUsd\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"85\" data-original-width=\"262\" height=\"64\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEh69EAXcrN8uA6Nmo9DcVlTgn4EJJ2f4Aqa9iWQenvcLJ2N9UL8l5_pHkB80SsIa5yno-r18Xi6uNeo7U5gm7FXtVpAKxU7_4-2dUSv1akMEiPhnZGDYS8NMa8d0zuimOwLjr0Lt8yILzR55oPyMWdfS7j8xqDmuLxT1N-RoW8V9tRtOCwYu0qM_JK-KUsd=w196-h64\" width=\"196\" /></a></div><ul style=\"text-align: left;\"><li>RMSE (Root Mean Squared Error): 예측값과 실제값의 편차 평균의 제곱근 값</li></ul><div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEgSQHJRltZB-1O4lzoVcSBAiL7NAhs3t6TEtxHNenraOIthWZl2H3e3GArY0nSY5NwAok0zx3p88tTwullR-i-AAuZZNGx2R1U0lDH3V5nArvaIfs6oJnIiKRiAnwh619tTqbOU2ocPS1vwPWdxaNPL2mWs0Y3AMeEd40nEITq2VEcwSdBXbpOCbwgheGkm\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"85\" data-original-width=\"307\" height=\"60\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEgSQHJRltZB-1O4lzoVcSBAiL7NAhs3t6TEtxHNenraOIthWZl2H3e3GArY0nSY5NwAok0zx3p88tTwullR-i-AAuZZNGx2R1U0lDH3V5nArvaIfs6oJnIiKRiAnwh619tTqbOU2ocPS1vwPWdxaNPL2mWs0Y3AMeEd40nEITq2VEcwSdBXbpOCbwgheGkm=w217-h60\" width=\"217\" /></a></div></div><ul style=\"text-align: left;\"><li>CvRMSE (Coefficent of Variation of RMSE): RMSE를 표준화하여 데이터 변동성을 알려주는 지표. RMSE의 백분율임. 데이터 평균에 대한 RMSE의 백분류로 계산</li></ul><div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEj2s3ScOnAhYLIEBw9NSKf3QWd2rGMoya9pPEHMm-VGGA5UV1aNQXo2iVcH9UlnrubLLV486LdFWuvbJezRn-7fF8uWqVhqFmuaWOsG4dwFnfYb6LbwlhdQV95S-8-BsHZEVDl-2CGFAQJMBjCELAsVyauUCJALAGEWFFxdKa3NYE9RxXcQcoQDfKGCKR7I\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"69\" data-original-width=\"303\" height=\"49\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEj2s3ScOnAhYLIEBw9NSKf3QWd2rGMoya9pPEHMm-VGGA5UV1aNQXo2iVcH9UlnrubLLV486LdFWuvbJezRn-7fF8uWqVhqFmuaWOsG4dwFnfYb6LbwlhdQV95S-8-BsHZEVDl-2CGFAQJMBjCELAsVyauUCJALAGEWFFxdKa3NYE9RxXcQcoQDfKGCKR7I=w218-h49\" width=\"218\" /></a></div></div><ul style=\"text-align: left;\"><li>MAE (Mean Absolute Error) : 편차값에 가까운 절대 차이 평균값</li></ul><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEjSaWNJ11wjB8baPUkxN5k1Ib2u0TnJnfsdpSXVkTm-I8CioNRokhfI72BBcV8b7LUCX9o3M7HOkarrql7SL_dTlSgPg4bnSZN_X2-OaT_Ap79OKeeBsbJ82NbZULsArpVaoIXny-A45IwbG1J2LQBUZNIyVZDYdgoLM4aII8ovy3JpVUsxskldFJOgB_UV\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"85\" data-original-width=\"265\" height=\"61\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjSaWNJ11wjB8baPUkxN5k1Ib2u0TnJnfsdpSXVkTm-I8CioNRokhfI72BBcV8b7LUCX9o3M7HOkarrql7SL_dTlSgPg4bnSZN_X2-OaT_Ap79OKeeBsbJ82NbZULsArpVaoIXny-A45IwbG1J2LQBUZNIyVZDYdgoLM4aII8ovy3JpVUsxskldFJOgB_UV=w189-h61\" width=\"189\" /></a></div><ul style=\"text-align: left;\"><li>Accuray : 모델이 정확히 값을 예측했는 지 지표. 분류 모델 등 사용. 여기서 1은 조건 부합 시 참(1), 거짓(0)을 반환하는 indicator function 임</li></ul><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEjO1y7n-M_EW3a3wnrFQKqTNdyvMOYfKVgYxf1yTAgt1wFEhJBVYd0C4ayetZuIgXoz-M2WZ5gvTVlrceOvtJPmaqZurwNOBSmwWKNIOUfe6YnVMAYG8FkPX3AmXOPcL2oTRErhideW_bzB_uFhnB-FbUZ8SjEReolSj-DHuScBruZmzTBZgXodRb5ydHO0\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"89\" data-original-width=\"315\" height=\"62\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjO1y7n-M_EW3a3wnrFQKqTNdyvMOYfKVgYxf1yTAgt1wFEhJBVYd0C4ayetZuIgXoz-M2WZ5gvTVlrceOvtJPmaqZurwNOBSmwWKNIOUfe6YnVMAYG8FkPX3AmXOPcL2oTRErhideW_bzB_uFhnB-FbUZ8SjEReolSj-DHuScBruZmzTBZgXodRb5ydHO0=w219-h62\" width=\"219\" /></a></div><ul style=\"text-align: left;\"><li>MAPE (Mean Abslute Percentage Error): 회귀분석 모델 예측에 주로 사용되는 지표.&nbsp;</li></ul></div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEhVLddPwwpb5fzh78A7OuFqjQcjW74tEgVtZZ5XWi---S8xLB9sFC-89hapWA2252PssVLYN9bNpg4CMdDra7JtHTbRoGiM5atC4m7XHna4odMw1EDDmXNSpOij2dFKtFY6k4jG78wUJVE8dvQqhSrZTxGFc83fWEd5pdaMMUGUq2l2UmEPIWOIK-OXcsKf\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"83\" data-original-width=\"336\" height=\"54\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEhVLddPwwpb5fzh78A7OuFqjQcjW74tEgVtZZ5XWi---S8xLB9sFC-89hapWA2252PssVLYN9bNpg4CMdDra7JtHTbRoGiM5atC4m7XHna4odMw1EDDmXNSpOij2dFKtFY6k4jG78wUJVE8dvQqhSrZTxGFc83fWEd5pdaMMUGUq2l2UmEPIWOIK-OXcsKf=w217-h54\" width=\"217\" /></a></div></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">이외에, LLM에 주로 사용되는 ROGUE, BLEU, 컴퓨터 비전에 주로 사용되는 mAP, IoU, F1, call, recall 등 지표가 있다.&nbsp;</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><br /></div>",
        "contentSnippet": "이 글은 인공지능 딥러닝 모델 성능 지표를 간략히 정리한 것이다. 개발 목적에 따라 다양한 딥러닝 모델을 사용하고, 그에 따라 적절한 성능 지표를 모니터링해 튜닝 등 품질을 개선하는 것이 좋다. 필요할 때 찾아보기 귀찮으니 정리해 놓기로 한다. 이와 관련된 다양한 성능지표를 확인해 본다.\n\n\n\n\n개요\n많은 딥러닝 모델 개발의 경우, 성능 지표를 미리 고민하지 않고, 개발을 하였다가 뒤늦게 관련 기능을 추가하느라 고생하는 경우가 있다. 이런 경우를 대비해, 미리 어떤 지표가 있는 지 확인해 본다.\n\n\n본론\n가장 단순한 성능 지표부터 시작해, LLM에서 많이 사용되는 지표도 확인해 보자. \n여기서 수식은 n개의 데이터 셋을 가진 입력 변수 x에 대한 참 값 y가 있다고 가정한다. ŷ 은 입력 x 에 대한 f(x)의 결과로 가정한다. f(x)의 참 값은 y이다. \n\nMSE (Mean Squared Error) : 편차의 제곱을 n으로 나눈 편차 평균\n\n\n\n\nRMSE (Root Mean Squared Error): 예측값과 실제값의 편차 평균의 제곱근 값\n\n\n\nCvRMSE (Coefficent of Variation of RMSE): RMSE를 표준화하여 데이터 변동성을 알려주는 지표. RMSE의 백분율임. 데이터 평균에 대한 RMSE의 백분류로 계산\n\n\n\nMAE (Mean Absolute Error) : 편차값에 가까운 절대 차이 평균값\n\n\n\nAccuray : 모델이 정확히 값을 예측했는 지 지표. 분류 모델 등 사용. 여기서 1은 조건 부합 시 참(1), 거짓(0)을 반환하는 indicator function 임\n\n\n\nMAPE (Mean Abslute Percentage Error): 회귀분석 모델 예측에 주로 사용되는 지표. \n\n\n\n\n\n이외에, LLM에 주로 사용되는 ROGUE, BLEU, 컴퓨터 비전에 주로 사용되는 mAP, IoU, F1, call, recall 등 지표가 있다.",
        "id": "tag:blogger.com,1999:blog-5201956450461596914.post-5149285106406844516",
        "isoDate": "2024-08-25T09:46:00.000Z"
      }
    ]
  },
  {
    "name": "권용진",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권영재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김승호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김병환",
    "category": "개인",
    "posts": []
  },
  {
    "name": "RSS feed for boyism Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성준의 린스타트업과 디자인씽킹",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권혁우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김준형",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강동혁",
    "category": "개인",
    "posts": []
  },
  {
    "name": "고종범",
    "category": "개인",
    "posts": [
      {
        "creator": "고종범",
        "title": "가을이 되는 느낌? 독서",
        "link": "https://brunch.co.kr/@@24SO/45",
        "pubDate": "Mon, 26 Aug 2024 00:54:06 GMT",
        "author": "고종범",
        "content": "여유가 주는 바쁨 진행하던 프로젝트가 너무 잘 진행되어 여유가 생겼다. 1차 고개를 넘은 터라 한 달 정도 여유 있을 거라 생각했는데 2달은 여유가 있을 듯싶다. 그래서 그 여유를 즐기기 위해 그동안 정리 못하고 있던 것들을 정리하기 시작했다. 잔뜩 쌓였던 나의 주제 단지를 열어보니 생각보다 많은 주제가 있었다. 게다가 끝난 줄 알았던 주제에 꼬리가 붙어서",
        "contentSnippet": "여유가 주는 바쁨 진행하던 프로젝트가 너무 잘 진행되어 여유가 생겼다. 1차 고개를 넘은 터라 한 달 정도 여유 있을 거라 생각했는데 2달은 여유가 있을 듯싶다. 그래서 그 여유를 즐기기 위해 그동안 정리 못하고 있던 것들을 정리하기 시작했다. 잔뜩 쌓였던 나의 주제 단지를 열어보니 생각보다 많은 주제가 있었다. 게다가 끝난 줄 알았던 주제에 꼬리가 붙어서",
        "guid": "https://brunch.co.kr/@@24SO/45",
        "isoDate": "2024-08-26T00:54:06.000Z"
      }
    ]
  },
  {
    "name": "cheese10yun",
    "category": "개인",
    "posts": []
  },
  {
    "name": "구자철",
    "category": "개인",
    "posts": []
  },
  {
    "name": "FSS",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권동준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김용일",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김도균",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김상훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민석",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김동우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권윤학",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강성훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김만수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "엘키",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권태환",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김슬기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김광현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성빈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김영우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강명훈",
    "category": "개인",
    "posts": [
      {
        "title": "비율 계산",
        "link": "https://kangmyounghun.blogspot.com/2024/08/blog-post_22.html",
        "pubDate": "2024-08-22T08:58:00.001Z",
        "author": "강명훈",
        "content": "<div>프로세스 단위별 합산 구하고,</div><div><br /></div>\n<div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOhXs36W9US2RiekjU5y8DAa8GsEORl5sxBBmgaw5HWBInPzfuQetcjYRWg7SV6rpPWzMjOqxAxGT0xFogcUkWYGgc4HD5Oibw80V1Huyo7K3Vc4aGbWBCaV9WJy65dPmsqWoAgaNgTWkkGTcOHHAaFN7amC7fywBDesHcp5u1StPRUtUAP128JSkh0feq/s1240/percent.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"720\" data-original-width=\"1240\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOhXs36W9US2RiekjU5y8DAa8GsEORl5sxBBmgaw5HWBInPzfuQetcjYRWg7SV6rpPWzMjOqxAxGT0xFogcUkWYGgc4HD5Oibw80V1Huyo7K3Vc4aGbWBCaV9WJy65dPmsqWoAgaNgTWkkGTcOHHAaFN7amC7fywBDesHcp5u1StPRUtUAP128JSkh0feq/s520/percent.png\" width=\"520\" /></a></div><div><br /></div><div><span><a name='more'></a></span>총합 <a href=\"https://docs.splunk.com/Documentation/Splunk/9.3.0/SearchReference/Appendcols\" target=\"_blank\">필드 추가</a> 후,</div><br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimln5NpC3Fdsuv5daWi5vziXQDbwY46Nvz3Zh_kgNQyg6o_Q4jLmahA_DvVO6YdsXWjCjOPk3Qf1h3YUuH-3N3Z7iPCNIG8QlNAD0HZhXpJnAzkSCY03YXRMLs1EmZAjfGSqa2XIojFMjuyZ1s9p_Mpdmc6d7zc1Q2iBTWVLKcncpPbhZzJDEKmCNpXXzv/s1240/percent2.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"720\" data-original-width=\"1240\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimln5NpC3Fdsuv5daWi5vziXQDbwY46Nvz3Zh_kgNQyg6o_Q4jLmahA_DvVO6YdsXWjCjOPk3Qf1h3YUuH-3N3Z7iPCNIG8QlNAD0HZhXpJnAzkSCY03YXRMLs1EmZAjfGSqa2XIojFMjuyZ1s9p_Mpdmc6d7zc1Q2iBTWVLKcncpPbhZzJDEKmCNpXXzv/s520/percent2.png\" width=\"520\" /></a></div><br />\n<div>비율 계산.</div><div><br /></div><div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4dpHdY-hZJxGnhlSoMHPWA6c0CK6WBgxavdAbIBWIiUFaA_vaFZuvXT6VDNr8WFILy6PByRHI6wSC1s-YI6Bo7PbulvuXoMwEDS4MqrwBB1jSF-nwmm2jl3JktWxINodvUqtb1fLocQwMjp6bBzpIiueU0QKCnScM7pdNuO5whFicWfCxcS1sHCH4XJsw/s1240/percent3.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"720\" data-original-width=\"1240\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4dpHdY-hZJxGnhlSoMHPWA6c0CK6WBgxavdAbIBWIiUFaA_vaFZuvXT6VDNr8WFILy6PByRHI6wSC1s-YI6Bo7PbulvuXoMwEDS4MqrwBB1jSF-nwmm2jl3JktWxINodvUqtb1fLocQwMjp6bBzpIiueU0QKCnScM7pdNuO5whFicWfCxcS1sHCH4XJsw/s520/percent3.png\" width=\"520\" /></a></div><div><br /></div><div>아니면 이벤트 단위로 총합, 단위별 합산 추가 후 계산.</div></div><div><br /></div><div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkrY1Cl8fh7lL04rj0SgMxiiaV8xXoTDk6VhMoI0lv27mzptIPny21eod4yLhZnrQHzEZfeKKdWzw_dXPoVUmaIJ5WtT_nCYndvk6ecNisXTtRXY9mVM7Yx3eTRden5ejxJVHEw2PERBKQ10LmbVTKfliPgnzNvUlHLaSwiVLkjQnijIscvdmHHxYdkxWJ/s1280/percent4.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"666\" data-original-width=\"1280\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkrY1Cl8fh7lL04rj0SgMxiiaV8xXoTDk6VhMoI0lv27mzptIPny21eod4yLhZnrQHzEZfeKKdWzw_dXPoVUmaIJ5WtT_nCYndvk6ecNisXTtRXY9mVM7Yx3eTRden5ejxJVHEw2PERBKQ10LmbVTKfliPgnzNvUlHLaSwiVLkjQnijIscvdmHHxYdkxWJ/s520/percent4.png\" width=\"520\" /></a></div><div><br /></div><div>그냥 <a href=\"https://docs.splunk.com/Documentation/Splunk/9.3.0/SearchReference/Top\" target=\"_blank\">top</a> 쓰자.</div><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN-n-UihLnfQ5CN9jamjwW55gQsHGen9R34TVjvnRmApkAp-FoKQPybxnUTfCRAph0GwlCC2Wx-Q_uwJ6mYVSDf211B6_FX0_7vt4_oawkkIHsL_ea9iXkKVuNVGOrLSsrr_mowxmwCU4e4nwdCHq4WwssJTWOahGXe0y4Fi8VEZ-q0MNJkzRoo9Q_6_I-/s1280/percent5.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"554\" data-original-width=\"1280\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN-n-UihLnfQ5CN9jamjwW55gQsHGen9R34TVjvnRmApkAp-FoKQPybxnUTfCRAph0GwlCC2Wx-Q_uwJ6mYVSDf211B6_FX0_7vt4_oawkkIHsL_ea9iXkKVuNVGOrLSsrr_mowxmwCU4e4nwdCHq4WwssJTWOahGXe0y4Fi8VEZ-q0MNJkzRoo9Q_6_I-/s520/percent5.png\" width=\"520\" /></a></div><div><br /></div></div><div><div><b>관련 글</b></div><div><ul><li><a href=\"https://kangmyounghun.blogspot.com/2021/08/splunk-eval-rex.html\">Splunk의 eval과 rex</a></li></ul></div></div>",
        "contentSnippet": "프로세스 단위별 합산 구하고,\n\n\n\n\n\n총합 필드 추가 후,\n\n\n\n비율 계산.\n\n\n\n\n\n아니면 이벤트 단위로 총합, 단위별 합산 추가 후 계산.\n\n\n\n\n\n\n그냥 top 쓰자.\n\n\n\n\n\n관련 글\n\nSplunk의 eval과 rex",
        "id": "tag:blogger.com,1999:blog-2597780270996323853.post-2918153940603938022",
        "isoDate": "2024-08-22T08:58:00.001Z"
      }
    ]
  },
  {
    "name": "김민장",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김덕홍",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성중",
    "category": "개인",
    "posts": []
  },
  {
    "name": "구교준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김덕기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "고명환",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강성희",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강성훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강병수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김봉현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강형석",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수로",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강미경",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강진우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권민재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권태관",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김도곤",
    "category": "개인",
    "posts": []
  },
  {
    "name": "칡토스의 게임 개발",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김선철",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김놀부",
    "category": "개인",
    "posts": []
  },
  {
    "name": "프리웨어 이야기",
    "category": "개인",
    "posts": [
      {
        "creator": "어떤오후의 프리웨어 이야기",
        "title": "Hasleo Disk Clone 4.5 PRO 버전 라이센스 무료 제공",
        "link": "http://muzbox.tistory.com/483463",
        "pubDate": "Wed, 28 Aug 2024 11:22:58 +0900",
        "author": "어떤오후의 프리웨어 이야기",
        "comments": "http://muzbox.tistory.com/483463#entry483463comment",
        "content": "<p data-ke-size=\"size16\">Hasleo Disk Clone Pro v4.5의 전체 버전을 무료로 얻으세요! 이 강력한 디스크 복제 소프트웨어로 Windows OS와 데이터를 쉽게 복제, 마이그레이션, 백업할 수 있습니다. 이 제한된 기간의 혜택을 놓치지 마세요.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"600\" data-origin-height=\"400\"><span data-url=\"https://blog.kakaocdn.net/dn/cQd2xe/btsJg64hTpF/WdrABJbGO33sCSj5Ky9G40/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/cQd2xe/btsJg64hTpF/WdrABJbGO33sCSj5Ky9G40/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/cQd2xe/btsJg64hTpF/WdrABJbGO33sCSj5Ky9G40/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcQd2xe%2FbtsJg64hTpF%2FWdrABJbGO33sCSj5Ky9G40%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"Hasleo Disk Clone 4.5 PRO 버전 라이센스 무료 제공\" data-origin-width=\"600\" data-origin-height=\"400\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;디스크 복제 소프트웨어는 데이터를 안전하게 유지하면서 시스템 업그레이드나 드라이브 교체를 가능하게 해주는 중요한 도구입니다. 특히, Hasleo Disk Clone Pro v4.5는 복잡한 절차 없이 간편하게 Windows 운영 체제와 데이터를 복제할 수 있는 강력한 솔루션을 제공합니다. 이 소프트웨어를 통해 시스템을 새 드라이브로 마이그레이션하거나, 기존 데이터를 백업하는 작업이 훨씬 쉬워집니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #009a87;\"><b>Hasleo Disk Clone Pro 소개</b></span></h2>\n<p data-ke-size=\"size16\"><b>Hasleo Disk Clone Pro</b>는 Windows 11, 10, 8, 7, Vista 및 Server에서 사용 가능한 올인원 디스크 복제 도구로, 사용자들이 새로운 드라이브로 운영 체제를 복제하거나, 디스크를 다른 디스크로 복사하거나, 파티션을 다른 파티션으로 복사할 수 있도록 도와줍니다. 이 도구는 시스템 업그레이드 중 이전 기기의 데이터를 안전하게 유지하며, 전체 드라이브 또는 파티션을 전송하고 Windows를 쉽게 마이그레이션할 수 있습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Hasleo Disk Clone Pro 01.jpg\" data-origin-width=\"720\" data-origin-height=\"521\"><span data-url=\"https://blog.kakaocdn.net/dn/baEpUP/btsJiqG89io/UDSqnmmtXFPl3JO8KVRsjk/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/baEpUP/btsJiqG89io/UDSqnmmtXFPl3JO8KVRsjk/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/baEpUP/btsJiqG89io/UDSqnmmtXFPl3JO8KVRsjk/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbaEpUP%2FbtsJiqG89io%2FUDSqnmmtXFPl3JO8KVRsjk%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"Hasleo Disk Clone Pro 01.jpg\" data-origin-width=\"720\" data-origin-height=\"521\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><b><span style=\"color: #000000;\"><i>주요 기능 및 장점</i></span></b></h3>\n<p data-ke-size=\"size16\">Hasleo Disk Clone Pro는 사용자가 드라이브 업그레이드를 위해 데이터를 동일하게 복제하거나, 파티션을 변경하지 않고 복사할 수 있는 기능을 제공합니다. 이 과정에서 데이터 손실의 걱정 없이 중요한 파티션의 백업도 가능하며, MBR에서 GPT로, 또는 GPT에서 MBR로 변환할 수 있는 기능도 제공합니다. 이 변환은 데이터 손실 없이 수행되며, 변환 후 Windows가 정상적으로 부팅될 수 있도록 보장합니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Hasleo Disk Clone Pro 03.jpg\" data-origin-width=\"720\" data-origin-height=\"521\"><span data-url=\"https://blog.kakaocdn.net/dn/dUaPxN/btsJiMwqC3A/K5GU5bbYA1KUKOxMeK86N0/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/dUaPxN/btsJiMwqC3A/K5GU5bbYA1KUKOxMeK86N0/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/dUaPxN/btsJiMwqC3A/K5GU5bbYA1KUKOxMeK86N0/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdUaPxN%2FbtsJiMwqC3A%2FK5GU5bbYA1KUKOxMeK86N0%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"Hasleo Disk Clone Pro 03.jpg\" data-origin-width=\"720\" data-origin-height=\"521\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><i><b>GUI의 편리함</b></i></h3>\n<p data-ke-size=\"size16\">Hasleo Disk Clone Pro는 마법사 기반의 그래픽 사용자 인터페이스(GUI)를 제공하여 사용자가 복제 작업을 쉽게 수행할 수 있도록 도와줍니다. 사용자는 소스와 대상, 기타 몇 가지 옵션만 설정하면 복제를 시작할 수 있으며, 복제 전에 파티션을 이동하거나 수정할 수 있는 기능도 제공합니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #009a87;\"><b>Hasleo Disk Clone Pro 무료 라이센스 등록방법</b></span></h2>\n<p data-ke-size=\"size16\">Hasleo Disk Clone Pro의 무료 라이선스를 활성화하려면 다음 단계를 따르십시오:</p>\n<ol style=\"list-style-type: decimal;\" data-ke-list-type=\"decimal\">\n<li><b>소프트웨어 다운로드:</b><br />개발자 페이지에서 소프트웨어의 무료 버전을 다운로드합니다.<br /><a href=\"https://www.easyuefi.com/disk-clone/download\" target=\"_blank\" rel=\"noopener\">Hasleo Disk Clone Pro 다운로드</a></li>\n<li><b>소프트웨어 설치 및 실행:</b><br />다운로드한 파일을 설치하고, 설치가 완료되면 소프트웨어를 실행합니다. 실행 후 기본 사용자 인터페이스(UI)가 나타납니다.</li>\n<li><b>메뉴 버튼 찾기:</b><br />UI에서 메뉴 버튼을 찾으십시오. 메뉴 버튼은 아래쪽 화살표가 있는 작은 상자처럼 보입니다.</li>\n<li><b>라이센스 코드 등록:</b><br />드롭다운 메뉴에서 \"등록\"을 선택하고 아래에 제공된 라이센스 코드를 입력합니다:<br /><b>A3TANDZ3-ZATGLVME-QL99LQ7H-X4D7T27P-TJNWZA33-L95HP4ML</b></li>\n<li><b>등록 완료:</b><br />코드를 입력하고 \"Register(등록)\" 버튼을 눌러 라이센스를 활성화합니다.</li>\n</ol>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Hasleo Disk Clone Pro 02.jpg\" data-origin-width=\"1177\" data-origin-height=\"704\"><span data-url=\"https://blog.kakaocdn.net/dn/b6jRIw/btsJhCaFjN2/1PstUEFkBj3jdxcaCiyHI1/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/b6jRIw/btsJhCaFjN2/1PstUEFkBj3jdxcaCiyHI1/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/b6jRIw/btsJhCaFjN2/1PstUEFkBj3jdxcaCiyHI1/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb6jRIw%2FbtsJhCaFjN2%2F1PstUEFkBj3jdxcaCiyHI1%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"Hasleo Disk Clone Pro 02.jpg\" data-origin-width=\"1177\" data-origin-height=\"704\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #009a87;\"><b>마치며</b></span></h2>\n<p data-ke-size=\"size16\">Hasleo Disk Clone Pro는 Windows 운영 체제와 데이터를 안전하게 복제하고 마이그레이션할 수 있는 필수 도구입니다. 이번 기회를 놓치지 말고, 무료 라이선스를 통해 이 강력한 소프트웨어를 경험해 보세요. 시스템 업그레이드, 데이터 보호, 디스크 변환 등 다양한 작업을 간편하게 수행할 수 있습니다.</p>",
        "contentSnippet": "Hasleo Disk Clone Pro v4.5의 전체 버전을 무료로 얻으세요! 이 강력한 디스크 복제 소프트웨어로 Windows OS와 데이터를 쉽게 복제, 마이그레이션, 백업할 수 있습니다. 이 제한된 기간의 혜택을 놓치지 마세요.\n\n\n \n 디스크 복제 소프트웨어는 데이터를 안전하게 유지하면서 시스템 업그레이드나 드라이브 교체를 가능하게 해주는 중요한 도구입니다. 특히, Hasleo Disk Clone Pro v4.5는 복잡한 절차 없이 간편하게 Windows 운영 체제와 데이터를 복제할 수 있는 강력한 솔루션을 제공합니다. 이 소프트웨어를 통해 시스템을 새 드라이브로 마이그레이션하거나, 기존 데이터를 백업하는 작업이 훨씬 쉬워집니다.\n \n \nHasleo Disk Clone Pro 소개\nHasleo Disk Clone Pro는 Windows 11, 10, 8, 7, Vista 및 Server에서 사용 가능한 올인원 디스크 복제 도구로, 사용자들이 새로운 드라이브로 운영 체제를 복제하거나, 디스크를 다른 디스크로 복사하거나, 파티션을 다른 파티션으로 복사할 수 있도록 도와줍니다. 이 도구는 시스템 업그레이드 중 이전 기기의 데이터를 안전하게 유지하며, 전체 드라이브 또는 파티션을 전송하고 Windows를 쉽게 마이그레이션할 수 있습니다.\n\n\n \n \n주요 기능 및 장점\nHasleo Disk Clone Pro는 사용자가 드라이브 업그레이드를 위해 데이터를 동일하게 복제하거나, 파티션을 변경하지 않고 복사할 수 있는 기능을 제공합니다. 이 과정에서 데이터 손실의 걱정 없이 중요한 파티션의 백업도 가능하며, MBR에서 GPT로, 또는 GPT에서 MBR로 변환할 수 있는 기능도 제공합니다. 이 변환은 데이터 손실 없이 수행되며, 변환 후 Windows가 정상적으로 부팅될 수 있도록 보장합니다.\n\n\n \n \nGUI의 편리함\nHasleo Disk Clone Pro는 마법사 기반의 그래픽 사용자 인터페이스(GUI)를 제공하여 사용자가 복제 작업을 쉽게 수행할 수 있도록 도와줍니다. 사용자는 소스와 대상, 기타 몇 가지 옵션만 설정하면 복제를 시작할 수 있으며, 복제 전에 파티션을 이동하거나 수정할 수 있는 기능도 제공합니다.\n \n \n \nHasleo Disk Clone Pro 무료 라이센스 등록방법\nHasleo Disk Clone Pro의 무료 라이선스를 활성화하려면 다음 단계를 따르십시오:\n소프트웨어 다운로드:\n개발자 페이지에서 소프트웨어의 무료 버전을 다운로드합니다.\nHasleo Disk Clone Pro 다운로드\n소프트웨어 설치 및 실행:\n다운로드한 파일을 설치하고, 설치가 완료되면 소프트웨어를 실행합니다. 실행 후 기본 사용자 인터페이스(UI)가 나타납니다.\n메뉴 버튼 찾기:\nUI에서 메뉴 버튼을 찾으십시오. 메뉴 버튼은 아래쪽 화살표가 있는 작은 상자처럼 보입니다.\n라이센스 코드 등록:\n드롭다운 메뉴에서 \"등록\"을 선택하고 아래에 제공된 라이센스 코드를 입력합니다:\nA3TANDZ3-ZATGLVME-QL99LQ7H-X4D7T27P-TJNWZA33-L95HP4ML\n등록 완료:\n코드를 입력하고 \"Register(등록)\" 버튼을 눌러 라이센스를 활성화합니다.\n\n\n \n \n마치며\nHasleo Disk Clone Pro는 Windows 운영 체제와 데이터를 안전하게 복제하고 마이그레이션할 수 있는 필수 도구입니다. 이번 기회를 놓치지 말고, 무료 라이선스를 통해 이 강력한 소프트웨어를 경험해 보세요. 시스템 업그레이드, 데이터 보호, 디스크 변환 등 다양한 작업을 간편하게 수행할 수 있습니다.",
        "guid": "http://muzbox.tistory.com/483463",
        "categories": [
          "NEWS/윈도우10 한시적 무료앱",
          "hasleo disk clone pro",
          "windows 복제",
          "데이터 백업",
          "디스크 복제",
          "무료 라이선스",
          "운영 체제 마이그레이션",
          "파티션 복사"
        ],
        "isoDate": "2024-08-28T02:22:58.000Z"
      },
      {
        "creator": "어떤오후의 프리웨어 이야기",
        "title": "윈도우용 추천 프리웨어 (2024.8.26) 모니터밝기조절, 파일관리, Linux에뮬레이션, CAD뷰어, 데이터시각화, 프로젝트관리",
        "link": "http://muzbox.tistory.com/483462",
        "pubDate": "Mon, 26 Aug 2024 08:37:38 +0900",
        "author": "어떤오후의 프리웨어 이야기",
        "comments": "http://muzbox.tistory.com/483462#entry483462comment",
        "content": "<p style=\"text-align: left;\" data-ke-size=\"size18\"><span style=\"background-color: #ffffff; color: #0d0d0d; text-align: start;\">&nbsp;네이버 소프트웨어와 같은 프로그램 소개 사이트가 종료된 후, 윈도우 운영체제를 사용하는 이용자들을 위해 공개 프리웨어 및 오픈소스 프로그램을 소개합니다. 유용한 무료 소프트웨어를 찾고자 하는 사용자들에게 정기적으로 알찬 정보를 제공합니다.</span></p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"freeware.png\" data-origin-width=\"500\" data-origin-height=\"500\"><span data-url=\"https://blog.kakaocdn.net/dn/l9JVa/btsJe7uOawY/WatS3qwKv992v68b9uBeE0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/l9JVa/btsJe7uOawY/WatS3qwKv992v68b9uBeE0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/l9JVa/btsJe7uOawY/WatS3qwKv992v68b9uBeE0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fl9JVa%2FbtsJe7uOawY%2FWatS3qwKv992v68b9uBeE0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"윈도우용 추천 프리웨어 (2024.8.26)\" data-filename=\"freeware.png\" data-origin-width=\"500\" data-origin-height=\"500\"/></span></figure>\n</p>\n<p style=\"text-align: left;\" data-ke-size=\"size18\">&nbsp;</p>\n<p style=\"text-align: left;\" data-ke-size=\"size18\"><span style=\"color: #333333; text-align: left;\">&nbsp;윈도우용 응용프로그램 (Application)은 수없이 많은 종류가 많은 개발자들에 의해 하루에도 수백,수천개가 새로 출시되고 그보다 더 많은 수의 프로그램들이 업데이트 됩니다. 이들 응용프로그램 (Application)은 비율을 지불해야하는<span>&nbsp;</span></span><b><span style=\"color: #009a87;\">상용프로그램</span></b><span style=\"color: #333333; text-align: left;\">과 정품 구매를 확대하기 위해 공급하는 일종의 샘플 개념의<span>&nbsp;</span></span><span style=\"color: #ee2323;\"><b>쉐어웨어</b></span><span style=\"color: #333333; text-align: left;\">, 무료로 사용할 수 있는<span>&nbsp;</span></span><b><span style=\"color: #ef6f53;\">프리웨어</span></b>등으로 크게 3가지로 나뉘게 되는데요.</p>\n<p style=\"text-align: left;\" data-ke-size=\"size18\"><br />&nbsp;물론 프리웨어에도 개인만 사용할 있다던가, 기업이나 관공서에서도 사용이 가능하다던가, 소스까지 같이 공개하여 맘대로 수정과 배포가 가능한 완전 무료등의 추가 분류가 필요합니다. 하지만, 개발자가 공개하는 무료배포의 의미가 정확하지 않는 프로그램도 많고, 저작권의 정의도 각양각색이라 본 블로그에서 소개하는 프리웨어도 <span style=\"color: #006dd7;\"><b>최대한 확인이 가능한 범위에서 개인 또는 기업에서 사용가능한지를 구분하여 소개</b></span>하고 있습니다.</p>\n<p style=\"text-align: left;\" data-ke-size=\"size18\">&nbsp;</p>\n<p style=\"text-align: left;\" data-ke-size=\"size18\">&nbsp;</p>\n<p style=\"text-align: center;\" data-ke-size=\"size18\">'어떤오후의 프리웨어 이야기'에서 추천하는<br /><span style=\"color: #409d00;\">&nbsp;<b>2024년 8월 26일자 공개자료실 윈도우용 추천 프리웨어</b></span>입니다.</p>\n<p id=\"no_1\" data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><i><b>1. Bright Master (트레이 아이콘에서 휠 회전으로 모니터 밝기 조정)<br /></b></i></span></h2>\n<p data-ke-size=\"size18\">&nbsp; 설치가&nbsp;필요&nbsp;없는&nbsp;모니터&nbsp;밝기&nbsp;조절&nbsp;도구입니다. <br />DDC/CI를&nbsp;지원하는&nbsp;외부&nbsp;모니터와&nbsp;WMI&nbsp;서비스가&nbsp;비활성화되지&nbsp;않은&nbsp;노트북,&nbsp;올인원&nbsp;PC,&nbsp;윈도우&nbsp;태블릿의&nbsp;내장&nbsp;디스플레이에서&nbsp;작동합니다. <br />주요&nbsp;기능으로는&nbsp;모든&nbsp;기기의&nbsp;동기화된&nbsp;밝기&nbsp;변경,&nbsp;제3자&nbsp;소프트웨어에&nbsp;의한&nbsp;밝기&nbsp;변경&nbsp;추적,&nbsp;디지털&nbsp;밝기&nbsp;표시,&nbsp;자동&nbsp;시작,&nbsp;마우스&nbsp;휠을&nbsp;이용한&nbsp;밝기&nbsp;조절,&nbsp;밝기&nbsp;조절&nbsp;단계&nbsp;선택,&nbsp;한&nbsp;번의&nbsp;클릭으로&nbsp;최소&nbsp;밝기&nbsp;설정&nbsp;등이&nbsp;있습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Bright Master.png\" data-origin-width=\"504\" data-origin-height=\"304\"><span data-url=\"https://blog.kakaocdn.net/dn/CCuKC/btsJfiXcAQ5/hkfwcbBPBrbsN3fro1pF70/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/CCuKC/btsJfiXcAQ5/hkfwcbBPBrbsN3fro1pF70/img.png\"><img src=\"https://blog.kakaocdn.net/dn/CCuKC/btsJfiXcAQ5/hkfwcbBPBrbsN3fro1pF70/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FCCuKC%2FbtsJfiXcAQ5%2FhkfwcbBPBrbsN3fro1pF70%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"Bright Master.png\" data-origin-width=\"504\" data-origin-height=\"304\"/></span></figure>\n</p>\n<p data-ke-size=\"size18\">▶프리웨어 - 개인</p>\n<p data-ke-size=\"size18\">▶ Windows 10/11</p>\n<p data-ke-size=\"size18\">▶무료 다운로드◀</p>\n<figure id=\"og_1724628763665\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"Bright Master / Screen dimmer\" data-og-description=\"Ru Bright Master Simple screen dimmer The program is designed to adjust the brightness of monitors, laptop displays and other equipment. Runs on Windows 7+ and does not require installation. Works with all external monitors that have DDC/CI support, as wel\" data-og-host=\"brightmaster.ru\" data-og-source-url=\"https://brightmaster.ru/en\" data-og-url=\"https://brightmaster.ru/en\" data-og-image=\"\"><a href=\"https://brightmaster.ru/en\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://brightmaster.ru/en\">\n<div class=\"og-image\" style=\"background-image: url();\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">Bright Master / Screen dimmer</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">Ru Bright Master Simple screen dimmer The program is designed to adjust the brightness of monitors, laptop displays and other equipment. Runs on Windows 7+ and does not require installation. Works with all external monitors that have DDC/CI support, as wel</p>\n<p class=\"og-host\" data-ke-size=\"size16\">brightmaster.ru</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p id=\"no_2\" data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><i><b>2. Cygwin (Linux 기능을 사용할 수 있는 유연한 에뮬레이션 도구)</b></i></span></h2>\n<p data-ke-size=\"size18\">&nbsp; Windows&nbsp;사용자가&nbsp;Linux의&nbsp;기능을&nbsp;경험할&nbsp;수&nbsp;있게&nbsp;해주는&nbsp;에뮬레이션&nbsp;레이어입니다. <br />사용자는&nbsp;원하는&nbsp;Linux&nbsp;기능을&nbsp;선택하여&nbsp;설치할&nbsp;수&nbsp;있으며,&nbsp;프로그램은&nbsp;사용자&nbsp;친화적이고&nbsp;잘&nbsp;조직된&nbsp;구조를&nbsp;가지고&nbsp;있습니다.&nbsp;오디오&nbsp;드라이버,&nbsp;그래픽&nbsp;소프트웨어,&nbsp;LXDE,&nbsp;GNOME,&nbsp;KDE&nbsp;등&nbsp;다양한&nbsp;Linux&nbsp;기능을&nbsp;제공합니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Cygwin.png\" data-origin-width=\"1083\" data-origin-height=\"668\"><span data-url=\"https://blog.kakaocdn.net/dn/b2AN3z/btsJfGXIURN/NeV5FdKmPcffTyNmWppCfk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/b2AN3z/btsJfGXIURN/NeV5FdKmPcffTyNmWppCfk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/b2AN3z/btsJfGXIURN/NeV5FdKmPcffTyNmWppCfk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb2AN3z%2FbtsJfGXIURN%2FNeV5FdKmPcffTyNmWppCfk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"Cygwin.png\" data-origin-width=\"1083\" data-origin-height=\"668\"/></span></figure>\n</p>\n<p data-ke-size=\"size18\">▶프리웨어 - 개인/기업(오픈소스)</p>\n<p data-ke-size=\"size18\">▶Windows 10/11</p>\n<p data-ke-size=\"size18\">▶무료 다운로드 ◀</p>\n<figure id=\"og_1724628796923\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"Cygwin\" data-og-description=\"Cygwin Get that Linux feeling - on Windows This is the home of the Cygwin project What... ...is it? Cygwin is: a large collection of GNU and Open Source tools which provide functionality similar to a Linux distribution on Windows. a DLL (cygwin1.dll) which\" data-og-host=\"www.cygwin.com\" data-og-source-url=\"https://www.cygwin.com/\" data-og-url=\"https://www.cygwin.com/\" data-og-image=\"\"><a href=\"https://www.cygwin.com/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://www.cygwin.com/\">\n<div class=\"og-image\" style=\"background-image: url();\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">Cygwin</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">Cygwin Get that Linux feeling - on Windows This is the home of the Cygwin project What... ...is it? Cygwin is: a large collection of GNU and Open Source tools which provide functionality similar to a Linux distribution on Windows. a DLL (cygwin1.dll) which</p>\n<p class=\"og-host\" data-ke-size=\"size16\">www.cygwin.com</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p id=\"no_3\" data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><i><b>3. Rename Master (다기능적인&nbsp;파일&nbsp;이름&nbsp;변경&nbsp;솔루션)</b></i></span></h2>\n<p data-ke-size=\"size18\">&nbsp;사용자가&nbsp;다양한&nbsp;도구를&nbsp;통해&nbsp;파일&nbsp;이름을&nbsp;효율적이고&nbsp;접근하기&nbsp;쉽게&nbsp;변경할&nbsp;수&nbsp;있게&nbsp;해주는&nbsp;프로그램입니다.&nbsp;주요&nbsp;기능은&nbsp;'Renaming&nbsp;Script'&nbsp;탭에서&nbsp;찾을&nbsp;수&nbsp;있으며,&nbsp;단어/숫자&nbsp;제거,&nbsp;표현&nbsp;변경,&nbsp;기존&nbsp;이름&nbsp;교체,&nbsp;카운터&nbsp;추가&nbsp;등&nbsp;다양한&nbsp;옵션을&nbsp;제공합니다. <br />프로그램은&nbsp;일반&nbsp;사용자를&nbsp;위한&nbsp;기본&nbsp;기능뿐만&nbsp;아니라&nbsp;고급&nbsp;사용자를&nbsp;위한&nbsp;'Textfile&nbsp;Wizard'&nbsp;같은&nbsp;기능도&nbsp;제공합니다.&nbsp;전반적으로&nbsp;Rename&nbsp;Master는&nbsp;사용하기&nbsp;쉬우면서도&nbsp;고급&nbsp;사용자를&nbsp;위한&nbsp;충분한&nbsp;구성&nbsp;옵션을&nbsp;제공하는&nbsp;신뢰할&nbsp;수&nbsp;있는&nbsp;종합&nbsp;솔루션입니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Rename Master.png\" data-origin-width=\"1030\" data-origin-height=\"647\"><span data-url=\"https://blog.kakaocdn.net/dn/vUqbY/btsJfElmKdd/sEbM4woMxpjLG6fgE8TpTk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/vUqbY/btsJfElmKdd/sEbM4woMxpjLG6fgE8TpTk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/vUqbY/btsJfElmKdd/sEbM4woMxpjLG6fgE8TpTk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FvUqbY%2FbtsJfElmKdd%2FsEbM4woMxpjLG6fgE8TpTk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"Rename Master.png\" data-origin-width=\"1030\" data-origin-height=\"647\"/></span></figure>\n</p>\n<p data-ke-size=\"size18\">▶프리웨어 - 개인</p>\n<p data-ke-size=\"size18\">▶Windows 10/11&nbsp;</p>\n<p data-ke-size=\"size18\">▶무료 다운로드 ◀</p>\n<figure id=\"og_1724628842166\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"JoeJoe's freeware utilities - [Rename Master]\" data-og-description=\"Rename Master is a freeware utility designed to rename multiple files with a just few clicks. Anyone that has worked with websites, file archives, or collections of music, videos, or pictures has probably spent way too much time renaming hundreds of files.\" data-og-host=\"www.joejoesoft.com\" data-og-source-url=\"https://www.joejoesoft.com/vcms/108/\" data-og-url=\"https://www.joejoesoft.com/vcms/108/\" data-og-image=\"\"><a href=\"https://www.joejoesoft.com/vcms/108/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://www.joejoesoft.com/vcms/108/\">\n<div class=\"og-image\" style=\"background-image: url();\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">JoeJoe's freeware utilities - [Rename Master]</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">Rename Master is a freeware utility designed to rename multiple files with a just few clicks. Anyone that has worked with websites, file archives, or collections of music, videos, or pictures has probably spent way too much time renaming hundreds of files.</p>\n<p class=\"og-host\" data-ke-size=\"size16\">www.joejoesoft.com</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p id=\"no_4\" data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><i><b>4. VariCAD Viewer (다기능&nbsp;CAD&nbsp;파일&nbsp;뷰어)<br /></b></i></span></h2>\n<p data-ke-size=\"size18\">&nbsp;단순히&nbsp;CAD&nbsp;도면을&nbsp;보는&nbsp;것&nbsp;이상의&nbsp;기능을&nbsp;제공합니다.&nbsp;2D&nbsp;DWG,&nbsp;3D&nbsp;STEP,&nbsp;2D/3D&nbsp;VariCAD&nbsp;등&nbsp;다양한&nbsp;형식의&nbsp;파일을&nbsp;지원하며,&nbsp;간단한&nbsp;조정과&nbsp;인쇄&nbsp;기능도&nbsp;갖추고&nbsp;있습니다.&nbsp;이&nbsp;프로그램은&nbsp;3D&nbsp;모델링에&nbsp;대해&nbsp;잘&nbsp;아는&nbsp;전문가들을&nbsp;위해&nbsp;설계되었습니다.</p>\n<p data-ke-size=\"size18\">&nbsp;인터페이스는 현대적이지만 다양한 기능으로 인해 다소 복잡해 보일 수 있습니다. 그러나 CAD 경험이 있는 사용자라면 직관적으로 사용할 수 있을 것입니다. 도구 모음의 배치를 사용자가 원하는 대로 조정할 수 있는 것도 장점입니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"VariCAD Viewer.png\" data-origin-width=\"1360\" data-origin-height=\"757\"><span data-url=\"https://blog.kakaocdn.net/dn/Zxvkp/btsJeQ7R4Ia/5zkpqgQItCCEUbKkidGFhk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/Zxvkp/btsJeQ7R4Ia/5zkpqgQItCCEUbKkidGFhk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/Zxvkp/btsJeQ7R4Ia/5zkpqgQItCCEUbKkidGFhk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FZxvkp%2FbtsJeQ7R4Ia%2F5zkpqgQItCCEUbKkidGFhk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"VariCAD Viewer.png\" data-origin-width=\"1360\" data-origin-height=\"757\"/></span></figure>\n</p>\n<p data-ke-size=\"size18\">▶프리웨어 - 개인</p>\n<p data-ke-size=\"size18\">▶Windows 10/11</p>\n<p data-ke-size=\"size18\">▶무료 다운로드 ◀</p>\n<figure id=\"og_1724628874152\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"Products | VariCAD - 3D/2D CAD system\" data-og-description=\"Products VariCAD - 3D / 2D mechanical CAD software We have a simple strategy - to sell VariCAD, with all its features, for one very affordable price. VariCAD is the best value you will find anywhere for this type of modeling and designing software. VariCAD\" data-og-host=\"www.varicad.com\" data-og-source-url=\"https://www.varicad.com/en/home/products/products/\" data-og-url=\"https://www.varicad.com/en/home/products/products/\" data-og-image=\"\"><a href=\"https://www.varicad.com/en/home/products/products/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://www.varicad.com/en/home/products/products/\">\n<div class=\"og-image\" style=\"background-image: url();\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">Products | VariCAD - 3D/2D CAD system</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">Products VariCAD - 3D / 2D mechanical CAD software We have a simple strategy - to sell VariCAD, with all its features, for one very affordable price. VariCAD is the best value you will find anywhere for this type of modeling and designing software. VariCAD</p>\n<p class=\"og-host\" data-ke-size=\"size16\">www.varicad.com</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p id=\"no_5\" data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><i><b>5. TreeSheets (다목적 데이터 조직화 및 시각화 도구)</b></i></span></h2>\n<p data-ke-size=\"size18\">&nbsp; 스프레드시트,&nbsp;마인드맵,&nbsp;노트&nbsp;애플리케이션의&nbsp;기능을&nbsp;하나의&nbsp;사용하기&nbsp;쉬운&nbsp;인터페이스로&nbsp;결합한&nbsp;프로그램입니다.&nbsp;복잡한&nbsp;데이터&nbsp;구조를&nbsp;쉽게&nbsp;다룰&nbsp;수&nbsp;있는&nbsp;것이&nbsp;주요&nbsp;특징입니다.</p>\n<p data-ke-size=\"size18\"><br />이&nbsp;프로그램은&nbsp;프로젝트&nbsp;관리,&nbsp;일정&nbsp;계획,&nbsp;브레인스토밍,&nbsp;할&nbsp;일&nbsp;목록&nbsp;작성&nbsp;등&nbsp;다양한&nbsp;용도로&nbsp;사용할&nbsp;수&nbsp;있습니다.&nbsp;사용자는&nbsp;데이터를&nbsp;계층적&nbsp;트리,&nbsp;스프레드시트&nbsp;그리드,&nbsp;마인드맵&nbsp;등&nbsp;다양한&nbsp;형태로&nbsp;시각화할&nbsp;수&nbsp;있습니다. <br />또한&nbsp;색상과&nbsp;테두리&nbsp;등을&nbsp;사용자&nbsp;지정하여&nbsp;시트의&nbsp;외관을&nbsp;개선할&nbsp;수&nbsp;있으며,&nbsp;작업을&nbsp;우선순위화하고&nbsp;마감일을&nbsp;설정하는&nbsp;등의&nbsp;기능을&nbsp;통해&nbsp;생산성을&nbsp;높일&nbsp;수&nbsp;있습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"TreeSheets.png\" data-origin-width=\"1149\" data-origin-height=\"761\"><span data-url=\"https://blog.kakaocdn.net/dn/bareS8/btsJelm7E7B/nMJiF5wlkyZskOHMQ57w20/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bareS8/btsJelm7E7B/nMJiF5wlkyZskOHMQ57w20/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bareS8/btsJelm7E7B/nMJiF5wlkyZskOHMQ57w20/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbareS8%2FbtsJelm7E7B%2FnMJiF5wlkyZskOHMQ57w20%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"TreeSheets.png\" data-origin-width=\"1149\" data-origin-height=\"761\"/></span></figure>\n</p>\n<p data-ke-size=\"size18\">▶프리웨어 - 개인/기업 (오픈소스)</p>\n<p data-ke-size=\"size18\">▶Windows 10/11</p>\n<p data-ke-size=\"size18\">▶무료 다운로드 ◀</p>\n<figure id=\"og_1724628900387\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"TreeSheets\" data-og-description=\"Open Source Free Form Data Organizer (Hierarchical Spreadsheet) D O W N L O A D : Download LATEST from github releases: Windows / OS X / Linux Click HERE, pick your platform from the latest release. &nbsp; A &quot;hierarchical spreadsheet&quot; that is a great replaceme\" data-og-host=\"strlen.com\" data-og-source-url=\"https://strlen.com/treesheets/\" data-og-url=\"https://strlen.com/treesheets/\" data-og-image=\"https://scrap.kakaocdn.net/dn/bndYPd/hyWSfzKILl/YLEhMi5yuQXXxgdNMPthdk/img.png?width=607&amp;height=498&amp;face=0_0_607_498\"><a href=\"https://strlen.com/treesheets/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://strlen.com/treesheets/\">\n<div class=\"og-image\" style=\"background-image: url('https://scrap.kakaocdn.net/dn/bndYPd/hyWSfzKILl/YLEhMi5yuQXXxgdNMPthdk/img.png?width=607&amp;height=498&amp;face=0_0_607_498');\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">TreeSheets</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">Open Source Free Form Data Organizer (Hierarchical Spreadsheet) D O W N L O A D : Download LATEST from github releases: Windows / OS X / Linux Click HERE, pick your platform from the latest release. &nbsp; A \"hierarchical spreadsheet\" that is a great replaceme</p>\n<p class=\"og-host\" data-ke-size=\"size16\">strlen.com</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "네이버 소프트웨어와 같은 프로그램 소개 사이트가 종료된 후, 윈도우 운영체제를 사용하는 이용자들을 위해 공개 프리웨어 및 오픈소스 프로그램을 소개합니다. 유용한 무료 소프트웨어를 찾고자 하는 사용자들에게 정기적으로 알찬 정보를 제공합니다.\n\n\n \n 윈도우용 응용프로그램 (Application)은 수없이 많은 종류가 많은 개발자들에 의해 하루에도 수백,수천개가 새로 출시되고 그보다 더 많은 수의 프로그램들이 업데이트 됩니다. 이들 응용프로그램 (Application)은 비율을 지불해야하는 상용프로그램과 정품 구매를 확대하기 위해 공급하는 일종의 샘플 개념의 쉐어웨어, 무료로 사용할 수 있는 프리웨어등으로 크게 3가지로 나뉘게 되는데요.\n 물론 프리웨어에도 개인만 사용할 있다던가, 기업이나 관공서에서도 사용이 가능하다던가, 소스까지 같이 공개하여 맘대로 수정과 배포가 가능한 완전 무료등의 추가 분류가 필요합니다. 하지만, 개발자가 공개하는 무료배포의 의미가 정확하지 않는 프로그램도 많고, 저작권의 정의도 각양각색이라 본 블로그에서 소개하는 프리웨어도 최대한 확인이 가능한 범위에서 개인 또는 기업에서 사용가능한지를 구분하여 소개하고 있습니다.\n \n \n'어떤오후의 프리웨어 이야기'에서 추천하는\n 2024년 8월 26일자 공개자료실 윈도우용 추천 프리웨어입니다.\n \n \n1. Bright Master (트레이 아이콘에서 휠 회전으로 모니터 밝기 조정)\n\n  설치가 필요 없는 모니터 밝기 조절 도구입니다. \nDDC/CI를 지원하는 외부 모니터와 WMI 서비스가 비활성화되지 않은 노트북, 올인원 PC, 윈도우 태블릿의 내장 디스플레이에서 작동합니다. \n주요 기능으로는 모든 기기의 동기화된 밝기 변경, 제3자 소프트웨어에 의한 밝기 변경 추적, 디지털 밝기 표시, 자동 시작, 마우스 휠을 이용한 밝기 조절, 밝기 조절 단계 선택, 한 번의 클릭으로 최소 밝기 설정 등이 있습니다.\n\n\n▶프리웨어 - 개인\n▶ Windows 10/11\n▶무료 다운로드◀\n\n \nBright Master / Screen dimmer\nRu Bright Master Simple screen dimmer The program is designed to adjust the brightness of monitors, laptop displays and other equipment. Runs on Windows 7+ and does not require installation. Works with all external monitors that have DDC/CI support, as wel\nbrightmaster.ru\n\n \n \n \n \n \n \n2. Cygwin (Linux 기능을 사용할 수 있는 유연한 에뮬레이션 도구)\n  Windows 사용자가 Linux의 기능을 경험할 수 있게 해주는 에뮬레이션 레이어입니다. \n사용자는 원하는 Linux 기능을 선택하여 설치할 수 있으며, 프로그램은 사용자 친화적이고 잘 조직된 구조를 가지고 있습니다. 오디오 드라이버, 그래픽 소프트웨어, LXDE, GNOME, KDE 등 다양한 Linux 기능을 제공합니다.\n\n\n▶프리웨어 - 개인/기업(오픈소스)\n▶Windows 10/11\n▶무료 다운로드 ◀\n\n \nCygwin\nCygwin Get that Linux feeling - on Windows This is the home of the Cygwin project What... ...is it? Cygwin is: a large collection of GNU and Open Source tools which provide functionality similar to a Linux distribution on Windows. a DLL (cygwin1.dll) which\nwww.cygwin.com\n\n \n \n \n \n \n \n3. Rename Master (다기능적인 파일 이름 변경 솔루션)\n 사용자가 다양한 도구를 통해 파일 이름을 효율적이고 접근하기 쉽게 변경할 수 있게 해주는 프로그램입니다. 주요 기능은 'Renaming Script' 탭에서 찾을 수 있으며, 단어/숫자 제거, 표현 변경, 기존 이름 교체, 카운터 추가 등 다양한 옵션을 제공합니다. \n프로그램은 일반 사용자를 위한 기본 기능뿐만 아니라 고급 사용자를 위한 'Textfile Wizard' 같은 기능도 제공합니다. 전반적으로 Rename Master는 사용하기 쉬우면서도 고급 사용자를 위한 충분한 구성 옵션을 제공하는 신뢰할 수 있는 종합 솔루션입니다.\n\n\n▶프리웨어 - 개인\n▶Windows 10/11 \n▶무료 다운로드 ◀\n\n \nJoeJoe's freeware utilities - [Rename Master]\nRename Master is a freeware utility designed to rename multiple files with a just few clicks. Anyone that has worked with websites, file archives, or collections of music, videos, or pictures has probably spent way too much time renaming hundreds of files.\nwww.joejoesoft.com\n\n \n \n \n \n \n \n4. VariCAD Viewer (다기능 CAD 파일 뷰어)\n\n 단순히 CAD 도면을 보는 것 이상의 기능을 제공합니다. 2D DWG, 3D STEP, 2D/3D VariCAD 등 다양한 형식의 파일을 지원하며, 간단한 조정과 인쇄 기능도 갖추고 있습니다. 이 프로그램은 3D 모델링에 대해 잘 아는 전문가들을 위해 설계되었습니다.\n 인터페이스는 현대적이지만 다양한 기능으로 인해 다소 복잡해 보일 수 있습니다. 그러나 CAD 경험이 있는 사용자라면 직관적으로 사용할 수 있을 것입니다. 도구 모음의 배치를 사용자가 원하는 대로 조정할 수 있는 것도 장점입니다.\n\n\n▶프리웨어 - 개인\n▶Windows 10/11\n▶무료 다운로드 ◀\n\n \nProducts | VariCAD - 3D/2D CAD system\nProducts VariCAD - 3D / 2D mechanical CAD software We have a simple strategy - to sell VariCAD, with all its features, for one very affordable price. VariCAD is the best value you will find anywhere for this type of modeling and designing software. VariCAD\nwww.varicad.com\n\n \n \n \n \n \n \n5. TreeSheets (다목적 데이터 조직화 및 시각화 도구)\n  스프레드시트, 마인드맵, 노트 애플리케이션의 기능을 하나의 사용하기 쉬운 인터페이스로 결합한 프로그램입니다. 복잡한 데이터 구조를 쉽게 다룰 수 있는 것이 주요 특징입니다.\n이 프로그램은 프로젝트 관리, 일정 계획, 브레인스토밍, 할 일 목록 작성 등 다양한 용도로 사용할 수 있습니다. 사용자는 데이터를 계층적 트리, 스프레드시트 그리드, 마인드맵 등 다양한 형태로 시각화할 수 있습니다. \n또한 색상과 테두리 등을 사용자 지정하여 시트의 외관을 개선할 수 있으며, 작업을 우선순위화하고 마감일을 설정하는 등의 기능을 통해 생산성을 높일 수 있습니다.\n\n\n▶프리웨어 - 개인/기업 (오픈소스)\n▶Windows 10/11\n▶무료 다운로드 ◀\n\n \nTreeSheets\nOpen Source Free Form Data Organizer (Hierarchical Spreadsheet) D O W N L O A D : Download LATEST from github releases: Windows / OS X / Linux Click HERE, pick your platform from the latest release.   A \"hierarchical spreadsheet\" that is a great replaceme\nstrlen.com",
        "guid": "http://muzbox.tistory.com/483462",
        "categories": [
          "NEWS/프리웨어 뉴스",
          "cad뷰어",
          "linux에뮬레이션",
          "기업무료프로그램",
          "데이터시각화",
          "모니터밝기조절",
          "무료프로그램",
          "추천프로그램",
          "파일관리",
          "프로젝트관리",
          "프리웨어"
        ],
        "isoDate": "2024-08-25T23:37:38.000Z"
      },
      {
        "creator": "어떤오후의 프리웨어 이야기",
        "title": "스마트폰으로 DSLR 부럽지 않은 사진 찍는 4가지 방법",
        "link": "http://muzbox.tistory.com/483461",
        "pubDate": "Fri, 23 Aug 2024 17:50:43 +0900",
        "author": "어떤오후의 프리웨어 이야기",
        "comments": "http://muzbox.tistory.com/483461#entry483461comment",
        "content": "<p data-ke-size=\"size16\">스마트폰으로 DSLR 못지않은 사진을 찍고 싶으신가요? 이 글에서 소개하는 간단한 비법들을 통해, DSLR 없이도 스마트폰만으로 멋진 사진을 촬영하는 방법을 알아보세요.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"0000.png\" data-origin-width=\"1234\" data-origin-height=\"694\"><span data-url=\"https://blog.kakaocdn.net/dn/dgYX4o/btsJcJnZHOm/sHaXaT6CAXv3XpQ5iSCWG0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/dgYX4o/btsJcJnZHOm/sHaXaT6CAXv3XpQ5iSCWG0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/dgYX4o/btsJcJnZHOm/sHaXaT6CAXv3XpQ5iSCWG0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdgYX4o%2FbtsJcJnZHOm%2FsHaXaT6CAXv3XpQ5iSCWG0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"스마트폰으로 DSLR 부럽지 않은 사진 찍는 4가지 방법\" data-filename=\"0000.png\" data-origin-width=\"1234\" data-origin-height=\"694\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">스마트폰이 우리 일상에서 없어서는 안 될 필수 아이템이 된 지도 오래입니다. 특히, 최근 몇 년간 스마트폰 카메라 기술이 비약적으로 발전하면서, 이제는 DSLR 없이도 훌륭한 사진을 촬영할 수 있게 되었습니다. 하지만 좋은 장비만으로는 뛰어난 사진을 찍을 수 없다는 사실, 알고 계셨나요? 이번 글에서는 별도의 DSLR 없이도 스마트폰으로 멋진 사진을 찍을 수 있는 몇 가지 비법을 소개하고자 합니다. 이제 언제 어디서나 스마트폰을 꺼내들어 전문가처럼 사진을 찍어보세요.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><b>1. 카메라 설정 최적화하기</b></span></h2>\n<p data-ke-size=\"size16\">스마트폰의 기본 카메라 앱은 많은 설정을 자동으로 처리해줍니다. 하지만 설정을 조금만 손봐도 사진의 퀄리티를 크게 향상시킬 수 있습니다. 예를 들어, HDR(High Dynamic Range) 기능을 활성화하면 밝고 어두운 영역이 고르게 노출되어 더 자연스럽고 선명한 사진을 얻을 수 있습니다. 또한, 촬영 전에 초점을 수동으로 맞추고, 노출을 조정하는 것 역시 중요합니다. 빛이 너무 많거나 적으면 사진이 흐릿해지거나, 디테일이 사라질 수 있습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"a.png\" data-origin-width=\"1234\" data-origin-height=\"694\"><span data-url=\"https://blog.kakaocdn.net/dn/eXksWk/btsJcQHmAMR/qlHktJNK87wm2pUXyqECgk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/eXksWk/btsJcQHmAMR/qlHktJNK87wm2pUXyqECgk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/eXksWk/btsJcQHmAMR/qlHktJNK87wm2pUXyqECgk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FeXksWk%2FbtsJcQHmAMR%2FqlHktJNK87wm2pUXyqECgk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"카메라 설정 최적화하기\" data-filename=\"a.png\" data-origin-width=\"1234\" data-origin-height=\"694\"/></span></figure>\n</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>HDR 기능 사용:</b> 강한 대비가 있는 장면에서 더 디테일한 사진을 촬영할 수 있습니다.</li>\n<li><b>초점 수동 조절:</b> 사진의 주제가 명확하게 보이도록 초점을 맞추세요.</li>\n<li><b>노출 조정:</b> 밝기를 조절하여 디테일을 더 살릴 수 있습니다.</li>\n</ul>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><b>2. 자연광을 활용하라</b></span></h2>\n<p data-ke-size=\"size16\">빛은 사진의 품질을 좌우하는 중요한 요소입니다. 스마트폰 카메라 센서의 크기는 DSLR보다 작기 때문에, 충분한 조명이 없다면 사진이 흐릿하게 나오기 쉽습니다. 자연광을 잘 활용하면 이런 문제를 해결할 수 있습니다. 예를 들어, 창가에서 들어오는 자연광을 활용하거나, 해가 질 무렵의 부드러운 빛을 이용해 촬영해 보세요. 실내 촬영 시에는 인공 조명보다는 자연광을 최대한 활용하는 것이 좋습니다. 단, 역광을 피하고 피사체가 빛을 충분히 받도록 각도를 조절해야 합니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"b.png\" data-origin-width=\"1234\" data-origin-height=\"694\"><span data-url=\"https://blog.kakaocdn.net/dn/ba9KfT/btsJdmThLeR/r6rk4BWLkFmtlBfZVDOu60/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/ba9KfT/btsJdmThLeR/r6rk4BWLkFmtlBfZVDOu60/img.png\"><img src=\"https://blog.kakaocdn.net/dn/ba9KfT/btsJdmThLeR/r6rk4BWLkFmtlBfZVDOu60/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fba9KfT%2FbtsJdmThLeR%2Fr6rk4BWLkFmtlBfZVDOu60%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"자연광을 활용하라\" data-filename=\"b.png\" data-origin-width=\"1234\" data-origin-height=\"694\"/></span></figure>\n</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>황금 시간대 촬영:</b> 일출이나 일몰 시의 부드러운 빛을 활용해보세요.</li>\n<li><b>역광 피하기:</b> 피사체가 어두워지지 않도록 주의하세요.</li>\n<li><b>반사된 빛 활용:</b> 직사광선이 강할 때는 반사된 빛을 이용하세요.</li>\n</ul>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><b>3. 구도와 각도에 신경 쓰기</b></span></h2>\n<p data-ke-size=\"size16\">좋은 사진은 구도에서 시작된다고 해도 과언이 아닙니다. 촬영할 때 단순히 피사체를 중앙에 두기보다는 3분할 구도를 활용하거나, 다양한 각도에서 찍어보는 것이 좋습니다. 예를 들어, 낮은 각도에서 촬영하면 피사체가 더 돋보이게 되며, 높은 각도에서는 넓은 배경을 효과적으로 담을 수 있습니다. 또한, 수평을 맞추는 것 역시 중요합니다. 스마트폰의 그리드 라인을 활용하면 수평을 쉽게 맞출 수 있습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"c.png\" data-origin-width=\"1234\" data-origin-height=\"694\"><span data-url=\"https://blog.kakaocdn.net/dn/QXn8G/btsJeAvZqmg/3DJuAfVCyf3jb2r1qEqH50/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/QXn8G/btsJeAvZqmg/3DJuAfVCyf3jb2r1qEqH50/img.png\"><img src=\"https://blog.kakaocdn.net/dn/QXn8G/btsJeAvZqmg/3DJuAfVCyf3jb2r1qEqH50/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FQXn8G%2FbtsJeAvZqmg%2F3DJuAfVCyf3jb2r1qEqH50%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"구도와 각도에 신경 쓰기\" data-filename=\"c.png\" data-origin-width=\"1234\" data-origin-height=\"694\"/></span></figure>\n</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>3분할 구도:</b> 사진의 중심을 3등분하여 구성하세요.</li>\n<li><b>다양한 각도 실험:</b> 여러 각도에서 촬영하여 가장 인상적인 장면을 찾아보세요.</li>\n<li><b>수평 맞추기:</b> 그리드 라인을 사용하여 수평을 정확히 맞추세요.</li>\n</ul>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><b>4. 간단한 후반 작업으로 완성도 높이기</b></span></h2>\n<p data-ke-size=\"size16\">사진을 찍고 난 후에도 스마트폰에서 바로 간단한 편집 작업을 통해 완성도를 높일 수 있습니다. 예를 들어, 밝기와 대비를 조정하거나, 색온도를 맞추는 것만으로도 사진의 분위기가 완전히 달라질 수 있습니다. 요즘 스마트폰에는 다양한 편집 앱이 있어서 전문가처럼 색상 보정, 노이즈 제거, 크롭 등을 손쉽게 할 수 있습니다. 하지만 과도한 편집은 오히려 사진의 자연스러움을 해칠 수 있으므로, 적절한 선에서 작업을 마무리하는 것이 좋습니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"d.png\" data-origin-width=\"1234\" data-origin-height=\"694\"><span data-url=\"https://blog.kakaocdn.net/dn/bHSy0O/btsJekmEDnB/IqyGjmZRrUxZx8mgErOMqK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bHSy0O/btsJekmEDnB/IqyGjmZRrUxZx8mgErOMqK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bHSy0O/btsJekmEDnB/IqyGjmZRrUxZx8mgErOMqK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbHSy0O%2FbtsJekmEDnB%2FIqyGjmZRrUxZx8mgErOMqK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"d.png\" data-origin-width=\"1234\" data-origin-height=\"694\"/></span></figure>\n</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><b>밝기 및 대비 조절:</b> 간단한 조정만으로도 사진이 선명해질 수 있습니다.</li>\n<li><b>색온도 맞추기:</b> 사진의 분위기를 원하는 대로 조정하세요.</li>\n<li><b>과한 편집 피하기:</b> 자연스러움을 잃지 않도록 주의하세요.</li>\n</ul>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\"><span style=\"color: #006dd7;\"><b>결론</b></span></h2>\n<p data-ke-size=\"size16\">스마트폰 카메라는 이제 우리 삶의 필수품이 되었으며, DSLR에 뒤지지 않는 성능을 자랑합니다. 하지만 그만큼 사용자의 노력도 중요합니다. 이번에 소개한 간단한 팁들을 활용하면, 언제 어디서나 스마트폰으로 전문가 못지않은 사진을 촬영할 수 있을 것입니다. 중요한 것은 장비가 아닌, 그 장비를 어떻게 활용하느냐입니다. 다음번에 사진을 찍을 때는 이 비법들을 꼭 기억하고 적용해 보세요.</p>",
        "contentSnippet": "스마트폰으로 DSLR 못지않은 사진을 찍고 싶으신가요? 이 글에서 소개하는 간단한 비법들을 통해, DSLR 없이도 스마트폰만으로 멋진 사진을 촬영하는 방법을 알아보세요.\n\n\n \n스마트폰이 우리 일상에서 없어서는 안 될 필수 아이템이 된 지도 오래입니다. 특히, 최근 몇 년간 스마트폰 카메라 기술이 비약적으로 발전하면서, 이제는 DSLR 없이도 훌륭한 사진을 촬영할 수 있게 되었습니다. 하지만 좋은 장비만으로는 뛰어난 사진을 찍을 수 없다는 사실, 알고 계셨나요? 이번 글에서는 별도의 DSLR 없이도 스마트폰으로 멋진 사진을 찍을 수 있는 몇 가지 비법을 소개하고자 합니다. 이제 언제 어디서나 스마트폰을 꺼내들어 전문가처럼 사진을 찍어보세요.\n \n \n1. 카메라 설정 최적화하기\n스마트폰의 기본 카메라 앱은 많은 설정을 자동으로 처리해줍니다. 하지만 설정을 조금만 손봐도 사진의 퀄리티를 크게 향상시킬 수 있습니다. 예를 들어, HDR(High Dynamic Range) 기능을 활성화하면 밝고 어두운 영역이 고르게 노출되어 더 자연스럽고 선명한 사진을 얻을 수 있습니다. 또한, 촬영 전에 초점을 수동으로 맞추고, 노출을 조정하는 것 역시 중요합니다. 빛이 너무 많거나 적으면 사진이 흐릿해지거나, 디테일이 사라질 수 있습니다.\n\n\n\nHDR 기능 사용: 강한 대비가 있는 장면에서 더 디테일한 사진을 촬영할 수 있습니다.\n초점 수동 조절: 사진의 주제가 명확하게 보이도록 초점을 맞추세요.\n노출 조정: 밝기를 조절하여 디테일을 더 살릴 수 있습니다.\n \n \n2. 자연광을 활용하라\n빛은 사진의 품질을 좌우하는 중요한 요소입니다. 스마트폰 카메라 센서의 크기는 DSLR보다 작기 때문에, 충분한 조명이 없다면 사진이 흐릿하게 나오기 쉽습니다. 자연광을 잘 활용하면 이런 문제를 해결할 수 있습니다. 예를 들어, 창가에서 들어오는 자연광을 활용하거나, 해가 질 무렵의 부드러운 빛을 이용해 촬영해 보세요. 실내 촬영 시에는 인공 조명보다는 자연광을 최대한 활용하는 것이 좋습니다. 단, 역광을 피하고 피사체가 빛을 충분히 받도록 각도를 조절해야 합니다.\n\n\n\n황금 시간대 촬영: 일출이나 일몰 시의 부드러운 빛을 활용해보세요.\n역광 피하기: 피사체가 어두워지지 않도록 주의하세요.\n반사된 빛 활용: 직사광선이 강할 때는 반사된 빛을 이용하세요.\n \n \n3. 구도와 각도에 신경 쓰기\n좋은 사진은 구도에서 시작된다고 해도 과언이 아닙니다. 촬영할 때 단순히 피사체를 중앙에 두기보다는 3분할 구도를 활용하거나, 다양한 각도에서 찍어보는 것이 좋습니다. 예를 들어, 낮은 각도에서 촬영하면 피사체가 더 돋보이게 되며, 높은 각도에서는 넓은 배경을 효과적으로 담을 수 있습니다. 또한, 수평을 맞추는 것 역시 중요합니다. 스마트폰의 그리드 라인을 활용하면 수평을 쉽게 맞출 수 있습니다.\n\n\n\n3분할 구도: 사진의 중심을 3등분하여 구성하세요.\n다양한 각도 실험: 여러 각도에서 촬영하여 가장 인상적인 장면을 찾아보세요.\n수평 맞추기: 그리드 라인을 사용하여 수평을 정확히 맞추세요.\n \n \n4. 간단한 후반 작업으로 완성도 높이기\n사진을 찍고 난 후에도 스마트폰에서 바로 간단한 편집 작업을 통해 완성도를 높일 수 있습니다. 예를 들어, 밝기와 대비를 조정하거나, 색온도를 맞추는 것만으로도 사진의 분위기가 완전히 달라질 수 있습니다. 요즘 스마트폰에는 다양한 편집 앱이 있어서 전문가처럼 색상 보정, 노이즈 제거, 크롭 등을 손쉽게 할 수 있습니다. 하지만 과도한 편집은 오히려 사진의 자연스러움을 해칠 수 있으므로, 적절한 선에서 작업을 마무리하는 것이 좋습니다.\n\n\n\n밝기 및 대비 조절: 간단한 조정만으로도 사진이 선명해질 수 있습니다.\n색온도 맞추기: 사진의 분위기를 원하는 대로 조정하세요.\n과한 편집 피하기: 자연스러움을 잃지 않도록 주의하세요.\n \n \n결론\n스마트폰 카메라는 이제 우리 삶의 필수품이 되었으며, DSLR에 뒤지지 않는 성능을 자랑합니다. 하지만 그만큼 사용자의 노력도 중요합니다. 이번에 소개한 간단한 팁들을 활용하면, 언제 어디서나 스마트폰으로 전문가 못지않은 사진을 촬영할 수 있을 것입니다. 중요한 것은 장비가 아닌, 그 장비를 어떻게 활용하느냐입니다. 다음번에 사진을 찍을 때는 이 비법들을 꼭 기억하고 적용해 보세요.",
        "guid": "http://muzbox.tistory.com/483461",
        "categories": [
          "ANDROID &amp; 모바일/안드로이드 꿀팁",
          "dslr 없이 사진",
          "hdr 기능",
          "사진 구도",
          "사진 편집",
          "스마트폰 사진 비법",
          "스마트폰 사진 촬영",
          "스마트폰 카메라 설정",
          "자연광 활용"
        ],
        "isoDate": "2024-08-23T08:50:43.000Z"
      }
    ]
  },
  {
    "name": "동우리의 블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "하테나",
    "category": "개인",
    "posts": []
  },
  {
    "name": "늑돌이네 라지온",
    "category": "개인",
    "posts": [
      {
        "creator": "늑돌이",
        "title": "한국 이북리더 명맥 잇나? 예스24 크레마 페블 예약판매",
        "link": "https://lazion.com/2513722",
        "pubDate": "Tue, 27 Aug 2024 18:45:01 +0900",
        "author": "늑돌이",
        "comments": "https://lazion.com/2513722#entry2513722comment",
        "content": "<h3 data-ke-size=\"size23\">예스24(YES24)에서 새로운 6인치 이북 리더 <b>크레마 페블(Crema&nbsp;Pebble)</b> 예약판매를 시작합니다.</h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><span style=\"color: #006dd7;\"><b>크레마 페블, 달라진 점</b></span><span style=\"color: #006dd7;\"></span></h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"1846260943_20240827112408_4712478486_web.jpg\" data-origin-width=\"3000\" data-origin-height=\"1688\"><span data-url=\"https://blog.kakaocdn.net/dn/cg9Fgs/btsJgIoRoqA/UkFx6aQKEKzdt1rPnjoKxk/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/cg9Fgs/btsJgIoRoqA/UkFx6aQKEKzdt1rPnjoKxk/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/cg9Fgs/btsJgIoRoqA/UkFx6aQKEKzdt1rPnjoKxk/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcg9Fgs%2FbtsJgIoRoqA%2FUkFx6aQKEKzdt1rPnjoKxk%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"1846260943_20240827112408_4712478486_web.jpg\" data-origin-width=\"3000\" data-origin-height=\"1688\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">6인치 화면 크기에 전작인 크레마 모티프에서 51g 경량화한 <b>139g의 무게</b>로 같은 화면 크기의 이북 리더 가운데에는 가장 가벼운 이 제품은 손에 착 감기는 그립감과 함께 조약돌(Pebble)처럼 부드러운 외관을 자랑합니다. </p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">1448x1072 해상도의 카르타(Carta) 6인치 e-ink 패널과 쿼드코어 프로세서, 안드로이드 11 OS를 가지고 있으며 구글 플레이도 지원합니다. RAM은 전작보다 1GB 늘어난 4GB로 더 쾌적한 반응 속도를 제공합니다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"upgradeDef01.jpg\" data-origin-width=\"430\" data-origin-height=\"409\"><span data-url=\"https://blog.kakaocdn.net/dn/s3qtj/btsJioPQqLt/VjLIqkNvbj6Gcvb3FkC0C0/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/s3qtj/btsJioPQqLt/VjLIqkNvbj6Gcvb3FkC0C0/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/s3qtj/btsJioPQqLt/VjLIqkNvbj6Gcvb3FkC0C0/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fs3qtj%2FbtsJioPQqLt%2FVjLIqkNvbj6Gcvb3FkC0C0%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"upgradeDef01.jpg\" data-origin-width=\"430\" data-origin-height=\"409\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">색상은 하양과 까망 2종이며 기본 저장소로 32GB 내장, 마이크로SD 슬롯을 통해 최대 512GB까지 확장 가능합니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h3 data-ke-size=\"size23\"><span style=\"color: #006dd7;\"><b>크레마 페블 가격과 예약판매 이벤트</b></span></h3>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\"><b>크레마 페블 본체의 가격은 21만9천원</b>이며, 액세서리와 함께 파는 패키지도 있습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">예스24는 크레마 페블 출시를 기념하여 <a href=\"http://app.ac/HqpJYK203\" target=\"_blank\" rel=\"noopener\"><b>예약판매 이벤트</b>도 진행</a>합니다. 예약 판매 기간인 9월 2일까지 예스24 홈페이지에서 크레마 페블을 구매할 경우 10명을 추첨해 YES상품권 5만원을 증정합니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"img0701.jpg\" data-origin-width=\"600\" data-origin-height=\"470\"><span data-url=\"https://blog.kakaocdn.net/dn/ZNW8a/btsJieGIpBP/8cY0qWkyyNZWcgvzPBVkvk/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/ZNW8a/btsJieGIpBP/8cY0qWkyyNZWcgvzPBVkvk/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/ZNW8a/btsJieGIpBP/8cY0qWkyyNZWcgvzPBVkvk/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FZNW8a%2FbtsJieGIpBP%2F8cY0qWkyyNZWcgvzPBVkvk%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" data-filename=\"img0701.jpg\" data-origin-width=\"600\" data-origin-height=\"470\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">새로운 6인치 e-ink 리더 <b>크레마 페블의 정식 출시는 9월 3일</b>입니다.<br /><br /></p>\n<p data-ke-size=\"size16\">더 자세한 내용은 <b><span style=\"color: #0593d3;\"><a href=\"http://app.ac/HqpJYK203\" target=\"_blank\" rel=\"noopener\">이곳에서 참고하시기 바랍니다.</a></span></b></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: right;\" data-ke-size=\"size16\"><span style=\"color: #1a5490;\">이 포스트에 포함된 링크를 통해 제품을 구매하는 경우 라지온에서 판매 수수료를 받을 수 있습니다.</span></p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p style=\"text-align: right;\" data-ke-size=\"size16\">(출처 : 예스24)</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<div class=\"notranslate\" style=\"all: initial;\">&nbsp;</div>",
        "contentSnippet": "예스24(YES24)에서 새로운 6인치 이북 리더 크레마 페블(Crema Pebble) 예약판매를 시작합니다.\n \n크레마 페블, 달라진 점\n \n\n\n \n6인치 화면 크기에 전작인 크레마 모티프에서 51g 경량화한 139g의 무게로 같은 화면 크기의 이북 리더 가운데에는 가장 가벼운 이 제품은 손에 착 감기는 그립감과 함께 조약돌(Pebble)처럼 부드러운 외관을 자랑합니다. \n \n1448x1072 해상도의 카르타(Carta) 6인치 e-ink 패널과 쿼드코어 프로세서, 안드로이드 11 OS를 가지고 있으며 구글 플레이도 지원합니다. RAM은 전작보다 1GB 늘어난 4GB로 더 쾌적한 반응 속도를 제공합니다.\n\n\n색상은 하양과 까망 2종이며 기본 저장소로 32GB 내장, 마이크로SD 슬롯을 통해 최대 512GB까지 확장 가능합니다.\n \n \n크레마 페블 가격과 예약판매 이벤트\n \n크레마 페블 본체의 가격은 21만9천원이며, 액세서리와 함께 파는 패키지도 있습니다.\n \n예스24는 크레마 페블 출시를 기념하여 예약판매 이벤트도 진행합니다. 예약 판매 기간인 9월 2일까지 예스24 홈페이지에서 크레마 페블을 구매할 경우 10명을 추첨해 YES상품권 5만원을 증정합니다.\n \n\n\n \n새로운 6인치 e-ink 리더 크레마 페블의 정식 출시는 9월 3일입니다.\n\n더 자세한 내용은 이곳에서 참고하시기 바랍니다.\n \n \n이 포스트에 포함된 링크를 통해 제품을 구매하는 경우 라지온에서 판매 수수료를 받을 수 있습니다.\n \n(출처 : 예스24)",
        "guid": "https://lazion.com/2513722",
        "categories": [
          "#작은PC/#태블릿#e북리더",
          "crema",
          "crema pebble",
          "e-ink",
          "ebook",
          "eBook Reader",
          "new",
          "Tablet",
          "yes24"
        ],
        "isoDate": "2024-08-27T09:45:01.000Z"
      }
    ]
  },
  {
    "name": "루리웹 - 루리웹 리뷰 게시판",
    "category": "게임",
    "posts": [
      {
        "creator": "샤말란의눈",
        "title": "[MULTI] ‘성검전설다움’을 아로새기는 여행, 비전스 오브 마나",
        "link": "https://bbs.ruliweb.com/news/board/11/read/2244",
        "pubDate": "Tue, 27 Aug 2024 10:19:16 +0900",
        "author": "샤말란의눈",
        "content": "<img width=\"236\" height=\"177\" src=\"https://i1.ruliweb.com/thumb/24/08/27/19191a184494c329e.jpg\">",
        "contentSnippet": "",
        "categories": [
          "리뷰"
        ],
        "isoDate": "2024-08-27T01:19:16.000Z"
      },
      {
        "creator": "｜RULIWEB｜",
        "title": "[게임툰] 하중 계산은 어려워, 월드 오브 구 2",
        "link": "https://bbs.ruliweb.com/news/board/11/read/2243",
        "pubDate": "Mon, 26 Aug 2024 20:04:42 +0900",
        "author": "｜RULIWEB｜",
        "content": "<img width=\"236\" height=\"177\" src=\"https://i3.ruliweb.com/thumb/24/08/26/1918e4c2d2151ad6b.jpg\">",
        "contentSnippet": "",
        "categories": [
          "게임툰"
        ],
        "isoDate": "2024-08-26T11:04:42.000Z"
      },
      {
        "creator": "샤말란의눈",
        "title": "[MULTI] 게임스컴 2024, 공식 방송 및 관련 기사 종합",
        "link": "https://bbs.ruliweb.com/news/board/11/read/2242",
        "pubDate": "Mon, 26 Aug 2024 08:12:38 +0900",
        "author": "샤말란의눈",
        "content": "<img width=\"236\" height=\"177\" src=\"https://i3.ruliweb.com/thumb/24/08/25/191884a1ee413b2a1.jpg\">",
        "contentSnippet": "",
        "categories": [
          "특집"
        ],
        "isoDate": "2024-08-25T23:12:38.000Z"
      }
    ]
  },
  {
    "name": "Reasontobe",
    "category": "개인",
    "posts": []
  },
  {
    "name": "자유로운 생활",
    "category": "개인",
    "posts": []
  },
  {
    "name": "에스티마의 인터넷이야기 EstimaStory.com",
    "category": "개인",
    "posts": []
  },
  {
    "name": "나긋한 개발 - 데비안 리눅스와 프로그램 언어",
    "category": "개인",
    "posts": []
  },
  {
    "name": "일상을 여행처럼...",
    "category": "개인",
    "posts": []
  },
  {
    "name": "khris'log",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Just hack'em",
    "category": "개인",
    "posts": []
  },
  {
    "name": "C++ Truths",
    "category": "개인",
    "posts": []
  },
  {
    "name": "jacking75",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Joel on Software",
    "category": "개인",
    "posts": []
  },
  {
    "name": "벤자민로그",
    "category": "개인",
    "posts": [
      {
        "title": "Ruby의 unless 쉽게 읽기",
        "link": "https://jeho.page/essay/2024/08/28/ruby-unless.html",
        "pubDate": "2024-08-28T02:30:00.000Z",
        "author": "김재호",
        "content": "<p>Ruby 에는 <code class=\"language-plaintext highlighter-rouge\">unless</code> 문법이 있습니다.<br />\n<code class=\"language-plaintext highlighter-rouge\">if !current_user</code>를 <code class=\"language-plaintext highlighter-rouge\">unless current_user</code>로 쓸 수 있습니다.</p>\n\n<p>간단한 조건에서는 좋은 것 같습니다.<br />\n자연어처럼 읽히거든요.</p>\n<div class=\"language-ruby highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">return</span> <span class=\"k\">unless</span> <span class=\"n\">user_logged_in</span> <span class=\"c1\"># 깔끔</span>\n</code></pre></div></div>\n\n<p>그런데 조건이 여러 개 있는 경우에는?</p>\n<div class=\"language-ruby highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">unless</span> <span class=\"n\">file</span><span class=\"p\">.</span><span class=\"nf\">exist?</span> <span class=\"o\">&amp;&amp;</span> <span class=\"n\">file</span><span class=\"p\">.</span><span class=\"nf\">owner</span> <span class=\"o\">==</span> <span class=\"n\">current_user</span> <span class=\"o\">&amp;&amp;</span> <span class=\"n\">permission</span> <span class=\"o\">!=</span> <span class=\"ss\">:readonly</span>\n  <span class=\"c1\"># do something</span>\n<span class=\"k\">end</span>\n</code></pre></div></div>\n\n<p>하, 머리가 팽팽 돕니다.<br />\n어떤 개새가 이렇게 코드를 써놨어?<br />\n<code class=\"language-plaintext highlighter-rouge\">unless</code> 로 복잡한 조건을 적은 코드를 볼 때마다 <code class=\"language-plaintext highlighter-rouge\">if</code> 로 고치곤 했습니다.</p>\n\n<p>어느 날, 같이 일하던 동생에게 흥미로운 조언을 들었습니다.</p>\n<blockquote>\n  <p><code class=\"language-plaintext highlighter-rouge\">unless</code>를 <code class=\"language-plaintext highlighter-rouge\">assert</code> 처럼 생각하고 읽으면 좋아요.</p>\n</blockquote>\n\n<p>아래 코드 처럼 생각하면 좋다는 것이었습니다.</p>\n<div class=\"language-ruby highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">assert</span><span class=\"p\">(</span><span class=\"n\">file</span><span class=\"p\">.</span><span class=\"nf\">exist?</span> <span class=\"o\">&amp;&amp;</span> <span class=\"n\">file</span><span class=\"p\">.</span><span class=\"nf\">owner</span> <span class=\"o\">==</span> <span class=\"n\">current_user</span> <span class=\"o\">&amp;&amp;</span> <span class=\"n\">permission</span> <span class=\"o\">!=</span> <span class=\"ss\">:readonly</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<p>즉, 반드시 파일이 존재해야 하고,<br />\n소유자는 현재 사용자이며,<br />\n읽기 전용이 아니어야 다음 코드로 진행할 수 있다.</p>\n\n<p>함수 시작부에 <code class=\"language-plaintext highlighter-rouge\">assert</code> 를 쓰듯이 <code class=\"language-plaintext highlighter-rouge\">unless</code>를 비슷한 용도로 쓰면 읽기가 오히려 편한 경우가 있다고.</p>\n\n<div class=\"language-ruby highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">raise</span> <span class=\"no\">FileNotExist</span> <span class=\"k\">unless</span> <span class=\"n\">file</span><span class=\"p\">.</span><span class=\"nf\">exist?</span>\n<span class=\"k\">raise</span> <span class=\"no\">AccessDenied</span> <span class=\"k\">unless</span> <span class=\"n\">file</span><span class=\"p\">.</span><span class=\"nf\">owner</span> <span class=\"o\">==</span> <span class=\"n\">current_user</span> <span class=\"o\">&amp;&amp;</span> <span class=\"n\">permission</span> <span class=\"o\">!=</span> <span class=\"ss\">:readonly</span>\n</code></pre></div></div>\n\n<p>오 뭐야? 좋은데?<br />\n낯설음을 뒤로하고 저는 이걸 받아들였습니다.<br />\n더 이상 <code class=\"language-plaintext highlighter-rouge\">unless</code>를 <code class=\"language-plaintext highlighter-rouge\">if</code> 로 고치지 않고 적절히 섞어 쓰게 됐습니다.</p>\n\n<p>오늘도 이런 <code class=\"language-plaintext highlighter-rouge\">unless</code> 코드를 쓰다가 조언해 준 동생 생각이 났습니다.<br />\n알려줘서 고맙다.😁</p>\n\n<p>P.S. 하지만… 알려줬던 동생은 이젠 기억도 못하는 것 같습니다.</p>\n\n<p><img src=\"/assets/img/unless_assert.png\" alt=\"얼간의 동생과의 카톡 대화\" /></p>\n\n<p><br />\n<em>함께 읽으면 좋은 글:</em></p>\n<ul>\n  <li><a href=\"/essay/2022/02/18/ruby.html\">Ruby는 프로그래머를 위한 선물</a></li>\n  <li><a href=\"/essay/2023/01/04/dont-say-ruby-is-slow.html\">루비가 느리다고?</a></li>\n</ul>",
        "contentSnippet": "Ruby 에는 unless 문법이 있습니다.\nif !current_user를 unless current_user로 쓸 수 있습니다.\n간단한 조건에서는 좋은 것 같습니다.\n\nreturn unless user_logged_in # 깔끔\n\n\n그런데 조건이 여러 개 있는 경우에는?\n\nunless file.exist? && file.owner == current_user && permission != :readonly\n  # do something\nend\n\n\n하, 머리가 팽팽 돕니다.\nunless 로 복잡한 조건을 적은 코드를 볼 때마다 if 로 고치곤 했습니다.\n어느 날, 같이 일하던 동생에게 흥미로운 조언을 들었습니다.\nunless를 assert 처럼 생각하고 읽으면 좋아요.\n아래 코드 처럼 생각하면 좋다는 것이었습니다.\n\nassert(file.exist? && file.owner == current_user && permission != :readonly)\n\n\n즉, 반드시 파일이 존재해야 하고,\n함수 시작부에 assert 를 쓰듯이 unless를 비슷한 용도로 쓰면 읽기가 오히려 편한 경우가 있다고.\n\nraise FileNotExist unless file.exist?\nraise AccessDenied unless file.owner == current_user && permission != :readonly\n\n\n오 뭐야? 좋은데?\nunless를 if 로 고치지 않고 적절히 섞어 쓰게 됐습니다.\n오늘도 이런 unless 코드를 쓰다가 조언해 준 동생 생각이 났습니다.\nP.S. 하지만… 알려줬던 동생은 이젠 기억도 못하는 것 같습니다.\n\n\n함께 읽으면 좋은 글:\nRuby는 프로그래머를 위한 선물\n루비가 느리다고?",
        "summary": "Ruby 에는 unless 문법이 있습니다. if !current_user를 unless current_user로 쓸 수 있습니다.",
        "id": "https://jeho.page/essay/2024/08/28/ruby-unless",
        "isoDate": "2024-08-28T02:30:00.000Z"
      },
      {
        "title": "매일 커밋에 실패했던 날",
        "link": "https://jeho.page/essay/2024/08/26/daily-commit-failed.html",
        "pubDate": "2024-08-26T05:00:00.000Z",
        "author": "김재호",
        "content": "<p>2020년, 코로나가 시작되고 루틴이라는 게 생기기 시작했습니다.<br />\n약속도 없고 방구석에만 있다 보니 자연스레 루틴이 잡혔습니다.</p>\n\n<p>아침 9시에 일어난다.<br />\n드르륵드르륵 커피를 간다.<br />\n커피잔을 들고 방에 들어가서 컴퓨터 앞에 앉는다.<br />\n코딩을 한다.</p>\n\n<p>매일 코딩을 했습니다.<br />\nGitHub 잔디도 빽빽이 잘 쌓였습니다.</p>\n\n<p>와 이거 멋진걸.<br />\n2021년에는 진짜로 잔디를 꽉 한 번 채워봐야겠다.</p>\n\n<p><img src=\"/assets/img/daily_coding.png\" alt=\"2021년 커밋 기록\" /><br />\n<em>2021년 GitHub</em></p>\n\n<p>하… 잘나가다가 실패해버렸습니다. 6/16일.</p>\n\n<p>6/16일은 가평에 놀러 갔던 날입니다.<br />\n호텔 방에 돌아왔더니 생각이 났습니다.<br />\n노느라 오늘 코딩을 못했네. 지금이라도 코딩해야지.<br />\n밤 10시쯤 집중해서 코딩하다가..<br />\n문득 시계를 보니 자정이 넘어버린 겁니다.</p>\n\n<blockquote>\n  <p>헉… 망했다. 아직 커밋 안 했는데.</p>\n</blockquote>\n\n<p>이렇게 무너진 건가? 끝난거야?<br />\n어찌나 허무하던지. 미리 커밋부터 정리해둘걸..</p>\n\n<p>아쉬워 하다가 이런 생각이 들었습니다.</p>\n<blockquote>\n  <p>잠깐. 난 잘못한 거 없잖아. 억울하다고.<br />\n놀다가 놓친 것도 아니고 진짜로 코딩하고 있었는걸.<br />\n시간을 돌려서 커밋을 집어넣을까?<br />\n이건 반칙이 아닌 것 같은데?</p>\n</blockquote>\n\n<p>1분쯤 고민했습니다.<br />\nGit 명령어를 찾아보기까지 했습니다.<br />\n하지만 그러지 않기로 했습니다.</p>\n\n<p>반칙이 아니긴 뭐가 아니야. 반칙 맞잖아.<br />\n그렇게 하는 순간 의미가 없어져 버려.<br />\n못한 건 못한 거지, 이게 뭐라고.</p>\n\n<p>마음이 편해졌습니다.<br />\n노력은 하되 실패한 날은 실패한 날대로 두자.</p>\n\n<p>이후로는 그다지 신경쓰며 하지 않았는데도 잔디는 여전히 푸릇푸릇합니다.<br />\n너무 신경 쓰며 할 때보다 오히려 지금이 더 좋습니다.</p>\n\n<p><img src=\"/assets/img/commits.png\" alt=\"최근 5년간 커밋\" /></p>\n\n<p><br />\n<em>함께 읽으면 좋은 글:</em></p>\n<ul>\n  <li><a href=\"/essay/2022/01/05/daily-coding.html\">매일매일 코딩하기</a></li>\n  <li><a href=\"/essay/2024/08/01/commit-count.html\">1년에 몇 개나 커밋하세요?</a></li>\n</ul>",
        "contentSnippet": "2020년, 코로나가 시작되고 루틴이라는 게 생기기 시작했습니다.\n아침 9시에 일어난다.\n매일 코딩을 했습니다.\n와 이거 멋진걸.\n\n2021년 GitHub\n하… 잘나가다가 실패해버렸습니다. 6/16일.\n6/16일은 가평에 놀러 갔던 날입니다.\n헉… 망했다. 아직 커밋 안 했는데.\n이렇게 무너진 건가? 끝난거야?\n아쉬워 하다가 이런 생각이 들었습니다.\n잠깐. 난 잘못한 거 없잖아. 억울하다고.\n1분쯤 고민했습니다.\n반칙이 아니긴 뭐가 아니야. 반칙 맞잖아.\n마음이 편해졌습니다.\n이후로는 그다지 신경쓰며 하지 않았는데도 잔디는 여전히 푸릇푸릇합니다.\n\n\n함께 읽으면 좋은 글:\n매일매일 코딩하기\n1년에 몇 개나 커밋하세요?",
        "summary": "2020년, 코로나가 시작되고 루틴이라는 게 생기기 시작했습니다. 약속도 없고 방구석에만 있다 보니 자연스레 루틴이 잡혔습니다.",
        "id": "https://jeho.page/essay/2024/08/26/daily-commit-failed",
        "isoDate": "2024-08-26T05:00:00.000Z"
      }
    ]
  },
  {
    "name": "악보쓰는 프로그래머",
    "category": "개인",
    "posts": []
  },
  {
    "name": "쭌안아빠",
    "category": "개인",
    "posts": []
  },
  {
    "name": "A Gangster World",
    "category": "개인",
    "posts": []
  },
  {
    "name": "요우의 내맘대로 블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "개발자스럽다",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "Against All Odds.",
    "category": "개인",
    "posts": []
  },
  {
    "name": "움직이는 게임서버",
    "category": "개인",
    "posts": []
  },
  {
    "name": "이상욱",
    "category": "개인",
    "posts": []
  },
  {
    "name": "임철재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "어쩐지 오늘은",
    "category": "개인",
    "posts": []
  },
  {
    "name": "oddpoet’s étude",
    "category": "개인",
    "posts": []
  },
  {
    "name": "0x00 - NULL",
    "category": "개인",
    "posts": []
  },
  {
    "name": "퇴근 후 서버다운",
    "category": "개인",
    "posts": []
  },
  {
    "name": "coolspeed",
    "category": "개인",
    "posts": []
  },
  {
    "name": "오늘도 끄적끄적",
    "category": "개인",
    "posts": []
  },
  {
    "name": "dx11 Vanica's Lifelog - 夢が夢で終わらないように",
    "category": "개인",
    "posts": []
  },
  {
    "name": "초코사랑",
    "category": "개인",
    "posts": []
  },
  {
    "name": "ZeroCho Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Hybrid's Notes",
    "category": "개인",
    "posts": []
  },
  {
    "name": "imays게임엔진개발자",
    "category": "개인",
    "posts": []
  },
  {
    "name": "RSS feed for hurinmon Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "기억보단 기록을",
    "category": "개인",
    "posts": [
      {
        "creator": "향로 (기억보단 기록을)",
        "title": "아무도 없어도 규칙을 지키는 사람",
        "link": "https://jojoldu.tistory.com/802",
        "pubDate": "Thu, 29 Aug 2024 00:45:39 +0900",
        "author": "향로 (기억보단 기록을)",
        "comments": "https://jojoldu.tistory.com/802#entry802comment",
        "content": "<p data-ke-size=\"size16\">늦은 시간에 퇴근을 하면 종종 신기한 경험을 하게 된다.<br />너무 늦은 시간에 퇴근을 하면 보통은 바로 회사 택시를 불러 타고 가곤 하는데, 그날은 머리도 식힐겸 조금은 걷다가 택시를 타려고 했다.</p>\n<p data-ke-size=\"size16\">저녁 11시가 넘는 늦은 시간.<br />주변에 아무도 없는, 지나가는 차도 없는 아주 조용한 횡단보도의 신호등을 기다리는 사람을 본 적이 있다.</p>\n<p data-ke-size=\"size16\"><b>지켜보는 사람 하나 없는 곳에서도 규칙을 지키는 사람</b>이 있다는 것에 걸음을 멈추고 한동안 지켜보게 되었다.<br />신호등의 불이 켜지고나서야 건너가는 모습을 보면서 멋지다는 생각을 했다.</p>\n<p data-ke-size=\"size16\">누군가에게는 융통성이 없는 것일 수 있다.<br />지나가는 차 하나 없어 사고날 일이 없고,<br />지켜보는 사람도 없으니 누군가 이상한 눈길로 볼 일도 없을 것이다.<br />그러면 굳이 기다리지 않고 지나가는게 늦은 시간 빨리 집에 갈 수 있는 융통성일 것이다.</p>\n<hr contenteditable=\"false\" data-ke-type=\"horizontalRule\" data-ke-style=\"style1\" />\n<p data-ke-size=\"size16\">아주 예전에 이경규씨가 진행한 \"양심냉장고\" 라는 프로그램이 있다.<br />1화의 양심 주제는 \"차량 정지선을 지키는 사람\" 을 찾는 것이였다.</p>\n<p data-ke-size=\"size16\">새벽 4시까지도 그 어느 차량도 지키지 않았다.<br />같이 진행하시는 분도 \"우리나라 사람은 법대로 살면 손해 본다고!\" 라고 외치는 와중에 <b>4시 13분이 되어서야 하나의 소형 차량이 아무도 없는 그 시간에 정지선을 지켰다</b>.</p>\n<p data-ke-size=\"size16\">알고보니 그 차량은 장애인 부부가 운전을 하고 있었고, 이경규씨는 \"왜 신호를 지키셨나요?\" 라는 질문을 한다.<br />\"아무도 지키지 않는데...\" 를 생략했지만 말이다.</p>\n<p data-ke-size=\"size16\">그리고 말을 제대로 하지 못하는 지체장애인 남편분께서 하신 답변은 \"내...가...늘...지켜요.\" 였다.</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><a href=\"https://cafe.daum.net/SoulDresser/FLTB/781737?svc=cafeapi\">무릎팍도사 - 양심냉장고 1화</a></li>\n</ul>\n<p data-ke-size=\"size16\">이 1화 방송이 나가고, 다음날 신문의 제목은 \"누가 장애인인가... 사실은 우리가 장애인!\" 이였다.</p>\n<hr contenteditable=\"false\" data-ke-type=\"horizontalRule\" data-ke-style=\"style1\" />\n<p data-ke-size=\"size16\">효율성, 융통성을 이야기하면서 눈에 보이는 것들의 이득을 취하게 된다.<br />그렇게 융통성을 이유로 \"<b>눈에 보이지 않는 어떤 무언가가 계속해서 마모되어오진 않았을까</b>\" 라는 생각을 하게 된다.<br />\"융통성\" 이란 말로 나는 얼마나 마모 되어왔을까?</p>\n<p data-ke-size=\"size16\">꼭 이런 점이 아니더라도,<br />나 스스로 약속한 것들이 있다.<br /><b>남이 보지 않아도,</b><br /><b>주변에서 알아봐주지 않아도&nbsp;</b><br /><b>나 스스로 한 약속이기에 지키는 그런 사람.</b><br /><br />그런 사람이 되고 싶다.</p>",
        "contentSnippet": "늦은 시간에 퇴근을 하면 종종 신기한 경험을 하게 된다.\n너무 늦은 시간에 퇴근을 하면 보통은 바로 회사 택시를 불러 타고 가곤 하는데, 그날은 머리도 식힐겸 조금은 걷다가 택시를 타려고 했다.\n저녁 11시가 넘는 늦은 시간.\n주변에 아무도 없는, 지나가는 차도 없는 아주 조용한 횡단보도의 신호등을 기다리는 사람을 본 적이 있다.\n지켜보는 사람 하나 없는 곳에서도 규칙을 지키는 사람이 있다는 것에 걸음을 멈추고 한동안 지켜보게 되었다.\n신호등의 불이 켜지고나서야 건너가는 모습을 보면서 멋지다는 생각을 했다.\n누군가에게는 융통성이 없는 것일 수 있다.\n지나가는 차 하나 없어 사고날 일이 없고,\n지켜보는 사람도 없으니 누군가 이상한 눈길로 볼 일도 없을 것이다.\n그러면 굳이 기다리지 않고 지나가는게 늦은 시간 빨리 집에 갈 수 있는 융통성일 것이다.\n아주 예전에 이경규씨가 진행한 \"양심냉장고\" 라는 프로그램이 있다.\n1화의 양심 주제는 \"차량 정지선을 지키는 사람\" 을 찾는 것이였다.\n새벽 4시까지도 그 어느 차량도 지키지 않았다.\n같이 진행하시는 분도 \"우리나라 사람은 법대로 살면 손해 본다고!\" 라고 외치는 와중에 4시 13분이 되어서야 하나의 소형 차량이 아무도 없는 그 시간에 정지선을 지켰다.\n알고보니 그 차량은 장애인 부부가 운전을 하고 있었고, 이경규씨는 \"왜 신호를 지키셨나요?\" 라는 질문을 한다.\n\"아무도 지키지 않는데...\" 를 생략했지만 말이다.\n그리고 말을 제대로 하지 못하는 지체장애인 남편분께서 하신 답변은 \"내...가...늘...지켜요.\" 였다.\n무릎팍도사 - 양심냉장고 1화\n이 1화 방송이 나가고, 다음날 신문의 제목은 \"누가 장애인인가... 사실은 우리가 장애인!\" 이였다.\n효율성, 융통성을 이야기하면서 눈에 보이는 것들의 이득을 취하게 된다.\n그렇게 융통성을 이유로 \"눈에 보이지 않는 어떤 무언가가 계속해서 마모되어오진 않았을까\" 라는 생각을 하게 된다.\n\"융통성\" 이란 말로 나는 얼마나 마모 되어왔을까?\n꼭 이런 점이 아니더라도,\n나 스스로 약속한 것들이 있다.\n남이 보지 않아도,\n주변에서 알아봐주지 않아도 \n나 스스로 한 약속이기에 지키는 그런 사람.\n그런 사람이 되고 싶다.",
        "guid": "https://jojoldu.tistory.com/802",
        "categories": [
          "생각정리",
          "규칙",
          "양심냉장고",
          "이경규"
        ],
        "isoDate": "2024-08-28T15:45:39.000Z"
      },
      {
        "creator": "향로 (기억보단 기록을)",
        "title": "1. TS를 만나고 - 디자인",
        "link": "https://jojoldu.tistory.com/801",
        "pubDate": "Mon, 26 Aug 2024 12:43:13 +0900",
        "author": "향로 (기억보단 기록을)",
        "comments": "https://jojoldu.tistory.com/801#entry801comment",
        "content": "<blockquote data-ke-style=\"style2\">\n<p data-ke-size=\"size16\">JVM 컨퍼런스가 있으면 \"Java 세상에서 살던 사람이 처음 TS 세계를 만나고 느낀 여러가지 차이점\" 을 발표하려고했는데, 시간도 너무 지났고, 발표 준비하기도 쉽지 않은 것 같아 블로그에 시리즈로 시작한다.<br />Java가 구린 언어다를 표현하기 위한 글이기 보다는 <a href=\"https://jojoldu.tistory.com/687\">전작(다른 언어로 성장하기)</a>과 마찬가지로 타 생태계를 통해 성장할 수 있음을 알리는 글이다</p>\n</blockquote>\n<p data-ke-size=\"size16\">자바를 처음 배울때 <code>getter/setter</code> 에 대한 이야기를 많이 들었다.<br />캡슐화 등의 장점을 들으면서 클래스 안에는 항상 무분별하게 <code>getter/setter</code> 를 생성했다.<br />(그때는 Lombok을 배우지 못했던 터라) IDE의 자동 생성 기능을 사용하면서 열심히 <code>getter/setter</code>를 생성했다.</p>\n<p data-ke-size=\"size16\">물론 예전부터 많은 분들은 무분별한 getter/setter를 사용하지말라는 글과 토론을 나누었다.</p>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><a href=\"https://www.yegor256.com/2014/09/16/getters-and-setters-are-evil.html\">2014.09.16 - getters-and-setters-are-evil</a></li>\n</ul>\n<p data-ke-size=\"size16\">다만, 내가 배울때 법칙?처럼 <code>getter/setter</code> 만든다로 배워서 그렇게 했다.<br />(지금이야 getter/setter를 쓰지 않고, <a href=\"https://martinfowler.com/bliki/TellDontAsk.html\">Tell, Don't Ask</a> 를 당연한 것처럼 다뤄지고 있지만 말이다.)</p>\n<p data-ke-size=\"size16\">여튼, 배우던 당시에는 무분별하게 <code>getter/setter</code>를 만들다보니 \"<b>어차피 단순한 data holders로 사용하는데 그냥 public으로 열면 안되나</b>\" 같은 생각이 들었다.</p>\n<p data-ke-size=\"size16\">이를 테면 다음과 같다.<br />매번 아래와 같이 생성하는데,</p>\n<pre class=\"cpp\"><code>public class Course {\n    private int price;\n\n    public int getPrice() {\n        return price;\n    }\n\n    public void setPrice(int price) {\n        this.price = price;\n    }\n}</code></pre>\n<p data-ke-size=\"size16\">아래와 같이 \"그냥 <code>public</code> 으로 열어두는 것과 무엇이 다르지?\" 라는 생각인 것이다.</p>\n<pre class=\"angelscript\"><code>public class Course {\n    public int price;\n}</code></pre>\n<p data-ke-size=\"size16\">어차피 값을 넣고 빼는 역할 밖에 없는데, 똑같지 않나? 라는 생각을 하곤 했다.</p>\n<p data-ke-size=\"size16\">그러다가 기능이 여러가지로 확장되는 상황을 만나니 접근자를 통해서 사용해야하는 것의 중요성을 알게 되었다.</p>\n<p data-ke-size=\"size16\">예를 들어 단순히 <code>price</code>에 값을 넣기만 하면 되는 로직에서 아래와 같이 <b>부가세 10%가 추가된 가격으로 반환해야한다</b>는 조건이 추가되니 단순히 <code>public</code> 으로 열어두는 것은 기존의 코드를 모두 바꿔야하는 문제가 생겼다.</p>\n<pre class=\"angelscript\"><code>public class Course {\n    ...\n\n    public int getPrice() {\n        return (int) (price * 1.1); // 부가세 10% 적용\n    }\n}\n\ncoursePrice = course.price; // &lt;&lt;&lt; 이렇게 호출되는 코드들이\ncoursePrice = course.getPrice(); // &lt;&lt;&lt; 이 코드로 모두 변경되어야만 했다.</code></pre>\n<p data-ke-size=\"size16\">접근자를 통해서 처음 구현해두지 않으니, 이런 경우가 발생했다.<br />그래서 \"<b>처음부터 접근자를 생성해두고, 이를 호출자들이 사용하도록 해야만 이런 문제를 겪지 않는 다</b>\" 는 것을 알게되었다.</p>\n<p data-ke-size=\"size16\">다만, 그럼 \"매번 이렇게 접근자를 생성해두고 코드를 작성해야하는 불편함과 귀찮음은 어쩔 수 없는 것인가\" 라는 생각은 계속 들었다.</p>\n<p data-ke-size=\"size16\">그러던 중, JavaScript/TypeScript 코드들을 보게 되었는데, 여기서는 <code>get/set</code> 접근자를 언어 레벨에서 지원했다.</p>\n<p data-ke-size=\"size16\">즉, <b>아래 2개 코드는 호출자 입장에서 동일한 방법으로 호출할 수 있다</b>.</p>\n<pre class=\"kotlin\"><code>// 1. 접근자 없이 public 필드\nclass Course {\n    public price: number;\n\n    constructor(price: number) {\n        this.price = price;\n    }\n}\n\n// 2. 접근자를 통한 접근\nclass Course {\n    private _price: number;\n\n    ...\n\n    get price(): number {\n        return this._price;\n    }\n}\n\nval price = course.price; // 1, 2 모두 동일한 코드로 호출</code></pre>\n<p data-ke-size=\"size16\">이건 코틀린도 동일한데, 코틀린 역시 <code>get/set</code> 접근자를 언어 레벨에서 지원한다.</p>\n<pre class=\"kotlin\"><code>// 1. 접근자 없이 public 필드\nclass Course(var price: Int)\n\n// 2. 접근자를 통한 접근\nclass Course(private var _price: Int) {\n    var price: Int\n        get() = (_price * 1.1).toInt() // 부가세 10% 적용\n}\n\nval coursePrice = course.price</code></pre>\n<p data-ke-size=\"size16\">(코틀린과 자바를 굳이 다른 생태계라고 구분하지는 않지만)<br />이런 개념이 요즘의 모던한 언어들에게는 기본적으로 내장되어있다.</p>\n<p data-ke-size=\"size16\">Java를 처음 배우는 입장에서는 \"public 필드를 사용하면 안되고, private 필드를 사용하고 이에 대한 접근자를 무조건 생성해야한다\" 를 배워야만 했다.<br />이걸 그나마 편하게 하기 위해 Lombok 라이브러리 등의 도입도 알아야만 한다.<br />만약 이 모든걸 무시하고 처음 배운 클래스에서 단순히 public 필드를 사용하면 이후에 큰 비용을 지불하게 된다.</p>\n<p data-ke-size=\"size16\">반면, <b>JS, TS, Kotlin 등의 언어를 배우는 입장에서는 이런 것이 고민 거리이자 배워야할 내용이 되지 않는다</b>.<br />public 필드를 사용하는 것과 접근자를 통한 접근이 모두 <b>동일한 인터페이스를 지원하기 때문에 내가 알게되는 지식이 늘어난다고해서 기존의 코드들이 전체 교체 될 일은 거의 없다</b>.<br />즉, 프로그래밍을 배우고 사용할때 중요한 고민거리가 하나 없어지게 되는 셈이다.</p>\n<p data-ke-size=\"size16\">이런 사례는 <b>언어 레벨에서 좋은 디자인 인터페이스를 지원한다</b> 라는 의미로 다가왔다.</p>\n<p data-ke-size=\"size16\">처음 클래스를 배울때 단순하게 public으로 모든 필드를 선언하고 코드를 작성하더라도 이후에 <b>접근자 (accessor) 개념을 배우고 클래스 코드를 변경하더라도 기존 호출자 코드에 변경이 필요하지 않는다</b>는 것을 초보 개발자들에게 알려줄 수 있는 셈이다.</p>\n<p data-ke-size=\"size16\">이걸로 변경에 유연한 디자인을 아주 쉽게 익히게 된다.<br />Lombok 같은 외부의 라이브러리를 도입해야한다거나,<br />접근자 (accessor) 에 대한 개념을 배운다던가 그런 언어 외적인 추가적인 학습없이 말이다.</p>\n<h2 data-ke-size=\"size26\">마무리</h2>\n<p data-ke-size=\"size16\">좋은 언어는 그 자체로 좋은 디자인을 배울 수 있어야 한다는 생각을 한다.<br />그런면에서 문법 설탕에 불과하다고 이야기하는 사람들도 있지만, 특정 프레임워크, 라이브러리의 도움으로 해결하는 문제들을 언어 레벨에서 직접 해결해주고 있다는 점이 참 좋았다.</p>\n<p data-ke-size=\"size16\">그리고 그런 개념들이 TS, Kotlin 등 Java가 아닌 다른 언어에서는 당연하게도 들어가 있다는 점이 섬세하게도 느껴졌다.<br />(물론 Kotlin과 Java 사이에 큰 차이가 있냐는 의견에도 동의하지만 말이다.)</p>\n<h2 data-ke-size=\"size26\">함께 보면 좋은 글</h2>\n<ul style=\"list-style-type: disc;\" data-ke-list-type=\"disc\">\n<li><a href=\"https://jojoldu.tistory.com/687\">다른 언어로 성장하기</a></li>\n</ul>",
        "contentSnippet": "JVM 컨퍼런스가 있으면 \"Java 세상에서 살던 사람이 처음 TS 세계를 만나고 느낀 여러가지 차이점\" 을 발표하려고했는데, 시간도 너무 지났고, 발표 준비하기도 쉽지 않은 것 같아 블로그에 시리즈로 시작한다.\nJava가 구린 언어다를 표현하기 위한 글이기 보다는 전작(다른 언어로 성장하기)과 마찬가지로 타 생태계를 통해 성장할 수 있음을 알리는 글이다\n자바를 처음 배울때 getter/setter 에 대한 이야기를 많이 들었다.\n캡슐화 등의 장점을 들으면서 클래스 안에는 항상 무분별하게 getter/setter 를 생성했다.\n(그때는 Lombok을 배우지 못했던 터라) IDE의 자동 생성 기능을 사용하면서 열심히 getter/setter를 생성했다.\n물론 예전부터 많은 분들은 무분별한 getter/setter를 사용하지말라는 글과 토론을 나누었다.\n2014.09.16 - getters-and-setters-are-evil\n다만, 내가 배울때 법칙?처럼 getter/setter 만든다로 배워서 그렇게 했다.\n(지금이야 getter/setter를 쓰지 않고, Tell, Don't Ask 를 당연한 것처럼 다뤄지고 있지만 말이다.)\n여튼, 배우던 당시에는 무분별하게 getter/setter를 만들다보니 \"어차피 단순한 data holders로 사용하는데 그냥 public으로 열면 안되나\" 같은 생각이 들었다.\n이를 테면 다음과 같다.\n매번 아래와 같이 생성하는데,\npublic class Course {\n    private int price;\n\n    public int getPrice() {\n        return price;\n    }\n\n    public void setPrice(int price) {\n        this.price = price;\n    }\n}\n아래와 같이 \"그냥 public 으로 열어두는 것과 무엇이 다르지?\" 라는 생각인 것이다.\npublic class Course {\n    public int price;\n}\n어차피 값을 넣고 빼는 역할 밖에 없는데, 똑같지 않나? 라는 생각을 하곤 했다.\n그러다가 기능이 여러가지로 확장되는 상황을 만나니 접근자를 통해서 사용해야하는 것의 중요성을 알게 되었다.\n예를 들어 단순히 price에 값을 넣기만 하면 되는 로직에서 아래와 같이 부가세 10%가 추가된 가격으로 반환해야한다는 조건이 추가되니 단순히 public 으로 열어두는 것은 기존의 코드를 모두 바꿔야하는 문제가 생겼다.\npublic class Course {\n    ...\n\n    public int getPrice() {\n        return (int) (price * 1.1); // 부가세 10% 적용\n    }\n}\n\ncoursePrice = course.price; // <<< 이렇게 호출되는 코드들이\ncoursePrice = course.getPrice(); // <<< 이 코드로 모두 변경되어야만 했다.\n접근자를 통해서 처음 구현해두지 않으니, 이런 경우가 발생했다.\n그래서 \"처음부터 접근자를 생성해두고, 이를 호출자들이 사용하도록 해야만 이런 문제를 겪지 않는 다\" 는 것을 알게되었다.\n다만, 그럼 \"매번 이렇게 접근자를 생성해두고 코드를 작성해야하는 불편함과 귀찮음은 어쩔 수 없는 것인가\" 라는 생각은 계속 들었다.\n그러던 중, JavaScript/TypeScript 코드들을 보게 되었는데, 여기서는 get/set 접근자를 언어 레벨에서 지원했다.\n즉, 아래 2개 코드는 호출자 입장에서 동일한 방법으로 호출할 수 있다.\n// 1. 접근자 없이 public 필드\nclass Course {\n    public price: number;\n\n    constructor(price: number) {\n        this.price = price;\n    }\n}\n\n// 2. 접근자를 통한 접근\nclass Course {\n    private _price: number;\n\n    ...\n\n    get price(): number {\n        return this._price;\n    }\n}\n\nval price = course.price; // 1, 2 모두 동일한 코드로 호출\n이건 코틀린도 동일한데, 코틀린 역시 get/set 접근자를 언어 레벨에서 지원한다.\n// 1. 접근자 없이 public 필드\nclass Course(var price: Int)\n\n// 2. 접근자를 통한 접근\nclass Course(private var _price: Int) {\n    var price: Int\n        get() = (_price * 1.1).toInt() // 부가세 10% 적용\n}\n\nval coursePrice = course.price\n(코틀린과 자바를 굳이 다른 생태계라고 구분하지는 않지만)\n이런 개념이 요즘의 모던한 언어들에게는 기본적으로 내장되어있다.\nJava를 처음 배우는 입장에서는 \"public 필드를 사용하면 안되고, private 필드를 사용하고 이에 대한 접근자를 무조건 생성해야한다\" 를 배워야만 했다.\n이걸 그나마 편하게 하기 위해 Lombok 라이브러리 등의 도입도 알아야만 한다.\n만약 이 모든걸 무시하고 처음 배운 클래스에서 단순히 public 필드를 사용하면 이후에 큰 비용을 지불하게 된다.\n반면, JS, TS, Kotlin 등의 언어를 배우는 입장에서는 이런 것이 고민 거리이자 배워야할 내용이 되지 않는다.\npublic 필드를 사용하는 것과 접근자를 통한 접근이 모두 동일한 인터페이스를 지원하기 때문에 내가 알게되는 지식이 늘어난다고해서 기존의 코드들이 전체 교체 될 일은 거의 없다.\n즉, 프로그래밍을 배우고 사용할때 중요한 고민거리가 하나 없어지게 되는 셈이다.\n이런 사례는 언어 레벨에서 좋은 디자인 인터페이스를 지원한다 라는 의미로 다가왔다.\n처음 클래스를 배울때 단순하게 public으로 모든 필드를 선언하고 코드를 작성하더라도 이후에 접근자 (accessor) 개념을 배우고 클래스 코드를 변경하더라도 기존 호출자 코드에 변경이 필요하지 않는다는 것을 초보 개발자들에게 알려줄 수 있는 셈이다.\n이걸로 변경에 유연한 디자인을 아주 쉽게 익히게 된다.\nLombok 같은 외부의 라이브러리를 도입해야한다거나,\n접근자 (accessor) 에 대한 개념을 배운다던가 그런 언어 외적인 추가적인 학습없이 말이다.\n마무리\n좋은 언어는 그 자체로 좋은 디자인을 배울 수 있어야 한다는 생각을 한다.\n그런면에서 문법 설탕에 불과하다고 이야기하는 사람들도 있지만, 특정 프레임워크, 라이브러리의 도움으로 해결하는 문제들을 언어 레벨에서 직접 해결해주고 있다는 점이 참 좋았다.\n그리고 그런 개념들이 TS, Kotlin 등 Java가 아닌 다른 언어에서는 당연하게도 들어가 있다는 점이 섬세하게도 느껴졌다.\n(물론 Kotlin과 Java 사이에 큰 차이가 있냐는 의견에도 동의하지만 말이다.)\n함께 보면 좋은 글\n다른 언어로 성장하기",
        "guid": "https://jojoldu.tistory.com/801",
        "categories": [
          "Architecture",
          "Java",
          "kotlin",
          "node.js",
          "typescript",
          "인터페이스",
          "자바",
          "코틀린",
          "타입스크립트"
        ],
        "isoDate": "2024-08-26T03:43:13.000Z"
      }
    ]
  },
  {
    "name": "WestwoodForever's Dev Log",
    "category": "개인",
    "posts": []
  },
  {
    "name": "허니몬(Honeymon)의 자바guru",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Wolf Loves Fox :: 일상",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Game Programmer Life",
    "category": "개인",
    "posts": []
  },
  {
    "name": "IT 프리랜서 이야기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "yuchi's dev",
    "category": "개인",
    "posts": [
      {
        "creator": "megayuchi",
        "title": "LightMap 구현전략",
        "link": "https://megayuchi.com/2024/08/22/lightmap-%ea%b5%ac%ed%98%84%ec%a0%84%eb%9e%b5/",
        "pubDate": "Thu, 22 Aug 2024 14:10:10 +0000",
        "content:encodedSnippet": "2024년 8월 22일 방송분입니다.",
        "dc:creator": "megayuchi",
        "comments": "https://megayuchi.com/2024/08/22/lightmap-%ea%b5%ac%ed%98%84%ec%a0%84%eb%9e%b5/#respond",
        "content": "2024년 8월 22일 방송분입니다.",
        "contentSnippet": "2024년 8월 22일 방송분입니다.",
        "guid": "http://megayuchi.com/?p=7047",
        "categories": [
          "Development",
          "Game Engine Development",
          "GPU"
        ],
        "isoDate": "2024-08-22T14:10:10.000Z"
      }
    ]
  },
  {
    "name": "만화로 나누는 자유/오픈소스 소프트웨어 이야기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "신현석(Hyeonseok Shin)",
    "category": "개인",
    "posts": [
      {
        "title": "Puppeteer로 웹페이지 열고 조작하기",
        "link": "https://hyeonseok.com/blog/916",
        "pubDate": "Sun, 25 Aug 2024 17:26:34 GMT",
        "content": "<p>화면 없이 웹 브라우징을 할 수 있는 제품으로 기억나는 최초는 <a href=\"https://phantomjs.org/\">팬텀(PhantomJS)</a>이다. 화면도 없이 브라우저를 열고 스크린 샷도 찍고 탐색하는 것이 엄청 신기했다. 지금은 개발이 중단되었다. 팬텀의 불편함을 극복해보고자 <a href=\"https://github.com/casperjs/casperjs\">캐스퍼(CasperJS)</a>도 나왔었는데 이것 역시 지금은 개발이 중단되었다. 아마도 <a href=\"https://pptr.dev/\">퍼피티어(Puppeteer)</a>와 <a href=\"https://playwright.dev/\">플레이라이트(Playwright)</a>같은 대안 프로젝트들이 구글과 마이크로소프트 같은 큰 회사에서 나와서 그렇게 된 것 같다.</p>\r\n\r\n<p>플레이라이트가 퍼피티어를 포크해서 나왔다고 하는데 두 프로젝트는 지향하는 바가 다르다. 퍼피티어가 헤드리스 크롬이나 <a href=\"https://hacks.mozilla.org/2024/08/puppeteer-support-for-firefox/\">파이어폭스</a>를 편하게 활용할 수 있게 고안되었다면 플레이라이트는 테스트를 위한 스위트에 가깝다. 나는 테스트를 돌리기 보다는 브라우저에서의 작업이 필요해서 퍼피티어를 선택했다.</p>\r\n\r\n<p>API는 상당히 직관적이어서 <a href=\"https://pptr.dev/#example\">예제</a>를 보면 바로 브라우저를 실행해서 테스트 해 볼 수 있다. 브라우저 열고 페이지 열고 등 코드만 봐도 바로 이해가 간다. 나는 처리해야 하는 작업이 많아서 별도 스크립트를 <a href=\"https://pptr.dev/api/puppeteer.page.__eval/\"><code>page.$$eval</code></a>을 사용해서 실행했다.</p>\r\n\r\n<pre><code>const imageCount = await page.$$eval(\"img\", (imgs) => imgs.length)</code></pre>\r\n\r\n<p>서로 다른 컨텍스트를 직관적으로 연결해줘서 아주 사용하기 편하다. 예전과 비교하면 정말 좋아졌다.</p>\r\n\r\n<p>페이지를 열려면 <code>const browser = await puppeteer.launch()</code>로 브라우저를 실행하고 <code>const page = await browser.newPage()</code>로 페이지를 열어서 사용하게 된다. 처음에는 매번 브라우저를 여는 시간도 아깝고 보통 브라우저 사용하듯이 하나 실행해 놓고 페이지만 열고 닫으며 사용해봤다. 이렇게 하니 평소에 브라우저가 메모리 많이 사용하고 있다고 욕하던 상황이 서버에서 벌어져서 서버가 죽었다. 프로세스가 열리는 것을 보니 페이지 하나만 열어도 크롬 관련 프로세스가 열개가 넘게 뜨고 <code>await page.close()</code>로 페이지를 닫아줘도 일부 프로세스들이 계속 남아 있었다. 결국 계속 쓸수록 메모리 점유가 늘어났다. <code>await browser.close()</code>로 실행된 브라우저를 닫아줘야 프로세스가 모두 정리가 된다.</p>\r\n\r\n<p><a href=\"https://accessibility.kr/\">accessibility.kr</a>에서 지금까지는 자바스크립트로 생성되는 페이지를 검사할 수 없었는데 이제 검사할 수 있는 사이트가 많이 늘어났다. 리다이렉션 처리도 많이 개선되었다.</p>",
        "contentSnippet": "화면 없이 웹 브라우징을 할 수 있는 제품으로 기억나는 최초는 팬텀(PhantomJS)이다. 화면도 없이 브라우저를 열고 스크린 샷도 찍고 탐색하는 것이 엄청 신기했다. 지금은 개발이 중단되었다. 팬텀의 불편함을 극복해보고자 캐스퍼(CasperJS)도 나왔었는데 이것 역시 지금은 개발이 중단되었다. 아마도 퍼피티어(Puppeteer)와 플레이라이트(Playwright)같은 대안 프로젝트들이 구글과 마이크로소프트 같은 큰 회사에서 나와서 그렇게 된 것 같다.\n\r\n\r\n플레이라이트가 퍼피티어를 포크해서 나왔다고 하는데 두 프로젝트는 지향하는 바가 다르다. 퍼피티어가 헤드리스 크롬이나 파이어폭스를 편하게 활용할 수 있게 고안되었다면 플레이라이트는 테스트를 위한 스위트에 가깝다. 나는 테스트를 돌리기 보다는 브라우저에서의 작업이 필요해서 퍼피티어를 선택했다.\n\r\n\r\nAPI는 상당히 직관적이어서 예제를 보면 바로 브라우저를 실행해서 테스트 해 볼 수 있다. 브라우저 열고 페이지 열고 등 코드만 봐도 바로 이해가 간다. 나는 처리해야 하는 작업이 많아서 별도 스크립트를 page.$$eval을 사용해서 실행했다.\n\r\n\r\nconst imageCount = await page.$$eval(\"img\", (imgs) => imgs.length)\n\r\n\r\n서로 다른 컨텍스트를 직관적으로 연결해줘서 아주 사용하기 편하다. 예전과 비교하면 정말 좋아졌다.\n\r\n\r\n페이지를 열려면 const browser = await puppeteer.launch()로 브라우저를 실행하고 const page = await browser.newPage()로 페이지를 열어서 사용하게 된다. 처음에는 매번 브라우저를 여는 시간도 아깝고 보통 브라우저 사용하듯이 하나 실행해 놓고 페이지만 열고 닫으며 사용해봤다. 이렇게 하니 평소에 브라우저가 메모리 많이 사용하고 있다고 욕하던 상황이 서버에서 벌어져서 서버가 죽었다. 프로세스가 열리는 것을 보니 페이지 하나만 열어도 크롬 관련 프로세스가 열개가 넘게 뜨고 await page.close()로 페이지를 닫아줘도 일부 프로세스들이 계속 남아 있었다. 결국 계속 쓸수록 메모리 점유가 늘어났다. await browser.close()로 실행된 브라우저를 닫아줘야 프로세스가 모두 정리가 된다.\n\r\n\r\naccessibility.kr에서 지금까지는 자바스크립트로 생성되는 페이지를 검사할 수 없었는데 이제 검사할 수 있는 사이트가 많이 늘어났다. 리다이렉션 처리도 많이 개선되었다.",
        "guid": "https://hyeonseok.com/blog/916",
        "isoDate": "2024-08-25T17:26:34.000Z"
      }
    ]
  },
  {
    "name": "한상곤 - Sigmadream",
    "category": "개인",
    "posts": []
  },
  {
    "name": "개발자 울이 노트",
    "category": "개인",
    "posts": []
  },
  {
    "name": "즐거운 개발자 :: 네이버  블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "황제펭귄의 게임개발이야기 [여기는 한국]",
    "category": "개인",
    "posts": []
  },
  {
    "name": "LINE ENGINEERING",
    "category": "기업",
    "posts": [
      {
        "title": "쿠버네티스 커스텀 리소스 정의하고 관리하기(feat.컨트롤러)",
        "link": "https://techblog.lycorp.co.jp/ko/define-and-manage-kubernetes-custom-resources-with-controller",
        "pubDate": "Wed, 28 Aug 2024 02:00:00 GMT",
        "content": "안녕하세요. Cloud DBS 팀에서 사내 클라우드인 Verda의 데이터베이스와 OpenSearch 서비스 개발을 맡고 있는 강인배, 문현균입니다. 이번 글에서는 저희 팀에서 쿠버...",
        "contentSnippet": "안녕하세요. Cloud DBS 팀에서 사내 클라우드인 Verda의 데이터베이스와 OpenSearch 서비스 개발을 맡고 있는 강인배, 문현균입니다. 이번 글에서는 저희 팀에서 쿠버...",
        "guid": "https://techblog.lycorp.co.jp/ko/define-and-manage-kubernetes-custom-resources-with-controller",
        "isoDate": "2024-08-28T02:00:00.000Z"
      },
      {
        "title": "Kafka와 ETL을 활용해 대용량 데이터 마이그레이션하기",
        "link": "https://techblog.lycorp.co.jp/ko/migrating-large-data-with-kafka-and-etl",
        "pubDate": "Mon, 26 Aug 2024 02:00:00 GMT",
        "content": "들어가며\n안녕하세요. LINE Plus에서 Global E-Commerce Platform 개발을 맡고 있는 장효택입니다. \nLINE Brand Catalog와 통합 커머스 검색 ...",
        "contentSnippet": "들어가며\n안녕하세요. LINE Plus에서 Global E-Commerce Platform 개발을 맡고 있는 장효택입니다. \nLINE Brand Catalog와 통합 커머스 검색 ...",
        "guid": "https://techblog.lycorp.co.jp/ko/migrating-large-data-with-kafka-and-etl",
        "isoDate": "2024-08-26T02:00:00.000Z"
      }
    ]
  },
  {
    "name": "뱅크샐러드 블로그",
    "category": "기업",
    "posts": [
      {
        "title": "이걸 진짜 만든다고요? 세상에 없던 게임, 샐러드게임 | 1편. BX",
        "link": "https://blog.banksalad.com/tech/banksalad-saladgame-1/",
        "pubDate": "Fri, 23 Aug 2024 00:00:00 GMT",
        "content": "“이걸 진짜 만든다고요? 🤯 ” 샐러드게임 탄생 배경 202…",
        "contentSnippet": "“이걸 진짜 만든다고요? 🤯 ” 샐러드게임 탄생 배경 202…",
        "guid": "https://blog.banksalad.com/tech/banksalad-saladgame-1/",
        "isoDate": "2024-08-23T00:00:00.000Z"
      },
      {
        "title": "이걸 진짜 만든다고요? 세상에 없던 게임, 샐러드게임 | 2편. UX",
        "link": "https://blog.banksalad.com/tech/banksalad-saladgame-2/",
        "pubDate": "Fri, 23 Aug 2024 00:00:00 GMT",
        "content": "…",
        "contentSnippet": "…",
        "guid": "https://blog.banksalad.com/tech/banksalad-saladgame-2/",
        "isoDate": "2024-08-23T00:00:00.000Z"
      }
    ]
  },
  {
    "name": "우아한형제들 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "TOAST Meetup",
    "category": "기업",
    "posts": [
      {
        "title": "NHN Cloud의 보안 가이드를 소개합니다",
        "link": "https://meetup.nhncloud.com/posts/384",
        "pubDate": "Sun, 25 Aug 2024 23:38:45 GMT",
        "content": "![NHN Cloud_meetup banner_security guides_202408_900.png](https://image.toast.com/aaaadh/real/2024/techblog/NHN%20Cloudmeetup%20bannersecurity%20guides202408900.png)\r\r\n\r\r\n## 들어가며\r\r\n\r\r\n다양한 산업 분야에서 클라우드 컴퓨팅의 확장성, 유연성, 데이터 분석 및 협업 기능을 활용하여 경쟁력을 강화하고, 서비스 운영 효율성을 높이고 있습니다. 이제 클라우드 전환은 선택이 아닌 필수가 된 것처럼 보입니다. 하지만 가상화 기술, 자원의 공유 및 인터넷을 통한 접속이라는 클라우드의 특성으로 인해 취약점 공격이나 악성 코드, DDoS와 같이 온프레미스 환경에서도 나타났던 보안 위협뿐만 아니라 기존에 없던 유형의 보안 위협도 다양하게 발생할 수 있다는 점에서 클라우드 보안에 대한 우려는 남아 있는 상태입니다.\r\r\n\r\r\n클라우드를 안전하게 이용하기 위해서는 온프레미스 인프라와 대비되는 클라우드의 구조와 보안 관리 방안을 정확히 이해하고, 클라우드 서비스 공급자와 이용자가 협력하여 각자의 책임 영역에서 적극적인 정보보호 활동을 수행할 수 있어야 합니다. 이를 [보안 책임 공유 모델(shared security responsibility, SSRM)](https://www.nhncloud.com/kr/security-center/ssrm)이라고 하는데요. 이 모델에서는 안전한 클라우드 서비스를 위해 클라우드 서비스 공급자와 이용자의 책임과 역할을 분류하고, 각자가 통제하는 요소를 보호하여 클라우드 전반의 안전하고 신뢰성 있는 보안을 확립합니다.\r\r\n\r\r\n보안 책임 공유 모델에서 클라우드 서비스 이용자는 가상 리소스와 데이터 보안을 위해 클라우드 및 보안 서비스를 활용해 보안을 고려한 서비스를 구성하고 관리할 책임을 갖습니다. 이를 위해서는 온프레미스 환경과 대비되는 클라우드 컴퓨팅 환경의 특성과 보안 전반에 대한 이해가 선행되어야 합니다. NHN Cloud는 보안을 위한 다양한 서비스와 기능을 제공할 뿐만 아니라, 다양한 보안 가이드를 제공하여 이용자가 데이터, 애플리케이션, 운영체제 단에서 직접 수행해야 하는 작업이나 반드시 숙지해야 할 사항 등을 안내합니다. 이를 통해 클라우드 서비스 이용자 책임 영역의 보안을 강화하고, 클라우드에 대한 위협과 대응 방안, 클라우드 영역별 보안, 정보보호 등에 대한 이해를 돕기 위한 노력을 지속하고 있습니다.\r\r\n\r\r\n이 글에서는 [NHN Cloud 보안 센터](https://www.nhncloud.com/kr/security-center)에서 제공하는 다양한 보안 가이드의 종류를 알아보고, 상황과 필요에 따라 적합한 문서를 활용하실 수 있도록 각 가이드에 대해 소개하고자 합니다.\r\r\n\r\r\n## NHN Cloud 보안 가이드의 종류와 소개\r\r\n\r\r\n![02_보안가이드메인.png](https://image.toast.com/aaaadh/real/2024/techblog/02uBCF4uC548uAC00uC774uB4DCuBA54uC778.png)\r\r\n\r\r\n### **NHN Cloud 보안 백서**\r\r\n\r\r\n고객이 클라우드 환경의 보안을 쉽게 이해할 수 있도록 돕고, NHN Cloud가 제공하는 보안 체계와 기술을 안내하기 위해 제작된 「NHN Cloud 보안 백서」는 NHN Cloud의 이름으로 발행된 첫 번째 백서입니다. 온프레미스 인프라와 대비되는 클라우드 컴퓨팅 환경의 특성과 그 특성에 기인한 보안 위협, 클라우드 보안 전반에 대해 안내하고, NHN Cloud가 고객의 클라우드 인프라와 서비스, 이용자의 개인정보와 데이터를 보호하기 위해 제공하는 다양한 보안 서비스와 기능을 소개합니다. 클라우드 서비스 이용 시 적절한 기술적 보안 대책을 마련하는 데 참고하거나 취약점 관리, 위협 탐지 및 대응, 인증, 컴플라이언스, 접근 통제, 데이터 보호 및 암호화 서비스를 이용한 안전한 서비스 인프라를 구성하는 데도 활용할 수 있을 것으로 기대합니다.\r\r\n\r\r\n [NHN Cloud 보안 백서 다운로드 신청 페이지 바로 가기](https://info.nhncloud.com/security-whitepaper.html?utm_source=nhncloud&utm_medium=home&utm_campaign=whitepapers&utm_content=security01&utm_term=cta)\r\r\n<br>\r\r\n### **NHN Cloud 개인정보보호 준수 가이드**\r\r\n\r\r\n개인정보는 살아 있는 개인에 대한 정보로 이름, 주민등록번호, 영상 등을 통해 개인을 식별할 수 있는 정보를 의미합니다. 하나의 정보만으로 개인을 식별할 수 없더라도, 다른 정보와 결합하여 개인을 특정할 수 있다면 개인정보로 간주합니다. 인터넷 기술이 발전하고, 소셜 네트워킹, 쇼핑, 금융 거래 등 일상의 거의 모든 행위가 디지털화됨에 따라 우리는 수많은 개인정보를 온라인에 저장하고 공유하는데요. 개인정보가 유출될 경우 신원 도용이나 금융 사기, 프라이버시 침해 등 심각한 문제를 초래할 수 있기 때문에 개인정보보호는 점점 더 중요한 이슈로 부각되고 있습니다.\r\r\n개인정보보호법은 이러한 개인정보의 처리 및 보호에 관한 사항을 정하여 개인의 자유와 권리를 보호하고, 개인의 존엄과 가치를 구현하기 위해 제정된 법입니다. NHN Cloud는 고객이 클라우드 환경에서 개인정보보호법을 준수할 수 있도록 다양한 서비스와 기능을 제공합니다. 「NHN Cloud 개인정보보호 준수 가이드」는 개인정보 및 개인정보보호법이 무엇인지 살펴보고, 개인정보의 안전성 확보조치 기준에 대해 알아본 뒤 NHN Cloud가 제공하는 보안 서비스를 이용해 개인정보를 보호할 수 있는 방안을 안내합니다.\r\r\n\r\r\n [NHN  Cloud 개인정보보호 준수 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN%20Cloud_Personal_Information_Compliance_Guide_2024.pdf)\r\r\n<br>\r\r\n### **NHN Cloud 콘솔 보안 가이드**\r\r\n\r\r\n콘솔은 클라우드 서비스 이용자가 클라우드 서비스를 직접 생성, 사용, 관리, 삭제할 수 있는 접점이며 환경입니다. 따라서 클라우드 콘솔 보안은 매우 중요하며, 이용자가 직접 구성하고 관리한다는 점에서 클라우드 및 네트워크 환경과 보안 요소, 보안 관리 방법, 그리고 클라우드에서 제공하는 보안 기능에 대한 정확한 이해가 선행되어야 합니다.\r\r\n많은 CSP에서 클라우드 보안을 위한 다양한 기능들을 선보이고 있고, NHN Cloud 역시 Network Firewall, Security Groups, Network ACL, DDoS Guard, Security Monitoring, Webshell Threat Detector, Security Advisor, NHN Bastion 등의 서비스를 제공하며 안전한 클라우드 컴퓨팅 환경을 제공하기 위해 노력하고 있습니다. 「NHN Cloud 콘솔 보안 가이드」는 이와 같은 NHN Cloud의 다양한 서비스에서 제공하는 보안 기능을 이용하여 클라우드 환경에서 애플리케이션 및 서비스를 안전하게 구성할 수 있도록 권장되는 보안 설정들에 대해 안내합니다. 계정 관리, 네트워크 보안, 서버 보안, 스토리지 보안, 데이터베이스 보안, 데이터 보호 및 키 관리, 로깅 및 모니터링과 같은 8개 영역에서 점검하고 관리할 수 있는 항목들을 제시하고 설명하며, 클라우드 서비스 이용자는 이를 기준으로 NHN Cloud 콘솔에서 직접 보안 기능을 설정하고 점검할 수 있습니다.\r\r\n\r\r\n [NHN Cloud 콘솔 보안 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN_Cloud_Console_Security_Guide_2023.pdf)\r\r\n<br>\r\r\n### **NHN Cloud 네트워크 아키텍처 보안 가이드**\r\r\n\r\r\n「NHN Cloud 네트워크 아키텍처 보안 가이드」는 NHN Cloud에서 제공하는 다양한 네트워크 기능과 네트워크 보안 서비스를 이용하여 클라우드 환경에서 가상 인프라 및 서비스를 안전하게 구성하고 제공할 수 있는 네트워크 보안 아키텍처를 설명합니다.\r\r\n클라우드 환경의 네트워크 구성 시 따라야 할 네트워크 분리, 접근 제어, 위협 대응 관련 컴플라이언스를 정리하여 안내하며, 이에 기반한 네트워크 아키텍처 보안 고려 사항 및 보안 설계 기준을 제시합니다. 또한 클라우드 환경에서 기업이나 공공 시스템을 설계할 때 참고할 수 있는 네트워크 보안 아키텍처를 사례별로 제시하기 때문에 환경에 따라 다를 수 있는 네트워크 분리 기준과 방법, 분리된 구간의 연결과 접근 통제 방식 등을 좀더 구체적이고 다양한 관점에서 살펴보고 이해할 수 있습니다.\r\r\n\r\r\n [NHN Cloud 네트워크 아키텍처 보안 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN_Cloud_Network_Architecture_Security_Guide_2024.pdf)\r\r\n<br>\r\r\n### **NHN Cloud 랜섬웨어 대응 가이드**\r\r\n\r\r\n랜섬웨어(ransomware)란 사용자의 컴퓨터 시스템을 장악하거나 데이터를 암호화하여 정상적으로 사용하지 못하게 만든 뒤 암호 키 또는 해제 방법을 알려주는 대가로 금전을 요구하는 악성 코드입니다. 최근 몇 년간 랜섬웨어가 사이버 공격의 주요 형태로 떠오르며 기업과 공공 기관, 개인 모두에게 심각한 피해를 입히고 있습니다.\r\r\n「NHN Cloud 랜섬웨어 대응 가이드」는 일반 악성코드와 대비되는 랜섬웨어의 특성과 공격 방식 및 유형을 살펴보고, 랜섬웨어가 사용하는 기술이 무엇인지 설명합니다. 그리고 랜섬웨어에 대비하여 클라우드 서비스 이용자가 취할 수 있는 효과적인 대응 전략과 방안을 제시합니다.\r\r\n\r\r\n [NHN Cloud 랜섬웨어 대응 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN_Cloud_Ransomware_Response_Guide_2024.pdf)\r\r\n<br>\r\r\n### **NHN Cloud 컨테이너 보안 가이드**\r\r\n\r\r\n컨테이너란 데스크톱, 온프레미스, 클라우드 등 어떤 환경에도 구애 받지 않고 실행할 수 있도록 애플리케이션 코드를 해당 라이브러리 및 종속 항목과 함께 패키징한 소프트웨어 실행 유닛입니다. NHN Cloud는 이러한 컨테이너 환경에서 다양한 애플리케이션을 개발하고 운영할 수 있도록 NHN Kubernetes Service(NKS), NHN Container Registry(NCR), NHN Container Service(NCS)를 제공합니다. 「NHN Cloud 컨테이너 보안 가이드」는 NHN Cloud의 컨테이너 서비스에서 제공하는 보안 기능과 Kubernetes 보안 방법을 소개합니다. 본 가이드를 통해 컨테이너의 개념과 주요 구성 요소, 컨테이너 환경의 보안 위협과 보호 대상, 컨테이너 환경과 컨테이너를 기반으로 서비스하는 워크로드를 보호하는 방법 등을 이해할 수 있습니다.\r\r\n\r\r\n [NHN Cloud 컨테이너 보안 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN%20Cloud_Container%20Security%20Guide_2024.pdf)\r\r\n<br>\r\r\n## 마치며\r\r\n\r\r\n여기까지 NHN Cloud의 다양한 보안 가이드의 종류와 활용에 대해 알아보았는데요. 2024년 8월 기준, NHN Cloud는 위와 같이 「NHN Cloud 보안 백서」, 「NHN Cloud 개인정보보호 준수 가이드」, 「NHN Cloud 콘솔 보안 가이드」, 「NHN Cloud 네트워크 아키텍처 가이드」, 「NHN Cloud 랜섬웨어 대응 가이드」, 「NHN Cloud 컨테이너 보안 가이드」를 제공하고 있습니다. 보안 가이드는 NHN Cloud를 활용해 서비스를 개발하고 운영하시는 아키텍트 및 보안 업무 담당자, NHN Cloud를 도입하고 구축하고자 하는 기업의 IT 담당자, 그리고 클라우드와 클라우드 환경 전반의 보안에 대해 알고 싶은 분들 모두가 자유롭게 이용하실 수 있으며, [NHN Cloud 홈페이지 > 보안 센터 > 보안 가이드](https://www.nhncloud.com/kr/security-center/guide) 페이지에서 확인하실 수 있습니다.\r\r\n\r\r\n긴 글을 읽어 주셔서 감사드리며, 앞으로도 NHN Cloud에서 제공하는 다양한 보안 가이드에 대해 많은 기대와 관심을 부탁드립니다. \r\r\n\r\r\n[![NHN Cloud_meetup banner_footer_black_202408_900.png](https://image.toast.com/aaaadh/real/2024/techblog/NHN%20Cloudmeetup%20bannerfooterblack202408900.png)](https://www.nhncloud.com/kr)",
        "contentSnippet": "![NHN Cloud_meetup banner_security guides_202408_900.png](https://image.toast.com/aaaadh/real/2024/techblog/NHN%20Cloudmeetup%20bannersecurity%20guides202408900.png)\r\r\n\r\r\n## 들어가며\r\r\n\r\r\n다양한 산업 분야에서 클라우드 컴퓨팅의 확장성, 유연성, 데이터 분석 및 협업 기능을 활용하여 경쟁력을 강화하고, 서비스 운영 효율성을 높이고 있습니다. 이제 클라우드 전환은 선택이 아닌 필수가 된 것처럼 보입니다. 하지만 가상화 기술, 자원의 공유 및 인터넷을 통한 접속이라는 클라우드의 특성으로 인해 취약점 공격이나 악성 코드, DDoS와 같이 온프레미스 환경에서도 나타났던 보안 위협뿐만 아니라 기존에 없던 유형의 보안 위협도 다양하게 발생할 수 있다는 점에서 클라우드 보안에 대한 우려는 남아 있는 상태입니다.\r\r\n\r\r\n클라우드를 안전하게 이용하기 위해서는 온프레미스 인프라와 대비되는 클라우드의 구조와 보안 관리 방안을 정확히 이해하고, 클라우드 서비스 공급자와 이용자가 협력하여 각자의 책임 영역에서 적극적인 정보보호 활동을 수행할 수 있어야 합니다. 이를 [보안 책임 공유 모델(shared security responsibility, SSRM)](https://www.nhncloud.com/kr/security-center/ssrm)이라고 하는데요. 이 모델에서는 안전한 클라우드 서비스를 위해 클라우드 서비스 공급자와 이용자의 책임과 역할을 분류하고, 각자가 통제하는 요소를 보호하여 클라우드 전반의 안전하고 신뢰성 있는 보안을 확립합니다.\r\r\n\r\r\n보안 책임 공유 모델에서 클라우드 서비스 이용자는 가상 리소스와 데이터 보안을 위해 클라우드 및 보안 서비스를 활용해 보안을 고려한 서비스를 구성하고 관리할 책임을 갖습니다. 이를 위해서는 온프레미스 환경과 대비되는 클라우드 컴퓨팅 환경의 특성과 보안 전반에 대한 이해가 선행되어야 합니다. NHN Cloud는 보안을 위한 다양한 서비스와 기능을 제공할 뿐만 아니라, 다양한 보안 가이드를 제공하여 이용자가 데이터, 애플리케이션, 운영체제 단에서 직접 수행해야 하는 작업이나 반드시 숙지해야 할 사항 등을 안내합니다. 이를 통해 클라우드 서비스 이용자 책임 영역의 보안을 강화하고, 클라우드에 대한 위협과 대응 방안, 클라우드 영역별 보안, 정보보호 등에 대한 이해를 돕기 위한 노력을 지속하고 있습니다.\r\r\n\r\r\n이 글에서는 [NHN Cloud 보안 센터](https://www.nhncloud.com/kr/security-center)에서 제공하는 다양한 보안 가이드의 종류를 알아보고, 상황과 필요에 따라 적합한 문서를 활용하실 수 있도록 각 가이드에 대해 소개하고자 합니다.\r\r\n\r\r\n## NHN Cloud 보안 가이드의 종류와 소개\r\r\n\r\r\n![02_보안가이드메인.png](https://image.toast.com/aaaadh/real/2024/techblog/02uBCF4uC548uAC00uC774uB4DCuBA54uC778.png)\r\r\n\r\r\n### **NHN Cloud 보안 백서**\r\r\n\r\r\n고객이 클라우드 환경의 보안을 쉽게 이해할 수 있도록 돕고, NHN Cloud가 제공하는 보안 체계와 기술을 안내하기 위해 제작된 「NHN Cloud 보안 백서」는 NHN Cloud의 이름으로 발행된 첫 번째 백서입니다. 온프레미스 인프라와 대비되는 클라우드 컴퓨팅 환경의 특성과 그 특성에 기인한 보안 위협, 클라우드 보안 전반에 대해 안내하고, NHN Cloud가 고객의 클라우드 인프라와 서비스, 이용자의 개인정보와 데이터를 보호하기 위해 제공하는 다양한 보안 서비스와 기능을 소개합니다. 클라우드 서비스 이용 시 적절한 기술적 보안 대책을 마련하는 데 참고하거나 취약점 관리, 위협 탐지 및 대응, 인증, 컴플라이언스, 접근 통제, 데이터 보호 및 암호화 서비스를 이용한 안전한 서비스 인프라를 구성하는 데도 활용할 수 있을 것으로 기대합니다.\r\r\n\r\r\n [NHN Cloud 보안 백서 다운로드 신청 페이지 바로 가기](https://info.nhncloud.com/security-whitepaper.html?utm_source=nhncloud&utm_medium=home&utm_campaign=whitepapers&utm_content=security01&utm_term=cta)\r\r\n\r\r\n### **NHN Cloud 개인정보보호 준수 가이드**\r\r\n\r\r\n개인정보는 살아 있는 개인에 대한 정보로 이름, 주민등록번호, 영상 등을 통해 개인을 식별할 수 있는 정보를 의미합니다. 하나의 정보만으로 개인을 식별할 수 없더라도, 다른 정보와 결합하여 개인을 특정할 수 있다면 개인정보로 간주합니다. 인터넷 기술이 발전하고, 소셜 네트워킹, 쇼핑, 금융 거래 등 일상의 거의 모든 행위가 디지털화됨에 따라 우리는 수많은 개인정보를 온라인에 저장하고 공유하는데요. 개인정보가 유출될 경우 신원 도용이나 금융 사기, 프라이버시 침해 등 심각한 문제를 초래할 수 있기 때문에 개인정보보호는 점점 더 중요한 이슈로 부각되고 있습니다.\r\r\n개인정보보호법은 이러한 개인정보의 처리 및 보호에 관한 사항을 정하여 개인의 자유와 권리를 보호하고, 개인의 존엄과 가치를 구현하기 위해 제정된 법입니다. NHN Cloud는 고객이 클라우드 환경에서 개인정보보호법을 준수할 수 있도록 다양한 서비스와 기능을 제공합니다. 「NHN Cloud 개인정보보호 준수 가이드」는 개인정보 및 개인정보보호법이 무엇인지 살펴보고, 개인정보의 안전성 확보조치 기준에 대해 알아본 뒤 NHN Cloud가 제공하는 보안 서비스를 이용해 개인정보를 보호할 수 있는 방안을 안내합니다.\r\r\n\r\r\n [NHN  Cloud 개인정보보호 준수 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN%20Cloud_Personal_Information_Compliance_Guide_2024.pdf)\r\r\n\r\r\n### **NHN Cloud 콘솔 보안 가이드**\r\r\n\r\r\n콘솔은 클라우드 서비스 이용자가 클라우드 서비스를 직접 생성, 사용, 관리, 삭제할 수 있는 접점이며 환경입니다. 따라서 클라우드 콘솔 보안은 매우 중요하며, 이용자가 직접 구성하고 관리한다는 점에서 클라우드 및 네트워크 환경과 보안 요소, 보안 관리 방법, 그리고 클라우드에서 제공하는 보안 기능에 대한 정확한 이해가 선행되어야 합니다.\r\r\n많은 CSP에서 클라우드 보안을 위한 다양한 기능들을 선보이고 있고, NHN Cloud 역시 Network Firewall, Security Groups, Network ACL, DDoS Guard, Security Monitoring, Webshell Threat Detector, Security Advisor, NHN Bastion 등의 서비스를 제공하며 안전한 클라우드 컴퓨팅 환경을 제공하기 위해 노력하고 있습니다. 「NHN Cloud 콘솔 보안 가이드」는 이와 같은 NHN Cloud의 다양한 서비스에서 제공하는 보안 기능을 이용하여 클라우드 환경에서 애플리케이션 및 서비스를 안전하게 구성할 수 있도록 권장되는 보안 설정들에 대해 안내합니다. 계정 관리, 네트워크 보안, 서버 보안, 스토리지 보안, 데이터베이스 보안, 데이터 보호 및 키 관리, 로깅 및 모니터링과 같은 8개 영역에서 점검하고 관리할 수 있는 항목들을 제시하고 설명하며, 클라우드 서비스 이용자는 이를 기준으로 NHN Cloud 콘솔에서 직접 보안 기능을 설정하고 점검할 수 있습니다.\r\r\n\r\r\n [NHN Cloud 콘솔 보안 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN_Cloud_Console_Security_Guide_2023.pdf)\r\r\n\r\r\n### **NHN Cloud 네트워크 아키텍처 보안 가이드**\r\r\n\r\r\n「NHN Cloud 네트워크 아키텍처 보안 가이드」는 NHN Cloud에서 제공하는 다양한 네트워크 기능과 네트워크 보안 서비스를 이용하여 클라우드 환경에서 가상 인프라 및 서비스를 안전하게 구성하고 제공할 수 있는 네트워크 보안 아키텍처를 설명합니다.\r\r\n클라우드 환경의 네트워크 구성 시 따라야 할 네트워크 분리, 접근 제어, 위협 대응 관련 컴플라이언스를 정리하여 안내하며, 이에 기반한 네트워크 아키텍처 보안 고려 사항 및 보안 설계 기준을 제시합니다. 또한 클라우드 환경에서 기업이나 공공 시스템을 설계할 때 참고할 수 있는 네트워크 보안 아키텍처를 사례별로 제시하기 때문에 환경에 따라 다를 수 있는 네트워크 분리 기준과 방법, 분리된 구간의 연결과 접근 통제 방식 등을 좀더 구체적이고 다양한 관점에서 살펴보고 이해할 수 있습니다.\r\r\n\r\r\n [NHN Cloud 네트워크 아키텍처 보안 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN_Cloud_Network_Architecture_Security_Guide_2024.pdf)\r\r\n\r\r\n### **NHN Cloud 랜섬웨어 대응 가이드**\r\r\n\r\r\n랜섬웨어(ransomware)란 사용자의 컴퓨터 시스템을 장악하거나 데이터를 암호화하여 정상적으로 사용하지 못하게 만든 뒤 암호 키 또는 해제 방법을 알려주는 대가로 금전을 요구하는 악성 코드입니다. 최근 몇 년간 랜섬웨어가 사이버 공격의 주요 형태로 떠오르며 기업과 공공 기관, 개인 모두에게 심각한 피해를 입히고 있습니다.\r\r\n「NHN Cloud 랜섬웨어 대응 가이드」는 일반 악성코드와 대비되는 랜섬웨어의 특성과 공격 방식 및 유형을 살펴보고, 랜섬웨어가 사용하는 기술이 무엇인지 설명합니다. 그리고 랜섬웨어에 대비하여 클라우드 서비스 이용자가 취할 수 있는 효과적인 대응 전략과 방안을 제시합니다.\r\r\n\r\r\n [NHN Cloud 랜섬웨어 대응 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN_Cloud_Ransomware_Response_Guide_2024.pdf)\r\r\n\r\r\n### **NHN Cloud 컨테이너 보안 가이드**\r\r\n\r\r\n컨테이너란 데스크톱, 온프레미스, 클라우드 등 어떤 환경에도 구애 받지 않고 실행할 수 있도록 애플리케이션 코드를 해당 라이브러리 및 종속 항목과 함께 패키징한 소프트웨어 실행 유닛입니다. NHN Cloud는 이러한 컨테이너 환경에서 다양한 애플리케이션을 개발하고 운영할 수 있도록 NHN Kubernetes Service(NKS), NHN Container Registry(NCR), NHN Container Service(NCS)를 제공합니다. 「NHN Cloud 컨테이너 보안 가이드」는 NHN Cloud의 컨테이너 서비스에서 제공하는 보안 기능과 Kubernetes 보안 방법을 소개합니다. 본 가이드를 통해 컨테이너의 개념과 주요 구성 요소, 컨테이너 환경의 보안 위협과 보호 대상, 컨테이너 환경과 컨테이너를 기반으로 서비스하는 워크로드를 보호하는 방법 등을 이해할 수 있습니다.\r\r\n\r\r\n [NHN Cloud 컨테이너 보안 가이드 다운로드 바로 가기](https://www.nhncloud.com/resources/download/NHN%20Cloud_Container%20Security%20Guide_2024.pdf)\r\r\n\r\r\n## 마치며\r\r\n\r\r\n여기까지 NHN Cloud의 다양한 보안 가이드의 종류와 활용에 대해 알아보았는데요. 2024년 8월 기준, NHN Cloud는 위와 같이 「NHN Cloud 보안 백서」, 「NHN Cloud 개인정보보호 준수 가이드」, 「NHN Cloud 콘솔 보안 가이드」, 「NHN Cloud 네트워크 아키텍처 가이드」, 「NHN Cloud 랜섬웨어 대응 가이드」, 「NHN Cloud 컨테이너 보안 가이드」를 제공하고 있습니다. 보안 가이드는 NHN Cloud를 활용해 서비스를 개발하고 운영하시는 아키텍트 및 보안 업무 담당자, NHN Cloud를 도입하고 구축하고자 하는 기업의 IT 담당자, 그리고 클라우드와 클라우드 환경 전반의 보안에 대해 알고 싶은 분들 모두가 자유롭게 이용하실 수 있으며, [NHN Cloud 홈페이지 > 보안 센터 > 보안 가이드](https://www.nhncloud.com/kr/security-center/guide) 페이지에서 확인하실 수 있습니다.\r\r\n\r\r\n긴 글을 읽어 주셔서 감사드리며, 앞으로도 NHN Cloud에서 제공하는 다양한 보안 가이드에 대해 많은 기대와 관심을 부탁드립니다. \r\r\n\r\r\n[![NHN Cloud_meetup banner_footer_black_202408_900.png](https://image.toast.com/aaaadh/real/2024/techblog/NHN%20Cloudmeetup%20bannerfooterblack202408900.png)](https://www.nhncloud.com/kr)",
        "isoDate": "2024-08-25T23:38:45.000Z"
      }
    ]
  },
  {
    "name": "ZUM 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "SK Planet",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Spoqa tech blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "팀 왈도 번역팀",
    "category": "게임",
    "posts": []
  },
  {
    "name": "이한",
    "category": "개인",
    "posts": []
  },
  {
    "name": "근원님",
    "category": "개인",
    "posts": []
  },
  {
    "name": "황의윤",
    "category": "개인",
    "posts": []
  },
  {
    "name": "호돌맨",
    "category": "개인",
    "posts": []
  },
  {
    "name": "박우빈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "문다영",
    "category": "개인",
    "posts": []
  },
  {
    "name": "유수민",
    "category": "개인",
    "posts": []
  },
  {
    "name": "안건주",
    "category": "개인",
    "posts": []
  },
  {
    "name": "손현호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "STARTUP BIBLE",
    "category": "개인",
    "posts": [
      {
        "creator": "Kihong Bae",
        "title": "자신감에 대해서 – part 1",
        "link": "https://www.thestartupbible.com/2024/08/on-confidence-part-1.html",
        "pubDate": "Wed, 28 Aug 2024 21:33:00 +0000",
        "content:encodedSnippet": "요새 나는 한국보단 해외 투자자들을 훨씬 더 많이 만나서 이들에게 돈을 받기 위해 노력하고 있다. 남에게 돈 받는 건 항상 어려운 일이고, 특히나 요새 같이 이자율이 높고 경제 상황이 좋지 않은 불경기엔 펀딩이 더욱더 힘들어 진다.(VC들의 펀딩이 이렇게 어렵다 보니, 우리 같은 VC에게 투자받아야 하는 창업가들의 펀딩은 더 어렵고 시간이 오래 걸릴 수밖에 없다).\n하지만, 몇 년 전과 비교해 보면 좋아진 점도 있는데, 그건 바로 한국이라는 나라의 이미지와 한국의 벤처 시장이 과거와는 비교할 수 없을 정도로 좋아졌다는 것이다. 전에는 잠재 LP들에게 왜 스트롱 같이 한국에 투자하는 VC에 출자해야 하는지 설명하는 데만 상당히 많은 시간을 할애했고, 이 설명의 기간이 어떤 경우엔 수년이 걸렸다. 하지만, 이젠 대부분의 투자자들이 한국이라는 시장에 대한 의문이나 의심은 없을 정도로 한국의 벤처생태계가 그동안 많은 발전을 했다.\n내가 잠재 LP들에게 최근에 가장 많이 받은 질문은 어떻게 이렇게 상대적으로 짧은 기간 안에 한국의 스타트업 시장이 좋아졌냐인데, 이 질문에 대한 짧은 답변은 아주 간단하게 그냥 한국 창업가들의 수준이 매우 높아졌다는 것이다. 그럼, 왜 한국 창업가들의 수준이 이렇게 좋아졌을까? 여기에 대한 여러 가지 이론, 생각, 그리고 각자의 경험이 있지만, 내가 딱 한 가지만 강조하자면, 그건 ‘자신감’이라고 생각한다.\n창업가의 자신감은 정말 중요하다. 내가 하는 사업에 대한 확신과 자신감, 내가 만들고 있는 제품에 대한 자신감, 그리고 나 스스로에 대한 자신감은 안 되는 사업도 되게 하고, 못 받던 펀딩도 받게 한다. 평소에 잘 안되던 것들이 자신감과 이로 인한 파급 효과로 인해서 하나씩 만들어지는 걸 경험하는 순간, 잠재의식 속에서는 더 큰 자신감이 무의식적으로 생기고, 이건 결국엔 성공의 확률을 높일 수밖에 없다. 창업가들이 이렇게 자신감으로 무장되면, 기업가치 300억 원의 회사를 만들겠다던 목표가 1,000억 원이 된다. 그리고 이 목표가 계속 커져서 결국엔 10조 원짜리 데카콘까지 바라볼 수 있게 만드는 게 창업가의 자신감이다.\n비공식적인 기록이지만, 한국에는 유니콘 기업이 22개나 있다. 작은 나라치곤 엄청나게 많은 유니콘이다. 이런 사실도 한국 창업가들에겐 큰 자신감을 준다. 이렇게 작은 나라에서 기업가치가 1조 원 이상인 비상장 회사가 이렇게 많다는 점, 이 중 몇 개의 기업은 본인이 개인적으로 아는 창업가들이 만들었는데, 그들도 그냥 나랑 비슷한 평범한 사람이라는 점, 그래서 어쩌면, 그리고 언젠가는 나도 수천억 원의 펀딩을 받고 유니콘 기업을 만들 수 있다는 생각만으로도 이 창업가의 자신감은 하늘을 찌를 수 있다.\n쿠팡의 뉴욕증권거래소 상장은 한국 창업가들의 마음에 큰불을 질렀다. 한국 시장만을 상대로 이커머스 사업을 하는 회사가 미국에서 IPO를 했고, 지금은 좀 내려갔지만, 한때는 기업가치가 100조 원에 육박했다는 사실은 한국 창업가들에게 엄청난 자신감을 준 큰 사건이었다. 그동안 항상 한국 시장이 작고, 한국 기업이 미국에서 상장하는 건 불가능하다는 부정적인 이야기만 들었고, 한국인들도 항상 곧 망할 거라고 확신했던 쿠팡이라는 회사를, 김범석이라는 창업가가 이런 비관론자들에게 마치 fuck you를 날리듯 보기 좋게 성공시켰다.\n배달의민족 엑싯도 한국 창업가들에게 큰 자신감을 줬다. 국내에서 학교를 다녔고, 국내에서만 일 한 경험이 있는 순수 토종 창업가 김봉진 대표가 만든 한국의 스타트업이 수조 원의 기업가치에 외국 회사에 인수됐을 때, 많은 한국의 창업가들이 “아, 유니콘은 외국에서 공부한 엄친아들만 만들 수 있는 줄 알았는데, 그게 아니구나. 나도 더 열심히 하면 배달의민족보다 훨씬 더 큰 회사를 만들 수도 있겠다.”와 비슷한 생각을 하면서 할 수 있다는 아주 큰 자신감이 생겼을 것이다.\n그리고 이 자신감은 창업가들에게만 해당하는 건 아니다. 이들에게 투자했던 VC들에도 해당한다. 스타트업에 투자해서 큰돈을 버는 건 외국 VC에만 해당하는 먼 나라 이야기라고 생각했던 국내 VC들도 투자한 회사들이 유니콘이 되고, 이들이 엑싯했을 때 엄청나게 큰돈을 벌면서, 앞으로 더욱더 많은 유니콘 회사를 발굴해서 투자할 수 있다는 자신감이 생겼다. 이 자신감으로 이들은 더 큰 펀드를 만들고, 더 큰 펀드로 더 많은 좋은 회사에 투자하고 있다.\n이런 자신감들이 처음에는 작게 생기다가, 시간이 지나면서 여기저기서 생기고, 이게 계속 쌓이면서 어느 순간 폭발적으로 커지는데, 이럴 때 대단한 일들이 벌어진다.\n지금이 바로 한국 스타트업 생태계에서 이런 일들이 일어나고 있다고 나는 생각하고, 앞으로 한국의 스타트업 시장은 더욱더 좋아질 거라고 확신한다.\nPart 2에서도 자신감 관련 이야기를 조금 더 해보고 싶다.",
        "dc:creator": "Kihong Bae",
        "comments": "https://www.thestartupbible.com/2024/08/on-confidence-part-1.html#respond",
        "content": "요새 나는 한국보단 해외 투자자들을 훨씬 더 많이 만나서 이들에게 돈을 받기 위해 노력하고 있다. 남에게 돈 받는 건 항상 어려운 일이고, 특히나 요새 같이 이자율이 높고 경제 상황이 좋지 않은 불경기엔 펀딩이 더욱더 힘들어 진다.(VC들의 펀딩이 이렇게 어렵다 보니, 우리 같은 VC에게 투자받아야 하는 창업가들의 펀딩은 더 어렵고 시간이 오래 걸릴 수밖에 없다). 하지만,(...)",
        "contentSnippet": "요새 나는 한국보단 해외 투자자들을 훨씬 더 많이 만나서 이들에게 돈을 받기 위해 노력하고 있다. 남에게 돈 받는 건 항상 어려운 일이고, 특히나 요새 같이 이자율이 높고 경제 상황이 좋지 않은 불경기엔 펀딩이 더욱더 힘들어 진다.(VC들의 펀딩이 이렇게 어렵다 보니, 우리 같은 VC에게 투자받아야 하는 창업가들의 펀딩은 더 어렵고 시간이 오래 걸릴 수밖에 없다). 하지만,(...)",
        "guid": "https://www.thestartupbible.com/?p=9192",
        "categories": [
          "Uncategorized",
          "confidence",
          "FoundersAtWork",
          "fundraising",
          "korea",
          "Strong",
          "vc"
        ],
        "isoDate": "2024-08-28T21:33:00.000Z"
      },
      {
        "creator": "Kihong Bae",
        "title": "회사는 놀이터가 아니다",
        "link": "https://www.thestartupbible.com/2024/08/your-company-is-not-your-playground.html",
        "pubDate": "Sun, 25 Aug 2024 21:38:00 +0000",
        "content:encodedSnippet": "내가 약 5개월 전에 쓴 글 ‘개발자도 회사의 조직원이다’가 최근에 여기저기서 공유가 많이 된 것 같다. 뭐, 이곳은 내 개인적인 블로그라서 남 눈치 안 보고 그냥 내 생각을 끄적거리는데, 사람들이 세상을 보는 관점도 다르고, 특정 주제에 대한 생각도 달라서 그런지 많은 분들이 여러 가지 의견을 댓글로 남겨줬다.\n댓글, 댓글의 대댓글, 그리고 여기에 대한 주인장의 댓글을 모두 합치면 50개가 넘는 코멘트가 있다. 이 중, 그래도 이성적이고 논리적인 이야기가 가능한 분위기의 댓글에는 내가 최대한 진정성 있는 코멘트를 남겼는데, 그냥 개싸움이 될 것 같은 분위기의 댓글은 건드리지 않았다. 대신, 그런 코멘트에 대해서는 이번 포스팅을 통해서 아주 간략하게 내 생각을 종합적으로 다시 한번 공유하고 싶다.\n일단, 이 글에 이렇게 격한 반응을 해주신 걸 보니, 한국에도 돈을 많이 벌고 싶어 하고, 성공에 목마른 개발자들이 많은 것 같아서 너무 다행이다. 이런 분들이 더 많아져야지 스타트업도 잘 되고, 경쟁력 있는 회사들이 많이 나온다고 생각한다.\n한 가지 사과하고 명확하게 하고 싶은 건, 내가 개발자들을 공격하려는 의도로 이전 글을 쓴 건 아니라는 점이다. 기획자이든 마케터이든 개발자이든, 모든 직원은 회사의 조직원인데 굳이 개발자를 꼭 집어서 글을 썼던 이유는 내가 아는 대부분의 조직에선 제품을 만들고 판매해서 돈을 버는 핵심 업무를 하는 그룹 군에서 돈을 버는 기능에 가장 관심이 적은 조직이 개발 조직이기 때문이다. 물론, 이건 개인적인 관점이다.\n몇 개의 댓글을 읽어보면, 회사가 잘 돼 봤자 사장만 돈 버는데 내가 굳이 열심히 할 필요가 없다는 내용이다. 특히나 회사의 지분도 없는데. 이런 분들은 내 블로그에서 불평하지 말고, 소속된 회사의 사장과 이런 이야기를 하는 걸 권장한다. 회사에 돈을 벌어 주는 일을 열심히 하는 직원에게 스톡옵션 또는, 그 어떤 보상도 하지 않는 사장이라면 굳이 이런 회사에 계속 다닐 필욘 없을 것 같다. 그냥 다른 곳으로 가면 된다. 만약 본인이 열심히 일하지 않거나 실력이 없어서 보상받을 수준이 안되면 그냥 불평하지 말고 그 회사 계속 다니면 된다. 어쨌든 이런 불평을 하면서도 계속 그 회사에 다니고 있다면, 본인 자신의 실력을 의심해 봐야 한다.\n개발자로서 기술적 모험이 제한된다면 굳이 스타트업에 갈 필요가 없다고 한 분도 있다. 이런 의견에 대한 내 생각 두 가지를 공유한다. 일단 본인이 기술적 모험을 하고 싶다면, 그리고 이 모험이 회사의 비즈니스 방향과 크게 상관없다면(=돈을 벌 수 있는 기술이 아니라면) 이걸 허락하는 다른 곳으로 가면 된다. 그런데 돈 버는 거와 상관없는 기술적 모험을 허락하는 내가 아는 곳들은 학교 아니면 연구소다. 회사는 아닐 것이다. 또 다른 생각은, 스타트업을 포함한 모든 회사는 개발자들이 기술적인 모험을 하는 놀이터가 아니다. 남의 돈으로 빨리 돈을 벌어서 압축적인 성장을 해야 하는 조직이다. 회사는 돈 받고 그냥 하루 종일 놀다 퇴근하는 곳이 아니다.\n또한, 회사라는 조직은 분명히 회사라는 집단의 목표가 있고, 이를 달성해야 하지만, 어떤 분들이 주장하는 개인적인 발전도 동시에 균형 있게 가져가야 한다. 나도 이건 동의한다. 하지만, 우선순위를 매기자면 무조건 회사의 목표가 먼저이고, 이게 어느 정도 된 후에 회사의 목표를 같이 만드는 개인의 발전에 신경 써줄 수 있다. 회사의 목표는 무조건 돈 버는 게 돼야 하고, 여기에 먼저 동참할 수 없다면 개발자든 마케터든 회사에겐 부채가 되고, 부채는 가장 먼저 제거해야 하는 짐이 될 수밖에 없다.\n그리고 어떤 분들의 댓글을 보고 나는 정말로 이 사람들이 일하는 회사가 어딘지 궁금해지기도 했다. 그 회사 동료들이 너무 불쌍해서…\n이 글 밑에 분명히 멋진 댓글도 많이 달릴 거지만, 거지 같은 댓글도 많이 올라올 것이다. 그 수준과 정도를 종합적으로 판단해서, 필요하면 또 한 번 내 의견을 공유하는 포스팅을 올릴 계획이다. 그런데 키보드 뒤에서 인신공격적인 코멘트를 달거나, 너무 멍청한 코멘트를 다는 분들은 익명이 아니라 실명을 밝혀주시면 오히려 더 건설적인 이야기가 가능하지 않을까 싶다.",
        "dc:creator": "Kihong Bae",
        "comments": "https://www.thestartupbible.com/2024/08/your-company-is-not-your-playground.html#comments",
        "content": "내가 약 5개월 전에 쓴 글 ‘개발자도 회사의 조직원이다’가 최근에 여기저기서 공유가 많이 된 것 같다. 뭐, 이곳은 내 개인적인 블로그라서 남 눈치 안 보고 그냥 내 생각을 끄적거리는데, 사람들이 세상을 보는 관점도 다르고, 특정 주제에 대한 생각도 달라서 그런지 많은 분들이 여러 가지 의견을 댓글로 남겨줬다. 댓글, 댓글의 대댓글, 그리고 여기에 대한 주인장의 댓글을(...)",
        "contentSnippet": "내가 약 5개월 전에 쓴 글 ‘개발자도 회사의 조직원이다’가 최근에 여기저기서 공유가 많이 된 것 같다. 뭐, 이곳은 내 개인적인 블로그라서 남 눈치 안 보고 그냥 내 생각을 끄적거리는데, 사람들이 세상을 보는 관점도 다르고, 특정 주제에 대한 생각도 달라서 그런지 많은 분들이 여러 가지 의견을 댓글로 남겨줬다. 댓글, 댓글의 대댓글, 그리고 여기에 대한 주인장의 댓글을(...)",
        "guid": "https://www.thestartupbible.com/?p=9188",
        "categories": [
          "Uncategorized",
          "korea",
          "people",
          "technology",
          "vc",
          "스타트업 바이블 QA"
        ],
        "isoDate": "2024-08-25T21:38:00.000Z"
      }
    ]
  },
  {
    "name": "Build a Great Product",
    "category": "개인",
    "posts": []
  },
  {
    "name": "지금 써보러 갑니다",
    "category": "개인",
    "posts": []
  },
  {
    "name": "매거진 입맛",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "요즘 IT",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "쿠팡 엔지니어링",
    "category": "기업",
    "posts": []
  },
  {
    "name": "지마켓 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "리멤버 (드라마 앤 컴퍼니)",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Kimchi hill",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "Hudi.blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "토스",
    "category": "기업",
    "posts": [
      {
        "title": "토스, 개발자 컨퍼런스 ‘슬래시24’ 참가 신청 시작",
        "link": "https://blog.toss.im/article/slash24-conference",
        "pubDate": "Mon, 26 Aug 2024 04:56:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-uswsmm{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;margin:24px 0 8px;padding:0;list-style:none;}.css-uswsmm ul,.css-uswsmm ol{margin:16px 0 0;}.css-uswsmm>li{margin-bottom:16px;padding-left:24px;}.css-uswsmm>li:last-of-type{margin-bottom:0;}.css-uswsmm>li>span{position:relative;}.css-uswsmm>li>span>:first-child::before{content:'•';font-weight:500;color:var(--adaptiveGrey800);position:absolute;left:-24px;}\n.css-1hwiibq{font-size:17px;line-height:1.6;word-break:keep-all;letter-spacing:0em;font-weight:400;color:var(--adaptiveGrey800);}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}.css-q3ktjb{white-space:pre-wrap;font-weight:bold;}9월 12일(목) 코엑스에서 역대 최초 오프라인 개최… 개발자 45명 연사로 나서\n총 1,500명 추첨 통해 선발, 참가 접수 9월 2일까지\n.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n.css-1kxrhf3{white-space:pre-wrap;}토스가 개발자 컨퍼런스 '슬래시24(SLASH24, 이하 슬래시)'의 참가 신청을 시작했다.\n슬래시는 토스 커뮤니티의 개발자들이 기술적인 성취와 러닝을 공유하는 자리다. 4회째를 맞은 올해의 주제는 ‘No Limit: 풀지 못할 문제는 없다'로, 끊임없이 도전하고 시도하며 끝내 문제를 해결해 내는 토스의 개발 문화를 반영했다.\n이번 행사는 오프라인에서 최초로 개최된다는 점에서 특별하다. 9월 12일(목) 오전 10시부터 오후 4시까지 코엑스 그랜드볼룸에서 열린다. 토스, 토스뱅크, 토스증권, 토스페이먼츠, 토스플레이스 등 5개 법인에서 총 45명의 개발자들이 연사로 나선다.\n첫 번째 순서로는 토스의 테크놀로지 총괄 이형석 CTO가 발표를 진행한다. 이형석 CTO는 핀테크 업계의 지형을 바꿔온 토스가 혁신을 이루기 위해 겪은 도전과 실패를 이야기한다. 또한, 그 과정에서 토스 엔지니어들이 견지하는 태도와 이를 가능하게 하는 조직문화를 공유할 예정이다.\n메인 세션은 안드로이드(Android), 데이터(Data), 데브옵스(DevOps), 프론트엔드(Frontend), 인프라(Infra), Node.js, 파이썬(Python), 서버(Server), QA 등 총 9개 직군의 29개 발표로 구성했다. 스페셜 세션으로는 ‘빠르게 성장하고 싶은 주니어 개발자를 위한 소프트 스킬’, ‘팀에 위닝 멘탈리티를 불어넣는 리더십 스킬’, 토스뱅크와 토스증권의 CTO가 전하는 ‘미래의 CTO에게’ 등 3개 발표와 파트너사인 아마존웹서비스(AWS), 노션(Notion), 세일즈포스(Salesforce)의 발표가 준비돼 있다.\n다양한 부대 행사도 함께 마련했다. 슬래시 연사와 세션에 대한 질의나 기술적 고민을 나눌 수 있는 ‘데브챗(DevChat) 존’, 다양한 프로그램에서 4개 이상의 스탬프를 모으면 참여할 수 있는 ‘럭키드로우 존', 직무 상담이나 이력서에 대한 피드백을 받을 수 있는 ‘리크루팅 존'을 운영한다. 더불어 AWS, 베스핀글로벌, 노션, 세일즈포스, 시디즈 등 특별 협업 부스도 별도로 운영된다.\n이형석 CTO는 “이번 슬래시는 ‘No Limit’이라는 주제에 맞게 기술적 도전에 대한 토스 엔지니어들의 열정과 도전 정신을 함께 나눌 수 있는 자리가 될 것”이라며 “처음으로 오프라인에서 행사를 진행하는 만큼, 참가자들이 네트워킹을 통해 서로의 경험을 공유하고 더 큰 임팩트를 얻으실 수 있기를 기대한다”라고 전했다.\n참가 신청은 9월 2일(월) 23시 59분까지 토스 슬래시24 홈페이지에서 할 수 있다. 참가 신청을 완료하면 행사 종료 후 세션 발표 자료를 이메일로 제공한다. 신청한 인원 중 추첨을 통해 총 1,500명을 현장에 초대하며, 선정된 참가자에게는 9월 4일(수)에 개별적으로 안내를 진행할 예정이다.",
        "content": "“No Limit: 풀지 못할 문제는 없다”",
        "contentSnippet": "“No Limit: 풀지 못할 문제는 없다”",
        "guid": "https://blog.toss.im/article/slash24-conference",
        "isoDate": "2024-08-26T04:56:00.000Z"
      },
      {
        "title": "TDF, 우리 모두의 은퇴 준비 필수품",
        "link": "https://blog.toss.im/article/retirement-plans-05",
        "pubDate": "Fri, 23 Aug 2024 06:20:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-nv7vyi{margin:24px 0 8px;padding:16px 40px 32px;border-radius:16px;background-color:var(--adaptiveGrey100);}.css-123co55{font-size:19px;letter-spacing:0em;line-height:1.6;margin:24px 0 0;font-weight:400;color:var(--adaptiveGrey900);background-color:transparent;}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}.css-q3ktjb{white-space:pre-wrap;font-weight:bold;}이 글에서 알 수 있는 것들\n.css-uswsmm{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;margin:24px 0 8px;padding:0;list-style:none;}.css-uswsmm ul,.css-uswsmm ol{margin:16px 0 0;}.css-uswsmm>li{margin-bottom:16px;padding-left:24px;}.css-uswsmm>li:last-of-type{margin-bottom:0;}.css-uswsmm>li>span{position:relative;}.css-uswsmm>li>span>:first-child::before{content:'•';font-weight:500;color:var(--adaptiveGrey800);position:absolute;left:-24px;}\n.css-1hwiibq{font-size:17px;line-height:1.6;word-break:keep-all;letter-spacing:0em;font-weight:400;color:var(--adaptiveGrey800);}\n.css-1kxrhf3{white-space:pre-wrap;}퇴직연금에서 중요한 비중을 차지하는 TDF 개념 이해하기\n내가 계획하는 은퇴시점에 따라 TDF 고르는 기준 알아보기\n\n.css-1feg9au{font-size:24px;letter-spacing:0em;line-height:1.6;font-weight:bold;color:var(--adaptiveGrey900);margin:24px 0 4px;}\n은퇴 준비를 위한 자산 배분 이해하기\n.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n은퇴 준비를 하겠다고 마음 먹었을 때 가장 중요한 첫 번째는 적은 금액이라도 퇴직연금 계좌에 꾸준하게 모으는 것, 두 번째는 자산 배분이다. 만약 이미 은퇴에 가까운 나이라면 힘들게 모은 자산을 안전하게 관리하면서 가져가야 한다. 위험자산보다는 안전자산의 비중을 높여 너무 무리하지 않는 선에서 투자와 관리를 지속하는 것이다. 시장 분위기가 좋을 때 높은 수익을 가져다주는 위험자산은 언제나 유혹적이지만, 최근처럼 한국 주식이 급락해버리면 열심히 모아온 은퇴 자금이 위태로워지고 만다.\n반대로 아직 은퇴까지 20~30년의 시간이 있다면 위험자산을 이용해 적극적인 투자가 가능하다. 물론 자신이 얼마나 위험을 감수할 수 있는 성향인지 이해하고 이를 바탕으로 위험자산의 비중을 조정해야 하지만, 일반적으로 20대와 30대일 때는 전체 자산에서 위험자산의 비중이 중년기보다 많아도 된다고 말한다. 일시적으로 시장이 급락할지라도 은퇴 시점까지 회복할 시간이 충분히 있기 때문이다.\n모두에게 공평하게 적용되는 한 가지는 바로 시간이다. 시간이 흘러 나이를 먹으면서 모두가 점차 은퇴 시점에 가까워진다. .css-16cuouw{white-space:pre-wrap;color:var(--adaptiveGrey800);background-color:#3fd59936;}그렇게 은퇴 시점에 가까워지면 자산 배분도 그에 맞게 변경되어야 한다. 30대의 주식과 채권 배분 비율과 50대의 비율은 당연히 다를 수밖에 없다. 시기에 따라 적절히 주식과 채권 상품을 사고 팔며 비율을 조정해야 한다는 뜻이다. 여기까지 이야기하면 대부분의 사람들이 이런 반응을 보인다.\n“먹고 살기도 바쁜데 어떻게 매번 그걸 체크하고 바꾸나요?”\n“배분 비율을 어떻게 바꿔야 하죠? 기준이 있나요?”\n투자에 관심이 많고 늘 시간을 들일 수 있는 사람이라면 충분히 가능한 일이다. 하지만 대다수의 사람들에게 ‘주기적으로 리밸런싱하고, 위험자산과 안전자산의 비율을 조정하라'는 건 사실 지속 가능하지 않은 방법이다. 그래서 등장한 은퇴용 투자상품이 바로 TDF(Target Date Fund)이다.\n은퇴를 위해 태어났어요, TDF\nTDF는 은퇴 시점을 정하고 투자하면 펀드를 운용하는 회사가 해당 시점에 맞춰 자산 배분을 알아서 관리해주는 상품이다. 따라서 보통 TDF 상품에는 숫자가 들어 있는데, 이 숫자가 바로 은퇴 연도를 뜻한다. 예를 들어 내가 은퇴하고자 하는 시점이 2040년이라면 상품명에 2040이 들어간 TDF를 고르면 된다.(이 숫자를 빈티지(vintage)라고 부른다.) 보통 TDF에 들어 있는 숫자는 2030, 2035, 2040처럼 5년 단위로 설정된다. 따라서 자신의 은퇴 희망 시점과 가장 가까운 숫자를 선택하면 된다.\n문제는 많은 사람들이 TDF에 대한 이해가 부족한 채 무분별하게 투자하고 있다는 데 있다. 실제로 IRP 계좌를 보유한 한 30대를 인터뷰해보니 TDF2035 상품과 TDF2050 상품을 모두 보유하고 있었다. 자신의 은퇴 희망 시점을 고려한 것이 아니라 상품을 권유한 판매자의 말대로 여러 TDF를 구매한 탓이었다. TDF가 만들어진 취지와 맞지 않을 뿐더러, 투자자가 투자 상품에 대해 이해하지 못한 채 돈을 넣어버린 잘못된 투자였다.\n한국 시장에서 TDF 상품이 수면 위로 떠오른 이유는 바로 디폴트옵션(사전지정운용제도) 때문이다. 4화 ‘.css-iynyr0{white-space:pre-wrap;cursor:pointer;color:var(--adaptiveGrey600);-webkit-text-decoration:underline!important;text-decoration:underline!important;}자꾸 디폴트옵션 설정하라고 알림이 와요’에서 말했듯이 오랫동안 저조했던 퇴직연금의 수익률을 개선하기 위해 등장한 디폴트옵션에는 TDF 상품이 매우 많이 포진되어 있다. 심지어 주식 배분이 80% 가까이 올라가도 TDF는 안전자산으로 분류되는 특징이 있다. 시간 흐름에 따라 은퇴 시점에 가까워질수록 안전자산 비중이 높아지면서 변화하기 때문이다. 은퇴 준비를 효과적으로 하기 위해서는 이러한 특징을 가진 TDF 상품을 적절하게 활용할 줄 알아야 한다.\nTDF의 핵심은 바로 글라이드 패스\n시간 흐름에 따라 알아서 자산을 배분해주는 TDF의 특징을 글라이드 패스(glide path)로 설명하곤 한다. 글라이드 패스는 비행기가 착륙할 때 그리는 경로를 뜻하는 말로, 하늘에서 땅으로 부드러운 곡선을 그리며 착륙하는 모습이 은퇴까지 아직 많은 시간이 남은 사회 초년기에는 주식 비중이 크고, 은퇴 시기에 가까워질수록 주식 비중이 줄어드는 흐름과 매우 닮아 있다. 따라서 TDF에서의 글라이드 패스를 쉽게 설명하자면, 가입 시점부터 은퇴 시점까지 시간 흐름에 따라 주식과 채권의 배분을 조정하는 방법이다. 시기에 따른 배분율은 사전에 정해져 있으며, 그 비율에 따라 은퇴 시점이 가까워지면 TDF는 포트폴리오의 주식 보유량을 줄이고 채권 보유량을 늘려준다. 이유는 간단하다. 채권이 주식에 비해 위험이 적다고 판단하기 때문이다.\n다음 그래프는 미국의 자산운용사 블랙록(Black Rock)의 2040 TDF 자산 배분 현황이다. 가로축에서 0은 은퇴 시점이고 5는 은퇴 시점 5년 전, 10은 10년 전을 나타낸다. 해당 포트폴리오의 2023년 상태, 즉 약 8년 정도 남은 상태를 보면 주식 비중은 약 55%이고, 은퇴 시점에는 40%로 줄어든 것을 확인할 수 있다.\n.css-of5acw{margin:24px 0 8px;text-align:center;}.css-1pgssrp{max-width:100%;border-radius:16px;}\n.css-18442ym{font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}.css-jfs1hr{white-space:pre-wrap;font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}은퇴를 기점으로 달라지는 블랙록의 2040 TDF 자산 배분 현황\n그래프처럼 주식 배분이 점점 줄어드는 스케줄이 바로 글라이드 패스다. 글라이드 패스의 높낮이는 TDF를 만드는 회사마다 다르다. 어떤 방식의 글라이드 패스가 정답이라고 말할 수는 없지만, 모든 글라이드 패스를 만들 때 공통적으로 고려하는 몇 가지가 있다.\n첫째, 은퇴 시점이 가까워질수록 주식보다 채권의 비중이 높아진다.\n둘째, 은퇴 시점이 가까워질수록 해외주식보다 자국 주식의 비중이 높아진다. 은퇴에 가까울수록 외환 위험에 노출되지 않도록 자국 주식의 비중을 높이는 방식이 미국에서는 정석으로 통용된다. (그런데 이것은 한국 주식시장의 저조한 실적을 고려하면 한국 TDF에도 맞다고 단언할 수는 없는 부분이다.)\n셋째, 은퇴 시점이 가까워질수록 수익률이 높고 만기가 긴 채권의 비중이 줄고 수익률이 좀 낮더라도 만기가 짧은 채권의 비중이 높아진다.\n넷째, 은퇴 시점부터는 물가연동채권(미국에서는 TIPS), 원자재, 부동산 등의 자산이 많이 포함된다.\n이처럼 글라이드 패스는 주식과 채권의 분배 비율을 담고 있기 때문에 TDF의 수익률을 좌우하는 매우 중요한 요소이다. TDF 상품을 이해하는 데 가장 중요한 정보이지만 아직까지는 어려운 용어로 가득한 투자상품 설명서를 읽어야 파악할 수 있게 되어 있어서 투자자에게 제일 전달되지 않는 정보이기도 하다.\n배분 비율에 따른 수익률 차이는?\nTDF 상품별로 얼마나 주식에 투자되고, 채권에 투자되는지에 따라 매년 수익률이 달라진다. 예를 들어 2024년 1월 1일부터 7월 31일까지의 수익률을 살펴보자. 물론 장기간 투자를 위해 만들어진 상품을 이렇게 단기간 수익률로 분석하는 방법은 상품의 본래 취지에 어긋날 순 있지만, 투자자들이 수익률 차이를 이해하는 데 있어서 중요한 부분이라고 생각한다.\n\n출처=.css-114ityv{white-space:pre-wrap;cursor:pointer;-webkit-text-decoration:underline!important;text-decoration:underline!important;font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}아이랩 글라이드\n2040 빈티지인 TDF 상품들의 2024년 동안의 수익률을 보면 가장 좋은 수익률을 기록한 것은 ‘한국투자TDF알아서ETF포커스’로 7개월 수익률이 13.19%이다. 수익률이 가장 낮은 상품과 비교하면 무려 8% 넘게 차이가 난다. 2040년을 은퇴 시점으로 타깃하는 경우에는 주식 배분율 차이가 최대 77%에서 최소 57%로 다양하기 때문에 상당한 차이가 날 수밖에 없다. 이런 점을 이해한다면 같은 빈티지 안에서도 왜 이렇게 수익률이 다른지 이해할 수 있고, TDF를 고를 때 내가 주식에 더 많이 투자하고 싶다면 주식 배분율을 기준으로 상품을 고르는 방법도 가능하다.(하위 개념인 어떤 종목을 보유했느냐 또한 중요한 요소이지만, 수익률 차이를 가르는 데는 상위 개념인 어떤 자산을 얼마나 보유했느냐가 더 결정적인 역할을 한다.)\nTDF를 고르는 6가지 기준\n만약 내가 은퇴자금을 공격적으로 투자하고 싶다면 상품별로 주식-채권 배분율을 확인해보고, 주식 배분율이 높은 TDF를 고르면 된다. 이 배분율 차이는 곧 수익률, 변동성과 직결된다는 점, 그리고 위에서 살펴본 수익률 순위는 2024년 상반기 한정이며, 투자 기간 전체의 수익률이 아니라는 것도 잊지 않아야 한다.\n\n출처=아이랩 글라이드\n또한 나에게 맞는 TDF를 고르기 위해서는 조금 더 고려해야 할 사항들이 있다. 아래 6가지 기준을 소개한다.\n첫째가 앞서 말한 나의 은퇴 시점, 둘째가 주식 배분의 정도이다. 예를 들어 2045년 은퇴를 목표로 한다면 TDF 2045 빈티지 안에서 주식 배분율을 최소 57%에서 최대 79%까지 고를 수 있다.\n셋째는 미국 주식에 어느 정도 투자하고 싶은가다. 한국 TDF 상품은 미국 주식에 많이 투자하는데 역시 배분율이 조금씩 다르다. 만약 미국 주식에 많이 투자하고 싶다면 최대치를 선택할 수 있다.\n넷째, 수수료 차이를 고려해 패시브 펀드와 액티브 펀드 중 원하는 것을 선택한다. 지수를 따라가는 패시브 펀드는 액티브 펀드보다 수수료가 저렴하다. 또한 패시브 펀드로 구성된 TDF는 펀드 매니저가 종목을 골라 넣는 방식이 아니기 때문에 시장 환경에 따른 수익률 예측이 가능하다.\n다섯째, 위험도 선택이다. 상품설명서를 보면 펀드운용자가 정한 위험등급을 볼 수 있다. 이는 매우 높은 위험의 1부터 매우 낮은 위험의 6까지 구분된다. 자신이 얼마나 위험을 감수할 수 있을지를 고려한다.\n여섯째, 은퇴 시점의 주식 배분율을 고른다. 같은 빈티지 안에서도 은퇴 시점에 주식 배분이 조금 더 높은 상품이 있고 낮은 상품이 있다. 은퇴가 다가왔을 때 예상치 못한 시장 상황에 의한 갑작스러운 손실을 낮추고 싶다면 더 낮은 상품을 고르면 된다.\n위와 같은 질문을 통해 자신의 계획에 맞는 TDF 상품을 찾아갈 수 있다. 단순히 수익률만으로 상품을 고르기보다는 나의 성향에 맞춰 고르는 심층적인 방법이다.\n과거 수익률은 판단하기 위한 최소한의 단서\n상품을 고르는 데 있어서 과거 수익률은 중요한 정보다. 과거의 수익률이 미래 수익률을 보장하지는 않지만 그동안 얼마나 잘해왔는지를 살펴볼 수 있는 중요한 척도이므로, 광고만 보고 고르는 것보다는 낫다. 수익률을 잘 비교하기 위해서는 내 펀드의 수익률만 살펴보는 것이 아니라, 같은 빈티지의 다른 상품이 동일한 기간에 얼마나 수익을 올렸는지 살필 것을 권한다. 앞서 살펴본 것처럼 똑같은 2040 타깃의 TDF여도 최소 5%에서 최대 13%로 차이가 있기 때문이다. 만기를 채우지 않은 채 수시로 상품을 갈아타는 것을 추천할 수는 없지만, 만약 꾸준히 더 좋은 수익률을 내고 나에게 잘 맞는 상품이 있다면 당연히 바꾸어야 하지 않을까?\nTDF는 한국 금융시장에서 은퇴 준비에 있어서 점점 더 큰 부분을 차지하고 있는 상품이므로, 이번 시간을 통해 TDF에 대해 제대로 이해하는 투자자들이 많아지길 기대한다. 그리하여 나에게 맞는 TDF를 골라서 투자하고 다음 단계로 나아가기를 바란다. 퇴직연금 계좌에는 보통 다른 계좌보다 제약이 있기는 하지만 ETF, 자산 배분 펀드 등 다양한 상품이 존재한다. 이를 이용해서 장기투자와 단기투자를 섞거나, 투자에 더 익숙한 투자자라면 ETF를 이용해 자신만의 포트폴리오를 잘 구성해볼 수 있다. 다음 화에서는 퇴직연금으로 투자하는 ETF에 대해 다루어 보도록 하겠다.\n*<노후 준비 액션플랜> 시리즈는 국내 159개 대표 운용펀드의 TDF 글라이드 패스와 수익률을 한눈에 보기 쉽도록 전달하는 .css-1vsqqzg{white-space:pre-wrap;cursor:pointer;-webkit-text-decoration:underline!important;text-decoration:underline!important;font-weight:bold;font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}아이랩 글라이드와 함께 만듭니다.\n.css-1ifza5r{border:0;margin-top:0;margin-bottom:0;height:1px;opacity:1;background:var(--tHairlineBackground);margin:0;}\nEdit 주소은, 김현미(아이랩) Graphic 조수희, 윤여진",
        "content": "퇴직연금을 운용하는 지속 가능한 방법",
        "contentSnippet": "퇴직연금을 운용하는 지속 가능한 방법",
        "guid": "https://blog.toss.im/article/retirement-plans-05",
        "isoDate": "2024-08-23T06:20:00.000Z"
      },
      {
        "title": "건강검진 지원부터 빈 용기 보증금 제도까지, 우리의 건강과 환경을 지키는 돈 되는 정책",
        "link": "https://blog.toss.im/article/money-policies-23",
        "pubDate": "Fri, 23 Aug 2024 01:00:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}.css-1kxrhf3{white-space:pre-wrap;}우리의 건강과 환경, 안전을 지키는 정책과 제도를 살펴봅니다.  \n.css-1feg9au{font-size:24px;letter-spacing:0em;line-height:1.6;font-weight:bold;color:var(--adaptiveGrey900);margin:24px 0 4px;}\n건강이 최우선 \n.css-q3ktjb{white-space:pre-wrap;font-weight:bold;}⓵ 생애주기별 국가건강검진제도\n일반건강검진부터 연령에 따른 암 검진, 영유아와 청소년까지 필수적인 검사를 지원합니다.\n.css-16cuouw{white-space:pre-wrap;color:var(--adaptiveGrey800);background-color:#3fd59936;}일반건강검진은 보통 2년 주기로 받아요. 비만, 시청각 이상, 고혈압, 폐결핵, 신장질환, 빈혈, 당뇨병 여부 등을 공통으로 검사하고요. 연령에 따라 이상지질혈증, B형간염, 골밀도, 인지기능장애, 우울증 등의 검사를 추가로 받을 수 있어요.\n자세한 내용은 국민건강보험공단(www.nhis.or.kr)에서 확인할 수 있습니다.\n⓶ 암 환자 의료비 지원\n암 환자가 있는 가정의 경제적 부담을 덜어드리고 힘든 치료 과정을 잘 견뎌낼 수 있도록 의료비를 지원해요.\n성인 환자 기준, 의료급여 수급자나 차상위 본인 부담 경감대상자인 경우, 최대 3년간 연속으로 연간 최대 300만원을 지원합니다. 주소지 관할 보건소에서 신청할 수 있어요.\n⓷ 찾아가는 금연지원 서비스\n담배를 끊고자 하는 금연 의지가 있지만 프로그램에 참가하기 어려운 상황이라면, 직접 찾아가 관리해드립니다. 위기 청소년, 여성, 장애인, 소규모 사업장 근로자 등이 지원 대상이에요.\n전국 17개 지역금연지원센터를 통해 신청하면 6개월간 9회 이상 지속적인 금연 상담을 받을 수 있고요, 니코틴 보조제나 금연치료제를 지원 받을 수 있어요. 한국건강증진개발원(02-3781-2220)에 더 자세한 내용을 문의해보세요.\n안전을 체감할 수 있는 사회 \n⓸ 여성 긴급전화 1366\n가정폭력, 성폭력, 성매매, 스토킹, 교제폭력 등 폭력 피해자에게 상담 서비스를 지원합니다. 365일 24시간 운영되니 도움이 필요할 때 언제든 전화하면 됩니다. \n초기 상담부터 관련 지원 기관으로의 연계, 피해자와 동반가족이 최대 7일까지 머무를 수 있는 긴급 피난처까지 제공합니다. \n⓹ 안심 상속 원스톱 서비스\n사망자가 남긴 재산과 채무를 한번에 조회하는 서비스입니다. 예금, 대출, 보험, 증권 등 금융거래 내역부터 연금 가입 유무까지 확인할 수 있어요.\n사망일이 속한 달의 말일부터 1년 이내에 상속인(또는 후견인)이 신청할 수 있어요. 가까운 행정복지센터에 방문해도 되고, 정부24 홈페이지에서 온라인 신청도 가능합니다.\n환경을 지키는 돈 되는 정책 \n⓺ 폐가전 무상방문수거\n온라인이나 전화로 폐가전 처리 신청하면, 수거 전담반이 찾아가 무상 수거해 갑니다. 집에서 사용하지 않는 TV나 냉장고, 세탁기, 에어컨 등 부피가 크고 무거운 폐가전 제품을 배출할 때 꼭 확인해보세요. 소형 가전은 5개 이상인 경우 수거 가능합니다.\n폐가전제품 배출예약시스템(www.15990903.or.kr)이나 전화(1599-0903)로 신청할 수 있어요.\n⓻ 빈 용기 보증금 제도\n재사용 표시가 있는 병을 가까운 슈퍼나 대형마트에 반환하면 보증금을 돌려 받을 수 있어요. 돌려받을 수 있는 금액은 병의 용량에 따라 70원~350원으로 달라요.\n예를 들면 소주병은 1병당 100원인데, 제사 지낼 때 많이 쓰는 대형 청주 병은 개당 350원이랍니다. 제품 라벨에 금액이 표시돼 있어요.\n\n.css-1ifza5r{border:0;margin-top:0;margin-bottom:0;height:1px;opacity:1;background:var(--tHairlineBackground);margin:0;}\nEdit 정경화 Graphic 조수희",
        "content": "지키고 살피는 건강, 안전, 환경 서비스",
        "contentSnippet": "지키고 살피는 건강, 안전, 환경 서비스",
        "guid": "https://blog.toss.im/article/money-policies-23",
        "isoDate": "2024-08-23T01:00:00.000Z"
      }
    ]
  },
  {
    "name": "모나미",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김진홍",
    "category": "개인",
    "posts": []
  }
]