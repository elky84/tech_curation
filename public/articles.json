[
  {
    "id": 1,
    "imageUrl": "",
    "title": "GitHub Copilot의 Visual Studio 자동 문서 댓글 생성 기능 소개",
    "description": "No description available",
    "reviews": [],
    "syllabus": [],
    "link": "https://jacking75.github.io/ai-github_copilot_20250602/",
    "pubDate": "Mon, 02 Jun 2025 00:00:00 +0900",
    "creator": "Unknown",
    "categories": []
  },
  {
    "id": 2,
    "imageUrl": "",
    "title": "Gemma3 기반 Ollama 활용 AI 에이전트 개발 핵심 Function Call 구현해보기",
    "description": "이 글은 AI 에이전트(Agent) 개발 시 필수적인 함수호출 방법을 오픈소스를 이용해 구현해 본다. 이를 위해, Gemma3(젬마) LLM(Large Language Model) 기반 Ollama 활용 Function Call(펑션콜) 실습 내용을 소개하고 실행 결과를 확인한다. 아울러, 이런 함수호출 방식의 한계점을 개선하기 위한 솔류션을 나눔한다. 이 실습의 결과는 다음과 같다. \n\n\n이 글은 다음 내용을 포함한다.\n\nAI 에이전트 구현을 위한 함수 호출 방법\nOllama 를 통한 Gemma3 사용법\n채팅 형식 프롬프트 및 메모리 사용법\nGradio 기반 웹 앱 개발\nFunction call 의 한계와 솔류션\n\n\nAI 에이전트 내부 Function call 메커니즘(Akriti, 2025)\n\n\n이 글의 구현 코드는 다음 링크에서 확인할 수 있다.\n\nmac999/AI_agent_simple_function_call\n\nGemma3 모델 특징\n\nGemma 3는 구글이 개발해  2025년 3월 10일에 출시한 LLM으로, 차세대 경량 오픈 멀티모달 AI 모델로, 텍스트와 이미지를 동시에 처리할 수 있는 기능을 갖추고 있다. 이 모델은 다양한 크기와 사양으로 제공되어 단일 GPU 또는 TPU 환경에서도 실행 가능하다.\nGemma 3는 1B, 4B, 12B, 27B의 네 가지 모델 크기로 제공되며, 각각 10억, 40억, 120억, 270억 개의 파라미터를 갖추고 있다. 1B 모델은 텍스트 전용으로 32K 토큰의 입력 컨텍스트를 지원하고, 4B, 12B, 27B 모델은 멀티모달 기능을 지원하며 128K 토큰의 입력 컨텍스트를 처리할 수 있다. 이는 이전 Gemma 모델보다 16배 확장된 크기로, 훨씬 더 많은 양의 정보를 한 번에 처리할 수 있게 해준다.\n이 모델은 텍스트와 이미지 데이터를 동시에 처리하고 이해하는 멀티모달 기능을 제공한다. 이미지 해석, 객체 인식, 시각적 질의응답 등 다양한 작업을 수행할 수 있으며, 텍스트 기반 작업에 시각적 정보를 효과적으로 활용할 수 있도록 지원한다. \n\n\n\n\n  \nWelcome Gemma 3: Google's all new multimodal, multilingual, long context open LLM\n\nGemma 3는 140개 이상의 언어를 지원하여 전 세계 다양한 언어 사용자를 대상으로 하는 AI 애플리케이션 개발에 매우 유리하다. 사용자는 자신의 모국어로 Gemma 3와 상호작용할 수 있으며, 다국어 기반의 텍스트 분석 및 생성 작업도 효율적으로 수행할 수 있다.\n이 모델은 다양한 작업 수행 능력을 갖추고 있다. 질문 답변, 텍스트 요약, 논리적 추론, 창의적인 텍스트 형식 생성(시, 스크립트, 코드, 마케팅 문구, 이메일 초안 등), 이미지 데이터 분석 및 추출 등 광범위한 자연어 처리 및 컴퓨터 비전 관련 작업을 수행할 수 있다. 또한, 함수 호출 및 구조화된 출력을 지원하여 개발자들이 특정 작업을 자동화하고 에이전트 기반의 경험을 구축하는 데 도움을 준다.\nGemma 3는 다양한 도구 및 프레임워크와 원활하게 통합된다. Hugging Face Transformers, Ollama, JAX, Keras, PyTorch, Google AI Edge, UnSloth, vLLM, Gemma.cpp 등 다양한 개발 도구 및 프레임워크와 호환되어 개발자들이 자신이 익숙한 환경에서 Gemma 3를 쉽게 활용하고 실험할 수 있다.\n이 모델은 다양한 벤치마크 테스트에서 동급 모델 대비 최첨단 성능을 입증했다. 특히, Chatbot Arena Elo Score에서 1338점을 기록하며, 여러 오픈 소스 및 상용 모델보다 높은 성능을 보였다. \nGemma 3는 오픈 모델로, 개방형 가중치를 제공하여 사용자가 자유롭게 조정하고 배포할 수 있다. Kaggle과 Hugging Face에서 다운로드 가능하며, Creative Commons 및 Apache 2.0 라이선스를 따름으로써, 개발자와 연구자에게 VLM 기술에 대한 접근성을 높여준다.\n\n개발 환경\n개발 환경은 다음과 같다. 미리 설치, 가입한다.\n\nollama:  https://ollama.com/download/windows\ngemma3: https://ollama.com/search\nserper 서비스: 가입. https://serper.dev/dashboard \n\n설치되어 있다면, 다음 명령을 터미널에서 실행한다.\nollama pull gemma3:4b\n\n\n\ngemma3:4b GPU VRAM 소모량\n\n\n이제 다음과 같이 모델을 실행해 볼 수 있다. \n\n\n\n\n참고로, GPU VRAM 등을 고려해 더 성능이 좋은 파라메터수 많은 대형 모델을 사용할 수도 있다.\n\n\ngemma3 지원 모델들\n\n처리 프로세스\n이 실습 프로그램의 프로세스는 다음과 같다.\n\n\nGradio 앱이 시작되면, 사용자의 입력이 발생하고 이 입력은 process_message 함수에 전달된다. 이 함수는 사용자의 메시지를 chat_history에 추가하여 대화 기록을 저장한다. 이후 모델에게 전달할 대화 문맥을 구성하기 위해 messages 리스트가 생성된다.\n\n\n그 다음 단계에서는 ollama.chat 함수를 통해 언어 모델에게 응답을 요청하게 되며, 이 응답 내에 함수 호출이 포함되어 있는지를 확인한다. 만약 응답에 함수 호출이 포함되어 있다면, 이를 parse_function_call 함수를 통해 파싱한다.\n\n\n파싱된 함수가 google_search라면, 모델이 검색을 원한다고 판단하여 검색 쿼리를 추출하고 검색 수행 예정임을 사용자에게 안내하는 메시지를 추가한다. 이후 실제로 google_search 함수를 실행하여 외부 검색을 수행한다.\n\n\n검색 결과는 다시 chat_history에 저장되며, 이 결과를 바탕으로 언어 모델에게 재질문을 하여 더 정확하고 완성된 응답을 유도한다. 모델이 생성한 최종 응답은 chat_history에 마지막으로 추가되고, 이 전체 대화 기록이 사용자에게 반환된다.\n\n\n이 구조는 사용자의 질의에 따라 외부 정보까지 능동적으로 검색하고 반영할 수 있는 LLM 기반 AI 에이전트의 대표적인 흐름을 보여준다.\n\n\n다음은 이 순서도를 보여준다.\n\n\n\n구현하기\n터미널에서 다음 라이브러리를 설치한다.\npip install langchain-core langchain-openai gradio ollama requests python-dotenv pydantic\n\n\n새로운 파이썬 파일(코드 참고)을 생성한 후, 우선, 필요한 라이브러리를 임포트한다. \n\nimport gradio as gr\nimport ollama\nimport requests, json, os\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, Dict, Any, List\n\nload_dotenv()\nSERPER_API_KEY = os.getenv('SERPER_API_KEY')\n\n\n\n그리고, 사용하는 API 키를 가져온다. 이를 위해, 미리 .env 파일을 다음과 같이 만들어 놓고, 해당 API를 입력해 놓야야 한다.\n\n# .env\nSERPER_API_KEY=<YOUR API KEY>\n\n\n\n파라메터에서 검색 질의문, 함수호출명과 파라메터를 정의한다. 아울러, 질의 결과를 명확히 데이터항목으로 추출하기 위해서 검색 결과가 될 데이타항목(타이틀, 링크, 스닙펫) 형식을 pydantic의 basemodel을 이용해 명확히 정의한다. 그리고, LLM 호출 결과를 펑션콜이 가능한 형식으로 변환하기 위한 파싱 함수인 parse_function_call 을 정의한다.\n\nclass SearchParameters(BaseModel):\n    query: str = Field(..., description=\"Search term to look up\")\n\nclass FunctionCall(BaseModel):\n    name: str\n    parameters: Dict[str, Any]\n\nclass SearchResult(BaseModel):\n    title: str\n    link: str\n    snippet: str\n\n    def to_string(self) -> str:\n        return f\"Title: {self.title}\\nLink: {self.link}\\nSnippet: {self.snippet}\"\n\ndef google_search(query: str) -> SearchResult:\n    \"\"\"Perform a Google search using Serper.dev API\"\"\"\n    try:\n        url = \"https://google.serper.dev/search\"\n        payload = json.dumps({\"q\": query})\n        headers = {\n            'X-API-KEY': SERPER_API_KEY,\n            'Content-Type': 'application/json'\n        }\n        \n        response = requests.post(url, headers=headers, data=payload)\n        response.raise_for_status()  # 잘못된 상태 코드에 대해 예외 발생\n        \n        results = response.json()\n        \n        if not results.get('organic'):\n            raise ValueError(\"No search results found.\")\n            \n        first_result = results['organic'][0]\n        return SearchResult(\n            title=first_result.get('title', 'No title'),\n            link=first_result.get('link', 'No link'),\n            snippet=first_result.get('snippet', 'No snippet available.')\n        )\n    except Exception as e:\n        print(f\"Search error: {str(e)}\")\n        raise\n\ndef parse_function_call(response: str) -> Optional[FunctionCall]:\n    \"\"\"Parse the model's response to extract function calls\"\"\"\n    try:\n        # Clean the response and find JSON structure\n        response = response.strip()\n        start_idx = response.find('{')\n        end_idx = response.rfind('}') + 1\n        \n        if start_idx == -1 or end_idx == 0:\n            return None\n            \n        json_str = response[start_idx:end_idx]\n        data = json.loads(json_str)\n        return FunctionCall(**data)\n    except Exception as e:\n        print(f\"Error parsing function call: {str(e)}\")\n        return None\n\n\n\ngemma에 지시할 시스템 프롬프트 명령을 정의한다. prompt_system_message는 이 챗봇이 어떻게 동작해야 하는지, 그리고 어떤 기준으로 답변을 해야 하는지에 대한 지침을 제공하는 역할을 한다. 이 메시지는 챗봇이 2024년까지의 정보를 학습한 AI 어시스턴트임을 명확히 하고, 사용자의 질문에 대해 가능한 경우에는 바로 답변을 하되, 최신 정보나 불확실한 내용, 시의성이 있는 질문에 대해서는 반드시 펑션콜을 통해 검색 기능을 활용해야 함을 명시한다. 이전 대화 내용이 함께 입력으로 주어지기 때문에, 챗봇은 이 대화 맥락을 참고하여 일관성 있고 상황에 맞는 답변을 해야 한다고 안내한다. 참고로, 준수해야 할 gemma3의 function call 형식은 다음과 같다. \n\ngemini-samples/examples/gemma-function-calling.ipynb at main · philschmid/gemini-samples\n\n검색이 필요한 상황과 그렇지 않은 상황을 구체적으로 구분하여, 챗봇이 임의로 정보를 추정하거나 추가하지 않고, 검색 결과에 기반한 사실만을 간결하게 전달하도록 유도한다. 검색이 필요한 경우에는 정해진 JSON 형식으로만 응답하도록 하여, 시스템이 함수 호출 방식으로 검색을 처리할 수 있게 한다.\n\n# 프롬프트 시스템 메세지 정의\nprompt_system_message = \"\"\"You are an AI assistant with training data up to 2024. Answer questions directly when possible, and use search when necessary.\n\nYou will receive previous conversation messages as part of the input. Use these prior messages to maintain context and provide coherent, context-aware answers.\n\nDECISION PROCESS:\n1. For historical events before 2024:\n   - Answer directly from your training data.\n2. For events in 2024:\n   - If you are certain, answer directly.\n   - If you are unsure, use search.\n3. For events after 2024 or current/recent information:\n   - Always use search.\n4. For timeless information (scientific facts, concepts, etc.):\n   - Answer directly from your training data.\n\nALWAYS USE SEARCH if the question:\n- Contains words like \"current\", \"latest\", \"now\", \"present\", \"today\", \"recent\"\n- Asks about someone in a changing position (champion, president, CEO, etc.)\n- Requests information that might have changed since 2024\n- Is time-sensitive and does not specify a time period\n\nFUNCTION CALL FORMAT:\nWhen you need to search, respond WITH ONLY THE JSON OBJECT, no other text, no backticks:\n{\n    \"name\": \"google_search\",\n    \"parameters\": {\n        \"query\": \"your search query\"\n    }\n}\n\nSEARCH FUNCTION:\n{\n    \"name\": \"google_search\",\n    \"description\": \"Search for real-time information\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"query\": {\n                \"type\": \"string\",\n                \"description\": \"Search term\"\n            }\n        },\n        \"required\": [\"query\"]\n    }\n}\n\nWHEN ANSWERING BASED ON SEARCH RESULTS:\n- Use ONLY facts found in the search results below.\n- Do NOT add any dates or information not present in the search results.\n- Do NOT make assumptions about timing or events.\n- Quote dates exactly as they appear in the results.\n- Keep your answer concise and factual.\n\"\"\"\n\n\n\ngemma에 전달할 메시지는 프롬프트 지시문, 사용자 질문을 포함한 이전 채팅 이력 메시지 등을 모두 포함한다. 이를 ollama LLM 에 전달할 수 있는 형식으로 변환하는 함수를 다음과 같이 준비한다. \n\n# 메시지 리스트를 생성하는 함수\ndef filter_memory(memory):\n    \"\"\"assistant의 검색 안내 메시지를 memory에서 제외\"\"\"\n    return [\n        msg for msg in memory\n        if not (\n            msg[\"role\"] == \"assistant\" and (\n                msg[\"content\"].startswith(\"Searching for:\") or\n                msg[\"content\"].startswith(\"Searched for:\")\n            )\n        )\n    ]\n\ndef build_messages(chat_history, user_input=None, prompt_system_message=prompt_system_message, N=6, search_result=None):\n    \"\"\"\n    최근 N개 메시지와 system 메시지를 합쳐 messages 리스트를 만듭니다.\n    search_result가 있으면, user_input 대신 검색 결과 기반 프롬프트를 추가합니다.\n    \"\"\"\n    memory = chat_history[-N:] if len(chat_history) > N else chat_history[:-1]\n    filtered_memory = filter_memory(memory)\n    messages = [{\"role\": \"system\", \"content\": prompt_system_message}] + filtered_memory\n    if search_result is not None:\n        messages.append({\n            \"role\": \"user\",\n            \"content\": (\n                \"Refer to the following search result and provide a concise, factual answer based only on this information:\\n\"\n                f\"{search_result.to_string()}\"\n            )\n        })\n    elif user_input is not None:\n        messages.append({\"role\": \"user\", \"content\": user_input})\n    return messages\n\n\n\n\n이제 process_message 함수를 구현한다. 이 함수는 사용자의 입력과 기존 채팅 기록을 받아 AI 모델과의 대화 흐름을 관리하는 역할을 한다.\n\n\n먼저 사용자의 메시지를 채팅 기록에 추가하고, 이전 대화 내용(메모리)을 추출하여 시스템 메시지와 함께 모델에 전달할 메시지 목록을 구성한다. 이 메시지 목록을 Ollama 모델에 전달하여 응답을 받는다. 모델의 응답이 함수 호출(JSON) 형태라면, 그 내용을 파싱하여 검색이 필요한 경우 검색 쿼리를 추출한다.\n\n\n검색이 필요하다고 판단되면, 검색 중임을 알리는 메시지를 채팅 기록에 추가하고, 실제로 검색을 수행한다. 검색 결과를 다시 채팅 기록에 반영한 뒤, 이 결과를 포함한 새로운 메시지 목록을 만들어 모델에 전달하여 최종 답변을 받는다. 최종적으로 받은 답변 역시 채팅 기록에 추가한다.\n\n\n검색이 필요하지 않은 경우에는 모델의 응답을 바로 채팅 기록에 추가한다. 이 과정에서 각 단계별로 최신 채팅 기록을 반환하여, 사용자 인터페이스가 실시간으로 대화 상태를 갱신할 수 있도록 한다.\n함수 실행 중 오류가 발생하면, 오류 메시지를 채팅 기록에 추가하여 사용자에게 알린다.\n\n# Model name\nMODEL_NAME = \"gemma3\"\n\ndef process_message(user_input, chat_history):\n    \"\"\"Process user message and update chat history\"\"\"\n    try:\n        # 사용자 메시지를 기록에 추가\n        chat_history.append({\"role\": \"user\", \"content\": user_input})\n        search_info = None\n\n        # 최근 N개 메시지만 memory에 포함 (예: 최근 6개)\n        N = 6\n        messages = build_messages(chat_history, user_input=user_input, N=N)\n\n        # 모델로부터 응답 받기\n        response = ollama.chat(\n            model=MODEL_NAME,\n            messages=messages       \n        )\n        \n        model_response = response['message']['content']\n        \n        # 함수 호출로 응답을 파싱 시도\n        function_call = parse_function_call(model_response)\n        \n        if function_call and function_call.name == \"google_search\":\n            # 검색 파라미터 검증\n            search_params = SearchParameters(**function_call.parameters)\n            search_query = search_params.query\n            \n            # 검색 정보 기록에 추가\n            search_info = f\"Searching for: {search_query}\"\n            chat_history.append({\"role\": \"assistant\", \"content\": search_info})\n            yield chat_history\n            \n            # 검색 실행\n            search_result = google_search(search_query)\n            \n            # 검색 결과로 정보 업데이트\n            search_info = f\"Searched for: {search_query}\\n\\nResult:\\n{search_result.to_string()}\"\n            chat_history[-1] = {\"role\": \"assistant\", \"content\": search_info}\n            yield chat_history\n\n            # 검색 결과 기반 메시지 생성\n            messages = build_messages(chat_history, N=N, search_result=search_result)\n      \n            # 검색 결과를 포함해 모델로부터 최종 응답 받기\n            final_response = ollama.chat(\n                model=MODEL_NAME,\n                messages=messages\n            )\n            \n            assistant_response = final_response['message']['content']\n        else:\n            # 함수 호출이 없으면 직접 응답 반환\n            assistant_response = model_response\n        \n        # 최종 응답을 기록에 업데이트\n        if search_info:\n            chat_history.append({\"role\": \"assistant\", \"content\": f\" Response:\\n{assistant_response}\"})\n        else:\n            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n        \n        yield chat_history\n            \n    except Exception as e:\n        error_msg = f\"An error occurred: {str(e)}\"\n        chat_history.append({\"role\": \"assistant\", \"content\": error_msg})\n        yield chat_history\n\n\n\n\n이제 Gradio UI 를 정의하고, 메인 엔트리에서 이 앱을 실행한다.\n\n\n# Gradio 인터페이스 생성\nwith gr.Blocks(css=\"footer {visibility: hidden}\") as demo:\n    gr.Markdown(\"\"\"\n    # Agent based on Gemma3 using Function Call\n    \n\n    \"\"\")\n    \n    chatbot = gr.Chatbot(\n        height=500,\n        show_label=False,\n        avatar_images=(None, \"https://api.dicebear.com/9.x/identicon/svg?seed=Mason\"),\n        type=\"messages\"\n    )\n    \n    with gr.Row():\n        msg = gr.Textbox(\n            scale=5,\n            show_label=False,\n            placeholder=\"Ask me anything...\",\n            container=False\n        )\n        submit_btn = gr.Button(\"Send\", scale=1)\n    \n    with gr.Row():\n        clear_btn = gr.Button(\"Clear Chat\")\n    \n\n    # 이벤트 핸들러 설정\n    msg.submit(\n        process_message,\n        [msg, chatbot],\n        [chatbot],\n    )\n    \n    submit_btn.click(\n        process_message,\n        [msg, chatbot],\n        [chatbot],\n    )\n    \n    clear_btn.click(\n        lambda: [],\n        None,\n        chatbot,\n        queue=False\n    )\n    \n    # 메시지 전송 후 텍스트박스 비우기\n    msg.submit(lambda: \"\", None, msg)\n    submit_btn.click(lambda: \"\", None, msg)\n\nif __name__ == \"__main__\":\n    demo.launch(inbrowser=True, share=True) \n\n\n\n실행\n앞에 구현된 앱을 실행한다. 그리고, 적절한 질문을 입력해 본다. 다음과 같이 실행되면 성공한 것이다.\n\n\n\n\n펑션콜 문제 개선 방법\n실제로 질의해보면 불명확한 프롬프트 입력 등에서 부적절한 함수 호출이 수행되는 것을 알 수 있다. 이를 개선하기 위해 다음 사항을 고려한다.\n\n프롬프트 설계의 명확성\n\n함수 호출이 필요한 상황, 호출 방식(JSON 포맷 등), 호출 예시를 SYSTEM_MESSAGE에 명확하게 안내해야 한다. 함수 호출이 아닌 일반 답변을 하면 안 된다는 점을 반복적으로 강조한다.\n예시 프롬프트:\n\"질문에 답변하기 위해 함수 호출이 필요하다고 판단되면 반드시 아래 JSON 형식으로만 응답하라. 다른 텍스트나 설명은 절대 포함하지 마라.\"\n\n함수 정의의 구체성\n\n함수의 목적, 파라미터, 반환값, 사용 예시를 상세하게 기술한다. 각 파라미터의 타입, 필수 여부, 설명을 명확히 한다. 함수가 처리할 수 없는 입력(예: 빈 문자열, 잘못된 타입 등)에 대한 예외 상황도 명시한다.\n\n예시 기반 Few-shot Prompting\n\nSYSTEM_MESSAGE 또는 user message에 함수 호출이 필요한 질문과 그에 대한 올바른 함수 호출 예시를 여러 개 포함시킨다. 예시가 많을수록 모델이 패턴을 더 잘 학습한다.\n\n함수 호출 실패 시 재시도 로직\n\n모델이 함수 호출을 하지 않거나 잘못된 형식으로 응답하면, 내부적으로 \"함수 호출이 필요합니다. 반드시 JSON 형식으로만 응답하세요.\"와 같은 추가 프롬프트로 재요청한다.\n\n출력 파싱의 견고성\n\n모델이 JSON 외의 텍스트를 섞어서 반환할 수 있으므로, 파싱 로직에서 JSON 부분만 추출하거나, 불완전한 JSON도 최대한 보완해서 파싱하도록 한다.\n\n함수 호출 의도 강화 프롬프트\n\nSYSTEM_MESSAGE에 \"함수 호출이 필요한 상황에서는 반드시 함수 호출을 우선적으로 고려하라\"는 문구를 추가한다. \"만약 함수 호출이 필요하지 않다고 판단되면, 그 이유를 설명하지 말고 바로 답변만 하라.\" 등 불필요한 설명을 억제한다.\n\n모델 버전 및 파라미터 최적화\n\n최신 GPT-4 Turbo 등 함수 호출에 최적화된 모델을 사용한다. temperature, top_p 등 파라미터를 낮춰 일관된 응답을 유도한다. \n\n함수 호출 실패 케이스 수집 및 개선 \n\n\n실제 사용자 입력 중 함수 호출이 누락된 사례를 수집하여, SYSTEM_MESSAGE나 예시 프롬프트를 지속적으로 개선한다.\n\n이외에 잘 활용되는 함수에 대한 파인튜닝을 수행해 본다. \n\n마무리\n본 글은 ollama 를 이용한 gemma3 모델을 로딩해 Agent 개발 시 핵심이 되는 function call을 구현해 보았다. 실행해 보면 알겠지만, 펑션콜은 프롬프트 입력에 따라 민감하게 동작한다는 것을 알 수 있다. 그러므로, 함수 호출 방식은 적절히 LLM 오케스트레이션 및 튜닝되어야 한다는 것을 알 수 있다. \n\n\n레퍼런스\n\nWelcome Gemma 3: Google's all new multimodal, multilingual, long context open LLM\ngemini-samples/examples/gemma-function-calling.ipynb at main · philschmid/gemini-samples\nFunction calling with Gemma3 using Ollama | by Arjun Prabhulal | Google Cloud - Community | Mar, 2025 | Medium | Google Cloud - Community\nEnhancing Gemma 3’s Capabilities with Fine-Tuning for Function Calling | by Akriti Upadhyay | May, 2025 | Medium\nGeneral AI agent framework for smart buildings based on  large language models and ReAct strategy\nOpen-Source Tools for Agents | Data Science Collective\nThe Era of High-Paying Tech Jobs is Over | by Somnath Singh | Level Up Coding\nAGI-Edgerunners/LLM-Agents-Papers: A repo lists papers related to LLM based agent\nLLM for Optimisation in 3-D Space: A comparison with Deterministic optimisation methods | by Peter Eze | Crayon Data & AI | Medium\nDemystifying Generative AI Agents | by Dr Sokratis Kartakis | Google Cloud - Community | Medium\ncookbook/quickstarts/Function_calling.ipynb at main · google-gemini/cookbook",
    "reviews": [],
    "syllabus": [],
    "link": "http://daddynkidsmakers.blogspot.com/2025/06/gemma3-ollama-function-call.html",
    "pubDate": "2025-06-04T06:56:00.000Z",
    "creator": "Unknown",
    "categories": []
  },
  {
    "id": 3,
    "imageUrl": "",
    "title": "다시 2부 - Composable Architecture - 라우터 소개",
    "description": "원래는 Action 시스템을 활용해서 Router를 쓰려고 설계했지만, 필자가 기존에 사용하던 방식 중 하나로 돌아가 거기서부터 다시 설계되었다.\n이 라우터는 크게 두 가지 상황에 활용 가능하다.\nActivity 간 화면 이동\nCompose Navigation 간 화면 이동\n만약 싱글 액티비티(Single Activity) 구조를 쓴다면 Compose Navigation 부분만 적용하면 되니 더 쉽게 사용 가능하다.\n여기서는 어떻게 활용되었을까?\nActivity는 Dagger의 IntoMap을 활용해서 Key/Value 매핑으로 ActivityRoute를 상속받아 구현한 객체를 정의해서 사용한다.\nCompose Navigation은 NavigationRoute를 상속받아 구현한다.\n이 글에서는\n새로운 Router의 설계 철학 및 동작 방식을 자세히 알아본다.\nActivity 및 Compose Navigation에서 Router를 활용하는 구체적인 코드 예시를 살펴본다.\nRoute\nNavigationRoute는 드로이드 나이츠(Droid Knights) 프로젝트에도 적용되었으며, 다음 링크를 통해 라우팅 방식을 확인해볼 수 있다.\ndroidknights 2025 - Router\n이 글에서는 TComposableArchitecture - GitHub - link에서 적용된 방법을 중심으로 설명한다.\n사용법\n사용법은 간단하다. Navigator를 주입받아 활용하며, 이동할 Activity 또는 Navigation을 Navigator에서 제공하는 navigate만 호출하면 된다.\n\n@HiltViewModel\ninternal class SearchViewModel @Inject constructor(\n    flowActionStream: FlowActionStream,\n    private val navigator: Navigator,\n) : ActionViewModel<SearchAction>(flowActionStream, SearchAction::class) {\n\n    override suspend fun handleAction(action: SearchAction) {\n        when (action) {\n            is SearchAction.ShowDetail -> { // Activity 이동 시\n                navigator.navigate(\n                    activityRoute = DetailActivityRouter::class,\n                    argumentMap = mapOf(\n                        DetailActivityRouter.PUT_DATA to DetailData(text = action.message),\n                    ),\n                )\n            }\n\n            is SearchAction.SwitchNavigation -> { // Compose Navigation 이동 시\n                navigator.navigate(\n                    navigationRoute = action.navItem.route,\n                    saveState = true,\n                )\n            }\n        }\n    }\n}\n\n\n여기서의 설계는?\n먼저 Compose Navigation에 대한 시퀀스를 살펴보자.\nView - Action - ViewModel - Router - RouterViewModel - RouterView로 이어지는 흐름을 알 수 있다.\n이를 쉽게 이해하기 위해 Repository 패턴에 비유하면, 두 개의 View에서 데이터 통신하는 방법을 설명한 그림으로 생각하면 된다. Router 부분이 Repository로 볼 수 있다.\nView - Action - ViewModel - Repository - RouterViewModel - RouterView\n이를 도식화하면\n\n이번에는 Activity를 이동한다면? 한단계가 더 추가됨을 알 수 있다.\nActivityJourney라는 개념을 활용해서 이동할 Activity가 포함되어 있는지 추가로 검증하고, Activity를 이동한다.\nView - Action - ViewModel - Router - (ActivityJourney) - RouterViewModel - RouterView\n\n이 코드를 이해하는 데 있어서 중요한 점은 두 개의 View가 어떻게 데이터를 통신하는가이다.\n가장 쉬운 방법은 싱글턴으로 처리된 Repository를 활용하는 것이거나, Activity 사이에 위치한 하나의 Repository를 활용하는 방법일 것이다. 보통은 Repository를 싱글턴으로 활용하니, 여기서도 동일하다고 이해하면 된다.\nRouter 코드\n먼저 우리가 활용할 Navigator는 인터페이스로 정의되어 있다. 구현체는 모두 숨겨져 있다.\n이때 구현체를 완전히 분리하고 싶다면, 인터페이스 모듈과 구현체 모듈을 두 개 만들어 활용하는 방식을 고려할 수 있다. 이 방식은 droidknights 2025 - Router에 포함되어 있으니 참고하길 바란다.\n\ninterface Navigator {\n\n    suspend fun <T : ActivityRoute> navigate(\n        activityRoute: KClass<T>,\n        argumentMap: Map<String, Parcelable> = emptyMap(),\n    )\n\n    suspend fun navigate(\n        navigationRoute: NavigationRoute,\n        saveState: Boolean = false,\n    )\n\n    suspend fun navigateBack()\n}\n\n\n위 코드에 대한 구현체는 다음과 같다.\nInternalNavigatorImpl은 Router 모듈 내부에서만 활용하기 위한 것으로, val channel: Channel<InternalRoute> 한 줄이 포함되어 있다.\n\n@ActivityRetainedScoped\ninternal class InternalNavigatorImpl @Inject constructor() : Navigator, InternalNavigator {\n\n    override val channel = Channel<InternalRoute>(Channel.BUFFERED)\n\n    // 아래에서 설명\n    override suspend fun <T : ActivityRoute> navigate(activityRoute: KClass<T>, argumentMap: Map<String, Parcelable>) {\n        channel.send(\n            InternalRoute.Activity(\n                activityRoute = activityRoute,\n                argumentMap = argumentMap,\n            )\n        )\n    }\n\n    override suspend fun navigate(navigationRoute: NavigationRoute, saveState: Boolean) {\n        channel.send(\n            InternalRoute.Navigation(\n                navigationRoute = navigationRoute,\n                saveState = saveState,\n            )\n        )\n    }\n\n    override suspend fun navigateBack() {\n        channel.send(InternalRoute.NavigateBack)\n    }\n}\n\n\n위 Router는 DI(Dependency Injection)를 통해 싱글턴으로 구성되어 있다.\n\n@Module\n@InstallIn(ActivityRetainedComponent::class)\ninternal abstract class RouterModel {\n\n    @Binds\n    @ActivityRetainedScoped\n    abstract fun provideNavigator(\n        navigator: InternalNavigatorImpl\n    ): Navigator\n\n    @Binds\n    @ActivityRetainedScoped\n    abstract fun provideInternalNavigator(\n        navigator: InternalNavigatorImpl\n    ): InternalNavigator\n}\n\n\nViewModel에서는 이를 활용해서, SideEffect로 View에서 collect할 수 있도록 구성되었으니, 기존 코드와 크게 다른 부분은 없다고 생각한다.\n\n@HiltViewModel\ninternal class InternalRouteViewModel @Inject internal constructor(\n    navigator: InternalNavigator,\n    private val journeyMapper: InternalActivityRouteMapper,\n) : ViewModel() {\n\n    val sideEffect by lazy(LazyThreadSafetyMode.NONE) {\n        navigator.channel.receiveAsFlow()\n            .map { router ->\n                when (router) {\n                    is InternalRoute.Activity<*> -> {\n                        journeyMapper.getJourneyOrNull(router.activityRoute)?.let {\n                            InternalRouteSideEffect.NavigateActivity(\n                                activityRoute = it,\n                                argumentMap = router.argumentMap,\n                            )\n                        }\n                    }\n\n                    is InternalRoute.Navigation -> {\n                        InternalRouteSideEffect.Navigate(\n                            navigationRoute = router.navigationRoute,\n                            saveState = router.saveState,\n                        )\n                    }\n\n                    is InternalRoute.NavigateBack -> {\n                        InternalRouteSideEffect.NavigateBack\n                    }\n                }\n            }\n            .filterNotNull()\n    }\n}\n\n\nView에서는\nView에서 SideEffect를 받아 처리하는 코드이다. 주로 화면 이동과 뒤로가기 이벤트를 처리할 수 있다.\nNavigateBack은 Activity와 Navigation 둘 다를 혼용 처리하기 위해 마지막 Entry가 있는지 한 번 더 체크하고 처리하도록 작성되었다.\n첫 화면에서는 보통 <나 X 버튼을 사용하지는 않지만, 적용은 해두었다.\n이 코드에서는 Navigation 처리에 대한 내용이므로 Activity 관련 코드는 제거되었다.\n\n@Composable\nprivate fun InternalLaunchedRouter(\n    navHostController: NavHostController? = null,\n    internalRouterViewModel: InternalRouteViewModel = viewModel(),\n) {\n    val activity = LocalActivity.current\n    val lifecycleOwner = LocalLifecycleOwner.current\n    LaunchedEffect(internalRouterViewModel, lifecycleOwner) {\n        lifecycleOwner.repeatOnLifecycle(Lifecycle.State.STARTED) {\n            internalRouterViewModel.sideEffect.collectLatest { sideEffect ->\n                when (sideEffect) {\n                    is InternalRouteSideEffect.NavigateBack -> {\n                        if (navHostController?.previousBackStackEntry != null) {\n                            navHostController.popBackStack()\n                        } else {\n                            activity?.finish()\n                        }\n                    }\n\n                    is InternalRouteSideEffect.Navigate -> {\n                        navHostController?.let { navigation ->\n                            navigation.navigate(sideEffect.navigationRoute) {\n                                navigation.graph.findStartDestination().route?.let {\n                                    popUpTo(it) {\n                                        saveState = sideEffect.saveState\n                                    }\n                                }\n                                restoreState = sideEffect.saveState\n                            }\n                        }\n                    }\n\n                    is InternalRouteSideEffect.NavigateActivity -> {\n                        activity?.startActivity(\n                            sideEffect.activityRoute.getActivity(activity).apply {\n                                sideEffect.argumentMap.entries.forEach { (key, value) ->\n                                    putExtra(key, value)\n                                }\n                            }\n                        )\n                    }\n                }\n            }\n        }\n    }\n}\n\n\n이 코드가 처음 소개된 Flow 다이어그램에 해당한다.\n\n정의는?\nNavigationRoute를 상속받아 Route를 구현하고, 이를 Navigation 화면에 적용해야 한다.\n\n@Serializable\nobject SettingsRoute : NavigationRoute\n\n\n그리고 연결할 지점에는 NavGraphBuilder를 통해 네비게이션에서 활용할 composable을 정의하면 된다.\n\nfun NavGraphBuilder.settingsNavGraph() {\n    composable<SettingsRoute> {\n        InternalSettingsScreen()\n    }\n}\n\n\n네비게이션을 활용하는 방법은 기존과 동일하므로, 여기서는 설명은 생략하고 일부 코드를 올려둔다.\n\nScaffold(\n    bottomBar = {\n        NavigationBar {\n            navigationUiState.navigation.forEach { navItem ->\n                NavigationBarItem(\n                    selected = navigationUiState.selectNav == navItem,\n                    onClick = {\n                        onClick(navItem)\n                    },\n                    // 생략\n                )\n            }\n        }\n    },\n    modifier = modifier\n) { innerPadding ->\n    Box(\n        modifier = Modifier\n            .padding(innerPadding)\n            .padding(horizontal = 10.dp)\n    ) {\n        NavHost( // 네비게이션\n            navController = navController,\n            startDestination = NavigationUiState.Default.selectNav.route,\n        ) {\n            searchNavGraph()\n            settingsNavGraph()\n        }\n    }\n}\n\n\nActivity\nActivity 라우팅은 Compose Navigation보다 조금 더 복잡할 수 있다.\n우선 다시 Router 코드로 돌아가서 중요한 부분만 살펴보자. 여기서는 DI인 Dagger를 활용했으니 Dagger를 활용한 방법으로 접근한다.\n먼저 RouterKey라는 MapKey를 정의했다. 이를 활용해서 클래스 정보를 Key/Value로 매핑할 수 있도록 한다.\n\n@MapKey\nannotation class RouteKey(\n    val value: KClass<out ActivityRoute>,\n)\n\n\n그리고 SomeActivityRouter를 정의하고, 이를 상속받아 구현한다. 여기서 getActivity()를 구현해서 Intent 정보를 함께 전달하게 된다.\n\ninternal class MainActivityRouteImpl @Inject constructor() : MainActivityRouter {\n\n    override fun getActivity(context: Context): Intent =\n        Intent(context, MainActivity::class.java)\n}\n\n\nDI를 활용해 이 RouterImpl이 MainActivityRouter임을 알려주기 위해 @IntoMap과 @RouteKey를 활용해서 처리한다.\n\n@Module\n@InstallIn(SingletonComponent::class)\ninternal abstract class MainModule {\n\n    @Binds\n    @IntoMap\n    @RouteKey(MainActivityRouter::class)\n    abstract fun bindMainActivityRoute(\n        mainActivityRoute: MainActivityRouteImpl,\n    ): ActivityRoute\n}\n\n\n위 코드는 KSP(Kotlin Symbol Processing)를 활용하면 더 쉽게 자동화할 수 있다. 어차피 수동으로 만들더라도 같은 패턴만 나오도록 만들면 되기 때문이다. 특히 Activity를 찾는 것이 중요하며, Intent 객체를 처음부터 만드는 것도 아니기에 이러한 접근이 가능하다.\nKSP 관련 글\nAndroid KSP(Kotlin Symbol Processing) 활용을 위한 준비단계! - link\nAndroid KSP(Kotlin Symbol Processing) 활용할 수 있는 샘플 코드 작업 - link\n코드를 실행하면 런타임에 아래의 map: Map<Class<out ActivityRoute>, @JvmSuppressWildcards ActivityRoute> 부분에서 ActivityRoute로 정의한 정보를 찾아 Map 형태로 전달하고 이를 활용할 수 있다.\n아쉽게도 KClass를 바로 활용하려고 했지만, 런타임에서 오류가 발생해서 Java Class를 활용하게 되었다.\n\n@Module\n@InstallIn(SingletonComponent::class)\ninternal object JourneyRouterModule {\n\n    @Provides\n    @Singleton\n    fun providerInternalActivityRouteMapper(\n        map: Map<Class<out ActivityRoute>, @JvmSuppressWildcards ActivityRoute>,\n    ): InternalActivityRouteMapper =\n        InternalActivityRouteMapper(map)\n}\n\n\n마지막으로 접근해야 할 정보는 getJourneyOrNull에서 찾아서 startActivity할 수 있다.\n\n@Singleton\ninternal class InternalActivityRouteMapper @Inject constructor(\n    @get:VisibleForTesting val mapper: Map<Class<out ActivityRoute>, ActivityRoute>,\n) {\n\n    /**\n     * Find and return ActivityRoute from the data stored in Mapper.\n     */\n    internal fun getJourneyOrNull(journeyKClass: KClass<*>): ActivityRoute? =\n        synchronized(mapper) {\n            mapper[journeyKClass.java]\n        }\n}\n\n\nActivity라서\nCompose Navigation에서는 @Serializable object SettingsRoute : NavigationRoute와 같이 @Serializable 데이터 클래스만 정의하면 동작에 문제가 없고, 필요한 데이터도 간단하게 넘겨줄 수 있다.\n하지만 Activity는 필요한 데이터를 넘기기 위해 별도의 arguments를 사용해야 한다는 차이점이 있다.\n그래서 Navigator 인터페이스에 아래와 같이 arguments를 받을 수 있도록 추가해두었다.\n\ninterface Navigator {\n\n    suspend fun <T : ActivityRoute> navigate(\n        activityRoute: KClass<T>,\n        argumentMap: Map<String, Parcelable> = emptyMap(),\n    )\n\n    suspend fun navigate(\n        navigationRoute: NavigationRoute,\n        saveState: Boolean = false,\n    )\n}\n\n\n데이터 전달을 위함이니 Parcelable로 제한해두었다.\nNavigator를 활용하면 getJourneyOrNull 부분을 ViewModel에서 처리하고있음을 아래와 같이 확인할 수 있다.\n\n@HiltViewModel\ninternal class InternalRouteViewModel @Inject internal constructor(\n    navigator: InternalNavigator,\n    private val journeyMapper: InternalActivityRouteMapper,\n) : ViewModel() {\n\n    val sideEffect by lazy(LazyThreadSafetyMode.NONE) {\n        navigator.channel.receiveAsFlow()\n            .map { router ->\n                when (router) {\n                    is InternalRoute.Activity<*> -> {\n                        journeyMapper.getJourneyOrNull(router.activityRoute)?.let {\n                            InternalRouteSideEffect.NavigateActivity(\n                                activityRoute = it,\n                                argumentMap = router.argumentMap,\n                            )\n                        }\n                    }\n                }\n            }\n            .filterNotNull()\n    }\n}\n\n\n이 코드 부분을 도식화한 부분이 아래와 같다.\n\n정리하면\nView에서 화면 이동 이벤트가 발생한다.\nViewModel에서 Navigator를 통해 Activity/Compose Navigation으로 이벤트를 이동 요청한다.\n    \nActivity 이동: DI를 통해 주입받은 ActivityRoute 정보를 자동으로 매핑하여 InternalActivityRouteMapper에 사전 보관된 정보와 매칭하여 startActivity를 호출한다.\nNavigation 이동: Compose Navigation을 통해 Map에 포함되어있을 경우 이동한다.\n남은 작업은 다음과 같다.\nActivityResult 처리\nNavigationResult 처리\n사용법\n앞서 소개한 Action과 다르게 Router는 Activity를 기반하여 동작한다. A/B Activity가 있다고 하더라도, InternalActivityRouteMapper는 싱글턴이지만 실제 동작은 서로 무관하게 동작한다.\n그래서 각 Activity마다 아래와 같은 코드를 선택적으로 주입해 주어야 한다. 싱글 액티비티 구조라면 당연히 하나면 충분하지만, 서브 네비게이션에 대한 구조를 적용하지 않아서 현재는 모든 경우에 대응되지는 않는다.\n\nval navHostController = rememberNavController()\nLaunchedRouter(navHostController)\n\n// or\nLaunchedRouter() // navHostController default null\n\n\nNext\nAction을 새로 짜면서 Router도 몇 번의 수정이 있었다.\n기존 Action 방식에서의 문제점은 다음과 같았다.\nA/B/C 화면 어디서든 동일한 이벤트인 뒤로가기(back) 이벤트를 받는다면, 모든 화면에서 action을 처리해버리고 원치 않는 종료가 일어날 수 있다는 점이었다. 사이드 이펙트를 통해 막을 수는 있었지만 근본적인 문제 해결도 아니고, 오히려 복잡성을 높이는 코드였기에 수정되면서 원래 사용하던 방식으로 되돌아갔다는 점이다.\n더 나아가 네비게이션에서만 Router를 활용하고 싶다면 드로이드 나이츠 코드를 참고해보면 좋을 것 같다.\ndroidknights 2025 - Router\n작성 글 이어보기\n1부 - 컴포즈에 사용할 Composable Architecture 설명(리엑트?)\n2부 - Composable Architecture는 만들었는데 문제가 있었네? 개선해보자.\n3부 - Composable Architecture에서는 Alert/Toast는 어떻게 사용할 수 있는가?\n다시 - Composable Architecture 설계 변경",
    "reviews": [],
    "syllabus": [],
    "link": "https://thdev.tech/architecture/2025/06/02/Android-Architecture-new-02/",
    "pubDate": "Mon, 02 Jun 2025 00:00:00 +0000",
    "creator": "Unknown",
    "categories": []
  },
  {
    "id": 4,
    "imageUrl": "",
    "title": "앱 개발 개척시대",
    "description": "AI 시대에는 개발자가 유리한가?\n다 똑같다고 생각합니다. 모두가 같은 출발선.\n제가 공부한 모든 컴퓨터 공학 지식의 가치를 0으로 설정했습니다.\n서부 개척시대에서 요이땅 하고 땅따먹기를 하러 가는 느낌입니다.\n어쩌면 이게 제 개발자 경력의 마지막은 아닐까?\n\n\n함께 읽으면 좋은 글:\n1인 개발자 전성시대\n1인 개발이란 전쟁터에서 혼자 살아남는 것",
    "reviews": [],
    "syllabus": [],
    "link": "https://jeho.page/essay/2025/06/05/at-the-app-development-race.html",
    "pubDate": "2025-06-05T02:44:00.000Z",
    "creator": "Unknown",
    "categories": []
  },
  {
    "id": 5,
    "imageUrl": "",
    "title": "누구를 위한 법이고, 누구를 위한 규제인가?",
    "description": "우리가 투자한 회사 중 콘택트렌즈 사업을 하는 옵틱라이프라는 곳이 있다. 이 회사는 본인들이 직접 콘택트렌즈를 제조하고, 다른 회사의 제품 또한 유통하고 있는데, 한국은 콘택트렌즈의 온라인 판매가 법으로 금지되어 있어서, 옵틱라이프에서 고객이 원하는 렌즈에 대한 결제를 하고, 실제 픽업은 전국 가맹점 중 하나에서 한다. 가맹 안경점은 특별한 회비나 수수료는 내지 않고, 옵틱라이프가 구매 건당 이들에게 수수료를(...)",
    "reviews": [],
    "syllabus": [],
    "link": "https://www.thestartupbible.com/2025/06/losers-will-be-losers.html",
    "pubDate": "Sun, 01 Jun 2025 21:33:00 +0000",
    "creator": "Kihong Bae",
    "categories": [
      "Uncategorized",
      "B2C",
      "FoundersAtWork",
      "healthcare",
      "internet",
      "korea",
      "regulation",
      "Strong"
    ]
  },
  {
    "id": 6,
    "imageUrl": "",
    "title": "Java Annotated Monthly – June 2025",
    "description": "Hi there, Java fans! It’s a new month, which means we’ve got a new batch of hot news, deep dives, and tasty tidbits from the Java world for you to enjoy. In this edition, Piotr Przybył joins us in the Featured Content section to share his cultivated list of content finds. We’re also testing a […]",
    "reviews": [],
    "syllabus": [],
    "link": "https://blog.jetbrains.com/idea/2025/06/java-annotated-monthly-june-2025/",
    "pubDate": "Fri, 06 Jun 2025 14:07:43 +0000",
    "creator": "Irina Mariasova",
    "categories": [
      "news",
      "ai",
      "java",
      "java-annotated-monthly"
    ]
  },
  {
    "id": 7,
    "imageUrl": "",
    "title": "대환대출로 신용대출 이자 줄일 수 있어요",
    "description": "이자 부담 줄일 수 있는 신용대출 갈아타기",
    "reviews": [],
    "syllabus": [],
    "link": "https://toss.im/tossfeed/article/tossmoment-11",
    "pubDate": "Thu, 05 Jun 2025 06:31:00 GMT",
    "creator": "Unknown",
    "categories": []
  },
  {
    "id": 8,
    "imageUrl": "",
    "title": "AI 리서치 끝판왕 - 제미나이 딥리서치 GEM 지침 배포 (무료 시각화까지!)",
    "description": "✨ 제미나이 딥리서치, GEM 지침으로 날개를 달아보세요! 여러분의 AI 활용 수준을 한 단계 업그레이드할 비밀 병기, GEM 지침을 소개합니다. 간단한 질문만으로는 얻을 수 없었던 전문가 수준의 분석 보고서를 직접 경험해보세요!\n\n\n \n혹시 AI에게 질문했을 때, \"음... 뭔가 알맹이가 빠진 것 같은데?\" 혹은 \"출처가 어디지?\" 하며 고개를 갸웃했던 경험, 다들 한 번쯤 있으시죠? 특히 과제나 보고서처럼 정확하고 깊이 있는 정보가 필요할 때는 더욱 답답함을 느끼셨을 거예요. 최신 정보, 실시간 데이터, 신뢰할 수 있는 출처에 기반한 분석. 이게 바로 우리가 AI에게 진짜 원하는 거잖아요!\n \n최근 ChatGPT, Claude, Gemini 같은 AI 모델들이 앞다투어 '딥리서치' 기능을 선보이는 이유도 바로 여기에 있습니다. 단순히 웹 검색 결과를 나열하는 것을 넘어, 실시간으로 웹을 탐색하고, 최신 논문을 찾고, 신뢰할 수 있는 데이터를 수집해 분석까지 해주는 정말 똑똑한 기능이죠. 하지만 이 강력한 딥리서치 기능도 제대로 활용하지 못하면 무용지물이랍니다. 오늘 제가 그 봉인 해제 방법을 알려드릴게요!  \n딥리서치, 왜 그냥 검색이랑 다를까요?  \n\"딥리서치? 그거 그냥 구글링 잘하는 거 아니야?\" 라고 생각하실 수도 있지만, 전혀 그렇지 않아요! 일반 웹 검색은 단순히 정보를 찾아 보여주는 데 그치지만, 딥리서치는 마치 숙련된 연구원처럼 정보를 탐색, 검증, 요약하고 비판적으로 재구성하는 과정을 거칩니다. 훨씬 더 깊이 있고 신뢰도 높은 결과물을 만들어내는 거죠.\n \n하지만 대부분의 사람들이 AI에게 \"○○에 대해 조사해줘\"처럼 너무 막연하게 질문하는 경우가 많아요. 이렇게 하면 딥리서치 기능을 켜도 AI가 어떤 관점에서, 무엇을 중점적으로 찾아야 할지 몰라 표면적인 정보만 가져오게 됩니다. 실제로 제가 간단한 프롬프트로 딥리서치를 요청했을 때는 일반화된 문장과 출처 불명의 정보만 얻었지만, 제가 개발한 GEM 지침을 활용한 구조화된 프롬프트를 사용했을 때는 실제 수치, 표, 그래프, 논문 인용까지 포함된 증거 기반의 답변을 받을 수 있었어요. 엄청난 차이죠?\n  알아두세요! 딥리서치의 작동 과정\n딥리서치는 다음과 같은 체계적인 단계를 거쳐요:\n의도 파악 및 쿼리 분해: 사용자의 복잡한 요청을 구체적인 질문들로 나눕니다.\n문서 검색: 웹, 학술 데이터베이스, API 등 다양한 소스에서 정보를 찾습니다.\n근거 중심 응답 구성: 수집된 정보를 바탕으로 신뢰할 수 있는 답변을 만듭니다.\n출처 명시 및 비판적 요약: 정보의 출처를 명확히 밝히고, 비판적인 시각에서 요약합니다.\n응답 구조화: 사용자가 이해하기 쉽도록 답변을 체계적으로 정리합니다.\n마법의 열쇠, GEM 지침이란 무엇일까요?  \n이 놀라운 차이를 만드는 것이 바로 제가 개발한 GEM 지침입니다! 여기서 GEM은 Google Gemini에서 제공하는 '사용자 맞춤형 AI 비서' 기능을 의미해요. 즉, 여러분만의 똑똑한 AI 어시스턴트를 만들 수 있는 기능이죠. 물론 이 GEM 지침은 ChatGPT나 Claude 같은 다른 AI 모델에서도 활용할 수 있지만, 제미나이의 GEM 기능과 함께 사용할 때 그 효과가 극대화된답니다.\n \n제미나이가 딥리서치에 특히 강력한 이유는 제미나이의 뛰어난 멀티모달 능력과 강력한 추론 능력 덕분이에요. 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 이해하고 분석할 수 있어서, 딥리서치를 통해 수집된 방대한 정보를 훨씬 더 심층적으로 파악하고 숨겨진 통찰까지 찾아낼 수 있죠. 게다가 제미나이의 압도적으로 큰 컨텍스트 윈도우(한 번에 기억하고 처리할 수 있는 정보의 양)와 효율적인 토큰 처리 능력은 딥리서치 과정의 효율성을 엄청나게 높여준답니다!  \n \nGEM 지침의 핵심 원리 dissected  \nGEM 지침이 어떻게 작동하는지 궁금하시죠? 크게 세 가지 핵심 원리가 있어요.\n응답 형식 준수: 일반적인 AI처럼 바로 답변을 내놓는 게 아니라, AI가 \"당신의 질문을 이렇게 분석했고, 이런 확장 기법을 적용해서 더 완벽한 프롬프트를 만들 거예요!\" 라고 먼저 설명해줘요. 그리고 나서 완전히 새로운 수준의 확장된 프롬프트를 생성해줍니다.\n확장 프롬프트의 내용 구성: 여기가 진짜 핵심인데요! 여러분의 단순한 질문을 세 가지 차원에서 확장시켜요.\n\n컨텍스트 설정: 시간적 맥락(예: \"2025년 5월 31일 현재\"), 현재 상황, 기술적 배경 등을 명시해서 AI가 최신 데이터를 우선적으로 탐색하도록 유도해요.\n목적 명시: 단순히 \"분석해줘\"가 아니라 \"무엇을 위해, 어떤 관점에서, 어떤 결과물을 원하는지\" 아주 구체적으로 알려주는 거죠.\n데이터 소스 지정: 어떤 정보원을 참고할지 (예: 한국언론진흥재단, 정보통신정책연구원), 어떤 형태의 시각자료가 필요한지까지 콕 집어 요청합니다.\n확장 기법 적용 (다차원 분석 요청): 이게 정말 중요해요! 예를 들어 \"블로그와 유튜브의 사회적 영향력\"이라는 주제라면, GEM 지침은 이 질문을 아래와 같이 여러 영역으로 나누어 심층 분석을 요청해요.\n\n정보 확산 및 여론 형성\n사회/문화적 영향\n경제적 파급효과\n윤리적 및 법적 고려사항\n미래 전망 및 제언\nGEM 지침, 어떻게 등록하고 사용할까요?  \n자, 그럼 이 강력한 GEM 지침을 어떻게 사용하냐고요? 아주 간단해요! 제가 영상 설명란과 고정 댓글에 남겨둔 링크를 통해 제 블로그로 오시면 GEM 지침이 담긴 텍스트 파일을 다운로드하실 수 있어요. 그 다음 Gemini에 접속하셔서 화면 왼쪽 메뉴의 \"GEM 관리자\"로 이동해 다운받은 지침을 등록하시면 끝! 자세한 등록 방법은 이전 영상을 참고해주세요.  \n \n일단 '리서치 전문가 GEM 지침'을 등록하셨다면, 이제 여러분이 할 일은 간단한 주제만 입력하는 거예요. 예를 들어, \"블로그와 유튜브의 사회적 영향력을 분석해줘\" 라고 입력하면, GEM 지침이 알아서 이 주제에 최적화된, 아주 상세하고 구조화된 리서치용 프롬프트를 뚝딱 만들어낼 거예요. 정보 확산 방식부터 경제적 파급 효과, 윤리적 쟁점, 미래 전망까지 다각도로 분석하고, 한국언론진흥재단이나 정보통신정책연구원 같은 신뢰도 높은 데이터 소스를 명시하며, 2020년 이후 최신 트렌드에 집중한다는 시간적 범위까지 설정해주는 식이죠.\n \n이렇게 잘 짜인 프롬프트는 단순한 질문이 아니라, AI에게 어떤 각도에서 얼마나 깊이 있게 분석해야 하는지 명확한 가이드라인을 제공하는 체계적인 연구 설계서나 마찬가지랍니다! 그 결과, 표면적인 정보 나열이 아니라 구체적인 데이터와 사례, 비판적 관점, 미래 전망까지 담긴 완성도 높은 종합 분석 보고서를 받아보실 수 있는 거예요. 프롬프트 설계에 쏟는 시간이 결과물의 질을 완전히 바꿔놓는 마법을 경험하게 되실 겁니다!\n  GEM 지침 활용 꿀팁!\n- 어떤 주제든 GEM 지침에 넣어보세요! 학술 연구, 시장 분석, 기술 동향 보고서 등 무궁무진하게 활용 가능해요.\n- 생성된 확장 프롬프트를 그대로 사용해도 좋지만, 필요에 따라 조금씩 수정해서 사용하면 더욱 맞춤화된 결과를 얻을 수 있어요.\n- AI 기술은 계속 발전하니, GEM 지침도 주기적으로 업데이트하면 좋겠죠? (제 블로그를 주목해주세요!)\n제미나이 딥리서치, 이렇게 강력합니다! ✨\nGEM 지침으로 생성된 상세 프롬프트를 복사해서, 제미나이 채팅창 하단의 '딥리서치' 버튼을 활성화한 후 붙여넣고 실행하면 끝! 잠시 후 제미나이가 리서치 계획을 보여줄 거예요. 이때 \"계획 수정\"을 눌러 세부 항목을 조정하거나, 바로 \"연구 시작\"을 눌러 리서치를 진행할 수 있습니다.\n딥리서치는 주제와 관련된 수많은 출처를 찾아 분석하고 종합하는 과정이 필요해서, 보고서가 완성되기까지 보통 10분 이상 소요될 수 있어요. (프로 모델에서는 더 많은 시간이 걸리기도 하고요!) 하지만 기다린 만큼의 가치가 충분하답니다. 커피 한 잔 하시면서 전문가 수준의 보고서가 탄생하는 과정을 지켜보세요! ☕\n⚠️ 딥리서치 사용 시 참고하세요!\n- 딥리서치는 방대한 정보를 처리하므로 일반적인 답변보다 시간이 더 오래 걸릴 수 있습니다. 인내심을 가져주세요!\n- 무료 버전과 유료 프로 모델 간에는 분석의 깊이나 속도에 차이가 있을 수 있습니다.\n- AI가 생성한 정보는 항상 교차 검증하고 비판적으로 수용하는 자세가 중요합니다.\n완성된 보고서, 시각화로 날개를 달다!  \n자, 드디어 딥리서치가 완료되면 정말 놀라운 보고서가 눈앞에 펼쳐질 거예요! 하지만 여기서 끝이 아니랍니다. 제미나이는 이 보고서를 더욱 빛나게 해줄 강력한 시각화 기능을 제공해요. 심지어 이 기능, 젠스파크 같은 다른 서비스에서는 크레딧을 소모해야 하지만 제미나이에서는 무료 플랜에서도 마음껏 사용할 수 있다는 사실!  \n \n보고서 오른쪽 상단의 '만들기' 버튼을 누르면 웹페이지나 인포그래픽 형태로 바로 변환할 수 있어요. 별도의 복잡한 프롬프트 없이 클릭 몇 번만으로 전문가가 만든 것 같은 시각자료가 뚝딱! 개인적으로는 젠스파크에서 Claude 기반으로 만든 시각화 결과물보다 제미나이 쪽이 더 깔끔하고 뛰어나다는 생각이 들더라고요. (여러분도 한번 비교해보세요!)\n \n물론, 시각화된 자료 외에 보고서 원문도 필요하겠죠? 제미나이 딥리서치 결과물은 텍스트로 복사하거나 구글 DOCS로 바로 내보낼 수 있어서, PDF를 포함한 다양한 형태로 저장하고 활용하기에도 정말 편리하답니다.\n✨ 시각화 마법 살짝 엿보기! (상상도)  ️\n웹페이지로 변신!  \n여러분의 딥리서치 보고서가 항목별로 깔끔하게 정리된 인터랙티브 웹페이지로 재탄생합니다. 스크롤하며 주요 내용을 확인하고, 관련 그래프나 표를 바로 볼 수 있어요. 마치 잘 만들어진 온라인 기사처럼 가독성이 뛰어나답니다!\n한눈에 쏙! 인포그래픽  \n복잡한 데이터와 분석 내용이 핵심만 담긴 세련된 인포그래픽으로 요약됩니다. 다양한 차트와 아이콘, 강조 색상을 활용해 정보 전달력을 극대화하죠. 발표 자료나 SNS 공유용으로도 안성맞춤!\n핵심만 쏙쏙! GEM 지침으로 딥리서치 역량 UP!  \n오늘 정말 많은 이야기를 나눴는데요, 핵심만 다시 한번 정리해볼까요?\n단순 검색은 이제 그만! 깊이 있는 분석과 통찰을 원한다면 딥리서치가 정답입니다.\nGEM 지침으로 AI를 조련하세요! 여러분의 생각을 정확히 반영한 맞춤형 고급 프롬프트를 생성하여 AI의 잠재력을 100% 끌어낼 수 있습니다.\n제미나이의 강력한 기능들을 활용하세요! 멀티모달 분석, 방대한 컨텍스트 처리, 그리고 무료 시각화 기능까지! 제미나이는 딥리서치를 위한 최고의 파트너입니다.\n결과물 활용도 UP! 전문가 수준의 보고서는 물론, 웹페이지, 인포그래픽 등 다양한 형태로 결과물을 활용하여 여러분의 과제나 업무 성과를 극대화하세요.\n이제 여러분도 AI를 단순한 검색 도구가 아닌, 진정한 연구 파트너로 활용하실 수 있을 거예요!\n \n \nGEM 지침: 딥리서치 초격차!\n✨ 맞춤형 프롬프트: GEM 지침이 단순 질문을 전문가급 리서치 설계도로 변환!\n  심층 분석 가능: 다차원 분석 요청으로 AI가 숨겨진 통찰까지 도출!\n⚙️ 리서치 효율 극대화:\n시간 절약 + 정보 품질 UP = 생산성 혁신!\n  전문가급 결과물: A+ 과제, 승진 보고서도 문제 없어요!\nGEM 지침으로 제미나이 딥리서치의 모든 가능성을 경험하세요!\n오늘 여러분께 소개해 드린 GEM 지침은 제미나이의 딥리서치 기능을 단순히 활용하는 수준을 넘어, 완전히 새로운 차원으로 끌어올릴 수 있는 강력한 도구입니다. 이제 단 몇 초 만에 전문가 수준의 프롬프트를 작성하고, 그 결과로 대학 논문이나 회사 보고서에 즉시 활용할 수 있는 고품질 분석 자료를 얻을 수 있게 되었습니다. 특히 다른 AI 서비스에서는 유료로 제공하는 시각화 기능을 제미나이에서는 무료로 사용할 수 있다는 점, 정말 놀랍지 않나요?  \n \n\"AI가 별로야\", \"내가 원하는 결과가 안 나와\" 라고 생각하셨던 분들도 이제는 그런 생각을 버리셔도 좋습니다. 문제는 AI가 아니라 우리가 어떻게 질문하느냐에 있었던 것이니까요! 혹시 지침 적용 과정에서 어려움이 있으시다면 오픈톡방에 남겨주세요. 최대한 빨리 도와드리겠습니다.  \n \n이런 AI 활용 꿀팁들이 더 필요하시다면, 지금 바로 구독과 좋아요 버튼을 꾹! 눌러주세요. 알림까지 켜두시면 앞으로 제가 공개할 더 많은 AI 활용 전략들을 가장 먼저 만나보실 수 있습니다. 여러분의 AI 활용 여정에 제 영상이 도움이 되었으면 좋겠습니다. 그럼 다음 영상에서 더 놀라운 내용으로 찾아뵙겠습니다. 감사합니다!  \n \n자주 묻는 질문 ❓\nQ: GEM 지침은 제미나이에서만 사용할 수 있나요?\nA: 아니요, GEM 지침의 원리는 ChatGPT나 Claude 등 다른 LLM에서도 충분히 활용 가능합니다. 하지만 제미나이의 GEM 기능(사용자 맞춤형 AI 비서)과 결합했을 때 가장 강력한 시너지를 내며, 특히 제미나이의 멀티모달 처리 능력과 방대한 컨텍스트 윈도우는 딥리서치에 최적화되어 있습니다.\nQ: 딥리서치 결과는 항상 100% 정확하고 최신 정보인가요?\nA: AI가 생성하는 정보는 매우 유용하지만, 항상 100% 완벽하다고 할 수는 없습니다. 딥리서치는 최신 정보를 포함하려고 노력하지만, 정보의 편향성이나 오류 가능성은 여전히 존재합니다. 따라서 중요한 결정이나 학술적 목적으로 사용할 경우, 반드시 여러 출처를 통해 교차 검증하고 비판적으로 검토하는 자세가 필요합니다.\nQ: GEM 지침을 직접 만들거나 수정하는 것이 어렵지 않을까요?\nA: 제가 제공하는 GEM 지침 템플릿을 활용하시면 누구나 쉽게 시작할 수 있습니다! 처음에는 제공된 지침을 그대로 사용해보시고, 익숙해지면 자신의 필요에 맞게 특정 데이터 소스를 추가하거나 분석 관점을 수정하는 등 커스터마이징 해보시는 것을 추천합니다. 혹시 어려움이 있으시면 언제든 오픈톡방에 질문해주세요!\nQ: 딥리서치와 일반 웹 검색의 가장 큰 차이점은 무엇인가요?\nA: 가장 큰 차이는 분석의 깊이와 정보의 질입니다. 일반 검색은 키워드에 맞는 정보를 찾아 나열하는 수준이라면, 딥리서치는 여러 정보를 종합하고, 검증하며, 다양한 관점에서 분석하여 새로운 통찰을 도출하려고 합니다. 마치 연구원이 연구를 수행하는 과정과 유사하다고 볼 수 있습니다.\nQ: 제미나이에서 제공하는 시각화 기능에는 어떤 종류가 있나요?\nA: 현재 제미나이는 딥리서치 결과를 바탕으로 웹페이지 형태와 인포그래픽 형태로 시각화하는 기능을 제공합니다. 웹페이지는 보고서 내용을 구조적으로 보여주며, 인포그래픽은 핵심 정보를 차트나 그림으로 요약하여 한눈에 파악하기 쉽게 만들어줍니다. 이 기능들은 모두 무료 플랜에서도 사용 가능합니다!\nGEM 무료 다운로드❓\n반응형\n\n    \n    (adsbygoogle = window.adsbygoogle || []).push({});\n  \n\n    \n\n    \nGemini를 위한 딥리서치 확장 프롬프트 생성 최적화 지침.zip\n0.00MB\n\n\n \n\n\n\n \n\n    {\n        \"@context\": \"https://schema.org\",\n        \"@type\": \"FAQPage\",\n        \"mainEntity\": [\n            {\n                \"@type\": \"Question\",\n                \"name\": \"GEM 지침은 제미나이에서만 사용할 수 있나요?\",\n                \"acceptedAnswer\": {\n                    \"@type\": \"Answer\",\n                    \"text\": \"  아니요, GEM 지침의 원리는 ChatGPT나 Claude 등 다른 LLM에서도 충분히 활용 가능합니다. 하지만 제미나이의 GEM 기능(사용자 맞춤형 AI 비서)과 결합했을 때 가장 강력한 시너지를 내며, 특히 제미나이의 멀티모달 처리 능력과 방대한 컨텍스트 윈도우는 딥리서치에 최적화되어 있습니다.\"\n                }\n            },\n            {\n                \"@type\": \"Question\",\n                \"name\": \"딥리서치 결과는 항상 100% 정확하고 최신 정보인가요?\",\n                \"acceptedAnswer\": {\n                    \"@type\": \"Answer\",\n                    \"text\": \"  AI가 생성하는 정보는 매우 유용하지만, 항상 100% 완벽하다고 할 수는 없습니다. 딥리서치는 최신 정보를 포함하려고 노력하지만, 정보의 편향성이나 오류 가능성은 여전히 존재합니다. 따라서 중요한 결정이나 학술적 목적으로 사용할 경우, 반드시 여러 출처를 통해 교차 검증하고 비판적으로 검토하는 자세가 필요합니다.\"\n                }\n            },\n            {\n                \"@type\": \"Question\",\n                \"name\": \"GEM 지침을 직접 만들거나 수정하는 것이 어렵지 않을까요?\",\n                \"acceptedAnswer\": {\n                    \"@type\": \"Answer\",\n                    \"text\": \"  제가 제공하는 GEM 지침 템플릿을 활용하시면 누구나 쉽게 시작할 수 있습니다! 처음에는 제공된 지침을 그대로 사용해보시고, 익숙해지면 자신의 필요에 맞게 특정 데이터 소스를 추가하거나 분석 관점을 수정하는 등 커스터마이징 해보시는 것을 추천합니다. 혹시 어려움이 있으시면 언제든 오픈톡방에 질문해주세요!\"\n                }\n            },\n            {\n                \"@type\": \"Question\",\n                \"name\": \"딥리서치와 일반 웹 검색의 가장 큰 차이점은 무엇인가요?\",\n                \"acceptedAnswer\": {\n                    \"@type\": \"Answer\",\n                    \"text\": \"  가장 큰 차이는 분석의 깊이와 정보의 질입니다. 일반 검색은 키워드에 맞는 정보를 찾아 나열하는 수준이라면, 딥리서치는 여러 정보를 종합하고, 검증하며, 다양한 관점에서 분석하여 새로운 통찰을 도출하려고 합니다. 마치 연구원이 연구를 수행하는 과정과 유사하다고 볼 수 있습니다.\"\n                }\n            },\n            {\n                \"@type\": \"Question\",\n                \"name\": \"제미나이에서 제공하는 시각화 기능에는 어떤 종류가 있나요?\",\n                \"acceptedAnswer\": {\n                    \"@type\": \"Answer\",\n                    \"text\": \"  현재 제미나이는 딥리서치 결과를 바탕으로 웹페이지 형태와 인포그래픽 형태로 시각화하는 기능을 제공합니다. 웹페이지는 보고서 내용을 구조적으로 보여주며, 인포그래픽은 핵심 정보를 차트나 그림으로 요약하여 한눈에 파악하기 쉽게 만들어줍니다. 이 기능들은 모두 무료 플랜에서도 사용 가능합니다!\"\n                }\n            }\n        ]\n    }",
    "reviews": [],
    "syllabus": [],
    "link": "http://muzbox.tistory.com/483602",
    "pubDate": "Sun, 1 Jun 2025 12:43:34 +0900",
    "creator": "어떤오후의 프리웨어 이야기",
    "categories": [
      "AI, 미래기술/AI 챗봇 및 지침 무료 배포",
      "ai 리서치",
      "ai 시각화",
      "AI 활용법",
      "gem 지침",
      "구글 제미나이",
      "데이터 분석 ai",
      "보고서 작성 ai",
      "사용자 맞춤형 ai",
      "제미나이 딥리서치",
      "프롬프트 엔지니어링"
    ]
  },
  {
    "id": 9,
    "imageUrl": "",
    "title": "Cloud Academy: Unlock Your Azure Skills and Accelerate Your Career",
    "description": "When we launched the Cloud Academy benefit for Visual Studio Professional and Enterprise subscribers back in March 2025, our goal was simple: give you the hands-on, practical learning experience you need to confidently master Azure and cloud technologies — without spending a dime beyond your subscription. Why? Because in today’s fast-changing tech world, knowing theory […]\nThe post Cloud Academy: Unlock Your Azure Skills and Accelerate Your Career appeared first on Visual Studio Blog.",
    "reviews": [],
    "syllabus": [],
    "link": "https://devblogs.microsoft.com/visualstudio/cloud-academy-benefit-for-visual-studio-subscribers/",
    "pubDate": "Tue, 03 Jun 2025 14:00:08 +0000",
    "creator": "Jim Harrer",
    "categories": [
      "Visual Studio"
    ]
  },
  {
    "id": 10,
    "imageUrl": "",
    "title": "[MOBILE] 그래 퍼드는 원래 재미있는 게임이었지, 퍼즐앤드래곤 제로",
    "description": "No description available",
    "reviews": [],
    "syllabus": [],
    "link": "https://bbs.ruliweb.com/news/board/11/read/2316",
    "pubDate": "Thu, 05 Jun 2025 14:19:53 +0900",
    "creator": "［RULIWEB］",
    "categories": [
      "리뷰"
    ]
  },
  {
    "id": 11,
    "imageUrl": "",
    "title": "악역영애 4컷 만화 - 5화, 등교 시작인데스와~★",
    "description": "No description available",
    "reviews": [],
    "syllabus": [],
    "link": "https://bbs.ruliweb.com/news/board/11/read/2315",
    "pubDate": "Wed, 04 Jun 2025 19:09:01 +0900",
    "creator": "｜RULIWEB｜",
    "categories": [
      "웹툰"
    ]
  },
  {
    "id": 12,
    "imageUrl": "",
    "title": "dotInsights | June 2025",
    "description": "Did you know? The Original Name of .NET Was “Next Generation Windows Services (NGWS)“. Before Microsoft officially named it “.NET,” the platform was internally referred to as NGWS: Next Generation Windows Services. The name “.NET” was adopted in the late 1990s to emphasize the platform’s focus on web-based development and interoperability, as opposed to being […]",
    "reviews": [],
    "syllabus": [],
    "link": "https://blog.jetbrains.com/dotnet/2025/06/03/dotinsights-june-2025/",
    "pubDate": "Tue, 03 Jun 2025 13:31:32 +0000",
    "creator": "Rachel Appel",
    "categories": [
      "net-tools",
      "dotinsights"
    ]
  },
  {
    "id": 13,
    "imageUrl": "",
    "title": "Context Collection Competition by JetBrains and Mistral AI",
    "description": "Build smarter code completions and compete for a share of USD 12,000! In AI-enabled IDEs, code completion quality heavily depends on how well the IDE understands the surrounding code – the context. That context is everything, and we want your help to find the best way to collect it. Join JetBrains and Mistral AI at […]",
    "reviews": [],
    "syllabus": [],
    "link": "https://blog.jetbrains.com/ai/2025/06/context-collection-competition/",
    "pubDate": "Mon, 02 Jun 2025 12:13:43 +0000",
    "creator": "Dmitry Ustalov",
    "categories": [
      "jetbrains-ai",
      "research",
      "news"
    ]
  },
  {
    "id": 14,
    "imageUrl": "",
    "title": "What’s Next for RubyMine",
    "description": "Hello everyone! The RubyMine 2025.2 Early Access Program is already available! In this blog post, we’ll share the upcoming features and updates planned for this release cycle. What’s coming in RubyMine 2025.2? Debugger improvements We’re introducing a number of changes aimed at enhancing the debugger installation experience. The entire process will now take less time, […]",
    "reviews": [],
    "syllabus": [],
    "link": "https://blog.jetbrains.com/ruby/2025/06/what-s-next-for-rubymine/",
    "pubDate": "Tue, 03 Jun 2025 07:55:54 +0000",
    "creator": "Alexey Varfolomeev",
    "categories": [
      "eap",
      "rubymine",
      "new-features"
    ]
  },
  {
    "id": 15,
    "imageUrl": "",
    "title": "Visual Studio Code 1.98의 AI 기능",
    "description": "No description available",
    "reviews": [],
    "syllabus": [],
    "link": "https://jacking75.github.io/ai-github_copilot_20250607/",
    "pubDate": "Sat, 07 Jun 2025 00:00:00 +0900",
    "creator": "Unknown",
    "categories": []
  }
]