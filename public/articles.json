[
  {
    "id": 1,
    "imageUrl": "",
    "title": "코드 품질 개선 기법 26편: 설명의 핵심은 첫 문장에 있다",
    "description": "이 글은 2024년 5월 23일에 일본어로 먼저 발행된 기사를 번역한 글입니다.LY Corporation은 높은 개발 생산성을 유지하기 위해 코드 품질 및 개발 문화 개선에 힘쓰고...",
    "reviews": [],
    "syllabus": [],
    "link": "https://techblog.lycorp.co.jp/ko/techniques-for-improving-code-quality-26",
    "pubDate": "Fri, 12 Dec 2025 02:00:00 GMT",
    "creator": "Unknown",
    "categories": []
  },
  {
    "id": 2,
    "imageUrl": "",
    "title": "Introducing the TeamCity Operator for Kubernetes",
    "description": "Running TeamCity in Kubernetes has always been possible, but it often required knowing your way around both systems pretty well. TeamCity is flexible and easy to tune, and Kubernetes provides excellent tools for running applications at scale. However, bringing the two together smoothly wasn’t always straightforward. The new TeamCity Operator is designed to change that. […]",
    "reviews": [],
    "syllabus": [],
    "link": "https://blog.jetbrains.com/teamcity/2025/12/teamcity-kubernetes-operator/",
    "pubDate": "Wed, 10 Dec 2025 11:13:43 +0000",
    "creator": "Dmitrii Korovin",
    "categories": [
      "best-practices",
      "news",
      "teamcity-2",
      "devopspains"
    ]
  },
  {
    "id": 3,
    "imageUrl": "",
    "title": "AI 빅테크가 다 가져가는 시대,&nbsp;틈새시장 공략법",
    "description": "1. AI 빅테크가 모든 시장을 장악하는 시대, 스타트업은 무엇을 해야 할까? 요즘 AI 시장은 하루가 다르게 변화하고 있습니다. 구글, 오픈AI, 메타, 텐센트 등 글로벌 빅테크 기업들은 더 강력한 모델 출시 AI 에이전트 공개 API 무료화 초거대 모델 경량화를 앞다투어 발표하며 사실상 전 세계 AI기술의 판도를 뒤픈들고 있습니다. 그래서 많은 스타트업",
    "reviews": [],
    "syllabus": [],
    "link": "https://brunch.co.kr/@@LOc/318",
    "pubDate": "Fri, 12 Dec 2025 03:54:52 GMT",
    "creator": "고명환",
    "categories": []
  },
  {
    "id": 4,
    "imageUrl": "",
    "title": "55권 – 2025년",
    "description": "내가 유일하게 해마다 세우는 목표가 있는데, 한 해의 독서량이다. 해마다 50권의 목표를 설정하고, 책 종류와 분야는 특별히 가리지 않는 잡식성 독서를 하는데, 올해도 좋은 책을 많이 읽었고, 지금까지 55권을 읽었으니, 아마도 58권 정도로 올해를 마무리할 것 같다. 최근 몇 해 동안 50권을 초과 독서했고, 어떤 해는 60권 넘게 읽어서 목표를 60권으로 설정해 볼 생각도 했는데,(...)",
    "reviews": [],
    "syllabus": [],
    "link": "https://www.thestartupbible.com/2025/12/reading-55-books-2025.html",
    "pubDate": "Wed, 10 Dec 2025 21:38:00 +0000",
    "creator": "Kihong Bae",
    "categories": [
      "Uncategorized",
      "books",
      "compounding",
      "consistency",
      "FoundersAtWork",
      "inspiring"
    ]
  },
  {
    "id": 5,
    "imageUrl": "",
    "title": "Deploying Qodana on Kubernetes: More Scalability, More Control",
    "description": "As engineering teams scale, so do the demands on their tooling. Code quality checks that worked well for one repository or service often need a more resilient, automated, and infrastructure-native setup when development ecosystems grow. That’s exactly why we’re excited to introduce first-class Kubernetes support for Qodana, bringing fully automated deployments, lifecycle management, and future-ready […]",
    "reviews": [],
    "syllabus": [],
    "link": "https://blog.jetbrains.com/qodana/2025/12/deploying-qodana-on-kubernetes-more-scalability-more-control/",
    "pubDate": "Fri, 12 Dec 2025 11:13:33 +0000",
    "creator": "Kerry Beetge",
    "categories": [
      "news",
      "releases",
      "kubernetes",
      "qodana",
      "qodana-cloud",
      "qodana-demo"
    ]
  },
  {
    "id": 6,
    "imageUrl": "",
    "title": "즉시 실행하기",
    "description": "지난 금요일 박소령 대표님과 “실패를 통과하는 일”의 라이브 방송을 진행했다.\n라이브에서 얘기해주신 내용 중 부산의 유명 카페의 대표님 이야기가 생각난다.\nB2B로 원두를 납품하는 것이 회사의 중요 사업 중 하나였지만,\n그 카페의 대표님은 영어를 할 줄 모르셨다.\n그래서 영어를 잘 하는 영업사원들에게 커피 원두를 아프리카에서 공수해오는 일을 위임했다.\n근데 그 영업 사원들이 모두 이탈하는 경우가 생기니 회사의 큰 위기가 왔다.\n이 카페의 가장 코어는 ‘좋은 원두’ 인데, 그걸 대표가 직접 챙기지 않으니 이런 큰 위기가 올 수도 있다는 깨달음이 있으셨다.\n그래서 그 길로 한국의 모든 사업은 팀원들에게 맡기고,\n6개월간 어학연수를 바로 출발하셨다.\n영어를 배워야만 회사의 코어인 ‘커피 원두’ 를 대표가 직접 챙길 수 있으니깐.\n그리고 현재는 모든 커피 원두는 대표님이 직접 공수해오고 계신다.\n직접 아프리카로 가시면서 직접 원두 하나하나를 다 테스팅 하시고 협상도 하신다.\n이 이야기에는 여러가지 고민할거리를 많이 줬다.\n‘회사의 코어 인데 내가 직접 하지 않는 건은 무엇이 있을까?’\n‘지금 당장 실행에 옮기지 않고, 기다리고 있는것은 무엇일까?’\n‘그걸 실행하지 않는 이유는 무엇일까?’\n…\n12월은 항상 애매하게 붕 뜬다.\n1년의 마무리를 하는 시점이다보니,\n어영부영 정리하면서 내년 1월부터 무언갈 새롭게 시작하자로 정리가 된다.\n그러다보니 12월에는 항상 무언갈 해낸 경험은 잘 없다.\n1월을 위한 준비로만 사용하기 때문이다.\n근데 12월도 한달이다.\n12개월 중 1개인 1년의 8%를 차지하는 긴 일정이다.\n이 한달을 정리만 하는것으로 마무리하기엔 너무 아쉽다.\n카페 대표님처럼,\n생각난 즉시 실행에 옮기려면 1월까지 기다릴 필요 없이 12월에 바로 공부를 시작해야겠다.\n그런 의미에서, 12월에도 저와 함께 공부하실 분들을 모집합니다 :)\nhttps://inf.run/kNSjJ\n신청 기간 : 12/4(목)~12/12(금)\n챌린지 기간 : 12월 14일(일)\n1월까지 기다리지말고,\n12월에 바로 공부해보시죠.\n12월에 조금이라도 공부한 경험, 완주한 경험은 1월의 새로운 시작을 성공 경험과 함께 더 알차게 시작할 수 있을거라고 믿습니다 :)",
    "reviews": [],
    "syllabus": [],
    "link": "https://jojoldu.tistory.com/855",
    "pubDate": "Mon, 8 Dec 2025 08:59:32 +0900",
    "creator": "향로 (기억보단 기록을)",
    "categories": [
      "생각정리",
      "마라톤 챌린지",
      "박소령",
      "실패를 통과하는 일",
      "실패를 통과하는일",
      "향로 챌린지"
    ]
  },
  {
    "id": 7,
    "imageUrl": "",
    "title": "조선시대에는 사교육비로 얼마를 썼을까?",
    "description": "조선시대 N수생, 논밭까지 다 팔아버린 과거시험 생존기",
    "reviews": [],
    "syllabus": [],
    "link": "https://toss.im/tossfeed/article/behindthemoney-14",
    "pubDate": "Mon, 08 Dec 2025 00:30:00 GMT",
    "creator": "Unknown",
    "categories": []
  },
  {
    "id": 8,
    "imageUrl": "",
    "title": "데이터 시스템의 미래 - 정확성을 목표로",
    "description": "마틴은 애플리케이션을 정확하게 만들고 결함에 견딜 수 있게 하는 최후의 방법이 전통적인 트랜잭션 접근법이라고 믿지 않는다. 데이터플로 아키텍처(마틴이 생각하는 미래)의 맥락에서 정확성에 관해 생각하는 몇 가지 방법을 제안한다.\n읽기 전용(stateless) 서비스는 장애가 발생하더라도 버그를 수정하고 재시작하면 정상 상태로 복귀하는 경우가 많다. 그러나 데이터베이스와 같은 상태 저장(stateful) 시스템은 “기억하는 시스템”이므로, 한 번 발생한 오류가 장기간 남아 데이터 손상으로 축적될 수 있다. 따라서 분산 시스템에서 정확성(correctness) 은 더 엄격한 설계와 검증을 요구한다.\n1. 트랜잭션(ACID)은 정확성의 만능 해법이 아니다\n수십 년간 원자성(Atomicity), 격리성(Isolation), 지속성(Durability)은 애플리케이션 정확성을 위한 핵심 도구로 활용되어 왔다. 그러나 현실에서는 다음과 같은 한계가 존재한다.\n약한 격리 수준(weak isolation)은 의미가 불명확하거나 오해되기 쉬워, 특정 애플리케이션에 안전한지 판단하기 어렵다.\n성능과 확장성을 위해 leaderless replication 등 트랜잭션 중심 모델을 대체하는 접근이 사용되기도 하나, 의미론(semantics)이 복잡해지고 “일관성(consistency)”의 정의가 모호해지는 경향이 있다.\n네트워크 장애나 크래시와 같은 현실 조건에서는 제품이 주장하는 보장과 실제 동작이 불일치할 수 있으며, 이러한 간극은 여러 실험적 검증(예: Jepsen)에서 반복적으로 드러난다.\n결론적으로, 강한 트랜잭션을 제공하는 데이터 시스템을 사용하더라도 애플리케이션이 자동으로 정확해지지는 않는다.\n2. “정확히 한 번(Exactly-once)” 실행은 본질적으로 어렵다\n분산 환경에서는 실패가 상시 발생한다. 메시지 처리 중 실패가 발생하면 일반적으로 다음 중 하나를 선택한다.\n처리를 포기하여 메시지를 드롭한다(데이터 손실 가능성이 발생한다).\n재시도를 수행한다(이미 성공했지만 성공 응답을 받지 못했을 가능성으로 인해 중복 처리 가능성이 발생한다).\n중복 처리는 결제 이중 청구, 카운터 과대 증가, 이체 중복 반영과 같은 형태로 나타나며, 이는 모두 데이터 오염의 한 형태이다. 따라서 “재시도”를 허용하는 시스템에서 정확성을 확보하려면 중복 억제 전략이 필요하다.\n3. TCP와 DB 트랜잭션만으로는 end-to-end 중복을 제거할 수 없다\n3.1. TCP의 중복 제거는 단일 연결 범위에서만 유효하다\nTCP는 시퀀스 번호를 이용하여 패킷 재전송 및 중복 제거를 수행한다. 그러나 이는 단일 TCP 연결 범위에서만 동작한다. 예를 들어, 클라이언트가 데이터베이스에 트랜잭션을 전송한 뒤 COMMIT을 전송했지만, 네트워크 단절로 인해 커밋 결과 응답을 받지 못한 경우를 생각할 수 있다. 클라이언트는 트랜잭션이 커밋되었는지 여부를 알 수 없으며, 재연결 후 동일 트랜잭션을 재시도할 가능성이 있다. 비멱등(non-idempotent) 연산이라면 결과가 중복 반영될 수 있다.\n3.2. 사용자(브라우저)–서버–DB 경로 전체에서 중복이 발생한다\nDB-클라이언트 구간에서 중복을 일부 억제하더라도, 사용자 단말(예: 브라우저)과 애플리케이션 서버 사이에서 동일 요청이 반복 제출될 수 있다. HTTP POST가 타임아웃되면 사용자는 재시도를 수행할 수 있으며, 서버 관점에서는 별도 요청으로, DB 관점에서는 별도 트랜잭션으로 인식된다. 이 경우 일반적인 중복 억제 메커니즘(TCP 수준, DB 연결 수준)은 충분하지 않다.\n4. 핵심 해법: 요청 ID를 end-to-end로 전달하여 중복을 억제한다\n중복 처리를 근본적으로 억제하려면 요청을 식별하는 Operation ID(요청 ID) 를 생성하고 이를 클라이언트에서 DB까지 end-to-end로 전달해야 한다.\n클라이언트는 요청마다 UUID 등 유일 식별자를 생성한다.\n요청 ID를 HTTP 요청 본문 또는 숨김 필드 등에 포함하여 서버로 전달한다.\n서버는 해당 요청 ID를 DB에 기록하고, DB는 request_id UNIQUE 제약 등을 통해 동일 요청이 2회 이상 반영되지 않도록 강제한다.\n이 방식은 요청 이벤트를 기록하는 테이블이 일종의 이벤트 로그(event log) 로 동작하도록 만들며, 이후의 상태(예: 잔액)는 이벤트로부터 파생될 수 있다.\n5. 종단 간 논증\n저수준 신회성 기능이 그 자체로 종단 간 정확성을 보장하기에 충분하지 않다.\nSaltzer, Reed, Clark(1984)의 End-to-End Argument는 다음 원칙을 제시한다.\n특정 기능(중복 제거, 무결성 검증, 암호화 등)은 통신 계층이나 저장 계층만으로 완전하고 정확하게 구현될 수 없으며, 끝단 애플리케이션의 지식과 협력이 필요하다.\n저수준 기능은 문제 발생 확률을 낮추는 데 유용하지만, end-to-end 정확성을 보장하는 충분조건은 아니다.\n따라서 트랜잭션, TCP, 메시징 시스템이 제공하는 신뢰성 기능을 활용하되, 애플리케이션 수준에서 end-to-end 정확성 요구사항을 직접 설계해야 한다.\n6. 제약 조건 강제하기\n유저명 중복 금지, 좌석 중복 예약 금지, 요청 ID 유일성 보장과 같은 유니크 제약은 분산 환경에서 본질적으로 합의를 요구한다.\n동시성 상황에서 여러 요청이 동일 값을 주장하면, 시스템은 어느 요청을 승인할지 결정해야 한다.\n일반적으로 단일 리더(leader)가 결정을 수행하게 하면 단순하지만, 리더 장애나 지리적 분산, 확장성 요구가 커지면 다시 합의 문제가 중요해진다.\n유니크 값을 기준으로 파티셔닝하면(예: username 해시) 같은 키의 요청이 동일 파티션으로 라우팅되도록 하여 처리량을 확장할 수 있다.\n7. 로그 기반 메시징의 유일성\n파티션 로그(예: Kafka)는 동일 파티션 내 메시지에 대해 소비자가 같은 순서를 관찰하게 한다. 이는 총순서(total order) 보장으로서 합의와 동등한 성질(총순서 브로드캐스트)로 이해할 수 있다. 이를 이용하면 유니크 제약 처리가 단순해진다.\n예시: username 선점 처리 흐름\nusername 요청을 메시지로 인코딩하여, username 해시 기반 파티션에 append한다.\n스트림 프로세서는 파티션을 단일 스레드로 순차 소비하며, 로컬 상태(예: 로컬 DB)에 점유 여부를 기록한다.\n사용 가능하면 성공 메시지를, 이미 점유된 경우 거절 메시지를 출력 스트림으로 발행한다.\n클라이언트는 출력 스트림에서 자신의 요청 결과를 관찰한다.\n핵심 원리는 충돌 가능성이 있는 쓰기들을 동일 파티션으로 모아 순차 처리함으로써 결정을 결정적이고 단순하게 만드는 것이다.\n8. 멀티 파티션 요청 처리\n예를 들어, \"이체 요청\"은 다음 파티션들과 관련될 수 있다.\n요청 ID 파티션\n출금 계좌(A) 파티션\n입금 계좌(B) 파티션\n전통적 DB 접근은 멀티 파티션 원자 커밋(예: 2PC)을 필요로 하며, 이는 처리량 및 운영 복잡도 비용을 동반한다. 데이터플로우 접근은 다음과 같이 분해하여 동등한 무결성을 달성한다.\n계좌 A에서 계좌 B로 송금하는 요청은 클라이언트에게 고유 요청 ID를 발급받아 요청 ID 기준으로 특정 로그 파티션에 추가된다.\n스트림 처리자는 요청 로그를 읽는다. 각 요청 메시지마다 보내는 사람 계좌 A의 출금 지시 메시지(A로 파티셔닝됨)와 받는 사람 계좌 B의 입금 지시 메시지(B로 파티셔닝됨) 두 가지를 출력 스트림으로 방출한다. 방출한 메시지에는 원 요청 ID가 포함된다.\n후속 처리자는 출금과 입금 지시 스트림을 소비해 요청 ID로 중복을 제거한 다음 변경 내용을 계좌 잔고에 반영한다.\n스트림 프로세서가 재시작되어 동일 이벤트를 재처리하더라도, 파생이 결정적이라면 동일 명령이 재발행될 뿐이며, 최종 반영 단계에서 요청 ID로 중복을 제거하면 무결성이 유지된다.\n9. 적시성과 무결성\n일관성이라는 용어는 서로 다른 요구를 혼합하여 혼란을 유발한다. 이를 다음 두 축으로 분리하여 해석한다.\n9.1. Timeliness(신선함/최신성)\n사용자가 최신 상태를 즉시 관찰하는 정도를 의미한다.\n복제 지연 등으로 인해 일시적 불일치가 발생할 수 있으나, 시간이 지나면 자연히 해소되는 성격을 가진다.\n9.2. Integrity(무결성)\n데이터 손실, 중복 적용, 모순 데이터, 잘못된 파생 데이터와 같은 “오염”이 없는 상태를 의미한다.\n한 번 무결성이 깨지면 기다린다고 복구되지 않으며, 명시적 검증과 수리가 필요하다.\n대부분의 애플리케이션에서 Integrity는 Timeliness보다 더 중요하다. 최신 반영이 늦어지는 것은 허용 가능한 경우가 많지만, 돈이 사라지거나 합계가 맞지 않는 오류는 치명적이다.\n10. 데이터플로우 시스템의 정확성\n스트리밍 기반 데이터플로우는 비동기 처리이므로 기본적으로 timeliness 보장이 약하다. 그러나 다음 조합을 통해 강한 무결성을 확보할 수 있다.\n쓰기를 단일 이벤트로 표현하여 원자적으로 기록한다.\n나머지 상태 업데이트는 이벤트로부터 결정적(Deterministic) 파생 함수로 계산한다.\n요청 ID를 end-to-end로 전달하여 중복 제거 및 멱등성을 확보한다.\n이벤트를 불변(Immutable)으로 유지하고 필요 시 재처리로 복구 가능하게 설계한다.\n이 조합은 분산 트랜잭션(2PC) 없이도 높은 무결성을 제공할 수 있는 유망한 방향이다.\n11. 느슨하게 해석되는 제약 조건\n엄격한 제약을 즉시 강제하려면 동기적 조정(합의/리더/선형화)이 필요하며 이는 성능 및 가용성 비용을 수반한다. 반면 많은 비즈니스는 다음과 같이 일시적 위반을 허용하고 사후 보정으로 해결할 수 있다.\n좌석 중복 예약: 한 명에게 사과하고 대체 좌석 또는 보상을 제공한다.\n재고 초과 판매: 입고 지연에 대해 사과하고 할인 등 보상을 제공한다.\n오버부킹: 환불, 업그레이드, 대체 제공 절차로 처리한다.\n즉, 제약 강제는 불일치로 인한 “사과”를 줄일 수 있으나, 시스템 성능 및 가용성을 낮춰 장애로 인한 “사과”를 늘릴 수 있다. 목표는 사과를 0으로 만드는 것이 아니라, 비즈니스에 적합한 균형점을 찾는 것이다.\n12. 믿어라, 하지만 확인하라\n앞선 논의는 어떤 장애는 발생할 수 있고(프로세스 크래시, 전원 장애, 네트워크 지연/드롭 등), 어떤 장애는 발생하지 않는다고(예: fsync 이후 데이터 유실 없음, 메모리 비트 오류 없음, CPU 연산은 항상 정확함) 가정하는 시스템 모델(system model) 위에서 진행된다. 이러한 가정은 대부분의 상황에서 타당하지만, 현실에서는 “가능/불가능”의 이분법이라기보다 확률의 문제로 이해하는 편이 정확하다. 즉, 매우 드문 사건이라도 규모가 커지면 실제로 발생할 수 있으며, 가정 위반이 실무에서 충분히 관측될 정도인지가 핵심이다.\n12.1. 하드웨어는 완벽한 추상화가 아니다\n디스크는 가만히 두어도 데이터가 조용히(silent) 손상될 수 있다.\n네트워크의 데이터 손상은 TCP 체크섬을 우회하는 경우가 발생할 수 있다.\n메모리의 무작위 비트 플립(bit-flip)은 매우 드물지만, 대규모 디바이스 환경에서는 관측될 수 있다.\n결함이 없는 메모리에서도 특정 접근 패턴으로 비트가 뒤집히는 현상이 보고되었으며(예: Rowhammer), 이는 운영체제 보안 메커니즘을 무력화하는 공격에도 활용된다.\n하드웨어 오류는 빈도가 낮더라도 “불가능”하지 않으므로, 시스템은 이를 완전히 배제하는 모델에만 의존해서는 안 된다.\n12.2. 소프트웨어 버그가 발생해도 무결성(integrity)유지하기\n하드웨어뿐 아니라 소프트웨어 버그는 더 빈번한 위험 요인이다. 네트워크/메모리/파일시스템의 체크섬은 소프트웨어 버그로 인한 잘못된 데이터 기록을 잡아내지 못한다.\n성숙한 DBMS조차 버그가 존재하며, 유니크 제약을 잘못 유지하거나(사례 존재), 직렬화 격리 수준에서 예상치 못한 이상 현상이 보고된 적이 있다.\n애플리케이션 코드는 DBMS보다 테스트/리뷰 강도가 낮은 경우가 많아, 버그 가능성이 더 높다.\n외래키/유니크 제약 등 DB가 제공하는 무결성 도구를 애플리케이션이 올바르게 사용하지 않는 경우도 많다.\nACID에서 말하는 “Consistency(일관성)”는 트랜잭션이 “일관된 상태에서 일관된 상태로” 시스템을 변환한다는 가정에 기반한다. 그러나 이는 트랜잭션(애플리케이션 로직)이 버그가 없다는 전제가 필요하며, 애플리케이션이 DB를 잘못 사용하면(예: 약한 격리 수준을 안전하지 않게 사용) 무결성은 보장되지 않는다.\n12.3. 약속을 맹목적으로 믿지 마라.\n하드웨어와 소프트웨어가 이상적으로 동작하지 않을 수 있다면, 데이터 손상은 언젠가 발생할 수 있다고 보는 편이 현실적이다. 따라서 중요한 것은 “손상이 없다고 믿는 것”이 아니라, 손상이 발생했는지 탐지하고 원인을 추적할 수 있는 메커니즘을 갖추는 것이다. 이러한 무결성 검증을 감사(auditing) 라고 한다.\n감사는 금융에만 필요한 개념이 아니며, 금융에서 특히 중요한 이유는 실수가 반드시 발생한다는 사실을 모두가 인정하기 때문이다.\n대규모 스토리지 시스템(HDFS, S3 등)은 디스크를 전적으로 신뢰하지 않고, 백그라운드에서 파일을 읽어 다른 복제본과 비교하거나 디스크 간 이동을 수행하여 조용한 손상(silent corruption) 위험을 완화한다.\n“데이터가 정말 존재하는지 확인하려면 읽어서 검사해야 한다”는 원칙이 성립한다.\n동일한 논리로 백업 복구 테스트를 주기적으로 수행해야 한다. 복구 테스트가 없다면, 백업이 깨졌다는 사실을 데이터 손실 이후에야 알 수 있다.\n12.4. 검증하는 문화\n많은 시스템은 정확성 보장을 절대적인 것으로 가정하고, 드문 데이터 손상 가능성을 고려한 자가 검증 체계를 갖추지 않는다. 그러나 앞으로는 다음과 같은 방향이 필요하다.\n시스템이 스스로 자신의 무결성을 지속적으로 점검하는 self-validating/self-auditing 메커니즘의 확산\nACID 문화가 만든 “트랜잭션을 믿으면 된다”는 습관에서 벗어나, 애플리케이션 설계 단계부터 감사 가능성(auditability) 을 투자 대상으로 포함\nNoSQL 확산과 약한 일관성, 그리고 성숙도가 낮은 저장 기술의 보편화는 “맹목적 신뢰” 전략을 더 위험하게 만들었다. 따라서 감사 가능성을 기반으로 한 설계가 중요해진다.\n12.5. 감사 기능 설계: 이벤트 기반이 유리하다\n여러 객체를 동시에 변경하는 트랜잭션은, 사후적으로 “왜 이런 변경이 일어났는지”를 해석하기 어렵다. CDC나 트랜잭션 로그를 확보하더라도, 여러 테이블의 INSERT/UPDATE/DELETE만으로는 의도(why) 와 맥락을 명확히 복원하기 어렵다. 또한 그 변경을 결정한 애플리케이션 로직 호출은 일회성(transient)이며 재현하기 어렵다.\n반면 이벤트 기반(event-based) 시스템은 감사 가능성 측면에서 장점이 있다.\n사용자 입력을 단일 불변 이벤트(immutable event) 로 기록한다.\n그 이벤트로부터 파생되는 상태 업데이트는 결정적(deterministic) 이고 반복 가능(repeatable)하도록 만든다.\n동일한 이벤트 로그와 동일한 버전의 파생 코드를 실행하면 동일한 결과가 재현된다.\n명시적인 데이터플로우는 데이터의 계보(provenance)를 선명하게 만들어 무결성 점검을 용이하게 한다.\n이벤트 로그는 해시(hash) 로 저장소 손상을 검증할 수 있다.\n파생 상태는 이벤트 로그로부터 재처리(reprocessing) 하여 동일 결과가 재현되는지 확인할 수 있다.\n필요하면 동일 파생을 중복 파이프라인으로 병렬 수행하여 결과를 상호 검증할 수도 있다.\n결정적 데이터플로우는 디버깅/트레이싱을 단순화하며, 문제 상황을 재현하는 시간 여행 디버깅(time-travel debugging) 성격의 진단 능력을 제공한다.\n12.6. 다시 종단 간 논증: 무결성 검증도 end-to-end가 최선이다\n개별 컴포넌트(디스크, 네트워크, 서비스, 알고리즘)가 완전히 무결하다고 신뢰할 수 없다면, 데이터 무결성은 주기적으로 점검해야 한다. 점검이 없다면 손상은 downstream에서 피해를 만든 뒤에야 드러나며, 그 시점에는 원인 추적 비용이 크게 증가한다.\n무결성 점검은 가능한 한 end-to-end 로 수행하는 것이 바람직하다.\n검증 범위에 더 많은 시스템을 포함할수록, 어떤 단계에서 손상이 “조용히” 지나갈 여지가 줄어든다.\n파생 데이터 파이프라인 전체를 end-to-end로 검증할 수 있다면, 경로 상의 디스크/네트워크/서비스/알고리즘이 암묵적으로 검증 범위에 포함된다.\n연속적인 end-to-end 무결성 검증은 신뢰도를 높이고, 이는 곧 변경에 대한 두려움을 줄여 시스템 진화를 빠르게 만든다. 이는 자동화 테스트가 변경 리스크를 낮추는 것과 같은 효과를 가진다.\n12.7. 감사 가능한 데이터 시스템용 도구: 암호학적 검증의 가능성\n현재 많은 데이터 시스템은 감사 가능성을 1급 요구사항으로 두지 않는다. 애플리케이션이 별도 감사 테이블에 변경 로그를 남길 수는 있으나, 감사 로그와 실제 DB 상태의 무결성을 동시에 보장하기는 여전히 어렵다. 트랜잭션 로그를 HSM으로 서명해 위변조를 방지할 수 있으나, “처음부터 올바른 트랜잭션이 기록되었는지”까지 보장하기는 어렵다.\n이 영역에서 암호학적 도구가 흥미로운 가능성을 제공한다.\n블록체인/분산 원장 기술은 “서로 신뢰하지 않는 조직”이 운영하는 복제본들이 서로를 검증하며, 합의 프로토콜로 실행할 트랜잭션을 결정하는 분산 DB로 이해할 수 있다.\n비잔틴 장애 허용(Byzantine fault tolerance) 측면과 작업 증명(Proof of Work)의 비용 문제, 처리량 한계 등에 대한 비판적 논의가 존재하더라도, 무결성 검증 아이디어는 주목할 가치가 있다.\n암호학적 감사는 종종 머클 트리(Merkle tree) 를 활용하며, 이는 특정 레코드가 데이터셋에 포함됨을 효율적으로 증명하는 데 사용된다.\n암호학적 감사 기법은 인증서 투명성(certificate transparency)처럼 이미 보안 영역에서 실전 적용 사례가 있으며, 향후 데이터 시스템 전반으로 확산될 가능성이 있다.\n암호학적 감사의 성능 오버헤드를 낮추고 대규모 시스템에서도 확장 가능하게 만드는 추가 연구가 필요하나, 장기적으로 주목할 만한 방향이다.\n결론\n데이터 시스템의 강한 보장은 end-to-end 정확성을 자동으로 제공하지 않는다.\n요청 ID 기반 end-to-end 중복 억제, 불변 이벤트, 결정적 파생, 중복 제거의 조합은 분산 트랜잭션 없이도 강한 무결성을 달성할 수 있다.\n엄격한 제약은 필요한 지점에 국소적으로 적용하고, 나머지는 느슨한 제약과 사후 보정 프로세스로 설계하는 전략이 실용적일 수 있다.\n참고\n데이터 중심 애플리케이션 설계 12장",
    "reviews": [],
    "syllabus": [],
    "link": "https://velog.io/@ahngj96/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%98-%EB%AF%B8%EB%9E%98-%EC%A0%95%ED%99%95%EC%84%B1%EC%9D%84-%EB%AA%A9%ED%91%9C%EB%A1%9C",
    "pubDate": "Sun, 07 Dec 2025 12:58:45 GMT",
    "creator": "Unknown",
    "categories": []
  },
  {
    "id": 9,
    "imageUrl": "",
    "title": "무(武)와 협(俠)이 펼쳐진 세계로! 연운",
    "description": "No description available",
    "reviews": [],
    "syllabus": [],
    "link": "https://bbs.ruliweb.com/news/board/11/read/2401",
    "pubDate": "Thu, 11 Dec 2025 17:09:17 +0900",
    "creator": "｜RULIWEB｜",
    "categories": [
      "게임툰"
    ]
  },
  {
    "id": 10,
    "imageUrl": "",
    "title": "BUN 앤트로픽에 (클로드코드) 에 인수되었습니다. / 성공담",
    "description": "영상: https://www.youtube.com/watch?v=RMwRhRyZ0Jo\n\n\n\nBUN 은 빠른 typescript 처리기 입니다.\n앤트로픽은 클로트 코드 라는 AI 자동 코딩 툴 회사입니다.\n \n처음 써보고 너무 빨라서 놀랐는데\n돈을 4년간 제대로 못벌고 있다가\n이번에 앤트로픽에서 인수해갔군요\n \n그 성공 스토리에 대한 이야기 구요\n두회사는 간접적으로 연관이 있었고 결국엔 성공했습니다.\n이번에도 레딧과 깃허브의 역활이 컷군요\n이 두개에서 성공 사례가 많이 나오는거 같습니다.\n제품의 본질에 충실하면 좋은 제품이 되었을때\n성공한다는 것을 배웠습니다.",
    "reviews": [],
    "syllabus": [],
    "link": "https://serverdown.tistory.com/1530",
    "pubDate": "Mon, 8 Dec 2025 13:52:27 +0900",
    "creator": "SIDNFT",
    "categories": [
      "유튜브",
      "Ai",
      "성공담"
    ]
  },
  {
    "id": 11,
    "imageUrl": "",
    "title": "Building AI Agents in Kotlin – Part 3: Under Observation",
    "description": "Previously in this series: Two articles in, and our coding agent can already do quite a bit. It can explore projects, read and write code, execute shell commands, and run tests. Adding a definition of done (DoD) in our last article gave it the feedback loop it needed – the agent now iterates until all […]",
    "reviews": [],
    "syllabus": [],
    "link": "https://blog.jetbrains.com/ai/2025/12/building-ai-agents-in-kotlin-part-3-under-observation/",
    "pubDate": "Fri, 12 Dec 2025 11:58:45 +0000",
    "creator": "Denis Domanskii",
    "categories": [
      "kotlin",
      "tutorials",
      "ai",
      "ai-agents"
    ]
  },
  {
    "id": 12,
    "imageUrl": "",
    "title": "Google Antigravity 바이브 코딩 도구 사용기",
    "description": "최근 장안의 화재인 Google Antigravity 바이브 코딩 도구 사용기를 나눔한다. \n\n\n설치 및 준비\n설치는 매우 간단하다. 다음 링크 방문해 다운로드 후 설치하면 된다.\n\nGoogle Antigravity\n\nvscode 통합 개발환경이므로, 설치 후, vscode 설정 방법대로 파이썬 등 애드인 설치하고, 사용하면 된다. \n\n\n바이브 코딩하기\n본 예에서는 간단히 PDF파일에서 텍스트와 이미지 레이아웃을 분리하고, 이를 JSON 과 이미지로 저장간하는 간단한 웹앱을 개발해 보도록 한다. 우선, vscode 바이브 도구 설정처럼 LLM 모델을 설정하고, 다음과 같이 프롬프트를 입력한다. \n\n\n\n파이썬으로 주어진 PDF파일을 업로드하면, 여기서 layout, text, image를 분리해서 이를 각 페이지별로 json으로 저장하는 파서 서비스를 개발해. 웹 기반 동작해야 함. 안정적이고 유명한 라이브러리만 사용해.\n\n그럼, 다음과 같이 PRD.md 파일을 우선 생성한다. \n\n\n이 파일대로 프로젝트 개발하라 요청한다. 다음은 그 결과이다.\n\n\n\n실행해본다. 그리고 웹 접속하면 다음 웹앱이 정상동작될 것이다.\n\n적당한 PDF파일로 테스트해본다.\n\n\n마무리\n\n지금까지 간단하게 구글 안티그레비티 바이브 코딩 도구를 사용하고, 웹 개발 후, 테스트해보았다. 기존 바이브 도구만큼이나 잘 동작하고, 깔끔하게 실행된다. 참고로, 구글 제미나이 프로 버전을 사용한다면 토큰 제한 그리 신경쓰지 않고 활용 가능하다.",
    "reviews": [],
    "syllabus": [],
    "link": "http://daddynkidsmakers.blogspot.com/2025/12/google-antigravity.html",
    "pubDate": "2025-12-10T02:05:00.000Z",
    "creator": "Unknown",
    "categories": []
  },
  {
    "id": 13,
    "imageUrl": "",
    "title": "Google Antigravity 사용법",
    "description": "안녕하세요. 이번 글은 Google의 Antigravity 사용법에 대해 작성한 글입니다.\n키워드 : Google Antigravity, Google Antigravity 사용법\n\n\nAntigravity\n이름의 유래\nAntigravity 소개글에 작성되어 있는 비전\n    \nOur vision is to ultimately enable anyone with an idea to experience liftoff and build that idea into reality.\nLiftoff라는 용어는 항공기 전체가 지면에서 완전히 떨어지는 시점을 의미하는데, 구글은 자신의 아이디어 구현할 때 누구나 사용할 수 있는 도구를 만들고 싶었던 것 같음. 즉, 개발할 때 생기는 여러 이슈들을 중력이라 생각하고 그것을 대항할 수 있는 도구\n여담으로 Python에서 import antigravity를 하면 XKCD 만화(353번)를 웹 브라우저로 열어주는 이스터에그가 발생하는데, 관련은 없지만 익숙한 용어였음\n\n\n핵심 원칙\nAntigravity의 핵심 원칙은 Trust, Autonomy, Feedback, Self-improvement\nTrust\n오늘날의 대부분의 제품은 두 케이스에 속함\n    \n(1) 에이전트가 수행한 모든 Action과 Tool 호출을 사용자에게 보여줌\n(2) 작업의 컨텍스트나 검증할 방법 없이 최종 코드 변경 사항만 보여줌\n이런 제품들은 에이전트가 왜 이 작업을 했는지, 어떻게 확인해야 하는지를 알기 어려움.\nAntigravity는 사용자가 에이전트가 수행할 작업에 대한 신뢰를 가질 수 있도록 Actifact와 Verfication result(검증 결과)를 제공해 컨텍스트를 제공함\n    \n단순히 개별 Tool 호출이 전부 보이는 것이 아닌, 호출들이 Task 단위로 묶어서 표시함\nTask에 대한 추상화된 정리와 진행 상황을 확인할 수 있음\n사용자가 검증하기 쉬운 형식의 Artifact를 생성함. 예를 들어 Task list(작업 목록), Implementation plan(구현 계획), Walkthrough(단계별 설명서), 스크린샷, 브라우저 녹화 영상\n사용하면서 인상 깊은 부분은 웹브라우저 작업을 할 때 녹화가 되는 기능으로 작업을 어떻게 진행했는지 확인할 수 있었음. Plan 버전으로 실행할 때 Implementation plan, Walkthrough를 제공하는 것도 좋았음\nAutonomy\n요즘 가장 직관적인 제품의 형태는 여러 Surface(편집기, 브라우저, 터미널)에 내장된 에이전트와 동기적으로 동작하는 형태\n    \n에이전트가 자율적으로 프론트엔드 코드를 구현하고, 터미널에서 로컬호스트 실행하고, 브라우저를 작동시켜 새로운 기능이 작동하는지 테스트하는 형태\n이런 자율성을 최적으로 사용하기 위해 사용자가 에이전트와 비동기적으로 상호 작용할 수 있도록 Manager View를 만들었음\nAgent Manager View는 여러 에이전트를 병렬로 생성하고 모니터링할 수 있는 관제탑 스타일의 화면\n기존에 자주 사용하던 VSCode, Cursor 같은 스타일은 Editor View로 존재함(Editor View와 Manager View로 전환하는 단축키는 커맨드(컨트롤) + e)\nFeedback\n에이전트의 지능이 좋아지면서 여러 작업이 가능해졌지만, 아직 완벽하지 않음\n    \n이런 상황에서 피드백을 제공하면 도움이 됨\nAntigravity는 모든 Surface나 Artifact에서 비동기 사용자 피드백을 허용함\nText Artifact에선 주석을 달 수 있고, 스크린샷에서 영역을 선택해서 주석을 달 수 있음\nSelf-improvement\n에이전트가 일하면서 배운 내용을 저장하고, 나중에 비슷한 일을 할 때 참조함\n    \n명시적 정보 : 유용한 코드 snippet, 아키텍처 정보\n\n\n설치\n웹페이지에서 다운로드 가능\nMac 사용자라면 brew로도 설치 가능\n\n  brew install --cask antigravity\n\n    \n처음에 설치하면 기존에 사용하던 IDE(VS Code, Windsurf, Cursor 등)의 설정을 불러올 수 있고, Start fresh를 눌러서 시작할 수 있음\nAgent 설정은 Agent-assisted development으로 설정했음\nAntigravity - 크롬 확장 프로그램 설치\n\n위와 같은 구성을 확인할 수 있고, VSCode와 다르게 우측 상단에 크롬 아이콘이 있어서 눌러봄\n\n설치가 필요한데, 이미 크롬이 설치되어 있어서 경로를 직접 수정함\n    \nSet custom Chrome binary path 클릭해서 경로 지정함\n\n설정에서 Browser로 이동해서 Chrome Binary Path를 지정할 수 있음\n\n다시 크롬 버튼을 누르니 온보딩 페이지로 나오고, 확장 프로그램을 설치함. 이를 통해 Antigravity에서 크롬을 제어할 수 있음\n\n\nAgent\nAgent의 Core Compoent\n    \nReasoning model\nTools\nArtifacts\nKnowledge\nCustomization(커스텀 요소)\n    \nAgent Modes / Settings\nMCP\nRules / Workflow\nModels\nGoogle Vertex Model Garden을 통해 모델 제공\n    \n사용자가 원하는 모델을 선택할 수 있음\nGemini 3 Pro (high)\nGemini 3 Pro (low)\nClaude Sonnet 4.5\nClaude Sonnet 4.5 (thinking)\nClaude Opus 4.5 (thinking)\nGPT-OSS\n사용량 제한은 Plans 문서에서 확인할 수 있음\n    \n현재(25년 12월 기준) Preview라서 무료로 사용 가능하며, Google AI Plan 구독 여부에 따라 제공량이 다름\n기본 제공\n        \n위에 제공하는 Model에 대한 기본 사용\nTab completion 무제한 제공\nCommand requests 무제한 제공\nAgent Manager, Borwser integration 사용 가능\nGoogle AI Ultra : 제일 많은 사용량, 5시간마다 쿼타 갱신\nGoogle AI Pro : 많은 사용량, 5시간마다 쿼타 갱신\nGoogle AI Plan에 가입하지 않은 경우 적당한 사용량을 제공하고 주마다 쿼타 갱신\n아직 key를 사용하는 방식은 지원하지 않으나, 추후에 지원할 가능성이 높음\n\nAntigravity 내부에서 사용하는 모델\nAntigravity가 동작하는 과정에서 모델을 사용하고 있으며, 사용자가 설정할 수 없음\nUI 목업 생성, 웹페이지나 앱에서 사용할 이미지 생성, 아키텍처 다이어그램 생성 : Nano Banana Pro\n브라우저 서브 에이전트가 브라우저에서 클릭, 스크롤, 입력 등 브라우저를 제어할 때 : Gemini 2.5 Pro UI Checkpoint(내부적으로 커스텀한 모델로 추측)\n체크포인팅, 컨텍스트 요약할 때 : Gemini 2.5 Flash\n코드의 Semantic Search(의미 기반 검색) : Gemini 2.5 Flash Lite\nTools(MCP)\nMCP를 사용해 Antigravity가 데이터베이스에 직접 접근해서 정보를 가져올 수 있음\n연결 방법\n우측 상단에 있는 Agent의 우측에 …을 클릭 -> MCP Servers 클릭\n\n필요한 MCP 검색(저는 supabase)\n\nInstall 클릭 후, 환경 설정\n\n\n이제 MCP의 Tool을 사용할 수 있음\n\nArtifacts\nArtifacts는 에이전트가 작업을 완료하거나 작업과 생각을 사용자에게 전달하기 위해 생성하는 모든 것\n예시\n    \nTask list(할 일 목록)\nImplementation plan(구현 계획)\nWalkthrough(워크스루) : 변경 사항, 작업 과정 요약\nScreenshots(화면 캡처)\nBrowser Recordings(브라우저 녹화)\nKnowledge(지식)\n아래 실제로 실행하는 부분에서 확인 가능\nAgent Modes / Settings\n에이전트의 모드나 설정에 대한 이해가 필요함\n대화 모드\n대화할 때 2가지 모드 중 선택할 수 있음\n    \nPlanning, Fast\nPlanning : 작업 실행 전에 Plan을 세움. 복잡한 작업에서 활용하는 것이 좋으며, 작업에 따라 아티팩트 생성, 계획 등을 수행\nFast : 에이전트가 작업을 바로 실행함. 간단한 작업에 사용하며, 속도가 중요하고 품질 저하에 대한 걱정이 적을 때 사용\n대화할 때 바로 설정할 수 있음\n\n전반적인 설정\nAntigravity Settings에서 설정할 수 있으며, Artifact Review Policy, Terminal Command Auto Execution Policy를 정의할 수 있음\n\nArtifact Review Policy(아티팩트 리뷰 정책)\n    \n에이전트가 아티팩트에 대해 리뷰를 어떻게, 언제 물어볼지 결정하는 정책\nAlways Proceed : 에이전트가 리뷰를 요청하지 않고 작업 진행\nAgent Decides : 에이전트가 리뷰 요청 시점을 스스로 결정\nRequest Review : 에이전트가 항상 리뷰 요청\nTerminal Command Auto Execution Policy(터미널 커맨드 자동 실행 정책)\n    \n에이전트가 터미널 컴맨드를 자동으로 실행할지 결정하는 정책\nAlways Proceed : 에이전트가 터미널 명령을 실행하기 전에 확인하지 않고 실행\nAgent Decides : 에이전트가 터미널 명령을 실행하기 전에 스스로 판단\nRequest Review : 에이전트가 터미널 명령을 실행하기 전에 항상 확인\n두가지 설정 모두 Agent Decides로 설정하고 사용 중\n유사하게 Browser Javascript Execution Policy도 존재함(설정 - Browser)\n    \nDisabled : 에이전트가 브라우저에서 자바스크립트 코드를 실행하지 않음\nAlways Ask : 에이전트가 항상 자바스크립트 코드를 실행하기 전에 확인\nModel Decides : 에이전트가 자바스크립트 코드를 실행하기 전에 스스로 판단\nTurbo : 확인 없이 자바스크립트 코드를 실행함\n\n그 외에 Agent Non-Workspace File Access 설정은 Agent가 지금 작업 폴더와 Antigravity 루트 폴더(`~/.antigravity/) 외의 폴더에 접근할 수 있게 설정할 수 있음\n\nRules / Workflows\nRules : 지켜야 할 원칙\nWorkflows : AI가 행동해야 하는 순서 제공\nAgent의 … 클릭 후 Customization을 누르면 설정할 수 있음\n\nRules\nAgent가 따라야 하는 작업 스타일, 가이드를 작성한 마크다운 파일\nClaude Code나 Gemini CLI를 사용하면 CLAUDE.MD, GEMINI.MD를 만드는데, 이 개념과 같다고 보면 됨\nGlobal Rules와 Workspace Rules로 나뉨\n    \nGlobal Rules : ~/.gemini/GEMINI.md에 저장\nWorkspace Rules : 워크스페이스의 .agent/rules에 저장하고, 해당 워크스페이스에서만 적용\nRules 파일 제한 : 12,000자(character)\nCustomizations에서 +workspace를 눌러서 Workspace Rules를 추가할 수 있음(현재 있는 Global 설정은 한국어 응답하라는 내용이 저장되어 있음)\n\nWorkspace Rule도 어떤 상황에 활성화를 할 것인지 옵션이 존재함\n    \nManual : 사용자가 에이전트 채팅창에서 @로 명시적으로 호출할 때 적용\nAlways On : 항상 모든 상황에 적용\nModel Decision : Rule 파일을 읽고 현재 작업에 이 Rule이 필요한지 모델이 스스로 결정\nGlob : 특정 파일 패턴과 일치하는 파일을 다룰 때 적용\n\nWorkflows\nworkflow는 반복되는 작업을 정리한 파일\n    \n작업들을 단계별로 정의하면 에이전트가 그 순서에 맞춰 일을 처리함\n실행할 때 채팅창에서 /워크플로우이름을 입력하면 실행됨\n.agent/workflows 폴더에 저장됨\nCustomizatios - Workflows - +workspace를 누르면 만들 수 있고, 채팅으로 만들라고 할 수도 있음\n\n실행하니 정상적으로 동작함\n\n\n\n실제 사용기\n웹페이지 취약점 확인 후 동작 테스트\n개인적으로 만들고 있던 웹페이지의 취약점을 확인하고, 그 후에 실제로 잘 동작하는지 확인해달라고 함\n\n하나씩 확인하는데, 영상을 찍어서 제시함. 이게 Screen Recording\n\n작업 완료한 후, 하단에서 여러 내용을 확인할 수 있음\n    \n수정된 파일, Artifact(Implementation Plan, Task, Walkthrough)\n\n이번에는 직접 로그인을 한 후 다시 확인해보라고 하니, 로그인 상황에서 테스트를 진행함\n\n관리자 계정이 아닌 계정으로 테스트하는게 좋을까?라고 물어보니 아래처럼 안내함(일단 false로 주석처리해서 테스트)\n\n그 후 테스트하고 성능 측정 후 내용을 공유함\n\nWalkthrough 문서에 댓글을 남긴 후, 피드백을 제출함\n\n\n제출한 후에 Agent Manager에서 내가 남긴 댓글에 대한 답변을 볼 수 있음\n\nScreen Recording 기능\nScreen Recording은 Antigravity가 browser_subagent라는 도구를 사용해 브라우저를 제어할 때만 자동으로 수행됨\n    \n녹화 시점:  “브라우저를 열어서 확인해줘”라는 요청을 받거나, 스스로 웹페이지 테스트가 필요하다고 판단하여 브라우저 도구를 실행할 때그 세션의 화면이 녹화\n녹화하는 것: 브라우저 창 내부에서 일어나는 일(클릭, 타이핑, 페이지 이동, 콘솔 로그 확인 등)만 녹화됨\nArtifact가 어디에 저장하는지 확인해보니 다음 경로에 저장됨(/Users/UserName/.gemini/antigravity/)\n    \n동영상 용량은 작진 않아서 나중에 관리가 필요할 것으로 예상됨\n\n대화 기록\n만약 지난 대화를 찾고 싶다면 이 버튼을 통해서 찾을 수 있음\n\n\n\n아키텍처 이미지 생성, 이미지 피드백\n이번엔 아키텍처를 그려달라고 함\n    \nCreated Task를 보면 Task가 생성되고, Task의 Progress Updates를 보여줌\n\n생성된 이미지에서 드래그를 하면 의견을 특정 부분에 줄 수 있음. 이 기능이 꽤 유용함\n\n다른 의견도 주고 Submit을 클릭해서 피드백을 제시함\n\n피드백을 반영해 이미지가 변경됨\n\n\n\nAgent Manager View\ncommand + e 또는 ctrl + e를 누르면 에이전트 매니저로 전환됨\nInbox\n아래 이미지는 Inbox에 진입한 상태\n\nPending만 확인할 수도 있음. 지금은 모두 다 확인해서 나오지 않음\n\nWorkspace\nworkspace에는 내가 대화한 내용들은 확인할 수 있음. 우측에 Review Changes를 누르면 변환된 부분만 확인할 수 있음\n\nworkspace에 질문을 하니, 에디터쪽에도 동일한 workspace가 열려있어서 동기화됨\n\n대화하다가 실수로 Conversation을 껐는데, option + command + b로 Toggle Agent를 다시 살렸음. 우측 최상단에 있는 이 버튼이였음\n\nPlayground\nPlaygrounds are independent workspaces perfect for quick prototypes or following your curiosity. Move to a dedicated workspace to continue exploring with multiple conversations라고 나와있는데, 간단히 요약하면 프로토타입을 만들 때 쓸 수 있는 공간\n여기서 구현한 기능을 지금 프로젝트로 move 하는 기능도 제공함\n    \n다만 지금 에디터로 열었던 파일들을 참고하지 않는다. 제로베이스에서 시작하는 것\n~/.gemini/antigravity/playground/ : 경로를 확인해보니 해당 폴더에 저장됨\nKnowledge\npersistent memory라고 나와있는데, 아직 생성된 지식이 없어서 아무것도 나오지 않음\n\n\nChangelog\nChangelog 페이지를 보면 어떤 기능을 업데이트했는지 확인할 수 있음\n25년 12월 4일 기준 Google AI Pro, Ultra 구독자에게 사용량을 늘려주고 Rate limit 주기를 더 빈번하게 수정함. 그 전에는 조금만 쓰면 쿼타가 찼다는 이야기가 나왔는데, Google AI 요금제랑 연동해서 점점 해결될 것으로 보임\n\n\n\n전반적인 후기\nAntigravity 장점\n현재 Preview라 무료 사용 가능(Google AI 멤버십을 사용한다면 더 많은 사용량을 받음. Gemini 구독 중이라면 꼭 써보는 것을 추천)\nGemini 3, Claude Opus도 사용 가능\n크롬 브라우저와 통합되어서 브라우저 제어가 수월함\n브라우저 작업시 Screen Recording을 해서 확인할 수 있음\nAgent Manager 구성으로 Agent가 어떻게 동작하는지 확인할 수 있음(다른 IDE와 차별점)\n이미지에 드래그를 해서 피드백을 줄 수 있음(디테일한 피드백)\n기존에 VSCode를 사용했는데, Antigravity를 메인으로 사용할 예정\n개선하면 좋을 부분\n나온지 얼마 되지 않아서 오류가 존재할 수 있음. 이는 Preview 단계라 그렇고 점점 개선될 것으로 예상\nBigQuery MCP 설정할 때 오류가 발생했는데, 디버깅이 어려웠음\n\nGoogle Cloud credits are provided for this project. #AISprint",
    "reviews": [],
    "syllabus": [],
    "link": "https://zzsza.github.io/ai/2025/12/08/how-to-use-antigravity/",
    "pubDate": "Mon, 08 Dec 2025 00:00:00 +0000",
    "creator": "Unknown",
    "categories": [
      "basic",
      "ai"
    ]
  },
  {
    "id": 14,
    "imageUrl": "",
    "title": "데이터 시스템의 미래 - 옳은 일 하기",
    "description": "Doing the Right Thing: 데이터 시스템 설계에서 “옳은 일”을 한다\n앞선 장에서는 데이터 시스템의 아키텍처를 비교하고, 신뢰성·확장성·유지보수성을 높이는 기법을 논의하였다. 그러나 기술적 논의만으로는 충분하지 않다. 모든 시스템은 목적을 가지고 만들어지며, 모든 행동은 의도한 결과뿐 아니라 의도하지 않은 결과도 낳는다. 엔지니어는 시스템이 사회에 미치는 영향을 고려하고, 어떤 세계를 만들고 싶은지에 대해 의식적으로 선택할 책임을 가진다.\n데이터는 추상적 대상처럼 보이지만, 실제로는 사람들의 행동·관심·정체성에 관한 데이터가 많다. 따라서 사람에 관한 데이터를 다룰 때는 인간 존엄과 존중을 최우선 가치로 두어야 한다.\n기술은 선악이 아니라 사용 방식이 문제이다\n기술 자체가 선하거나 악한 것이 아니라, 그것이 어떻게 사용되고 사람에게 어떤 영향을 미치는지가 중요하다. 검색 엔진과 같은 소프트웨어도, 총과 같은 무기도 “사용과 결과”가 본질을 결정한다. 엔지니어가 기술만 바라보고 결과를 외면하는 태도는 충분하지 않으며, 윤리적 책임은 엔지니어에게도 존재한다.\n예측 분석(Predictive Analytics)은 개인의 삶을 직접적으로 바꾼다\n날씨나 질병 확산을 예측하는 분석과 달리, 다음과 같은 예측은 개인의 삶에 직접 영향을 준다.\n재범 가능성 판단\n대출 부도 가능성 판단\n보험금 청구 위험 판단\n채용에서의 “리스크” 판단\n조직은 위험 회피 관점에서 “의심스러우면 거절”을 선택하기 쉽다. 그러나 알고리즘이 개인을 “위험하다”고 라벨링하면, 그 사람은 반복적으로 거절당하며 사회 참여에서 배제될 수 있다. 이러한 체계적 배제는 개인의 자유를 크게 제한하며, “알고리즘 감옥(algorithmic prison)”이라고 불리기도 한다. 이는 죄가 증명되기 전까지 무죄를 전제로 하는 인권 원칙과 정면으로 충돌할 수 있다.\n편향과 차별: 알고리즘은 공정함을 자동으로 만들지 않는다\n알고리즘 판단이 인간 판단보다 반드시 공정하거나 불공정한 것은 아니다. 인간도 편향을 가지고 있으며 차별이 제도화될 수 있다. 데이터 기반 판단이 오히려 더 공정할 수 있다는 기대도 존재한다.\n그러나 예측 분석에서 규칙 자체를 데이터로부터 학습시키면 다음 문제가 발생한다.\n학습된 패턴이 불투명(opaque)하여 “왜 그렇게 판단했는지” 알기 어렵다.\n입력 데이터가 편향되어 있으면, 모델은 그 편향을 학습하여 증폭할 가능성이 높다.\n보호 특성(인종/성별/나이/장애/종교 등)을 직접 사용하지 않더라도, 우편번호·IP 주소 등은 보호 특성과 강하게 상관될 수 있어 사실상 대리 변수(proxy)가 된다.\n따라서 편향된 과거 데이터로부터 “공정한 미래”가 자동으로 만들어진다고 믿는 태도는 위험하다. 예측 모델은 과거를 외삽할 뿐이며, 과거가 차별적이었다면 그 차별을 제도화한다. 더 나은 미래를 만들기 위해서는 인간의 도덕적 상상력과 의식적 개입이 필요하다. 데이터와 모델은 도구이지 주인이 아니다.\n책임과 설명 가능성: 누가 책임지는가\n자동화된 의사결정이 확산되면 책임과 설명 가능성 문제가 발생한다.\n인간이 실수하면 책임을 물을 수 있고 항소 절차가 가능하다.\n알고리즘이 실수하면 누가 책임지는지 불분명해진다.\n자율주행차 사고, 신용평가 알고리즘의 차별 등에서 책임 주체가 모호해진다.\n사법적 검토가 들어오면, 모델이 왜 그런 결정을 했는지 설명할 수 있어야 한다.\n전통적 신용점수는 대체로 “개인의 과거 상환 기록” 같은 관련 사실에 기반하며, 오류 수정의 가능성이 존재한다(절차가 어렵더라도). 반면 머신러닝 기반 평가는 입력이 훨씬 넓고 불투명하여, 특정 개인이 왜 불이익을 받았는지 확인하기 어렵고 부당 대우 여부도 판단하기 어렵다.\n또한 예측 분석은 종종 “당신은 과거에 무엇을 했는가”가 아니라 “당신과 유사한 사람들이 과거에 무엇을 했는가”에 근거한다. 이는 고정관념과 범주화(stereotyping)를 내포하며, 잘못된 범주에 배치된 개인에게는 구제가 어렵다. 예측은 본질적으로 확률적이므로, 전체적으로는 맞더라도 개별 사례에서 틀릴 수 있다. 따라서 데이터 기반 결정의 절대적 우월성을 믿는 태도는 위험하다.\n필요한 과제는 다음과 같다.\n알고리즘의 책임성과 투명성 확보\n편향 증폭 방지\n오류가 발생했을 때 수정 및 구제(recours) 메커니즘 구축\n데이터가 사람을 해치는 데 쓰이지 않도록 방지\n피드백 루프: 예측이 현실을 고정시키는 악순환\n추천 시스템처럼 비교적 “가벼운” 예측에서도 사회적 문제는 발생한다.\n사용자가 이미 동의하는 콘텐츠만 노출되어 에코 챔버가 강화될 수 있다.\n고정관념, 허위정보, 양극화가 증폭될 수 있다.\n개인의 삶에 직접 영향을 주는 예측에서는 더 위험한 자기강화 피드백 루프가 생긴다. 예를 들어 채용에서 신용점수를 활용하면, 불운으로 점수가 떨어진 사람은 취업이 어려워지고, 실업은 빈곤을 심화하며, 점수는 더 악화되는 하향 나선이 발생한다. 수학적 엄밀성과 데이터가 이러한 가정을 가리는 위장막이 될 수 있다.\n이 문제는 컴퓨터만이 아니라 사람과 제도를 포함한 전체 시스템을 보는 시스템 사고(systems thinking) 가 필요하다. 시스템이 기존 격차를 증폭하는지, 불의를 완화하는지, 의도치 않은 결과는 무엇인지 점검해야 한다.\n프라이버시와 트래킹: 데이터 수집 자체의 윤리 문제\n사용자가 명시적으로 입력한 데이터를 저장·처리하는 시스템은 사용자를 위한 서비스를 수행하며, 사용자와 서비스의 관계가 비교적 명확하다. 반면 사용자의 활동이 “부수 효과”로 추적·기록되는 경우 관계는 달라진다. 서비스는 사용자가 시킨 일만 수행하는 것이 아니라, 자체 이해관계를 갖게 되며, 이는 사용자의 이해관계와 충돌할 수 있다.\n트래킹은 검색 품질 향상, 추천, A/B 테스트, UI 개선 등 사용자에게 이득을 주는 측면도 있다. 그러나 광고 기반 비즈니스 모델에서는 사용자가 아니라 광고주가 실질적 고객이 되며, 추적은 더 정교해지고 보유 기간이 길어지며, 개인 프로파일링이 강화된다. 이 관계는 “감시(surveillance)”로 볼 수 있다.\n감시(surveillance)라는 관점의 사고 실험\n“data”라는 단어를 “surveillance(감시)”로 치환하면, 평소 미화되던 표현이 다른 의미로 들린다. 대규모 감시 인프라가 사실상 구축되고 있으며, IoT의 확산은 마이크·센서가 상시 연결되는 환경을 만들고 있다. 많은 기기는 보안 수준도 낮다.\n과거의 권위주의 국가도 상상하기 어려웠던 “항상 휴대되는 위치 추적 장치”와 “공간마다 존재하는 마이크”가 기업 주도의 방식으로 현실화되고 있다. 감시는 정부만이 아니라 기업에 의해 수행될 수 있으며, 그 결과가 개인의 보험·고용 등 삶의 핵심 영역을 좌우하면 더 이상 “무해한 개인화”로 보기 어렵다.\n동의(consent)와 선택의 자유는 형식적으로만 존재할 수 있다\n이용약관과 프라이버시 정책에 동의했으므로 사용자가 자발적으로 선택했다고 주장할 수 있다. 그러나 다음 한계가 존재한다.\n사용자는 어떤 데이터가 수집되고 어떻게 결합·보존·처리되는지 충분히 알기 어렵다.\n프라이버시 정책은 이해를 돕기보다 은폐하는 방식으로 작성되는 경우가 많다.\n한 사람의 데이터는 다른 사람(서비스 비사용자)의 정보도 유추할 수 있어, 당사자의 동의만으로 충분하지 않다.\n데이터 제공과 서비스 제공의 교환은 상호 호혜적 협상 관계가 아니라, 서비스가 일방적으로 조건을 정한다.\n서비스가 사회 참여에 사실상 필수라면(네트워크 효과 포함), “사용하지 말라”는 선택지는 실질적 자유가 아니다.\n결국 감시에 동의하지 않는 사람에게 의미 있는 선택권이 없을 수 있으며, 감시는 특권층을 제외하면 회피하기 어려워진다.\n프라이버시는 비밀이 아니라 “결정권”이다\n“프라이버시는 죽었다”는 주장은 프라이버시 개념을 오해한 것이다. 프라이버시는 모든 것을 숨긴다는 뜻이 아니라, 무엇을 누구에게 공개할지 스스로 결정할 자유를 의미한다. 감시 인프라를 통해 데이터가 추출되면, 이 결정권은 개인에서 데이터 수집자로 이전된다.\n기업은 “올바르게 사용하겠다”고 요구하며, 사용자에게는 선택권이 제한된다. 또한 기업은 감시의 결과를 공개하지 않는 경향이 있으며, 광고 타기팅 같은 간접 형태로만 드러난다. 이는 개인이 자신의 민감 정보(질병 여부 등)의 공개에 대한 주도권을 잃는 결과를 낳는다.\n프라이버시 설정은 일부 통제권을 되돌리는 시작점이지만, 서비스 제공자는 여전히 데이터에 광범위한 접근권과 내부 분석권을 가진다. 개인의 프라이버시 권리가 대규모로 기업에 이전되는 현상은 역사적으로 전례가 드물다.\n데이터는 자산이자 위험 자산(toxic asset)이다\n행동 데이터는 “데이터 배출물(data exhaust)”처럼 가치 없는 부산물로 묘사되기도 한다. 그러나 광고 기반 모델에서는 행동 데이터가 사실상 핵심 자산이며, 사용자 서비스는 데이터 수집을 위한 수단으로 전락할 수 있다.\n데이터 브로커 산업은 개인 데이터를 구매·결합·추론·재판매한다.\n정부도 비공식 협력, 강제력, 법적 요구, 또는 탈취를 통해 데이터를 획득하려 한다.\n기업 파산 시 개인 데이터는 매각될 수 있다.\n데이터 유출은 빈번하며, 데이터는 “유해 물질”에 비유되기도 한다.\n따라서 데이터 수집은 “오늘의 정치 환경”뿐 아니라 “미래의 모든 정권”까지 고려해야 한다. 언젠가 인권을 존중하지 않는 정권이 등장할 가능성을 배제할 수 없으므로, 장차 경찰 국가를 가능케 할 기술을 설치하는 것은 시민적 위생(civic hygiene) 관점에서 위험하다.\n데이터와 지식은 권력이며, 타인을 감시하면서 자신은 감시받지 않는 것은 강력한 권력이다. 기술 기업이 정치 권력을 노리지 않더라도, 축적된 데이터는 공적 감독 밖에서 사람들의 삶에 영향을 미치는 잠재적 권력이 된다.\n산업혁명과 정보화 시대: 데이터는 “정보 시대의 오염”이다\n정보화 시대는 산업혁명과 유사하게 장기적으로는 생활 수준을 높일 수 있으나, 단기적으로는 심각한 부작용이 발생할 수 있다. 산업혁명은 오염, 열악한 노동 환경, 아동 노동 등을 낳았고, 이후 규제와 안전장치가 구축되면서 사회 전체가 큰 이익을 얻었다.\n이와 유사하게, 정보화 시대에서 데이터 수집·사용·남용은 중대한 문제이며, 프라이버시 보호는 환경 보호에 비견될 수 있다. 데이터는 정보 시대의 “오염”이며, 이를 어떻게 관리·격리·폐기하는지가 정보 경제의 건강을 좌우한다.\n법과 자율규제: 균형과 문화 변화가 필요하다\n데이터 보호 법제는 개인 권리를 보존하는 데 도움을 줄 수 있다(예: 목적 제한, 최소 수집, 과잉 수집 금지). 그러나 인터넷 환경에서 실효성은 논쟁적이며, “가능한 많이 모아 나중에 쓰자”는 빅데이터 철학과 충돌한다. 동시에 의료 데이터 분석처럼 사회적 효익이 큰 영역에서는 과규제가 혁신을 방해할 위험도 있어, 기회와 위험의 균형이 어렵다.\n근본적으로는 다음 문화 변화가 요구된다.\n사용자를 최적화할 “지표”로 보지 않고, 존엄·대리권(agency)을 가진 인간으로 대한다.\n데이터 수집과 처리 관행을 자율적으로 절제하여 신뢰를 구축한다.\n사용자에게 데이터 사용 방식에 대한 교육과 투명성을 제공한다.\n데이터는 필요 기간만 보관하고, 필요가 끝나면 폐기한다(“영원 보관”을 당연시하지 않는다).\n정책만이 아니라 암호학적 접근 제어 등 기술적 수단으로 권한을 강제하는 접근을 고려한다.\n장 요약: 데이터플로우·무결성·감사·윤리\n본 장의 논의는 다음 흐름으로 요약된다.\n단일 도구가 모든 요구를 만족시키지 못하므로, 애플리케이션은 여러 컴포넌트를 조합해야 한다.\n배치 처리와 이벤트 스트림을 통해 시스템 간 데이터 변경을 흐르게 하여 데이터 통합 문제를 해결할 수 있다.\n일부 시스템은 기록의 원천(system of record)이 되고, 다른 데이터는 변환을 통해 파생된다(인덱스, 물질화 뷰, ML 모델, 통계 요약 등).\n파생을 비동기·느슨 결합으로 구성하면, 한 영역의 문제가 다른 영역으로 전파되는 것을 줄여 견고성과 장애 내성을 높인다.\n변환 단계의 변경은 입력 전체에 대해 재실행하여 출력 재파생이 가능하며, 이는 애플리케이션 진화를 쉽게 한다.\n분산 트랜잭션 대신, end-to-end 요청 식별자에 기반한 멱등성 및 비동기 제약 검증을 통해 강한 무결성을 더 확장 가능하고 견고하게 달성할 수 있다.\n제약 위반은 즉시 차단하거나, 사후 보정(Compensation)으로 처리할 수 있으며, 이는 실제 비즈니스와 잘 맞는다.\n무결성 검증을 위해 감사(auditing)를 도입하여 데이터 손상과 오류를 조기에 탐지해야 한다.\n마지막으로, 데이터 집약 애플리케이션은 윤리적 책임을 수반하며, 예측 분석·차별·감시·프라이버시 침해·유출·의도치 않은 결과를 고려해야 한다.\n엔지니어는 인간 존엄과 존중을 기반으로, 자신이 만들고 싶은 세계를 의식적으로 선택해야 한다.\n참고\n데이터 중심 애플리케이션 설계 12장",
    "reviews": [],
    "syllabus": [],
    "link": "https://velog.io/@ahngj96/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%98-%EB%AF%B8%EB%9E%98-%EC%98%B3%EC%9D%80-%EC%9D%BC-%ED%95%98%EA%B8%B0",
    "pubDate": "Mon, 08 Dec 2025 13:38:39 GMT",
    "creator": "Unknown",
    "categories": []
  },
  {
    "id": 15,
    "imageUrl": "",
    "title": "크리피 비하인드 / 게임소재 / 게임 분석 / 미니게임 많이 모아둔 것",
    "description": "게임 제목이 바꼈나봅니다.\n\n\n \n \n영상: https://www.youtube.com/watch?v=fq4jDVb-bwA\n\n\n\n \nAI 로 스토리 만들어서 게임에 넣자고 하길레 이게임이 생각나서 다시 찾아봤습니다.\n상황 시작 -> 게임 -> 실패 or 성공 연출\n여기서 AI 령상을\n상황시작 ( 이건 없어도 되겠고 )\n게임중 ( 안절 부절한 영상 )\n실패 ( 망한 영상 )\n성공 ( 좋은 영상 )\n게임중 영상은 뒤에 배경으로 깔고\n이것저것 하면 될꺼 같습니다.",
    "reviews": [],
    "syllabus": [],
    "link": "https://serverdown.tistory.com/1535",
    "pubDate": "Wed, 10 Dec 2025 00:11:51 +0900",
    "creator": "SIDNFT",
    "categories": [
      "게임",
      "게임소재"
    ]
  }
]