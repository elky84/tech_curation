[
  {
    "id": 1,
    "imageUrl": "",
    "title": "프로그래머와 협업을 위한 git 설치와 사용 / TortoiseGit",
    "description": "코딩하는 사람은 \n이미지나 ui 의 모양은 개발하는데 중요하진 않습니다.\n개발은 더미 이미지나 버튼 배치로 얼마든지 진행할 수 있습니다.\n하지만 디자이너들에겐 이런것들이 중요하겠죠\n개발자에게 이미지를 바꿀때마다 파일을 던져는 식이라면 양쪽모두에게 절차가 길어집니다.\n그래서 디자이너가 직접 파일을 바꿀 수 있게 해주는 것이 좋습니다.\n디자이너는 매번 덮어쓰면되기 때문에 GIT 에 대해 알아햘 사항이 많지는 않습니다. \n그래서 익혀두시면 좋습니다.\n \n구글에서 TortoiseGit  이걸 검색하면 설치페이지를 알려줍니다.\n거북이 깃 최신버전 링크: https://tortoisegit.org/\n보통 64bit 쓰니까 64bit 로 설치합니다.\n다음다음 해서 설치하시구요\n여기까지 하시면\n\n\n윈도우 탐색기에서 우클릭 하시면 clone 을 할 수 있습니다.\n처음에 소스를 가져오는 행동이 클론 clone 입니다.\n눌러모시면 에러가 납니다.\n\n\ngit 명령어 프로그램이 필요하기 때문입니다.\n일단 Set Git Path 를 누릅니다.\n\n\n화살표를 보시면 git 을 받을 수 있는 링크를 제공합니다. \n눌러서 웹페이지가 열리면 다운받을 수 있습니다.\n실행하시면 여러가지를 물어보는데 \n따로 만질것은 없고 다음다음 눌러 설치하면 됩니다.\n위에 Set Git Path 창은 이제 필요 없습니다. 닫으시구요\nclone 다시 진행합니다.\n \n \n \n \n \n \n거북이 깃은 git 명령어가 어렵기 때문에 ui 로 만질수 있게 해주는 \n\n\n\nurl 주소 넣으시구요 ok 누릅니다.\n각 주소에 따라 접근권한이 막혀있으면 각종 로그인 창이 뜹니다.\n로그인하시면 됩니다.\n \n이제 뭔가 수정했다고 치고요 수정이 되면 파일이 빨간색 아이콘이 뜨게됩니다.\n파일을 업로드하는 과정인  commit 과 push 를 해봅시다.\n\n\n우클릭해서 commit 누르세요\n\n\n상단부에 설명을 써야합니다. \n비어있으면 안올라가기 때문에 아무거나 쓰세요\n아래쪽엔 내가 고친 파일들이 보입니다. 확인하시구요\n\n\n하단에 Commit 버튼을 보셔야합니다.\ncommit 만 하시면 서버에 안올라갑니다. push 를 해야 서버까지 올라갑니다.\n화살표 보고 Commit & Push 를 누르세요\n누르면 바로 진행합니다.\n권한 설정에 따라 성공하거나 실패할 수 있습니다. \n실패하면 git 소스 소유자에게 물어보세요\n \n최신소스 받는법 Pull\n처음엔 clone 을 해서 받아왔지만 이후에는 Pull 을 이용해 가져와야합니다.\n매번 clone 을 하면 오래걸리 때문에 바뀐거만 받는 것입니다.\n우클릭하구요\n\n\nGit Sync 누릅니다.\n\n\n아래에 보시면 Pull 있습니다. 누르시면 받아집니다.\n참고로 이 창에선 Commit Push 다 별도로 할 수 있습니다.\n \n \n잘못 고쳤어요 Revert\nCommit & Push  하기전엔 되돌릴 수 있습니다.\n해당파일에 직접 가서 우클릭하시면\n\n\nRevert 가 있습니다. 누른다고 진행되는것이 아니고 다시 파일을 고를 수 있는 창이 표시됩니다.\n\n\n어떤파일을 Revert 되돌리기 할지 선택할 수 있습니다.\n파일 체크 하시고 ok 누르시면 됩니다.\n \n요약\n처음에 받기 clone\n고치고 올리기 Commit & Push\n최신소스 다시 받기 Pull\n고친거 버리기 Revert \n이렇게 네가지를 만 하시면 될것 같습니다.\n점점 불황으로 빠지고 있어서 효율을 높이지 않으면 경쟁력에서 밀릴 것입니다.\n적극적으로 사용해서 경쟁력을 올리시 바랍니다.",
    "reviews": [],
    "syllabus": [],
    "link": "http://serverdown.tistory.com/1163",
    "pubDate": "Sat, 1 Mar 2025 18:59:21 +0900",
    "creator": "SIDNFT",
    "categories": [
      "프로그래밍/개발메모"
    ]
  },
  {
    "id": 2,
    "imageUrl": "",
    "title": "A case for QLC SSDs in the data center",
    "description": "The growth of data and need for increased power efficiency are leading to innovative storage solutions. HDDs have been growing in density, but not performance, and TLC flash remains at a price point that is restrictive for scaling.  QLC technology addresses these challenges by forming a middle tier between HDDs and TLC SSDs.   QLC [...]\nRead More...\nThe post A case for QLC SSDs in the data center appeared first on Engineering at Meta.",
    "reviews": [],
    "syllabus": [],
    "link": "https://engineering.fb.com/2025/03/04/data-center-engineering/a-case-for-qlc-ssds-in-the-data-center/",
    "pubDate": "Tue, 04 Mar 2025 17:00:26 +0000",
    "creator": "Unknown",
    "categories": [
      "Data Center Engineering"
    ]
  },
  {
    "id": 3,
    "imageUrl": "",
    "title": "배달 라이더라면 꼭 알아야 하는 종합소득세 가이드",
    "description": "한 번 읽어두면 두고두고 도움 될 거예요",
    "reviews": [],
    "syllabus": [],
    "link": "https://blog.toss.im/article/tossmoment-1",
    "pubDate": "Thu, 27 Feb 2025 11:35:00 GMT",
    "creator": "Unknown",
    "categories": []
  },
  {
    "id": 4,
    "imageUrl": "",
    "title": "눈 깜짝할 사이에 당하는 스맛폰 도난",
    "description": "영상: https://www.youtube.com/watch?v=UEnx08n0amk\n\n\n\n저도 보고 도 어리둥절 했습니다.\n이런 방식이 있다니 ㄷㄷㄷ",
    "reviews": [],
    "syllabus": [],
    "link": "http://serverdown.tistory.com/1165",
    "pubDate": "Tue, 4 Mar 2025 01:32:22 +0900",
    "creator": "SIDNFT",
    "categories": []
  },
  {
    "id": 5,
    "imageUrl": "",
    "title": "인생을 0.5배속으로 살아가기",
    "description": "올해가 카운트다운이 된 지 벌써 60일이 훌쩍 지났습니다. 아직도 저는 날짜를 입력할 때 2024로 쓰고 지우길 반복하는데 말이죠",
    "reviews": [],
    "syllabus": [],
    "link": "https://minieetea.com/20250304-0-5x-speed-for-life/",
    "pubDate": "Tue, 04 Mar 2025 12:38:51 GMT",
    "creator": "minieetea",
    "categories": []
  },
  {
    "id": 6,
    "imageUrl": "",
    "title": "식자재 품목 검색을 더 쉽게! 검색 엔진 도입과 개선",
    "description": "안녕하세요. 스포카 백엔드팀 프로그래머 이지민입니다.\n스포카에서는 식당 점주분들이 식자재 주문을 더 편리하게 하기 위한 많은 노력들을 하고 있습니다.\n그중에서도, 주문하려는 품목을 검색하여 원하는 품목을 빠르게 찾을 수 있도록 품목 검색 기능을 제공하고 있는데요.\n검색 엔진 도입부터 지금의 검색이 되기까지의 과정들을 이야기해보려고 합니다.\n도입 초기에는 검색 엔진에 대한 이해가 깊지 않아, 논리적인 의사결정보다는 다양한 테스트를 통해 더 나은 결과를 찾는 방식으로 기능을 결정, 구현하였습니다.\n이 점을 고려해 읽어주시길 바라며, 이 글은 검색 엔진의 점진적인 발전 과정을 다루는 이야기이니, 순차적으로 읽어보시면 개선 과정이 더욱 잘 이해되실 것 같습니다!\n검색 엔진 도입 배경\n품목 검색 기능 초기에는 Database의 LIKE 질의를 통한 검색만 제공되었습니다. 이로 인해 품목명에 띄어쓰기가 다르거나 맞춤법이 정확히 일치하지 않는 경우, 사용자가 원하는 결과를 찾기가 어려웠습니다.\n예를 들어, 깐마늘을 검색했을 때 마늘/깐 이라고 저장되어 있는 유통사 품목은 검색되지 않아 점주들은 깐마늘을 유통사가 취급하지 않는다고 오해하는 상황이 발생하곤 했습니다.\n이와 같은 문제와 사용하는 점주의 수가 증가하고 품목의 종류가 다양해짐에 따라, DB 검색 기능의 한계가 더 드러나게 되었고 이를 해결하기 위해 검색 엔진 도입의 필요성이 대두되었습니다.\n이번 검색 엔진 도입이 스포카에서 최초 도입은 아닌데요.(하지만 제가 처음이에요.) (구)도도카트 서비스 운영 당시 많은 명세표 품목을 검색하는데 Elasticsearch 검색 엔진을 활용했었습니다.\n우선 별도의 검색 품질에 대한 기준이 마련되어 있지 않았기 때문에 (구)도도카트 서비스의 검색 엔진 설정을 참고해 Elasticsearch(이하 ES)를 POC 해보기로 했습니다.\n\nDB LIKE 검색과 ES 검색 비교 POC\n품목 데이터는 product 라는 인덱스에 다음과 같은 setting 으로 구성했습니다.\n\n{\n  \"product\":{\n    \"mappings\":{\n      \"properties\":{\n        // .. 생략\n        \"name\":{\n          \"type\":\"text\",\n          \"analyzer\":\"korean\",\n          \"fields\":{\n            \"ngram\":{\n              \"type\":\"text\",\n              \"analyzer\":\"korean_ngram\"\n            }\n          }\n        }\n      }\n    },\n    \"settings\":{\n      \"index\":{\n        \"analysis\":{\n          \"filter\":{\n            \"edge_ngram_back\":{\n              \"min_gram\":\"1\",\n              \"side\":\"back\",\n              \"type\":\"edge_ngram\",\n              \"max_gram\":\"5\"\n            },\n            \"edge_ngram_front\":{\n              \"min_gram\":\"1\",\n              \"side\":\"front\",\n              \"type\":\"edge_ngram\",\n              \"max_gram\":\"5\"\n            }\n          },\n          \"analyzer\":{\n            \"korean\":{\n              \"filter\":[\n                \"lowercase\",\n                \"trim\"\n              ],\n              \"type\":\"custom\",\n              \"tokenizer\":\"nori_mixed\"\n            },\n            \"korean_ngram\":{\n              \"filter\":[\n                \"lowercase\",\n                \"edge_ngram_front\",\n                \"edge_ngram_back\",\n                \"trim\"\n              ],\n              \"type\":\"custom\",\n              \"tokenizer\":\"nori_mixed\"\n            }\n          },\n          \"tokenizer\":{\n            \"nori_mixed\":{\n              \"type\":\"nori_tokenizer\",\n              \"decompound_mode\":\"mixed\"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n\n품목명을 저장할 name field, name.ngram field 구현\n한글 검색의 정확성과 유연성을 높이기 위해 Nori Tokenizer와 Edge N-gram 필터를 활용해 띄어쓰기나 일부 단어만으로도 검색이 가능하도록 설정\n또한, LIKE 질의를 사용하는 기존 DB 검색과 ES 검색의 결과를 비교한 POC 결과는 다음과 같습니다.\n\n결과를 통해 볼 수 있듯이, 정확하지 않은 키워드로 검색했을 때도 기존 DB의 LIKE 질의보다 ES 검색이 훨씬 더 나은 결과를 제공하는 것을 확인할 수 있었습니다.\n비용 증가와 관리 포인트가 늘어남에도 불구하고 앞서 언급한 문제들이 해소되었고 검색 품질을 크게 항상시킬 수 있을거 같아 검색 엔진 도입을 최종적으로 결정하게 되었습니다!\n다만, 이번 테스트는 전체 검색어가 아닌 일부 검색어를 대상으로 진행된 POC 였기 때문에, 실제 사용자의 피드백을 바탕으로 지속적인 수정과 개선이 필요할 것으로 예상하고 있었습니다.\n이러한 부분을 미리 인지하고 마음의 준비(?)와 공부를 하고 있었죠.\n개선 작업\n1) 가중치 조절 및 N-gram 조정\n이슈 및 원인 분석\n검색 엔진을 적용한 후에 아래와 같은 피드백이 들어왔습니다.\n\n주문하려고 했던 품목은 통베이컨(에스푸드)였지만, 통베, 통베이 키워드로 검색했을 때 상위에 노출되지 않는다는 이슈였습니다.\n이 문제를 해결하기 위해, 우선 통베라는 검색어를 중심으로 원인을 분석해보았습니다.\n문제 원인 파악을 위해 _analyze API를 활용하여 name 필드에 적용된 분석기(analyzer)가 검색어를 어떻게 토큰화하는지 살펴보았습니다.\n\nGET product/_analyze\n{\n  \"field\" : \"name\",\n  \"text\" : \"통베이컨(에스푸드)\"\n}\n\nResponse\n{\n  \"response\":{\n    \"tokens\":[\n      {\n        \"token\":\"통\",\n        \"start_offset\":0,\n        \"end_offset\":1,\n        \"type\":\"word\",\n        \"position\":0\n      },\n      {\n        \"token\":\"베이컨\",\n        \"start_offset\":1,\n        \"end_offset\":4,\n        \"type\":\"word\",\n        \"position\":1\n      },\n      {\n        \"token\":\"에스\",\n        \"start_offset\":5,\n        \"end_offset\":7,\n        \"type\":\"word\",\n        \"position\":2\n      },\n      {\n        \"token\":\"푸드\",\n        \"start_offset\":7,\n        \"end_offset\":9,\n        \"type\":\"word\",\n        \"position\":3\n      }\n    ]\n  }\n}\n\n\n결과는 [통, 베이컨, 에스, 푸드] 로 예측 가능하게 나오네요.\n그러나 문제의 검색어인 통베는 다음과 같이 토큰화되었습니다:\n[통, 베, 베, 어]\n잠깐 어 는 뭐지? 라고 생각하실 수 있는데요. 통베라는 단어 어디에도 어 라는 단어는 찾아볼 수 없기 때문이죠.\n이는 ES의 Nori Tokenizer 가 한국어 문장에서 어미를 추출하는 방식을 따라 토큰화하기 때문입니다.\n예를 들어, “강아지가 밥을 먹습니다”라는 문장은 [강아지, 가, 밥, 을, 먹, 습니다]로 명사와 어미를 구분하여 토큰화됩니다.\n따라서 정확히 알기는 어렵지만 어는 Nori Tokenizer 가 베에서 어미로 분리한 결과로 예상하고 있어요.\n이런 토큰화 방식을 보고 Nori Tokenizer 의 토큰화는 단어별로 검색하는 패턴이 많은 저희 서비스에서 예측 불가능할 수 있겠다라는 깨달음을 얻었어요.\n하지만 Nori를 아예 제거하기엔 Nori 가 해주는 명사 추출의 이점이 있을 수 있어 조심스러웠습니다.\n결론적으로, Nori 기능을 완전히 제거하는 대신, 다른 접근을 시도하기로 했습니다.\n쿼리 가중치 조절 POC\n문제 해결을 위해 쿼리의 가중치를 조절해보기로 했습니다.\n기존엔 쿼리 가중치를 순수 Nori Tokenizer 가 적용된 name 필드와 Nori Tokenizer 와 N-gram filter 가 적용된 name.ngram 필드에 각각 10과 5 를 주고 있었는데요.\n따라서 N-gram 에 의해 검색된 품목보다 순수 Nori 에 의해 검색된 품목의 유사도가 높아져 상위에 올라가게 됩니다.\n통베 라고 검색했을때 통베이컨(에스푸드) 품목이 올라오기 위한 가중치 조절과 N-gram Filter 조정이 필요해보였습니다.\n\n최적의 가중치와 N-gram Filter 설정을 찾기 위해 통베이컨 품목을 기준으로 삽질을 테스트를 아래와 같이 해보았는데요.\n\n\nNori Tokenizer 에만 의존하기에는 무리가 있을거 같아 ES 기본 Tokenizer 인 Standard Tokenizer 도 추가해서 테스트 해봤습니다. \n가중치의 경우, 순수 Nori 인 nori 와 N-gram 을 적용한 ngram, standard 필드의 가중치를 조정해보며 각각 2.0, 3.0, 2.0 일 때 통베이컨(에스푸드)와 세척당근이 가장 잘 검색되는 것을 확인 했습니다.\n여기에서 Edge N-gram 에 대해서 간단히 설명드리자면요.\n\n{\n  \"min_gram\": \"1\",\n  \"max_gram\": \"3\",\n  \"side\": \"front\",\n  \"type\": \"edge_ngram\"\n}\n\n\nmin_gram : 최소 토큰 길이\nmax_gram : 최대 토큰 길이\nside : 단어의 어느 부분부터 토큰화 할지 설정(front/back)\nside 가 front 인 위 예시로 안녕하세요를 토큰화해보면 [안, 안녕, 안녕하]로 토큰화되고 side 가 back 일 경우엔 [요, 세요, 하세요]로 토큰화 됩니다.\n때문에 front 의 경우 주로 첫 글자부터 검색하는 자동완성과 같은 곳에서 사용하고 back 은 주로 뒷글자부터 검색하는 경우, 예를들면 영어로 ion 을 검색했을 때 action, station, evolution 같은 것들을 검색할 때 유용하게 사용할 수 있을거예요.\n저희는 식자재 검색라는 특성이 있어 통베, 세척당과 같이 앞글자부터 검색하는 경우가 많기 때문에 back 은 제거하고 front 만 남기기로 했습니다. max_gram 도 기존엔 5로 토큰화가 많이 되어 오히려 정확성을 떨어트리는 것을 발견했고 적절해보이는 3으로 조정했습니다.\n결론\n결론적으로 아래 조정 작업으로 문제가 되었던 품목이 검색 상위에 안정적으로 노출되도록 검색 품질을 향상시켰습니다.\nN-gram 조정: max_gram 값을 5 -> 3으로 하향 조정하고 side: front 만 사용\n가중치 조정: Nori, N-gram, Standard 분석기의 가중치를 적절히 분배\n2) Wildcard 검색\n이슈 및 원인 분석\n위 작업을 배포하고 내부에서 아래와 같은 피드백을 받았습니다.\n\n칠성사이다/355ml*24캔라는 품목이 있는데도 불구하고 사이다 라고 검색했을때 검색이 되지 않는 이슈였는데요.\n각 분석기에서 칠성사이다/355ml*24캔이 토큰화된 결과는 다음과 같았습니다.\n\nNori: 칠성사, 칠, 성사, 이, 다, 355, ml, 24, 캔\n\nN-gram: 칠, 칠성, 칠성사, 칠, 성, 성사, 이, 다, 3, 35, 355, m, ml, 2, 24, 캔\n\nStandard: 칠성사이다, 355ml, 24캔\n\n\n결과에서 확인할 수 있듯이, 사이다라는 토큰이 생성되지 않아 검색 결과에서 제외된 것입니다.\n위에서 가중치와 N-gram 을 조정했는데도 불구하고 왜 사이다로 토큰화되지 않았을까요?\n이는 N-gram 은 Filter 이기 때문에 Nori 분석기에서 생성된 토큰을 기반으로 토큰을 더 잘게 나누는 필터링을 수행하기 때문이에요.\n즉, Nori 분석기가 사이다를 하나의 단어로 인식하지 못하고 어미(이, 다)로 나누어버렸기 때문에, 사이다라는 토큰 자체가 존재하지 않았던 것이죠,,\n만약 칠성고구마였다면 어떻게 되었을까요?\n\nnori: [칠성, 고구마, 355, ml, 24, 캔]\n\nngram: [칠, 칠성, 고, 고구, 고구마, 3, 35, 355, m, ml, 2, 24, 캔]\n\nstandard: [칠성고구마, 355ml, 24캔]\n\n\n\n이처럼 명사 단위로 토큰화하기 때문에 Nori 명사 사전에 명사 존재 여부에 따라 토큰화가 다르게 됩니다. 명사 사전에 존재하는 품목의 경우, 고구마처럼 검색이 훨씬 매끄러울 수 있을거예요.\n사이다도 명사 사전에 등록되어 있었다면 칠성사이다도 [칠성, 사이다] 로 토큰할 수 있었겠죠.\nUser Dictionary\n따라서, 사용자 사전(user_dictionary) 도입을 고려했었는데요. Nori Tokenizer 에게 사이다는 명사야, 혹은 칠성사이다는 [칠성, 사이다] 라고 토큰화 해! 라고 인식할 수 있는 기준을 마련해줄수있는 방법이에요.\n하지만 몇가지 한계가 있었어요.\n관리 포인트 증가\n    \n관리해야 할 품목의 종류가 너무 많아 어려움이 발생\n농산물, 곡류, 축산물, 수산물 등 수백에서 수천 가지 품목을 주기적으로 업데이트하기 어려운 환경\n표준화되지 않은 품목명\n    \n유통사마다 다른 표기 방식으로 인해 같은 품목도 명칭이 다름\n예: “무”와 “무우”, “샐러드”와 “셀러드” 등 비표준어와 잘못된 외래어 표기\n이러한 다양한 표기법을 모두 관리하기엔 부담이 큼\n이러한 이유로 사용자 사전을 유지 관리하는 것이 현실적으로 어렵다고 판단하여 다른 접근 방식을 찾기로 했습니다.\nWildcard Field\n문제를 다시 분석한 결과, 검색어 자체가 포함된 품목을 반환하는 것이 핵심이라는 점을 확인했습니다.\n이는 마치 DB의 LIKE 쿼리처럼 검색어가 포함된 품목을 반환하는 것이죠.\nES에서는 이러한 기능을 제공하는 Wildcard 필드를 활용할 수 있었습니다.\nWildcard 필드를 추가하는 방법은 간단합니다.\n\n{\n  \"mappings\":{\n    \"properties\":{\n      \"name\":{\n        \"type\":\"text\",\n        \"analyzer\":\"korean\",\n        \"fields\":{\n          // .. 생략\n          \"wildcard\":{\n            \"type\":\"wildcard\"\n          }\n        }\n      }\n      //.. 생략\n    }\n  }\n}\n\n\nWildcard 필드는 역색인 구조가 아닌 패턴 매칭 방식을 사용하기 때문에 성능 문제가 발생할 수 있어서 신중히 사용해야 합니다. 모든 토큰을 검사해야 하기 때문에 데이터가 많아질수록 메모리 사용량이 많아지고 성능이 떨어질 수 있습니다.\n따라서 Wildcard 필드 대신 정교한 N-gram 을 사용하거나 Query-String 쿼리를 권장합니다.\n하지만 저희는 데이터량이 많지 않고, 필터를 통해 조회되는 데이터 수를 제한할 수 있었기 때문에 성능 부담이 아직까진 크지 않아 Wildcard 필드를 사용하기로 결정했습니다.\nWildcard 필드를 활용한 쿼리는 다음과 같이 구성했습니다:\n\n{\n  \"query\":{\n    \"bool\":{\n      \"must\":[\n        {\n          \"bool\":{\n            \"should\":[\n              {\n                \"wildcard\":{\n                  \"name.wildcard\":{\n                    \"boost\":100.0,\n                    \"wildcard\":\"*사이다*\"\n                  }\n                }\n              },\n              {\n                \"multi_match\":{\n                  \"fields\":[\n                    \"name^3.0\",\n                    \"name.ngram^4.0\",\n                    \"name.standard^3.0\"\n                  ],\n                  \"query\":\"사이다\"\n                }\n              }\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n검색어가 포함된 결과는 상단으로 올리되, 포함된 결과 내에서도 유사도에 맞게 정렬되도록 쿼리를 수정했습니다. 위처럼 할 경우 사이다가 포함된 단어는 100 점을 추가로 받고 match 되는 필드에 따라 점수를 추가로 더해지게 됩니다.\n예를들어 칠성사이다 , 칠십성사이다 , 칠성사이 라는 품목이 있을때, 사이다 라고 검색하면 wildcard 에 의해 칠성사이다, 칠십성사이다 가 가장 상단으로 나오게 될테고, 품목의 이름이 더 짧아 유사도가 더 높은 칠성사이다 가 최상단으로 나오게 될거예요.\n기존의 가중치는 유지하되 검색어가 포함된 결과만 올리기 위한 쿼리입니다.\n결론\nUser Dictionary 도입을 고려했으나 유지보수에 대한 한계로 제외\nWildcard 필드와 쿼리로 검색어가 포함된 품목의 점수를 높임\n3) 초성 검색 feat. ICU\nWildcard 검색까지 구현하고 나니 검색 되지 않는 품목 없이 꽤 안정화된 검색 결과를 제공할 수 있었는데요. 더 편리한 검색을 위한 초성 검색 니즈가 들어 왔습니다.\n어떤 extension 을 사용할 것인가\n초성검색을 위해 지금 시스템에 도입할 수 있고, 적당한 레퍼런스가 있는 두가지 extension으로 POC 를 진행해봤어요.\nelasticsearch-jaso-analyzer(이하 JASO)\nanalysis-icu(이하 ICU)\n결론적으로 ICU 를 선택했는데요, JASO 에 대한 설명이 너무 길어질거 같아 자세한 설정 방법과 설명은 위 주소에서 참고주시길 바랍니다.\n두 분석기를 비교한 결과는 아래 표로 정리되었습니다.\n\n개발 난이도\n    \nJASO: \"chosung\" 옵션만 추가하면 간단히 초성 검색이 가능\nICU: 직접 초성 필터를 구현해야 하는 추가 작업 필요\n유지보수 및 확장성\n    \nJASO: 커스텀 확장(extension)으로 기본 제공되지 않기 때문에, 사용하는 ES 버전과 플랜에 따라 제약이 있을 수 있음\nICU: 기본 확장(extension)으로 계속 지원되며, 다른 기능으로의 확장이 자유로움\n버전 지원\n    \nJASO: Elasticsearch 8.6.2까지만 지원. 이후 버전은 직접 설정 필요\nICU: 최신 버전까지 지원\n토큰 생성 방식\n    \nJASO: 영어 오타 교정, 쌍자음 분리 등 추가 기능 지원\nICU: 필요에 따라 초성 검색뿐만 아니라 다양한 확장 가능\nJASO 가 더 많은 옵션을 제공한다는 이점이 있지만 불필요한 토큰이 생성되고 큰 max_gram을 주어 토큰을 많이 생성해야 된다는 점,\n유지보수를 직접 해야된다는 점에서 ICU extension 을 직접 확장하여 사용하기로 하였습니다.\nICU Analyzer\n그럼, ICU analyzer 의 설정을 좀더 자세히 살펴보겠습니다.\n\n{\n    \"orderable_vendor_product_v4\":{\n        \"aliases\":{\n            \"orderable_vendor_product\":{}\n        },\n        \"mappings\":{\n            \"properties\":{\n                // .. 생략\n                \"name\":{\n                    \"type\":\"text\",\n                    \"fields\":{\n                        \"icu\":{\n                            \"type\":\"text\",\n                            \"analyzer\":\"icu_analyzer\",\n                            \"search_analyzer\":\"icu_search_analyzer\",\n                            \"similarity\":\"scripted_no_idf\"\n                        }\n                    }\n                }\n            }\n        }\n    },\n    \"template\":{\n        \"settings\":{\n            \"index\":{\n                \"analysis\":{\n                    \"filter\":{\n                        // .. 생략\n                        \"ngram_filter\":{\n                            \"type\":\"ngram\",\n                            \"min_gram\":1,\n                            \"max_gram\":2,\n                            \"token_chars\":[\n                                \"letter\",\n                                \"digit\"\n                            ]\n                        }\n                    },\n                    \"analyzer\":{\n                        // .. 생략\n                        \"icu_analyzer\":{\n                            \"type\":\"custom\",\n                            \"filter\":[\n                                \"lowercase\",\n                                \"ngram_filter\"\n                            ],\n                            \"char_filter\":[\n                                \"nfd_normalizer\",\n                                \"make_chosung_filter\"\n                            ],\n                            \"tokenizer\":\"icu_tokenizer\"\n                        },\n                        \"icu_search_analyzer\":{\n                            \"type\":\"custom\",\n                            \"filter\":[\n                                \"lowercase\",\n                                \"ngram_filter\"\n                            ],\n                            \"char_filter\":[\n                                \"chosung_only_filter\",\n                                \"nfd_normalizer\"\n                            ],\n                            \"tokenizer\":\"icu_tokenizer\"\n                        }\n                    },\n                    \"char_filter\":{\n                        \"nfd_normalizer\":{\n                            \"mode\":\"decompose\",\n                            \"name\":\"nfkc\",\n                            \"type\":\"icu_normalizer\"\n                        },\n                        \"make_chosung_filter\":{\n                            \"type\":\"pattern_replace\",\n                            \"pattern\":\"[^\\u1100-\\u1112^0-9a-zA-Z가-힣ㄱ-ㅎ ㅏ-ㅑ]\",\n                            \"replacement\":\"\"\n                        },\n                        \"chosung_only_filter\":{\n                            \"type\":\"pattern_replace\",\n                            \"pattern\":\"[^ㄱ-ㅎa-zA-Z0-9]\",\n                            \"replacement\":\"\"\n                        }\n                    }\n                    //.. 생략 Tokenizer\n                }\n            }\n        }\n    }\n}\n\n\n위 analzyer 를 그림으로 나타내면 아래와 같습니다.\n\nicu field 를 보시면 analyzer 와 search_analyzer 를 구분해서 설정해준걸 보실 수 있는데요.\nAnalyzer : 저장되는 document 에 대해서 토큰화하여 그림과 같이 역색인화 구조로 저장합니다.\nSearch Analyzer : 검색어에 대해서 토큰화를 수행해서 저장되어 있는 토큰을 검색해서 결과를 내는 역할을 합니다.\n기존엔 Analyzer 와 Search Analyzer 를 구분해서 지정해줄 필요가 없었지만 초성 검색의 경우엔 초성으로 검색 했을 때만 초성 검색이 되길 바랬었는데요.\n예를 들어 초성이 아닌 통베이컨을 Analyzer 로 검색했을 경우, ㅌㅂㅇㅋ 으로 초성화 될 것이고 사용자가 초성 검색을 하지 않았는데도 초성을 포함하는 품목이 많이 나오게 될 것 입니다. \n그래서 최대한 기존의 쿼리 score 에는 영향이 가지 않고 초성 검색을 했을때만 초성으로 품목을 찾기 위해서 초성을 제외한 글자는 모두 제거하는 필터를 넣는 Search Analyzer 를 따로 지정해주었습니다.\n즉, 통베이컨이 Search Analyzer 를 거치게 되면 아무 토큰도 생성되지 않게 되어 초성 검색에 대해서는 수행이 되지 않게 되는거죠.\nICU_Normalizer 에 대한 설정은 문서를 보시면 더 자세히 볼 수 있을거예요.\n요약하자면 유니코드의 정규화(Normalization)를 수행하는 역할을 합니다.\n저희는 decompose 옵션을 사용하여 통이라는 글자를 ㅌㅗㅇ 으로 문자를 분해할 수 있도록 했고, NFKC(Normalization Form Compatibility Composition) 옵션을 사용하여 호환가능한 문자를 호환시키고, 조합 가능한 문자는 조합하도록 설정하였습니다.\n즉, 통베이컨™①é를 정규화하면 ㅌㅗㅇㅂㅔㅇㅣㅋㅓㄴTM1é로 정규화 됩니다. \n초성검색 구현을 위해선 정규화 방식보다는 초성 분리를 위한 mode 를 잘 설정하는게 더 핵심이라고 할 수 있을거예요.\n요약하면 품목을 ICU Normalizer 를 활용하여 초성 토큰 형태로 저장하고, 초성 검색어에 대해서만 초성 검색을 수행할 수 있도록 Search Analyzer 를 구분하여 구현해주었습니다.\nICU 는 JASO와 비교했을 때 초기 구현은 다소 복잡했지만, 유지보수와 추후 확장성 측면에서 더 적합한 선택이었으면 합니다!! (Extension 교체 작업만은 다시 하고 싶지 않아요..)\nIDF 제외\n이제 마지막 개선 작업이네요.\n잘 운영하고 있던 중 아래와 같은 의견이 들어왔습니다.\n이슈 및 원인 분석\n\n사용자가 스위트콘을 검색하려 했으나 스위트곤으로 오타가 발생한 경우, 기대했던 스위트콘이 아닌 곤약이 상위에 노출되는 문제가 있었습니다.\n일반적으로 사용자가 기대하는 결과와는 다른 결과였죠.\n다행히도 로컬 테스트 환경에서 원인 분석을 해볼 수 있었는데요, 그런데 조금 충격적이게도 로컬에선 스위트콘이 더 상위노출 되었습니다. 이럴수가..\n그렇다는 것은 로컬 테스트코드와 실제 환경의 검색 결과가 다르다는 것이고, 지금까지의 테스트가 유효한게 맞을까.. 하는 생각이 들었는데요.\n\n더 자세한 원인을 파악하기 위해 ES의 explain API를 사용해 점수 산출 과정을 분석했습니다.\n결과를 요약하자면 아래와 같은데요.\n#곤약/면곤약\n    \nN-gram Boost : 8.8\nIDF: 6.05\nTF: 0.78\n8.8 * 6.05 * 0.78 = 41.66\n스위트콘/리치스/2.95kg\n    \nN-gram Boost : 8.8\nIDF : 3.14\nTF : 0.81696963\n8.8 * 3.14 * 0.81696963 = 22.568493\nES 에서 Boost * IDF * TF 계산 로직을 통해 score 를 산출하고 있었습니다. 곤약이 상위로 올라온 이유는 숫자를 보면 알수있듯 두배 가까이 차이나는 IDF 때문인 것을 파악할 수 있었는데요.\n우선 처음 보는 개념인 IDF 와 TF 가 무엇인지 알아보았습니다.\nIDF(Inverse Document Frequency): 특정 단어가 전체 문서에서 얼마나 드물게 나타나는지\nTF(Term Frequency): 특정 단어가 문서 내에서 얼마나 자주 나타나는지\nIDF 는 전체 문서를 기준으로 계산되기 때문에 테스트코드와 실제 환경의 결과가 다른 이유가 여기에 있었습니다.\n테스트코드의 테스트를 위해 생성해놓은 문서는 상대적으로 너무나도 적은 양의 데이터이기 때문에 IDF 의 값이 대부분 동일하고 TF 값으로 대부분 유사도가 정해질거예요.\n반면, 테스트코드에 비해 방대한 품목 데이터가 있는 실제 환경에선 곤약과 스위트콘처럼 IDF 의 차이가 클 수 있습니다.\nIDF 가 유의미한 결과를 내주기 위해서는 전체 문서가 모두 한 유통사의 품목으로, 품목명의 구조나 맥락이 동일해야 할 것 같은데요.\n하지만 유통사마다 품목명이 모두 제각각인데도 IDF가 모든 유통사의 데이터를 포함한 전체 품목 데이터를 기준으로 계산되면서 오히려 유사도 계산에 역효과를 내고 있었습니다.\n또한, 실제환경과 로컬 환경에서의 테스트 결과가 보장되지 않아 문제 재현 및 원인 파악이 어려워보였습니다.\n따라서, IDF 를 제외하고 score 계산하는 방법을 알아봤습니다.\nIDF를 제외한 점수 계산을 위해 크게 세 가지 방법을 검토해봤습니다.\n1. 점수 고정 (Constant Score)\n첫번째 방법은 아래와 같이 쿼리에 boost 값을 명시하여 점수를 고정 시키는 Constant Score Query 를 활용한 방법입니다.\n\n{\n  \"query\":{\n    \"bool\":{\n      \"must\":[\n        {\n          \"bool\":{\n            \"should\":[\n              {\n                \"constant_score\":{\n                  \"filter\":{\n                    \"wildcard\":{\n                      \"name.wildcard\":{\n                        \"wildcard\":\"*스위트곤*\"\n                      }\n                    }\n                  },\n                  \"boost\":100.0\n                }\n              },\n              {\n                \"constant_score\":{\n                  \"filter\":{\n                    \"match\":{\n                      \"name\":\"스위트곤\"\n                    }\n                  },\n                  \"boost\":3.0\n                }\n              },\n              {\n                \"constant_score\":{\n                  \"filter\":{\n                    \"match\":{\n                      \"name.ngram\":\"스위트곤\"\n                    }\n                  },\n                  \"boost\":4.0\n                }\n              }\n              // .. 생략\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n결과를 봤을 때, Boost 만으로 Scoring 되기 때문에 같은 필드에서 검색된 품목은 동일한 점수를 가지는 것을 확인했습니다. \n예를 들어 name.ngram 로 검색된 품목들은 모두 Boost 4 인 동일한 점수로 정렬이 제대로 되지 않았습니다.\n결론적으로, 이 방식은 유사도 기반 정렬이 필요한 우리의 요구사항에 적합하지 않았습니다.\n2. 유사도 모델 변경\n기본적으로 ES는 Boost * IDF * TF 식을 사용하는 BM25(Best Matching 25) 모델을 사용합니다. 이를 대신해 DFR(Deviation From Randomness) 모델을 사용해 보았습니다.\nDFR 은 현 document에 얼마나 드물게 등장하는지, 문서 길이에 따라 통계적 랜덤성 기반으로 검색됩니다. type 뿐만 아니라 아래처럼 옵션에 가중치나 옵션을 설정해줄수있습니다.\n아래는 DFR 모델 설정 예시입니다.\n\n{\n  \"settings\": {\n    \"similarity\": {\n      \"custom_similarity\": {\n        \"type\": \"DFR\",\n        \"basic_model\": \"g\", // 문서 내에 특정 용어가 얼마나 드물게 발생하는지\n        \"after_effect\": \"b\", // 검색어가 얼마나 여러번 등장하는지\n        \"normalization\": \"h2\" // 문서 길이에 따라 빈도 설정\n      }\n    }\n  }\n}\n\n\n보시다시피 DFR 모델에 대한 지식이 없으면 유지보수가 무척 어려워보입니다.\n문제가 생기거나 개선의 여지가 생겼을 때 더 복잡한 계산식을 사용하는 모델이기 때문에 쉽게 커스텀하기가 힘들어보이고, 모델의 대한 이해뿐만 아니라 옵션에 대한 각 알고리즘도 알아야하기 때문에 유지보수가 정말 쉽지 않을거라 생각이 들었죠.\n우리 팀에 검색 엔진에 대한 전문적인 지식을 가진 분이 없었기 때문에 더 복잡한 모델로 변경하는건 과감하게 제외했습니다.\n3. Scripted Similarity 사용\nIDF를 제외하고 직접 계산식을 정의하는 방법입니다. 아래는 Scripted Similarity 설정 예시입니다.\n\n{\n  \"setting\":{\n    \"similarity\":{\n      \"scripted_no_idf\":{\n        \"type\":\"scripted\",\n        \"script\":{\n          \"source\":\"double tf = Math.sqrt(doc.freq); return query.boost * tf;\"\n        }\n      }\n    }\n  }\n}\n\n\n위처럼 setting 을 변경하고 쿼리의 explain 을 해보면 script 에 있는 식이 아래와 같이 idOrCode에 들어가 scoring 되는것을 볼 수 있어요.\n\nExplain 결과\n{   \n    // .. 생략\n    \"description\":\"sum of:\",\n    \"details\":[\n        {\n            \"details\":[\n                {\n                    \"description\":\"score from ScriptedSimilarity(weightScript=[null], script=[Script{type=inline, lang='painless', idOrCode='double tf = Math.sqrt(doc.freq); return query.boost * tf;', options={}, params={}}]) computed from:\",\n                    \"details\":[\n                        // .. 생략\n                    ],\n                    \"value\":8\n                }\n            ]\n        }\n        // .. 생략\n    ]\n}\n\n\n\n위 세가지 방법 중 가장 마지막 방법인 Scripted Similarity 로 계산식을 넣기로 했어요. Scripted Similarity 를 선택한 이유는 다음과 같습니다.\n유연한 계산식 사용\n    \n계산식을 직접 조정 가능\n기존 BM25 모델에서 IDF만 제거할 수 있는 계산식을 직접 부여 가능\n유지보수 용이성\n    \n계산식이 명확히 노출되어 있어 비교적 수정이 간단\n쿼리와 독립적\n    \n점수 계산이 쿼리와 독립적으로 이루어져 다른 쿼리 추가 시에도 영향을 받지 않음\nScripted Similarity 계산식 결정\n그후에는 Scripted Similarity 에 적용할 계산식 결정이 필요했습니다.\n후보 1. 단순한 TF 계산식\n\ndouble tf = Math.sqrt(doc.freq);\nreturn query.boost * tf;\n\n/* freq : 문서 내의 토큰 등장 수 */\n\n\n후보 2. BM 모델과 동일한 TF 계산식\n\ndouble freq = doc.freq;\ndouble k1 = 1.2;\ndouble b = 0.75;\ndouble dl = doc.length;\ndouble avgdl = 7;\ndouble tf = freq / (freq + k1 * (1 - b + b * dl / avgdl));\nreturn query.boost * tf;\n\n/*\n    freq : 문서 내의 토큰 등장 수\n    k1: TF 영향도 가중치\n    b : 문서 길이 보정 파라미터\n    dl : 문서 길이\n    avgdl : 전체 문서의 문서 길이 평균값\n */\n\n\n\n\n결과만 보았을 때 확실히 IDF 를 포함했을 때보다 IDF 를 제외했을 때 결과가 개선되긴 했지만, 여전히 단순 TF 계산식, BM 모델 계산식의 결과는 크게 차이가 없었습니다.\n하지만 위 결과의 정렬은 비슷한듯 하지만 실제 점수는 다릅니다. 단순 TF 의 경우 문서 내의 토큰 등장 수로만 TF 를 계산하기 때문에 동일한 점수를 가진 결과가 다수 나타나 매번 정렬이 달라질 수 있습니다.\n국산쌀와 찰떡(쌀 국산)는 단순 TF 계산식에 의하면 둘은 동일한 점수를 반환하게 됩니다.\n반면 BM 모델과 동일한 TF 계산식의 경우, 문서 내 토큰 등장 수와 문서 길이를 함께 고려하기 때문에 저희가 흔히 생각한대로 국산쌀의 유사도가 더 높은 결과로 나오게 됩니다.\n따라서, BM 모델에서 IDF 만 제거한, 후보2 계산식을 사용하기로 결정하였습니다.\n다만, avgdl 를 현재의 기준으로 7로 고정시켜 가중치를 조정했지만 이후 문서의 평균 길이가 변경될 경우엔 유지보수가 필요한 부분이 있을 수 있습니다.\n하지만 avgdl 가 크게 변하지 않을 것이라는 가정과 점수 계산에 크게 영향을 주지 않는다고 생각해 고정된 계산식으로 가게 되었습니다.\n그럼에도 이후에 유지보수가 필요하거나 변경이 필요할 수 있습니다. 꽤 해석이 필요한 계산식을 가지고 있기 때문에\n어떤 계기로 IDF를 제거하게 되었는지, Scripted Similarity 를 왜 적용하게 되었는지, 계산식은 어떻게 결정되었고 어떤 의미인지를 자세히 문서화하도록 노력하였습니다.\n결과\n결과적으로, 아래와 같은 구조를 갖게 되었습니다.\n\n이러한 구조를 통해 현재 검색 정확도는 어떨까요?\n아래는 매장 사이드 관련 데이터입니다.\n2025년 1월 기준\n    \n검색 결과 성공률: 검색 시 하나 이상의 품목이 결과에 노출되는 경우 → 98.6%\n검색 목적 달성률: 검색한 품목을 선택한 후, 실제 액션(주문 등)을 수행하는 경우 → 73.6%\n이를 통해 상당히 높은 검색 정확도와 달성률을 기록하고 있음을 확인할 수 있습니다.\n또한, 아래는 유통사에서 매장의 주문서를 생성할 때, ES 검색이 매우 편리했다는 피드백을 받은 메시지입니다.\n키친보드의 검색 기능이 타 ERP와 달리 오타가 있어도 품목을 정확하게 검색할 수 있어 큰 편리함을 느꼈다고 합니다!\n\n\n개발 과정에서 어려움이 있었지만 결국 사용자한테 좋은 영향을 주는 기능을 개발한거 같아 아주 뿌듯하네요!\n소소한 Tip\n중단 시간을 줄인 Reindexing\nES 의 비용을 최소화하기 위해서 최소한의 리소스로 구동하고 있습니다. 그러다보니 인덱스에 새로운 필드가 추가되거나 설정이 변경되는 개선이 될때마다 인덱스를 새롭게 동기화해야하기 때문에 중단이 불가피했는데요.\n하지만 주문에서의 검색은 점주들이 꼭 해야만하는 중요한 기능이기 때문에 중단을 최소화하고 싶었습니다.\n알아보던 중 찾아낸 것은 별칭(Alias)를 이용한 배포입니다.\n\nOld Index Template 을 템플릿으로 하여 생성된 Old Index(별칭: Product) 가 존재한다.\n새로운 매핑 정보가 있는 New Index Template 을 생성한다.\nNew Index Template 을 템플릿으로 하여 New Index 를 생성한다.\nOld Index 의 문서를 New Index 로 reindex 한다.\nOld Index 를 삭제한다.\nNew Index 에 Product alias 를 부여 한다.\n서버에서 alias 기준으로 쿼리한다.\n이렇게 되면 5번과 6번 사이, Product 를 가진 인덱스가 존재하지 않을 시에만 검색 중단이 발생하게 됩니다.\n이 방법이 가장 좋은 방법이라고 표현하기는 어렵지만 기존 20만개의 품목을 마이그레이션 하느라 10분이 넘는 중단 시간을 3초 정도의 중단 시간으로 낮출 수 있는 방법이었습니다.\n대량 데이터 조회\nDB 의 품목 데이터와 ES 인덱스 동기화를 위해 하루에 한번씩 스케줄러가 실행됩니다.\nDB 품목 데이터가 삭제될 때 ES 데이터도 삭제되어야 하는데 어떠한 이유로 삭제되지 않는 경우가 있었습니다. 즉, 품목이 DB 데이터엔 없지만 ES 데이터에는 있는 경우죠.\n다만 전체 문서를 조회하는 과정의 부하가 많이 걱정되긴 했는데요. 그래서 처음엔 페이징을 활용했습니다.\n\n fun deleteOrphanDocuments() {\n    val pageSize = 1000\n\n    var pageable = PageRequest.of(0, pageSize, Sort.by(Sort.Order.asc(\"id.keyword\")))\n\n    do {\n        val indexPage = productIndexRepository.findAll(pageable)\n        val documentProductIds = indexPage.content.map { it.id }\n\n        val dbProductIds = productRepository.findAllById(documentProductIds).map { it.id }\n\n        val documentIdsNotInDB = documentProductIds - dbProductIds\n        productIndexRepository.deleteAllById(documentIdsNotInDB)\n\n        pageable = pageable.next()\n    } while (indexPage.hasNext())\n}\n\n\n\nES야.. 죽지마.. 테스트서버에서 확인해보니 동기화 할 때마다 자꾸 ES 서버가 다운되더라고요..\n페이징보다 Scroll API 를 이용하는 것이 훨씬 성능상 좋다고 하여서 Scroll API 를 적용해봤습니다.\n\nfun deleteOrphanDocuments() {\n    val toDeleteProductIds = mutableListOf<UUID>()\n\n    val batchSize = 1000\n\n    var documentIdsWithScroll = productIndexRepository.findAllIdWithScroll(size = batchSize)\n\n    while (documentIdsWithScroll.productDocumentIds.isNotEmpty()) {\n        val documentIds = documentIdsWithScroll.productDocumentIds\n        val dbProductIds = productRepository.findAllById(documentIds).map { it.id }\n\n        toDeleteProductIds.addAll(documentIds - dbProductIds)\n\n        documentIdsWithScroll =\n            productIndexRepository.findAllIdByScrollId(\n                scrollId = documentIdsWithScroll.nextScrollId,\n            )\n    }\n\n    productIndexRepository.clearScroll(documentIdsWithScroll.nextScrollId)\n\n    productIndexRepository.deleteAllById(toDeleteProductIds)\n}\n\n\nES 에서 Scroll 을 생성할 때 설정한 시간만큼 Scroll 정보를 저장하고 있고 해당 Scroll ID 를 가지고 다음 데이터를 처리단위만큼 반환해줍니다. \n받은 데이터엔 또 다음 Scroll 조회를 위한 Scroll ID 를 반환하는 형식입니다.\n장점은 페이징 조회보다 계산과 조회가 빠르고 부하가 적은 것인데요. 다만, Scroll 정보를 저장하고 있어야한다는 단점이 있습니다.\n조회 후 clearScroll 해주고 만료 시간을 적게 설정하면 큰 문제가 되지 않을거예요.\n변경 후엔 ES가 건강해졌습니다. 대량 조회가 필요할 땐 Scroll API 를 사용하는걸 추천드립니다.\n검색 품질 유지하기\n추가적으로 쿼리나 스코어링 방식을 변경하다보면 현재의 요구사항에는 만족했지만 이전의 요구사항에 반할 때가 생길 수 있습니다.\n그래서 테스트코드에 이전 요구사항에 관련된 테스트케이스를 꼭 남겨두고 현재 만족할 수 있는 테스트케이스를 더해서\n이전과 현재 요구사항을 모두 만족할 수 있도록 구현하는걸 목표로 했습니다.\n특정 케이스에 모두 만족하는지 일일이 확인하는 것은 현실적으로 어려울 수 있으니 테스트를 든든하게! 잘 짜두는 것이 많은 도움이 될 겁니다!\n이후엔 어떤 것을 할 것인가\nWildcard 검색, 초성 검색, IDF 제거 등 여러 개선들을 해왔는데요.\n요구사항에 맞게, 문제상황에 맞게 조치하는 형식으로 개선해나가다보니 조금은 불필요한 옵션들로 검색을 무겁게 만든 부분들이 분명 있을 수 있을것 같아요.\n또한 검색 엔진에 대한 전문적인 지식 없이 점차 학습하여 개발하다보니 초반과 후반에 대한 지식의 차이가 커서 초반에 했던 방법들이 논리보단 여러 검색어 테스트 POC 로 적용한 것들이 많아요.\n그래서 조금 뚱뚱해진 ES 의 다이어트가 필요해보입니다.\n일단 생각나는 것은 부하를 줄 수 있는 Wildcard 를 없애는 것인데요. N-gram 과 Tokenizer 의 적절한 조합으로 Wildcard 없이 포함된 단어가 잘 조회될 수 있도록 방법을 찾아봐야할 것 같아요.\n그리고 이젠 품목 데이터가 많이 쌓여서 한국 식자재의 대부분 데이터가 존재할 것인데요. 이 데이터로 사용자 사전을 만들어서 검색의 정확도를 높이는 작업을 해볼 수 있을 겁니다.\n추가되는 데이터에 맞춰 사전이 관리되어야 하기 때문에 개발자뿐만 아니라 타부서 팀원분들도 쉽게 관리하기 위한 시스템을 마련해야 할 것 같습니다.\n외래어를 포함하는 새로운 식자재를 등록하는 것도 중요하지만 “셀러드”처럼 식자재이지만 표기를 잘못한 경우도 있어서 이런 경우엔 품목명 자체의 수정이 필요합니다.\nERP에 등록된 품목명과 키친보드의 등록된 품목명의 강한 결합성을 끊을 수 있도록, 검색 엔진의 설정뿐만 아니라 키친보드 품목명의 데이터 클렌징이 필요할거예요.\n마무리\n끝으로, 저희의 지금 검색 엔진은 완성이 아닙니다. 지금도 많이 부족할 수 있지만 사용자의 보이스를 들으며 꾸준히 발전하고 있습니다.\n키친보드를 사용하는 점주 모두가 검색이 너무 편해요! 라고 할때까지, 대충 검색해도 마음속으로 생각했던 품목이 결과에 나오는 그날까지, 발전은 계속 됩니다!\nES 도입과 설정에 도움주시고 자기 일처럼 고민해주신 백엔드 개발자분들과 개선을 위한 데이터를 제공해주신 데이터팀분들, 검색 개선을 위해 피드백 주신 사업팀분들 모두 감사합니다!",
    "reviews": [],
    "syllabus": [],
    "link": "https://spoqa.github.io/2025/03/04/es-dev.html",
    "pubDate": "2025-03-04T00:00:00.000Z",
    "creator": "Unknown",
    "categories": []
  },
  {
    "id": 7,
    "imageUrl": "",
    "title": "Beat processors - 6th",
    "description": ";을 구분자로 사용한 데이터 분리.\n\n- script:\n    lang: javascript\n    source: >\n      function process(evt) \n        var str = evt.Get('message').split(';')\n        evt.Put('result', str)\n      }\n\n\n\n\n{\n  \"@timestamp\": \"2025-02-27T03:49:19.802Z\",\n  \"@metadata\": {\n    \"beat\": \"filebeat\",\n    \"type\": \"_doc\",\n    \"version\": \"8.17.0\"\n  },\n  \"message\": \"type:  5 crl.root-x1.letsencrypt.org.edgekey.net;type:  5 e8652.dscx.akamaiedge.net;::ffff:23.207.177.83;\",\n  \"result\": [\n    \"type:  5 crl.root-x1.letsencrypt.org.edgekey.net\",\n    \"type:  5 e8652.dscx.akamaiedge.net\",\n    \"::ffff:23.207.177.83\",\n    \"\"\n  ]\n}\n\n\n\n\n마지막 구분자 때문에 빈 값이 추출된다. 정규표현식으로 구분자만 제외.\n\n- script:\n    lang: javascript\n    source: >\n      function process(evt) {\n        var str = evt.Get('message').match(/[^;]+/);\n        evt.Put('result', str)\n      }\n\n\n\n\n{\n  \"@timestamp\": \"2025-02-27T03:53:10.537Z\",\n  \"@metadata\": {\n    \"beat\": \"filebeat\",\n    \"type\": \"_doc\",\n    \"version\": \"8.17.0\"\n  },\n  \"message\": \"type:  5 crl.root-x1.letsencrypt.org.edgekey.net;type:  5 e8652.dscx.akamaiedge.net;::ffff:23.207.177.83;\",\n  \"result\": [\n    \"type:  5 crl.root-x1.letsencrypt.org.edgekey.net\"\n  ]\n}\n\n\n\n\n첫 번째 데이터만 추출된다. 정규표현식은 최초 검사 성공 후 중지되기 때문. 모든 데이터를 검사하도록 글로벌(g) 수정자 추가.\n\n- script:\n      lang: javascript\n      source: >\n        function process(evt) {\n          var str = evt.Get('message').match(/[^;]+/g);\n          evt.Put('result', str)\n        }\n\n\n\n\n{\n  \"@timestamp\": \"2025-02-27T03:54:35.866Z\",\n  \"@metadata\": {\n    \"beat\": \"filebeat\",\n    \"type\": \"_doc\",\n    \"version\": \"8.17.0\"\n  },\n  \"message\": \"type:  5 crl.root-x1.letsencrypt.org.edgekey.net;type:  5 e8652.dscx.akamaiedge.net;::ffff:23.207.177.83;\",\n  \"result\": [\n    \"type:  5 crl.root-x1.letsencrypt.org.edgekey.net\",\n    \"type:  5 e8652.dscx.akamaiedge.net\",\n    \"::ffff:23.207.177.83\"\n  ]\n}\n\n\n\n\n원하는 구조가 나왔으니 정규표현식만 좀 더 깔끔하게 수정.\n\n- script:\n    lang: javascript\n    source: >\n      function process(evt) {\n        var str = evt.Get('message').match(/[^ :;]+\\.[^;]+/g);\n        evt.Put('result', str)\n      }\n\n\n\n\n{\n  \"@timestamp\": \"2025-02-27T03:56:28.797Z\",\n  \"@metadata\": {\n    \"beat\": \"filebeat\",\n    \"type\": \"_doc\",\n    \"version\": \"8.17.0\"\n  },\n  \"message\": \"type:  5 crl.root-x1.letsencrypt.org.edgekey.net;type:  5 e8652.dscx.akamaiedge.net;::ffff:23.207.177.83;\",\n  \"result\": [\n    \"crl.root-x1.letsencrypt.org.edgekey.net\",\n    \"e8652.dscx.akamaiedge.net\",\n    \"23.207.177.83\"\n  ]\n}\n\n\n\n\n관련 글\n\nBeat processors - 5th\nconvert ip to decimal\n엘라스틱의 key-value 처리\n도메인 정규화 Processor: registered_domain",
    "reviews": [],
    "syllabus": [],
    "link": "https://kangmyounghun.blogspot.com/2025/02/filebeat-processors-6th.html",
    "pubDate": "2025-02-27T03:59:00.004Z",
    "creator": "Unknown",
    "categories": []
  },
  {
    "id": 8,
    "imageUrl": "",
    "title": "On the Horizon: Top Three Updates Coming Soon From Qodana",
    "description": "As we head deeper into Q1 of 2025, some exciting new developments are underway. From expanding Qodana’s integrations with new IDEs to consolidating organizational data, many more promising new projects are in the works to help increase code quality in your team. Let’s take a look at the top three! An organization-wide dashboard Insights into […]",
    "reviews": [],
    "syllabus": [],
    "link": "https://blog.jetbrains.com/qodana/2025/03/qodana-roadmap-2025-q1/",
    "pubDate": "Tue, 04 Mar 2025 14:05:56 +0000",
    "creator": "Kerry Beetge",
    "categories": [
      "qodana",
      "code-quality-dashboard",
      "roadmap",
      "roadmap2025",
      "visual-studio-plugin"
    ]
  },
  {
    "id": 9,
    "imageUrl": "",
    "title": "쉽고 효과좋은 코어운동",
    "description": "영상: https://www.youtube.com/watch?v=m-gRpx4NyxY&list=WL\n\n\n\n쉽고 재밌으며 효과적입니다.\n\n\n흔들흔들",
    "reviews": [],
    "syllabus": [],
    "link": "http://serverdown.tistory.com/1159",
    "pubDate": "Fri, 28 Feb 2025 13:15:51 +0900",
    "creator": "SIDNFT",
    "categories": [
      "유튜브",
      "코어운동"
    ]
  },
  {
    "id": 10,
    "imageUrl": "",
    "title": "NotebookLM을 이용한 AI 활용 - 기술 자료 분석",
    "description": "No description available",
    "reviews": [],
    "syllabus": [],
    "link": "https://jacking75.github.io/tech-ai_20250304/",
    "pubDate": "Tue, 04 Mar 2025 00:00:00 +0900",
    "creator": "Unknown",
    "categories": []
  },
  {
    "id": 11,
    "imageUrl": "",
    "title": "엘라스틱 Runtime field - 10th",
    "description": "beat 기본 인덱스 템플릿을 사용하면 1,500여 개 필드로 구성된 인덱스가 생성된다.\n\n\n\n\n이게 싫으면 별도 템플릿을 사용하면 됨.\n\n\n\n\n\n그런데 갑자기 geo_point 타입 필드가 필요하면?\n\n\n\n\n미리 계획된 템플릿을 준비하지 못했더라도 rest api로 간단히 원하는 필드 추가 가능.\n\nPUT winevent-2025/_mapping\n{\n    \"properties\": {\n        \"geoip\": {\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"geo_point\"\n                }\n            }\n        }\n    }\n}\n\n\n\n\n이게 번거로우면 런타임 필드 기능을 써도 된다. geo_point 타입 빈 필드 생성.\n\n\n\n\n\nset value 옵션으로 새로운 값 만들 때만 썼는데 이렇게도 활용할 수 있구나.\n\n\n관련 글\n\n엘라스틱 Runtime field - 9th\n엘라스틱 Runtime field\nElasticsearch 활용(scripted field)\n정규표현식 몰라도 된다 - 2nd\nconvert ip to decimal\nWinlogbeat 8.12의 변화",
    "reviews": [],
    "syllabus": [],
    "link": "https://kangmyounghun.blogspot.com/2025/03/runtime-field-10th.html",
    "pubDate": "2025-03-03T12:27:00.002Z",
    "creator": "Unknown",
    "categories": []
  },
  {
    "id": 12,
    "imageUrl": "",
    "title": "Cursor 0.46, 뭔가 달라졌다! Agent의 변화를 체크해보세요.",
    "description": "Cursor 0.46 업데이트에서 중요한 변화들이 있었네요! 주요 포인트를 정리해보면 다음과 같습니다.\n🔥 Cursor 0.46 업데이트 핵심 요약\n💡 Agent 기본 모드로 통합\nChat, Composer, Agent가 하나의 인터페이스로 정리됨.\n기존 모드 이름 변경\n          \nChat → Ask\nComposer (일반) → Edit\nComposer (Agent 모드) → Agent\n단축어: ⌘ + . (모드 전환 가능)\nAsk ↔ Agent 같은 컨텍스트에서 자유롭게 전환 가능!\n🌐 웹 검색 기본 제공\n이제 프롬프트에 @web을 포함하지 않아도 웹 검색이 자동 적용됨.\n만약 웹 검색이 되지 않는다고 느껴진다면, \"웹에서 공식 문서를 검색한 다음 구현을 진행\" 같은 문장을 포함하면 해결 가능.\n🖥️ 터미널 참조 기능 추가 (@terminals)\n@terminals를 사용하여 터미널 내용을 바로 참조 가능\n기존에 Agent에 터미널 내용을 전달하는 과정이 불편했는데, 이제 더 직관적인 디버깅 환경 제공\n✨ 이 업데이트의 의미는?\n더 자연스럽고 편리한 워크플로우 → 하나의 인터페이스에서 모든 기능을 수행 가능\n웹 검색이 더 직관적 → @web을 따로 입력할 필요 없이 자동 제공\n터미널 디버깅이 개선 → @terminals로 바로 참조 가능\n더 자세한 정보는?\nCursor 공식 변경 로그\n  \n이제 코딩할 때 더 빠르고 매끄럽게 작업할 수 있겠네요! 🚀🔥",
    "reviews": [],
    "syllabus": [],
    "link": "https://blog.gaerae.com/2025/03/cursor-046-agent.html",
    "pubDate": "Sun, 02 Mar 2025 14:57:00 +0000",
    "creator": "noreply@blogger.com (Unknown)",
    "categories": [
      {
        "_": "ai",
        "$": {
          "domain": "http://www.blogger.com/atom/ns#"
        }
      }
    ]
  },
  {
    "id": 13,
    "imageUrl": "",
    "title": "비디오프록 컨버터 videoproc / AI 이미지 동영상 화질 향상 / 프레임 향상 / 소음제거 프로그렘 / 유료 / 4만원",
    "description": "영상: https://www.youtube.com/watch?v=X-Zjm18F3gY\n\n\n\n \n비디오프록 : https://www.videoproc.com/kr/video-converting-software/?ttref=y1bd-nr2502-isumo\n\n \n비디오프록 컨버터(VideoProc Converter) – 원스톱 AI 미디어 개선 & 변환 프로그램\n비디오프록 컨버터(VideoProc Converter)는 4K/HD 동영상, 오디오, DVD를 전환하고, 완전 GPU 가속화를 통해 동영상을 압축, 편집, 수정, 녹화, 다운로드할 수 있는 올인원 툴입니다.\nwww.videoproc.com\n\n \n중국꺼 같은데 해킹이 묻어있을 수 있으니 주의 해야합니다.\n기능 자체는 좋아서 보안에 취약해도 되는 PC 에서 구동하면 될꺼 같군요.\n은행\n주식\n암호화폐 \n관련된 작업을 하닌 PC 에선 하지말라는 이야기\n나중에 저렴해질 수도 있으니 자주 확인해봐야겠습니다.",
    "reviews": [],
    "syllabus": [],
    "link": "http://serverdown.tistory.com/1157",
    "pubDate": "Thu, 27 Feb 2025 20:37:49 +0900",
    "creator": "SIDNFT",
    "categories": []
  },
  {
    "id": 14,
    "imageUrl": "",
    "title": "미니PC 강자 미니스포럼(MINISFORUM), 한국어 공식 홈페이지 오픈",
    "description": "미니PC 업계에서 유명한 미니스포럼(MINISFORUM)의 한국어 공식 홈페이지(https://www.minisforum.kr/)가 열렸습니다.\n \n미니스포럼은 Kommend Technology Co., Limited의 브랜드로, 중국 심천(Shenzen)에서 2012년부터 시작하여 만들어진 컴퓨터 제조사입니다. 다만 글로벌 페이지에서는 2018년부터 미니스포럼 브랜드가 만들어졌다고 하니 좀 혼란스럽네요.\n아무튼 다양한 미니PC 관련 제품을 내놓으면서 조금씩 유명해진 회사입니다.\n \n국내에서는 AMD 라이젠 프로세서를 채용한 UM 시리즈로 유명합니다만, 공식 판매 채널이 없어서 직구로만 구입 가능했고 AS는 사실상 불가능했죠.\n \n\n\n \n이번에 오픈한 미니스포럼 한국어 페이지에서는 현재 몇몇 미니PC를 판매 중입니다. 다만 가격 면에서 직구보다는 아직 많이 비싸군요. 제품 보증이 중요한 분들이 선택하실 것 같습니다.\n \n참고로 미니스포럼 한국어 페이지에서 구입하는 경우 12개월의 제한적인 보증을 제공합니다. 그리고 제품을 받고 30일 안에 초기 불량에 대해서는 교환 또는 환불을 지원합니다.\n물론 우리나라에는 대리점이 없으므로 본사와 직접 제품을 주고 받아야 하니 시간은 좀 들어갈 것 같습니다.\n \n더 자세한 내용은 이곳을 확인하시기 바랍니다.\n \n \n당연한 이야기지만 여전히 리셀러를 통한 직구 방식으로 구입한 제품에 대해서는 어떤 보증도 해주지 않습니다. 구입처에 문의하거나 스스로 부품을 사서 해결하는 방법이 가능하겠습니다.\n \n(출처 : 미니스포럼)\n \n실제로 아직 미니스포럼 한국어 페이지에서 직접 구입하여 후기를 남긴 고객이 없는지라 직구에 비해 얼마나 이득을 줄지는 모르겠지만 그래도 공식 창구가 생겼다는 점에서 일단 환영합니다.\n \n \n \n관련 글\n\n \n미니스포럼 고성능 미니워크스테이션 MS-A1, 라이젠 8700G AM5 플랫폼의 미니PC 출시\n미니PC로 유명한 미니스포럼(Minisforum)에서 AMD 라이젠7 8700G를 지원하는 MS-A1을 출시합니다.  Minisforum MS-A1의 주요 특징 이 제품은 미니스포럼에서 내놓은 인텔 플랫폼 기반의 MS-01과 비슷한 하우\nlazion.com\n\n\n \n미니스포럼, 최초의 터치스크린 코어 울트라 9 고성능 미니PC AtomMan X7 Ti 발표\n소형 PC 제조사 미니스포럼(MINISFORUM)에서 최초로 터치스크린과 인텔 코어 울트라 9 프로세서를 갖고 나온 미니PC AtomMan X7 Ti를 발표했습니다.  미니스포럼의 하이엔드 브랜드인 아톰맨(AtomMan)으\nlazion.com\n\n\n \n인텔 N100 스틱PC, MINISFORUM S100(미니스포럼 S100) 출시\n초소형 PC를 만드는 미니스포럼(MINISFORUM)에서 S100이라는 이름으로 인텔 N100 프로세서가 들어간 스틱PC를 출시합니다.  마치 좀 큰 USB 메모리스틱을 연상시키는 모양의 스틱PC는 주로 모니터 뒤에\nlazion.com",
    "reviews": [],
    "syllabus": [],
    "link": "https://lazion.com/2513739",
    "pubDate": "Fri, 28 Feb 2025 15:23:51 +0900",
    "creator": "늑돌이",
    "categories": [
      "#작은PC/#미니PC",
      "miniPC",
      "Minisforum",
      "News",
      "PC"
    ]
  },
  {
    "id": 15,
    "imageUrl": "",
    "title": "Building multimodal AI for Ray-Ban Meta glasses",
    "description": "Multimodal AI – models capable of processing multiple different types of inputs like speech, text, and images – have been transforming user experiences in the wearables space. With our Ray-Ban Meta glasses, multimodal AI helps the glasses see what the wearer is seeing. This means anyone wearing Ray-Ban Meta glasses can ask them questions about [...]\nRead More...\nThe post Building multimodal AI for Ray-Ban Meta glasses appeared first on Engineering at Meta.",
    "reviews": [],
    "syllabus": [],
    "link": "https://engineering.fb.com/2025/03/04/virtual-reality/building-multimodal-ai-for-ray-ban-meta-glasses/",
    "pubDate": "Tue, 04 Mar 2025 21:24:18 +0000",
    "creator": "Unknown",
    "categories": [
      "AI Research",
      "ML Applications",
      "Virtual Reality",
      "Meta Tech Podcast"
    ]
  }
]