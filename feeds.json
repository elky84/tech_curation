[
  {
    "name": "C++ Team Blog",
    "category": "기업",
    "posts": [
      {
        "creator": "Sinem Akinci",
        "title": "C++ symbol context and CMake build configuration awareness for GitHub Copilot in VS Code",
        "link": "https://devblogs.microsoft.com/cppblog/c-symbol-context-and-cmake-build-configuration-awareness-for-github-copilot-in-vs-code/",
        "pubDate": "Thu, 19 Feb 2026 16:13:03 +0000",
        "content:encodedSnippet": "C++ code navigation and build system tooling play an important role in the developer inner-loop. Code navigation tooling provides a precise, semantic understanding of your codebase, while build system tooling helps you express build configurations and variants for reproducible builds. In the VS Code ecosystem, these powerful capabilities are available through our C/C++ and CMake Tools extensions.\nWith the latest updates to GitHub Copilot in VS Code, we’re bringing the same C++-specific intelligence directly into agent mode by surfacing key language and build system capabilities as tools the agent can invoke. The goal is simple: make AI-assisted C++ workflows more consistent and performant by grounding them in the same symbol and build context developers already use and trust.\nThese tools are available through the new C/C++ DevTools extension, which ships as a part of the C/C++ extension pack for VS Code. To view the full documentation, please visit our C++ DevTools docs.\nC++ code understanding tools\nWith the new C++ code understanding tools, agent mode now has access to rich C++ symbol context. Instead of relying solely on text search or file search, the agent can now reason about C++ code at the symbol level across your workspace, which it can leverage to intelligently perform code editing operations across a codebase.\nThe current tools available for Copilot Chat include:\nGet symbol definition – Retrieve detailed information about a C++ symbol, including where it is defined and its associated metadata\nGet symbol references – Find all references to a given symbol across the codebase\nGet symbol call hierarchy – Surface incoming and outgoing calls for a function to understand call patterns and dependencies\nTo enable these tools, select the Enable Cpp Code Editing Tools setting in your VS Code user settings.\n\nExample use cases\nMemory safety\nMemory safety issues in C++ are rarely isolated to a single line of code. Safely modernizing or hardening code requires understanding where memory is allocated, who owns it, and how it flows through the system. With C++ code understanding tools, agent mode can reason about these questions using symbol-level context rather than text search alone. For example, in the following refactor, Copilot was able to quickly locate relevant symbol information and all references to the symbol to rapidly collect relevant context rather than manually searching through .cpp and .h files.\n\nDependency analysis\nBefore moving a component to a new library or changing an API surface, developers need to understand what components depend on it. Call hierarchies let Copilot analyze dependency chains and highlight potential ripple effects before changes are made. For example, Copilot is able to determine call hierarchies related to btDdvtBroadphase to determine the relevant calls to and from a given function.\n\nCMake build and test configuration tools\nIn C++ development, every change must compile, link, and pass unit tests across the project’s active build configuration, not just look correct in the editor.\nCMake build and test configuration tools leverage the build configurations identified and provided by the CMake tools extension, so Copilot Chat seamlessly builds and tests your project using the exact configuration you already have selected in VS Code. By working with the same CMake Tools integration you use in the editor, Copilot avoids relying on ad-hoc command-line invocations and stays aligned with your chosen targets, presets, and build state. This enables the agent to perform end-to-end C++ workflows greater accuracy and reliability.\nThe current tools available to Copilot Chat for build configuration include:\nBuild with CMake: Build a CMake project using active configuration\nRun CTests: Run CTest tests using active test suite\nList Build Targets: List the available set of build targets for a CMake project\nList CTest tests: List the available set of tests for a CMake project\nExample use cases\nFixing build errors\nIf a change to a codebase introduces a compiler or build error, Copilot can invoke the new CMake build tool using the active configuration, inspect the failure, and iterate on this fix until the project builds successfully with CMake.\n\nModify code to pass test suite\nWhen a change is introduced to code, Copilot can run relevant tests and adjust the code until they pass, using the same test infrastructure developers rely on manually, ensuring that the code not only builds successfully but passes the test suite.\n\nTips for best results\nBe specific: Identify the exact symbol, file, or component you’re asking about (for example, “refactor the getConfig() function” rather than “make this faster”)\nReference context: Ask Copilot Chat to consider specific files, functions, or modules when analyzing changes.\nDirectly reference tools: Directly reference relevant tools using # in chat to ensure invocation.\n\nUse custom instructions: Set up custom instructions to guide Copilot Chat.\nLeverage latest models: Use the latest AI models that support tool-calling for the most accurate code understanding and tool usage.\nOptimize tool performance: Only enable relevant tools to your development workflow to avoid context bloat.\nLet us know your feedback!\nWe’re excited to continue improving these tools and other C++ integration points based on feedback, and we encourage you to try them out and let us know how they fit into your C++ workflows. Download the C/C++ DevTools extension and give it a try. Please file any issues or feedback in the appropriate repository. For CMake-related functionality: Issues · microsoft/vscode-cmake-tools and for C++-related functionality: Issues · microsoft/vscode-cpptools\nThe post C++ symbol context and CMake build configuration awareness for GitHub Copilot in VS Code appeared first on C++ Team Blog.",
        "dc:creator": "Sinem Akinci",
        "comments": "https://devblogs.microsoft.com/cppblog/c-symbol-context-and-cmake-build-configuration-awareness-for-github-copilot-in-vs-code/#respond",
        "content": "<p>C++ code navigation and build system tooling play an important role in the developer inner-loop. Code navigation tooling provides a precise, semantic understanding of your codebase, while build system tooling helps you express build configurations and variants for reproducible builds. In the VS Code ecosystem, these powerful capabilities are available through our C/C++ and CMake [&#8230;]</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/cppblog/c-symbol-context-and-cmake-build-configuration-awareness-for-github-copilot-in-vs-code/\">C++ symbol context and CMake build configuration awareness for GitHub Copilot in VS Code</a> appeared first on <a href=\"https://devblogs.microsoft.com/cppblog\">C++ Team Blog</a>.</p>\n",
        "contentSnippet": "C++ code navigation and build system tooling play an important role in the developer inner-loop. Code navigation tooling provides a precise, semantic understanding of your codebase, while build system tooling helps you express build configurations and variants for reproducible builds. In the VS Code ecosystem, these powerful capabilities are available through our C/C++ and CMake […]\nThe post C++ symbol context and CMake build configuration awareness for GitHub Copilot in VS Code appeared first on C++ Team Blog.",
        "guid": "https://devblogs.microsoft.com/cppblog/?p=36293",
        "categories": [
          "C++",
          "CMake",
          "Copilot",
          "Visual Studio Code"
        ],
        "isoDate": "2026-02-19T16:13:03.000Z"
      },
      {
        "creator": "Augustin Popa",
        "title": "Microsoft C++ (MSVC) Build Tools v14.51 Preview Released: How to Opt In",
        "link": "https://devblogs.microsoft.com/cppblog/microsoft-c-msvc-build-tools-v14-51-preview-released-how-to-opt-in/",
        "pubDate": "Thu, 19 Feb 2026 02:37:48 +0000",
        "content:encodedSnippet": "Today we are releasing the first preview of the Microsoft C++ (MSVC) Build Tools version 14.51. This update, shipping in the latest Visual Studio 2026 version 18.4 Insiders release, introduces many C++23 conformance changes, bug fixes, and runtime performance improvements. Check out the release notes for an in-progress list of what’s new. Conformance improvements and bug fixes will be detailed in an upcoming blog post and Insiders release notes in the near future.\nWe plan to ship more frequent, incremental MSVC Build Tools previews, just as we are shipping more frequent IDE updates. As a result, we have adjusted the process for enabling and using MSVC previews, and this post describes the new process.\nWe encourage you to explore MSVC previews to adapt to breaking changes and report issues early. MSVC previews do not receive servicing patches and thus should not be used in production environments.\nHow to opt in\nVisual Studio 2026 has changed the process for opting in to MSVC Build Tools previews. Most Visual Studio updates will include fresh MSVC previews, bringing compiler changes to you far faster than ever before. These updates will occur more frequently in the Insiders channel. Soon, you will also be able to install MSVC previews from the Stable channel, though these will be less recent than the builds available in Insiders.\nInstalling MSVC previews\nTo install MSVC v14.51 Preview, you must select one or both of these components in the Visual Studio installer depending on what architectures you are targeting for your builds:\nMSVC Build Tools for x64/x86 (Preview)\nMSVC Build Tools for ARM64/ARM64EC (Preview)\nYou can install these from the Workloads tab under Desktop development with C++ or from the Individual components tab.\nHow to install MSVC v14.51 Preview from the C++ desktop workload\n\nMSVC v14.51 components under “Individual components”\n\nAn easy way to find the relevant components under Individual components is to search for “preview”. Here you will also find support libraries and frameworks like MFC, ATL, C++/CLI, and Spectre-mitigated libraries compatible with this MSVC preview.\nThe components are the same as stable MSVC releases, except they are marked with “(MSVC Preview)” rather than “(Latest)” or a specific version number. Whenever you update Visual Studio, your MSVC preview will also be updated to the latest available build in that installer channel. Preview MSVC builds are not designed with version pinning in mind and do not receive servicing updates, though you can always download fresh builds as you update the IDE.\nIf you only want to build in the command line, you can also install MSVC v14.51 Preview using the Build Tools for Visual Studio 2026, by selecting the same checkboxes.\nConfiguring Command Prompts\nYou can configure MSVC Preview command-line builds by navigating to this path and running the appropriate vcvars for your desired environment:\ncmd.exe example for x64 builds:\ncd \"C:\\Program Files\\Microsoft Visual Studio\\18\\Insiders\\VC\\Auxiliary\\Build\"\r\n.\\vcvars64.bat -vcvars_ver=Preview\n\nConfiguring MSBuild Projects\nFor MSBuild projects, you must enable MSVC preview builds in the project system by setting the new Use MSVC Build Tools Preview property to “Yes” and making sure the MSVC Build Tools Version property is set to “Latest supported”. If MSVC Build Tools Version is set to something other than “Latest supported”, that MSVC version will be used for builds instead. If you wish to switch back to a stable MSVC build, you should set Use MSVC Build Tools Preview to “No”.\nInstructions – Enabling MSVC previews in MSBuild projects\nFirst, right-click the project you want to modify in Solution Explorer, select Properties.\nNext, make sure your Configuration and Platform at the top are set to what you want to modify.\nUnder the General tab (open by default), set Use MSVC Build Tools Preview to “Yes”.\n\nMake sure the MSVC Build Tools Version property is set to “Latest supported”, or else your project will build with the version specified there instead.\nLastly, run a build to make sure it works. Your project will now build using the latest preview tools.\nNote: For command-line builds, you can also set the new property by running:\nmsbuild <project_or_solution_file> /p:MSVCPreviewEnabled=true\nConfiguring CMake Projects\nFor CMake projects, you should specify the MSVC version in a CMakePresets.json file under the toolset property. The same process applies regardless of what version of MSVC you want to use (and whether it’s a Preview or not).\nInstructions – Enabling MSVC previews in CMake projects\nFirst, open your CMake project in Visual Studio. Ensure your workspace has a CMakePresets.json file in the root directory. See Configure and build with CMake Presets | Microsoft Learn if you need help configuring a CMakePresets file.\nThen, add a base preset under configurePresets that specifies MSVC v14.51:\n{\r\n    \"name\": \"windows-msvc-v1451-base\",\r\n    \"description\": \"Base preset for MSVC v14.51\",\r\n    \"hidden\": true,\r\n    \"inherits\": \"windows-base\",\r\n    \"toolset\": {\r\n        \"value\": \"v145,host=x64,version=14.51\"\r\n    }\r\n}\nNext, add more specific presets for each individual architecture, e.g.:\n{\r\n    \"name\": \"x64-debug-msvc-v1451-preview\",\r\n    \"displayName\": \"x64 Debug (MSVC v14.51 Preview)\",\r\n    \"inherits\": \"windows-msvc-v1451-base\",\r\n    \"architecture\": {\r\n        \"value\": \"x64\",\r\n        \"strategy\": \"external\"\r\n    },\r\n    \"cacheVariables\": {\r\n        \"CMAKE_BUILD_TYPE\": \"Debug\"\r\n    }\r\n}\nNext, slect the new build configuration from the list of targets beside the Play button at the top of the IDE.\nLastly, run a build to make sure it works. You can create additional presets the same way for other MSVC versions to easily swap between them.\nKnown issues\nThere are several known issues that will be fixed in a future MSVC Build Tools Preview and/or Visual Studio Insiders release.\nCMake targets using Visual Studio generator\nThere is a bug configuring CMake targets using the Visual Studio (MSBuild) generator. A workaround is described below.\nFirst, open Developer Command Prompt for VS Insiders (or the prompt for the version of Visual Studio you are using) as an administrator.\nThen, run the following commands, which create a new folder and copy a file from another location to it:\npushd %VCINSTALLDIR%\\Auxiliary\\Build\r\nmkdir 14.51\r\ncopy .\\v145\\Microsoft.VCToolsVersion.VC.14.51.props .\\14.51\\Microsoft.VCToolsVersion.14.51.props\r\ncopy .\\v145\\Microsoft.VCToolsVersion.VC.14.51.txt .\\14.51\\Microsoft.VCToolsVersion.14.51.txt\nLastly, run a build to make sure it works.\nCommand-line builds using PowerShell\nCommand line builds in PowerShell (including via Launch-VsDevShell.ps1) are not yet configured for the preview.\nC++ CMake tools for Windows dependency on latest stable MSVC\nIf you are using the CMake tools in Visual Studio, their installer component still has a dependency on the latest stable version of MSVC. Therefore you will need to install both latest stable and latest preview MSVC Build Tools until we correct this dependency relationship.\nTry out MSVC v14.51 Preview in Visual Studio 2026!\nWe encourage you to try out Visual Studio 2026 version 18.4 on the Insiders Channel, along with MSVC version 14.51 Preview. For MSVC, your feedback can help us address any bugs and improve build and runtime performance. Submit feedback using the Help > Send Feedback menu from the IDE, or by navigating directly to Visual Studio Developer Community.\nThe post Microsoft C++ (MSVC) Build Tools v14.51 Preview Released: How to Opt In appeared first on C++ Team Blog.",
        "dc:creator": "Augustin Popa",
        "comments": "https://devblogs.microsoft.com/cppblog/microsoft-c-msvc-build-tools-v14-51-preview-released-how-to-opt-in/#respond",
        "content": "<p>Today we are releasing the first preview of the Microsoft C++ (MSVC) Build Tools version 14.51. This update, shipping in the latest Visual Studio 2026 version 18.4 Insiders release, introduces many C++23 conformance changes, bug fixes, and runtime performance improvements. Check out the release notes for an in-progress list of what’s new. Conformance improvements and [&#8230;]</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/cppblog/microsoft-c-msvc-build-tools-v14-51-preview-released-how-to-opt-in/\">Microsoft C++ (MSVC) Build Tools v14.51 Preview Released: How to Opt In</a> appeared first on <a href=\"https://devblogs.microsoft.com/cppblog\">C++ Team Blog</a>.</p>\n",
        "contentSnippet": "Today we are releasing the first preview of the Microsoft C++ (MSVC) Build Tools version 14.51. This update, shipping in the latest Visual Studio 2026 version 18.4 Insiders release, introduces many C++23 conformance changes, bug fixes, and runtime performance improvements. Check out the release notes for an in-progress list of what’s new. Conformance improvements and […]\nThe post Microsoft C++ (MSVC) Build Tools v14.51 Preview Released: How to Opt In appeared first on C++ Team Blog.",
        "guid": "https://devblogs.microsoft.com/cppblog/?p=36310",
        "categories": [
          "C++",
          "Visual Studio",
          "MSVC",
          "visual studio"
        ],
        "isoDate": "2026-02-19T02:37:48.000Z"
      }
    ]
  },
  {
    "name": "Facebook Engineering",
    "category": "기업",
    "posts": []
  },
  {
    "name": "eBay Tech Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Twitter Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Netflix TechBlog",
    "category": "기업",
    "posts": [
      {
        "creator": "Netflix Technology Blog",
        "title": "Scaling LLM Post-Training at Netflix",
        "link": "https://netflixtechblog.com/scaling-llm-post-training-at-netflix-0046f8790194?source=rss----2615bd06b42e---4",
        "pubDate": "Fri, 13 Feb 2026 08:05:33 GMT",
        "content:encodedSnippet": "Baolin Li, Lingyi Liu, Binh Tang, Shaojing Li\nIntroduction\nPre-training gives Large Language Models (LLMs) broad linguistic ability and general world knowledge, but post-training is the phase that actually aligns them to concrete intents, domain constraints, and the reliability requirements of production environments. At Netflix, we are exploring how LLMs can enable new member experiences across recommendation, personalization, and search, which requires adapting generic foundation models so they can better reflect our catalog and the nuances of member interaction histories. At Netflix scale, post-training quickly becomes an engineering problem as much as a modeling one: building and operating complex data pipelines, coordinating distributed state across multi-node GPU clusters, and orchestrating workflows that interleave training and inference. This blog describes the architecture and engineering philosophy of our internal Post-Training Framework, built by the AI Platform team to hide infrastructure complexity so researchers and model developers can focus on model innovation — not distributed systems plumbing.\nA Model Developer’s Post-Training Journey\nPost-training often starts deceptively simply: curate proprietary domain data, load an open-weight model from Hugging Face, and iterate batches through it. At the experimentation scale, that’s a few lines of code. But when fine-tuning production-grade LLMs at scale, the gap between “running a script” and “robust post-training” becomes an abyss of engineering edge cases.\nFigure 1. Simple steps to post-train an open-weight model.\nGetting the data right\nOn paper, post-training is straightforward: choose a tokenizer, preprocess the dataset, and build a dataloader. In practice, data preparation is where things break. High-quality post-training — instruction following, multi-turn dialogue, Chain-of-Thought — depends on precisely controlling which tokens contribute to the loss. Hugging Face chat templates serialize conversations, but don’t specify what to train on versus ignore. The pipeline must apply explicit loss masking so only assistant tokens are optimized; otherwise the model learns from prompts and other non-target text, degrading quality.\nVariable sequence length is another pitfall. Padding within a batch can waste compute, and uneven shapes across FSDP workers can cause GPU synchronization overhead. A more GPU-efficient approach is to pack multiple samples into fixed-length sequences and use a “document mask” to prevent cross-attention across samples, reducing padding and keeping shapes consistent.\nSetting up the model\nLoading an open-source checkpoint sounds simple until the model no longer fits on one GPU. At that point you need a sharding strategy (e.g., FSDP, TP) and must load partial weights directly onto the device mesh to avoid ever materializing the full model on a single device.\nAfter loading, you still need to make the model trainable: choose full fine-tuning vs. LoRA, and apply optimizations like activation checkpointing, compilation, and correct precision settings (often subtle for RL, where rollout and policy precision must align). Large vocabularies (>128k) add a further memory trap: logits are [batch, seq_len, vocab] and can spike peak memory. Common mitigations include dropping ignored tokens before projection and computing logits/loss in chunks along the sequence dimension.\nStarting the training\nEven with data and models ready, production training is not a simple “for loop”. The system must support everything from SFT’s forward/backward pass to on-policy RL workflows that interleave rollout generation, reward/reference inference, and policy updates.\nAt Netflix scale, training runs as a distributed job. We use Ray to orchestrate workflows via actors, decoupling modeling logic from hardware. Robust runs also require experiment tracking (model quality metrics like loss and efficiency metrics like MFU) and fault tolerance via standardized checkpoints to resume cleanly after failures.\nThese challenges motivate a post-training framework that lets developers focus on modeling rather than distributed systems and operational details.\nThe Netflix Post-Training Framework\nWe built Netflix’s LLM post-training framework so Netflix model developers can turn ideas like those in Figure 1 into scalable, robust training jobs. It addresses the engineering hurdles described above, and also constraints that are specific to the Netflix ecosystem. Existing tools (e.g., Thinking Machines’ Tinker) work well for standard chat and instruction-tuning, but their structure can limit deeper experimentation. In contrast, our internal use cases often require architectural variation (for example, customizing output projection heads for task-specific objectives), expanded or nonstandard vocabularies driven by semantic IDs or special tokens, and even transformer models pre-trained from scratch on domain-specific, non-natural-language sequences. Supporting this range requires a framework that prioritizes flexibility and extensibility over a fixed fine-tuning paradigm.\nFigure 2. The post-training library within Netflix stack\nFigure 2 shows the end-to-end stack from infrastructure to trained models. At the base is Mako, Netflix’s internal ML compute platform, which provisions GPUs on AWS. On top of Mako, we run robust open-source components — PyTorch, Ray, and vLLM — largely out of the box. Our post-training framework sits above these foundations as a library: it provides reusable utilities and standardized training recipes for common workflows such as Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), Reinforcement Learning (RL), and Knowledge Distillation. Users typically express jobs as configuration files that select a recipe and plug in task-specific components.\nFigure 3. Main components developed for the post-training framework\nFigure 3 summarizes the modular components we built to reduce complexity across four dimensions. As with most ML systems, training success hinges on three pillars — Data, Model, and Compute — and the rise of RL fine-tuning adds a fourth pillar: Workflow, to support multi-stage execution patterns that don’t fit a simple training loop. Below, we detail the specific abstractions and features the framework provides for each of these dimensions:\n\nData: Dataset abstractions for SFT, reward modeling, and RL; high-throughput streaming from cloud and disk for datasets that exceed local storage; and asynchronous, on-the-fly sequence packing to overlap CPU-heavy packing with GPU execution and reduce idle time.\nModel: Support for modern architectures (e.g., Qwen3, Gemma3) and Mixture-of-Experts variants (e.g., Qwen3 MoE, GPT-OSS); LoRA integrated into model definitions; and high-level sharding APIs so developers can distribute large models across device meshes without writing low-level distributed code.\nCompute: A unified job submission interface that scales from a single node to hundreds of GPUs; MFU (Model FLOPS Utilization) monitoring that remains accurate under custom architectures and LoRA; and comprehensive checkpointing (states of trained parameters, optimizer, dataloader, data mixer, etc.) to enable exact resumption after interruptions.\nWorkflow: Support for training paradigms beyond SFT, including complex online RL. In particular, we extend Single Program, Multiple Data (SPMD) style SFT workloads to run online RL with a hybrid single-controller + SPMD execution model, which we’ll describe next.\n\nToday, this framework supports research use cases ranging from post-training large-scale foundation models to fine-tuning specialized expert models. By standardizing these workflows, we’ve lowered the barrier for teams to experiment with advanced techniques and iterate more quickly.\nLearnings from Building the Post-Training Framework\nBuilding a system of this scope wasn’t a linear implementation exercise. It meant tracking a fast-moving open-source ecosystem, chasing down failure modes that only appear under distributed load, and repeatedly revisiting architectural decisions as the post-training frontier shifted. Below are three engineering learnings and best practices that shaped the framework.\nScaling from SFT to RL\nWe initially designed the library around Supervised Fine-Tuning (SFT): relatively static data flow, a single training loop, and a Single Program, Multiple Data (SPMD) execution model. That assumption stopped holding in 2025. With DeepSeek-R1 and the broader adoption of efficient on-policy RL methods like GRPO, SFT became table stakes rather than the finish line. Staying close to the frontier required infrastructure that could move from “offline training loop” to “multi-stage, on-policy orchestration.”\nSFT’s learning signal is dense and immediate: for each token position we compute logits over the full vocabulary and backpropagate a differentiable loss. Infrastructure-wise, this looks a lot like pre-training and maps cleanly to SPMD — every GPU worker runs the same step function over a different shard of data, synchronizing through Pytorch distributed primitives.\nOn-policy RL changes the shape of the system. The learning signal is typically sparse and delayed (e.g., a scalar reward at the end of an episode), and the training step depends on data generated by the current policy. Individual sub-stages — policy updates, rollout generation, reference model inference, reward model scoring — can each be implemented as SPMD workloads, but the end-to-end algorithm needs explicit coordination: you’re constantly handing off artifacts (prompts, sampled trajectories, rewards, advantages) across stages and synchronizing their lifecycle.\nIn our original SFT architecture, the driver node was intentionally “thin”: it launched N identical Ray actors, each encapsulating the full training loop, and scaling meant launching more identical workers. That model breaks down for RL. RL required us to decompose the system into distinct roles — Policy, Rollout Workers, Reward Model, Reference Model, etc. — and evolve the driver into an active controller that encodes the control plane: when to generate rollouts, how to batch and score them, when to trigger optimization, and how to manage cluster resources across phases.\nFigure 4. Architectural differences of SFT and RL framework\nFigure 4 highlights this shift. To add RL support without reinventing distributed orchestration from scratch, we integrated the core infrastructure from the open-source Verl library to manage Ray actor lifecycle and GPU resource allocation. Leveraging Verl’s backend let us focus on the “modeling surface area” — our Data/Model/Compute abstractions and internal optimizations — while keeping orchestration concerns decoupled. The result is a hybrid design: a unified user interface where developers can move between SFT and RL workflows without adopting an entirely different mental model or API set.\nHugging Face-Centric Experience\nThe Hugging Face Hub has effectively become the default distribution channel for open-weight LLMs, tokenizers, and configs. We designed the framework to stay close to that ecosystem rather than creating an isolated internal standard. Even when we use optimized internal model representations for speed, we load and save checkpoints in standard Hugging Face formats. This avoids “walled garden” friction and lets teams pull in new architectures, weights, and tokenizers quickly.\nThis philosophy also shaped our tokenizer story. Early on, we bound directly to low-level tokenization libraries (e.g., SentencePiece, tiktoken) to maximize control. In practice, that created a costly failure mode: silent training–serving skew. Our inference stack (vLLM) defaults to Hugging Face AutoTokenizer, and tiny differences in normalization, special token handling, or chat templating can yield different token boundaries — exactly the kind of mismatch that shows up later as inexplicable quality regressions. We fixed this by making Hugging Face AutoTokenizer the single source of truth. We then built a thin compatibility layer (BaseHFModelTokenizer) to handle post-training needs — setting padding tokens, injecting generation markers to support loss masking, and managing special tokens / semantic IDs — while ensuring the byte-level tokenization path matches production.\nWe do take a different approach for model implementations. Rather than training directly on transformers model classes, we maintain our own optimized, unified model definitions that can still load/save Hugging Face checkpoints. This layer is what enables framework-level optimizations — e.g., FlexAttention, memory-efficient chunked cross-entropy, consistent MFU accounting, and uniform LoRA extensibility — without re-implementing them separately for every model family. A unified module naming convention also makes it feasible to programmatically locate and swap components (Attention, MLP, output heads) across architectures, and provides a consistent surface for Tensor Parallelism and FSDP wrapping policies.\nThe trade-off is clear: supporting a new model family requires building a bridge between the Hugging Face reference implementation and our internal definition. To reduce that overhead, we use AI coding agents to automate much of the conversion work, with a strict logit verifier as the gate: given random inputs, our internal model must match the Hugging Face logits within tolerance. Because the acceptance criterion is mechanically checkable, agents can iterate autonomously until the implementation is correct, dramatically shortening the time-to-support for new architectures.\nToday, this design means we can only train architectures we explicitly support — an intentional constraint shared by other high-performance systems like vLLM, SGLang, and torchtitan. To broaden coverage, we plan to add a fallback Hugging Face backend, similar to the compatibility patterns these projects use: users will be able to run training directly on native transformers models for rapid exploration of novel architectures, with the understanding that some framework optimizations and features may not apply in that mode.\nProviding Differential Value\nA post-training framework is only worth owning if it delivers clear value beyond assembling OSS components. We build on open source for velocity, but we invest heavily where off-the-shelf tools tend to be weakest: performance tuned to our workload characteristics, and integration with Netflix-specific model and business requirements. Here are some concrete examples:\nFirst, we optimize training efficiency for our real use cases. A representative example is extreme variance in sequence length. In FSDP-style training, long-tail sequences create stragglers: faster workers end up waiting at synchronization points for the slowest batch, lowering utilization. Standard bin-packing approaches help, but doing them offline at our data scale can add substantial preprocessing latency and make it harder to keep datasets fresh. Instead, we built on-the-fly sequence packing that streams samples from storage and dynamically packs them in memory. Packing runs asynchronously, overlapping CPU work with GPU compute. Figure 5 shows the impact: for our most skewed dataset, on-the-fly packing improved the effective token throughput by up to 4.7x.\nFigure 5. Training throughput on two of our internal datasets on A100 and H200 GPUs\nWe also encountered subtler performance cliffs around vocabulary expansion. Our workloads frequently add custom tokens and semantic IDs. We found that certain vocabulary sizes could cause the language model head to fall back from a highly optimized cuBLAS kernel to a much slower CUTLASS path, tripling that layer’s execution time. The framework now automatically pads vocabulary sizes to multiples of 64 so the compiler selects the fast kernel, preserving throughput without requiring developers to know these low-level constraints.\nSecond, owning the framework lets us support “non-standard” transformer use cases that generic LLM tooling rarely targets. For example, some internal models are trained on member interaction event sequences rather than natural language, and may require bespoke RL loops that integrate with highly-customized inference engines and optimize business-defined metrics. These workflows demand custom environments, reward computation, and orchestration patterns — while still needing the same underlying guarantees around performance, tracking, and fault tolerance. The framework is built to accommodate these specialized requirements without fragmenting into one-off pipelines, enabling rapid iteration.\nWrap up\nBuilding the Netflix Post-Training Framework has been a continual exercise in balancing standardization with specialization. By staying anchored to the open-source ecosystem, we’ve avoided drifting into a proprietary stack that diverges from where the community is moving. At the same time, by owning the core abstractions around Data, Model, Compute, and Workflow, we’ve preserved the freedom to optimize for Netflix-scale training and Netflix-specific requirements.\nIn the process, we’ve moved post-training from a loose collection of scripts into a managed, scalable system. Whether the goal is maximizing SFT throughput, orchestrating multi-stage on-policy RL, or training transformers over member interaction sequences, the framework provides a consistent set of primitives to do so reliably and efficiently. As the field shifts toward more agentic, reasoning-heavy, and multimodal architectures, this foundation will help us translate new ideas into scalable GenAI prototypes — so experimentation is constrained by our imagination, not by operational complexity.\nAcknowledgements\nThis work builds on the momentum of the broader open-source ML community. We’re especially grateful to the teams and contributors behind Torchtune, Torchtitan, and Verl, whose reference implementations and design patterns informed many of our training framework choices — particularly around scalable training recipes, distributed execution, and RL-oriented orchestration. We also thank our partner teams in Netflix AI for Member Systems for close collaboration, feedback, and shared problem-solving throughout the development and rollout of the Post-Training Framework, and the Training Platform team for providing the robust infrastructure and operational foundation that makes large-scale post-training possible.\n\nScaling LLM Post-Training at Netflix was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Netflix Technology Blog",
        "guid": "https://medium.com/p/0046f8790194",
        "categories": [
          "ai-infrastructure",
          "llm",
          "reinforcement-learning"
        ],
        "isoDate": "2026-02-13T08:05:33.000Z"
      }
    ]
  },
  {
    "name": "JetBrains: Developer Tools for Professionals and Teams – Company Blog | JetBrains",
    "category": "기업",
    "posts": [
      {
        "creator": "Dmitrii Korovin",
        "title": "TeamCity 2025.11.3 Is Here",
        "link": "https://blog.jetbrains.com/teamcity/2026/02/teamcity-2025-11-3-bug-fix/",
        "pubDate": "Thu, 19 Feb 2026 16:48:20 +0000",
        "content:encodedSnippet": "Today we’re rolling out another bug-fix update for your TeamCity 2025.11 On-Premises servers. This update addresses a number of issues, inlcuding:\nQoL updates for the bundled IntelliJ Platform Plugin;\nVariable GID value for Docker inside TeamCity Docker images;\nNPE login fails;\nConnection settings are reset after changing the parent project.\nAll TeamCity bug-fix updates include performance and security improvements, so we recommend that you never skip these minor updates. See TeamCity 2025.11.3 Release Notes for the complete list of resolved issues.\nWhy update?\nStaying up to date with minor releases ensures your TeamCity instance benefits from the following:\nPerformance improvements.\nBetter compatibility with integrations.\nFaster, more stable builds.\nEnhanced security for your workflows.\nCompatibility\nTeamCity 2025.11.3 shares the same data format as all 2025.11.x releases. You can upgrade or downgrade within this series without the need for backup and restoration.\nHow to upgrade\nUse the automatic update feature in your current TeamCity version.\nDownload the latest version directly from the JetBrains website.\nPull the updated TeamCity Docker image.\nNeed help?\nThank you for reporting issues and providing feedback! If you have questions or run into any problems, please let us know via the TeamCity Forum or Issue Tracker.\nHappy building!",
        "dc:creator": "Dmitrii Korovin",
        "content": "Today we&#8217;re rolling out another bug-fix update for your TeamCity 2025.11 On-Premises servers. This update addresses a number of issues, inlcuding: All TeamCity bug-fix updates include performance and security improvements, so we recommend that you never skip these minor updates. See TeamCity 2025.11.3 Release Notes for the complete list of resolved issues. Why update? Staying [&#8230;]",
        "contentSnippet": "Today we’re rolling out another bug-fix update for your TeamCity 2025.11 On-Premises servers. This update addresses a number of issues, inlcuding: All TeamCity bug-fix updates include performance and security improvements, so we recommend that you never skip these minor updates. See TeamCity 2025.11.3 Release Notes for the complete list of resolved issues. Why update? Staying […]",
        "guid": "https://blog.jetbrains.com/?post_type=teamcity&p=682588",
        "categories": [
          "bug-fix"
        ],
        "isoDate": "2026-02-19T16:48:20.000Z"
      },
      {
        "creator": "Alina Dolgikh",
        "title": "Java to Kotlin Conversion Comes to Visual Studio Code",
        "link": "https://blog.jetbrains.com/kotlin/2026/02/java-to-kotlin-conversion-comes-to-visual-studio-code/",
        "pubDate": "Thu, 19 Feb 2026 16:25:00 +0000",
        "content:encodedSnippet": "At JetBrains, we aim to make Kotlin development as accessible and efficient as possible across the entire ecosystem. While IntelliJ IDEA remains the premier IDE for Kotlin, we recognize that many developers use Visual Studio Code for a variety of tasks and projects.\nTo help streamline the transition from Java to Kotlin for VS Code users, we are pleased to introduce the Java to Kotlin (J2K) converter extension.\nDownload the Extension\n         \nSeamless Conversion in VS Code\nThis new extension allows you to convert individual Java files into Kotlin code with a simple context menu action, significantly reducing the manual effort required when migrating legacy codebases or switching languages mid-project.\nBecause the extension leverages the same underlying engine used in our primary IDEs, you can expect a reliable conversion that respects Kotlin idioms and syntax requirements.\nSee it in action\nThe converter is designed to be unobtrusive and easy to use. Watch the short demo below to see how it handles the conversion process.\n\n\n\n\n\n\nHow to get started\nTo begin using the converter, simply:\nInstall the Java to Kotlin Converter extension from the Visual Studio Marketplace.\nOpen any .java file in your workspace.\nRight-click anywhere in the editor or on the file in the Explorer and select Convert to Kotlin.\nOur Commitment to the Ecosystem\nThis extension is part of our ongoing effort to support Kotlin users wherever they choose to write code. It joins other initiatives aimed at improving the developer experience outside of IntelliJ IDEA, such as the Kotlin LSP, which provides IDE-independent language support via the Language Server Protocol.\nAs this is a new release, we highly value your feedback. If you encounter any issues or have suggestions for improvements, please report them on YouTrack or through the extension’s marketplace page.",
        "dc:creator": "Alina Dolgikh",
        "content": "At JetBrains, we aim to make Kotlin development as accessible and efficient as possible across the entire ecosystem. While IntelliJ IDEA remains the premier IDE for Kotlin, we recognize that many developers use Visual Studio Code for a variety of tasks and projects. To help streamline the transition from Java to Kotlin for VS Code [&#8230;]",
        "contentSnippet": "At JetBrains, we aim to make Kotlin development as accessible and efficient as possible across the entire ecosystem. While IntelliJ IDEA remains the premier IDE for Kotlin, we recognize that many developers use Visual Studio Code for a variety of tasks and projects. To help streamline the transition from Java to Kotlin for VS Code […]",
        "guid": "https://blog.jetbrains.com/?post_type=kotlin&p=682675",
        "categories": [
          "kotlin",
          "news",
          "release"
        ],
        "isoDate": "2026-02-19T16:25:00.000Z"
      },
      {
        "creator": "Katie Fraser",
        "title": "How Students Are Using AI-Powered Hints in Programming Courses",
        "link": "https://blog.jetbrains.com/research/2026/02/how-ai-powered-hints-used/",
        "pubDate": "Thu, 19 Feb 2026 12:02:23 +0000",
        "content:encodedSnippet": "Learning to code in an online course can feel like hitting a wall again and again, without much help moving forward. Our research team wanted to change that. That’s why our Education Research team built an AI-powered hints tool that doesn’t just point out errors but helps students understand them. As described in our blog post last summer, students can use the tool in JetBrains Academy courses to get tailored guidance nudging them towards the solution instead of staring blankly at a stubborn piece of code or waiting for help on a forum. \nTry our plugin\n                                                    \nSince then, our researchers have investigated how students actually use the tool and published the results in a paper that they will present at the Technical Symposium on Computer Science Education (SIGCSE TS) February 19 in St. Louis, Missouri. In this blog post, we will explain how the study was set up, its results, and our analysis.\nLearning to code with the help of smart feedback\nIn the last post, we covered the history of online learning and the importance of feedback in any type of learning environment. Online programming courses can differ a lot in format, especially in how they encourage students to practice writing and running code within the course. \nMany online learning platforms, such as Udemy, Codecademy, and edX, provide online editors for students to complete the exercises. Learning programming inside a code editor is easier than installing and learning new software, but in the long run it means that the student is unfamiliar with the relevant professional environment. Other platforms, such as Hyperskill, offer a hybrid option, where the student can also learn in a professional integrated developer environment (IDE) if they choose to do so. The downside of a hybrid course is that some students might not choose to use the IDE, for example if they consider it a hurdle to install and learn to navigate new software.\nFor our JetBrains Academy courses, we released the JetBrains Academy plugin a few years ago, which lets educators build complete courses directly inside the IDE. With this setup, students can read theory and immediately apply it by solving practical tasks in the very same environment they use to write code. As a result, they can become comfortable working in IDEs early on and are better prepared for real-world software development roles. \nA recent addition to our plugin is the AI-powered hints tool, developed by our   Education Research team. The tool leverages powerful IDE features by combining static analysis and code quality checks with LLMs to provide personalized hints to students. In the previous paper, we conducted evaluation studies to gauge how useful students found the tool while learning, as described in the previous blog post.\nTo help students even further, we wanted to know more about how they are really interacting with the tool and discover possible new behavior patterns. In the rest of this blog post, we will talk about exactly this: how students are actually interacting with the AI-powered hints tool.\nSmart feedback in online learning\nRecent years have seen many studies exploring how students interact with intelligent tutoring systems, offering useful directions for future system design. In this short subsection, we will provide a summary of recent studies, plus describe what our study adds to the field.\nResearchers in this study examined an intelligent next-step hint system to understand why students ask for help, looking at factors such as time elapsed and code completeness after each hint, and identifying reasons like difficulty starting an exercise. This work provides general insights into students’ hint-seeking patterns, but without the details (e.g. keystroke data). This study analyzed when and how to provide feedback and hints using keystroke datasets annotated with points where experts would intervene, but did not look at students’ actual interactions with hint systems. \nIn this paper, researchers investigated how different hint levels support task completion and why students request hints, using a think‑aloud study with pre‑ and post‑tests for 12 students. However, we would have liked to see broader usage patterns or how learners cope with unhelpful hints.\nMore recently, researchers studied how students use on-demand automated hints in an SQL course and how these hints affect learning, using A/B testing to identify behavioral differences and simple patterns. For example, they found that students often request a second hint within 10 seconds of the first one. While this research sheds light on basic behavioral trends, it mainly asks whether adding a hint system supports learning, rather than closely examining the interaction patterns themselves. \nOverall, prior work has focused on why and when hints are given, but far less on how students actually work with them in practice. In our study, we applied process mining techniques to uncover detailed behavioral patterns in how students interact with hints while they solve programming tasks. In addition, we conducted interviews with a subset of the students to learn more about how they interact with the hints tool in specific scenarios.\nOur investigation into students’ behavioral patterns\nWith our study, we wanted to better understand how the students are really interacting with the tool, as well as how students navigate situations with less helpful hints. Specifically, our two research questions were:\nWhat behavioral patterns can be identified based on students’ interaction with the next-step hint system?\nWhat strategies do students use to overcome unhelpful hints?\nOur study collected data in two steps. First, we collected information about all of the students’ interactions with the hints system, analyzing these interactions quantitatively. Then, we selected a subset of six students to interview for about 30 minutes, so that we had a qualitative and in-depth complement to the quantitative analysis. In the next subsection, we will tell you about the methodology for each step. The subsection after that will present the results and analysis.\nQuantitative behavioral analysis\nFor the first part of the study, we analyzed detailed problem-solving data from first- and second-year Bachelor’s students working on programming tasks in our introductory Kotlin course. The selected projects represent different levels of complexity and cover basic topics such as variables, loops, conditional operators, and functions. The 34 participants had previously completed at least one programming course in another language, and they had no experience with Kotlin before. We asked these students to complete three projects from the course, using the hint system whenever they wanted. \nWe used KOALA to capture keystrokes, IDE actions, the windows they accessed, and rich information about their hint usage. Hint-usage information included details like when hints were requested and for which specific tasks. The resulting dataset contains:\n6,658,936 lines of code\n1,364,943 IDE activity events\n960 textual hint requests\n453 code hint requests \nNote: This dataset is open source, and we encourage you to take a look. It can be used for further analysis in a research project, or even just for educators to see how beginner learners are programming.\nProcess mining and our dataset: Preliminaries\nAfter collecting this data, we conducted a quantitative analysis of the students’ behavior. To do this, we used a process mining technique. Process mining turns log or event data into behavioral models using action analysis, taking into account both event frequency and sequence. In business settings, the technique can help companies see where improvements are needed, as is argued for in this process mining manifesto, and save a lot of money, for example, by finding instances of duplicate payments or minimizing complex order delays.\nAlthough it is primarily used in industry settings, process mining has already been applied in educational domains (see this paper and this paper for examples). As far as we know, we are the first to use process mining to analyze programming hint systems.\nTo apply this technique to our hint-systems dataset, we first extracted all hint-request sessions from the dataset (1,050 in total). The data from these sessions are the input for the analysis, which uses specialized algorithms to map the activities and transitions between them.\nThe sessions began when a student requested a hint. By clicking Get Hint or Retry, they ended when it was clear that the hint was processed, either by accepting the code hint or closing the text hint banner. The session was also considered “over” if the student ran into an error or there was no activity for five minutes.\nWe also defined different activities within sessions. Here are the activities:\nHint Button Clicked: The student clicks Get Hint to generate a hint.\nHint Retry Clicked: The student clicks Retry to regenerate a hint.\nHint Code Generated: The code hint is generated. By tool design, this action is triggered every time Get Hint is clicked.\nHint Banner Shown: The banner with a textual hint is shown to the student. Although this action is carried out by the system, not by the student, it marks the moment when the student has viewed the hint. This action will be used for a more detailed analysis.\nShow Code Hint Clicked: The student clicks Show Code to show a code hint.\nHint Accepted: The student clicks Accept to accept a code hint.\nHint Banner Closed: The student closes the textual hint banner.\nHint Canceled: The student clicks Cancel to decline a code hint.\nError Occurred: This action includes cases where internal errors occurred during the hint generation process, e.g. problems with the internet connection, or when the LLM did not provide a response.\nAll sessions began with items 1 or 2, i.e. clicking Get Hint or Retry. A session could end with different activities, with the most common being items 6, 8, and 9: Hint Accepted, Hint Canceled, or Error Occurred. \nIn the analysis, we coded the sequence of activities within each session. We also grouped the sessions into the following three main types:\nPositive Student accepted the code hint\nNeutral Student saw a hint but didn’t clearly accept or reject it\nNegative Student canceled a hint or got an error\nThe first and last types are straightforward: For the positive type, the session ended when the student accepted the code hint. For the negative, the session ended because the student did not use the hint, having received an error or actively cancelled the hint. \nThe neutral type will be looked at in more detail in the next section, as it required additional analysis to determine how helpful the student found the hint. We know by the actions carried out that the student saw a text or code hint, but they proceeded without explicitly accepting the code hint. Examples of neutral sessions include the following (Hint Code Generated has been omitted, as it automatically occurs as the second step in each):\nHint Retry Clicked → Hint Banner Shown \nHint Button Clicked → Hint Banner Shown → Show Code Hint Clicked\nHint Retry Clicked → Hint Banner Shown → Hint Banner Closed\nThe numbers below shows the distribution of session types among the 1,050 total sessions.\nPositive: 299 (28%)\nNeutral: 420 (40%)\nNegative: 284 (27%)\n\n\n\n\nThe above shows that strictly positive sessions make up about a third of all sessions. As will be discussed below, we found out that the neutral cases often actually were successful in helping the students with their tasks, even if not positive as defined above. Namely, in these cases students clearly analyzed and used the content of the hints but did not automatically apply them.\nNote that 266 of the 284 (94%) negative sessions ended with errors, rather than the student cancelling the hint. These errors were either internet connection issues on the student side or a system issue on our side; we have already fixed any such system errors. In addition to these three types listed above, we collected 47 data points, accounting for 4,5% of the sessions, that were not classifiable based on the available data. \nWith these preliminary data about the “events”, we were able to move on to applying the process mining technique for our analysis. The next subsection describes these results. \nProcess mining and our dataset: Analysis\nWe turned the data described in the previous subsection into a map of how students moved through the system. This sort of map is also called a process behavior graph. The graph for our dataset is displayed below; as it is quite complex, we will explain the most important parts individually.\n\n\n\n\nIn the above graph, some boxes are colored, while others are not. The blue boxes at the top represent activities that begin a session, either by generating a new hint or retrying a hint; the red boxes on the left represent activities that end a negative session; the green box at the bottom is the activity that can end a positive session; and the other activities represent activities that can otherwise happen during a session. \nThe arrows indicate the direction of transitions, and the numbers next to the arrows indicate the average time in seconds for each transition. Longer times, particularly those around 900 seconds, indicate that the student was solving the task and then returned for another hint. In those cases, a new event would have been triggered.\nEach arrow’s transparency and thickness represent information about the frequency and average prevalence. As can be seen in the fragment below, the most frequent transitions occurred from the blue box Hint Button Clicked to white boxes Hint Code Generated and then to Hint Banner Shown. \n\nThe arrows that circle back to themselves, such as in the snippet below, represent those instances in which students repeatedly clicked the same button. In the case of the white box Show Code Hint Clicked, we think this suggests that the student might not have understood how the code hint worked, and that a future iteration of the tool would need to provide more explanation to help the student.\n\nLooking more closely at the negative sessions, we can see in the following image two arrows coming from the red box labelled ERR. The right arrow with an 18.7-second average leads to HRC, or Hint Retry Clicked. The left arrow with > 900 s leads to HBC, or Hint Button Clicked.\n\nThese patterns indicate that even when the student encountered an error, such as a bad internet connection, they persisted in generating a new hint, either within the same task or in another. This is an important insight because it shows that students are motivated to use the hints tool even when encountering errors or negative scenarios.\nIn the image below, we can also see that between the blue Hint Button Clicked and various white boxes, there is an arrow going back to the blue HBC button. We interpret these transitions as indicating that the students modified their code to generate additional variations of the same hint before integrating the information into a solution. That is, the pattern suggests that they did not simply click Accept Hint, but tried to solve the task themselves.\n\nIn the classification part of the analysis described in the previous subsection, these types of activities usually comprised the sessions labelled as neutral, as they often did not end with the student accepting the hint. These and other unexpected patterns were why we wanted to hear more from these students about why they behaved the way they did.\nQualitative behavioral analysis\nAs described in the previous section, we observed three types of sessions when the students interacted with the AI-powered hint system. We categorized a session as positive when the student ended the trajectory by clicking Accept Hint, as the tool is working as intended. We categorized a session as negative if the student received an error message or they themselves canceled the hint. Interestingly, the students still persevered in trying for new hints despite errors. \nFinally, we categorized sessions as neutral when the student clearly saw the hint but did not end the session like in positive or negative sessions. This means they saw the hint, but did not apply it to their code; for this reason, we do not consider these sessions negative or unsuccessful. Our analysis shows that neutral sessions make up 40% of all sessions, and we were curious about what exactly the students’ strategies were. \nIn order to learn more about what the students were thinking, we selected a subset of the participants so that they represented different experience levels, as well as locations. We conducted six semi-structured interviews, each lasting approximately 30 minutes. The interviews had two main sections: asking the participants about their experience with AI-powered hints and about any other forms of assistance they might have used while completing the tasks.\nLet’s look at what is going on in the neutral scenarios. We will focus on reporting two kinds of behaviors in these neutral scenarios: selective use of hints and combining partial solutions.\nSelective use of hints\nFrom these interviews, we learned that some participants chose to work with the code hints manually instead of clicking Accept Hint. The reasons for this depended on the situation: sometimes they chose to manually alter their code for improved learning, i.e. so that they could better remember how to fix the problem next time; sometimes they only wanted to use part of the code hint. We call this behavior selective use of hints.\nIn the log data, we could see that the students opened the visual diff window in the code editor. This window allows the student to view both their own code and the code hint. After opening this window, the students then manually typed in the recommended code from the hint – if the hint was clear. \nWe can show you two examples where the hints were unclear. In the first, the text hint suggested that an image should be trimmed before applying filters, and that this trimming could be done by storing a temporary result in a variable. This can be seen in the image below, inside the box bordered by a dotted line.\n\nAs can be seen above, the student did not completely follow the hint; instead they only trimmed the image and did not create a new variable. An advantage of the in-IDE format over a course in an online editor is that the IDE will show the student an optimized way to proceed with the removed variable, even though the hint had suggested otherwise. The student’s strategy in this example suggests that students are not unquestioningly accepting hints, but are actually analyzing and adapting them for their benefit. \nIn a second case, the text hint suggested that the student should add a Boolean variable to store the result of a game. The image below displays the hint, as well as the various student attempts, which included a few compilation errors. \n\nThe text hint was not wrong, but it did not provide the full story: in Kotlin, Boolean variables can be initialized later when used in a loop, and so it provided the code without any initial value. Based on the student’s attempts to compile, they did not understand this language-specific feature. \nFrom looking more closely at the students’ behavior concerning selective use of hints, we learned both that students are actively trying to adapt the hints so that they can learn better and that, in creating the hints, we should take into account language-specific features that a beginner student might not yet know about. Both takeaways form a solid basis for future investigations. \nCombining partial solutions\nIn the qualitative analysis of the AI-powered hints tool, we additionally learned that the students sometimes combined information from multiple hint attempts, instead of using only one hint. In the log data, this appeared as clusters of closely spaced hint requests on the same portion of code, followed by students editing the solution themselves rather than just copy-and-pasting code. This behavior is depicted schematically in the image below.\n\nThis behavior, which we call combining partial solutions, can be seen as a strategy to develop a working solution by troubleshooting and making small changes to their solutions, each time generating a new hint. As with the previous behavior, students did not accept the code hint, which is why we categorized it as a neutral and not a positive scenario. \nWithin this analysis, we found that students often toggled between their original code version and multiple hints: they kept the previous code hint windows open. We did not expect this behavior from students using the hint tool and will investigate further in future work.\nWe also observed that students sometimes copied their original code outside the task environment, reusing it later alongside new attempts. In the end, they were able to obtain a working solution with this strategy. This is another behavior that we did not expect from the students, and we are interested in investigating it further in the future. More specifically, we would like to provide the participants with multiple hints simultaneously (instead of sequentially), so that they can construct a working solution efficiently and effectively. \nFrom this analysis, we have learned that the hint tool could benefit from better support for partial adoption of hints and designs that embrace the way students naturally mix system suggestions with their own understanding and ideas, so that the student does not have to manually open multiple windows or copy-and-paste code from outside the IDE.\nFurthermore, what we can understand from this behavior of combining partial solutions is that the students are active problem solvers. That is, they are not just clicking through the hints so that they can advance to the next step or lesson. Instead, students analyze the hints and adapt them to get even a better solution – critical thinking skills like these are invaluable in the LLM era.\nExplore it yourself \nInterested in learning programming with the help of our AI-powered hints tool? Check out our JetBrains Academy course catalog. \n\nYou can also use our data to do your own research to discover something new about how students use smart feedback in online programming courses. \nExplore our dataset\n                                                    \nAnd finally, if you’re interested in collaborating with our team, let us know!",
        "dc:creator": "Katie Fraser",
        "content": "Learning to code in an online course can feel like hitting a wall again and again, without much help moving forward. Our research team wanted to change that. That’s why our Education Research team built an AI-powered hints tool that doesn’t just point out errors but helps students understand them. As described in our blog [&#8230;]",
        "contentSnippet": "Learning to code in an online course can feel like hitting a wall again and again, without much help moving forward. Our research team wanted to change that. That’s why our Education Research team built an AI-powered hints tool that doesn’t just point out errors but helps students understand them. As described in our blog […]",
        "guid": "https://blog.jetbrains.com/?post_type=research&p=682019",
        "categories": [
          "research",
          "ai-based-hints",
          "education-research",
          "jetbrains-academy",
          "jetbrains-research"
        ],
        "isoDate": "2026-02-19T12:02:23.000Z"
      },
      {
        "creator": "Cheuk Ting Ho",
        "title": "LangChain Python Tutorial: 2026’s Complete Guide",
        "link": "https://blog.jetbrains.com/pycharm/2026/02/langchain-tutorial-2026/",
        "pubDate": "Thu, 19 Feb 2026 10:40:15 +0000",
        "content:encodedSnippet": "If you’ve read the blog post How to Build Chatbots With LangChain, you may want to know more about LangChain. This blog post will dive deeper into what LangChain offers and guide you through a few more real-world use cases. And even if you haven’t read the first post, you might still find the info in this one helpful for building your next AI agent.\nLangChain fundamentals\nLet’s have a look at what LangChain is. LangChain provides a standard framework for building AI agents powered by LLMs, like the ones offered by OpenAI, Anthropic, Google, etc., and is therefore the easiest way to get started. LangChain supports most of the commonly used LLMs on the market today.\nLangChain is a high-level tool built on LangGraph, which provides a low-level framework for orchestrating the agent and runtime and is suitable for more advanced users. Beginners and those who only need a simple agent build are definitely better off with LangChain.\nWe’ll start by taking a look at several important components in a LangChain agent build.\nAgents\nAgents are what we are building. They combine LLMs with tools to create systems that can reason about tasks, decide which tools to use for which steps, analyze intermittent results, and work towards solutions iteratively.\n\n\n\n\nCreating an agent is as simple as using the `create_agent` function with a few parameters:\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n\n   \"gpt-5\",\n\n   tools=tools\n\n)\nIn this example, the LLM used is GPT-5 by OpenAI. In most cases, the provider of the LLM can be inferred. To see a list of all supported providers, head over here.\nLangChain Models: Static and Dynamic\nThere are two types of agent models that you can build: static and dynamic. Static models, as the name suggests, are straightforward and more common. The agent is configured in advance during creation and remains unchanged during execution.\nimport os\n\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nmodel = init_chat_model(\"gpt-5\")\n\nprint(model.invoke(\"What is PyCharm?\"))\n\nDynamic models allow you to build an agent that can switch models during runtime based on customized logic. Different models can then be picked based on the current state and context. For example, we can use ModelFallbackMiddleware (described in the Middleware section below) to have a backup model in case the default one fails.\nfrom langchain.agents import create_agent\n\nfrom langchain.agents.middleware import ModelFallbackMiddleware\n\nagent = create_agent(\n\n   model=\"gpt-4o\",\n\n   tools=[],\n\n   middleware=[\n\n       ModelFallbackMiddleware(\n\n           \"gpt-4o-mini\",\n\n           \"claude-3-5-sonnet-20241022\",\n\n       ),\n\n   ],\n\n)\nTools\nTools are important parts of AI agents. They make AI agents effective at carrying out tasks that involve more than just text as output, which is a fundamental difference between an agent and an LLM. Tools allow agents to interact with external systems – such as APIs, databases, or file systems. Without tools, agents would only be able to provide text output, with no way of performing actions or iteratively working their way toward a result.\nLangChain provides decorators for systematically creating tools for your agent, making the whole process more organized and easier to maintain. Here are a couple of examples:\nBasic tool\n@tool\n\ndef search_db(query: str, limit: int = 10) -> str:\n\n   \"\"\"Search the customer database for records matching the query.\n\n   \"\"\"\n\n...\n\n   return f\"Found {limit} results for '{query}'\"\nTool with a custom name\n@tool(\"pycharm_docs_search\", return_direct=False)\n\ndef pycharm_docs_search(q: str) -> str:\n\n   \"\"\"Search the local FAISS index of JetBrains PyCharm documentation and return relevant passages.\"\"\"\n\n...\n\n   docs = retriever.get_relevant_documents(q)\n\n   return format_docs(docs)\nMiddleware\nMiddleware provides ways to define the logic of your agent and customize its behavior. For example, there is middleware that can monitor the agent during runtime, assist with prompting and selecting tools, or even help with advanced use cases like guardrails, etc.\nHere are a few examples of built-in middleware. For the full list, please refer to the LangChain middleware documentation.\n\nMiddlewareDescription\nSummarizationAutomatically summarize the conversation history when approaching token limits.\nHuman-in-the-loopPause execution for human approval of tool calls.\nContext editingManage conversation context by trimming or clearing tool uses.\nPII detectionDetect and handle personally identifiable information (PII).\n\n\n\n\n\nReal-world LangChain use cases\nLangChain use cases cover a varied range of fields, with common instances including: \nAI-powered chatbots\nDocument question answering systems\nContent generation tools\nAI-powered chatbots\nWhen we think of AI agents, we often think of chatbots first. If you’ve read the How to Build Chatbots With LangChain blog post, then you’re already up to speed about this use case. If not, I highly recommend checking it out.\nDocument question answering systems\nAnother real-world use case for LangChain is a document question answering system. For example, companies often have internal documents and manuals that are rather long and unwieldy. A document question answering system provides a quick way for employees to find the info they need within the documents, without having to manually read through each one.\nTo demonstrate, we’ll create a script to index the PyCharm documentation. Then we’ll create an AI agent that can answer questions based on the documents we indexed. First let’s take a look at our tool:\n@tool(\"pycharm_docs_search\")\n\ndef pycharm_docs_search(q: str) -> str:\n\n   \"\"\"Search the local FAISS index of JetBrains PyCharm documentation and return relevant passages.\"\"\"\n\n   # Load vector store and create retriever\n\n   embeddings = OpenAIEmbeddings(\n\n       model=settings.openai_embedding_model, api_key=settings.openai_api_key\n\n   )\n\n   vector_store = FAISS.load_local(\n\n       settings.index_dir, embeddings, allow_dangerous_deserialization=True\n\n   )\n\n   k = 4\n\n   retriever = vector_store.as_retriever(\n\n       search_type=\"mmr\", search_kwargs={\"k\": k, \"fetch_k\": max(k * 3, 12)}\n\n   )\n\n   docs = retriever.invoke(q)\nWe are using a vector store to perform a similarity search with embeddings provided by OpenAI. Documents are embedded so the doc search tool can perform similarity searches to fetch the relevant documents when called. \ndef main():\n\n   parser = argparse.ArgumentParser(\n\n       description=\"Ask PyCharm docs via an Agent (FAISS + GPT-5)\"\n\n   )\n\n   parser.add_argument(\"question\", type=str, nargs=\"+\", help=\"Your question\")\n\n   parser.add_argument(\n\n       \"--k\", type=int, default=6, help=\"Number of documents to retrieve\"\n\n   )\n\n   args = parser.parse_args()\n\n   question = \" \".join(args.question)\n\n   system_prompt = \"\"\"You are a helpful assistant that answers questions about JetBrains PyCharm using the provided tools.\n\n   Always consult the 'pycharm_docs_search' tool to find relevant documentation before answering.\n\n   Cite sources by including the 'Source:' lines from the tool output when useful. If information isn't found, say you don't know.\"\"\"\n\n   agent = create_agent(\n\n       model=settings.openai_chat_model,\n\n       tools=[pycharm_docs_search],\n\n       system_prompt=system_prompt,\n\n       response_format=ToolStrategy(ResponseFormat),\n\n   )\n\n   result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": question}]})\n\n   print(result[\"structured_response\"].content)\n \nSystem prompts are provided to the LLM together with the user’s input prompt. We are using OpenAI as the LLM provider in this example, and we’ll need an API key from them. Head to this page to check out OpenAI’s integration documentation. When creating an agent, we’ll have to configure the settings for `llm`, `tools`, and `prompt`.\nFor the full scripts and project, see here.\nContent generation tools\nAnother example is an agent that generates text based on content fetched from other sources. For instance, we might use this when we want to generate marketing content with info taken from documentation. In this example, we’ll pretend we’re doing marketing for Python and creating a newsletter for the latest Python release.\nIn tools.py, a tool is set up to fetch the relevant information, parse it into a structured format, and extract the necessary information.\n@tool(\"fetch_python_whatsnew\", return_direct=False)\n\ndef fetch_python_whatsnew() -> str:\n\n   \"\"\"\n\n   Fetch the latest \"What's New in Python\" article and return a concise, cleaned\n\n   text payload including the URL and extracted section highlights.\n\n   The tool ignores the input argument.\n\n   \"\"\"\n\n   index_html = _fetch(BASE_URL)\n\n   latest = _find_latest_entry(index_html)\n\n   if not latest:\n\n       return \"Could not determine latest What's New entry from the index page.\"\n\n   article_html = _fetch(latest.url)\n\n   highlights = _extract_highlights(article_html)\n\n   return f\"URL: {latest.url}\\nVERSION: {latest.version}\\n\\n{highlights}\"\nAs for the agent in agent.py. \nSYSTEM_PROMPT = (\n\n   \"You are a senior Product Marketing Manager at the Python Software Foundation. \"\n\n   \"Task: Draft a clear, engaging release marketing newsletter for end users and developers, \"\n\n   \"highlighting the most compelling new features, performance improvements, and quality-of-life \"\n\n   \"changes in the latest Python release.\\n\\n\"\n\n   \"Process: Use the tool to fetch the latest 'What's New in Python' page. Read the highlights and craft \"\n\n   \"a concise newsletter with: (1) an attention-grabbing subject line, (2) a short intro paragraph, \"\n\n   \"(3) 4–8 bullet points of key features with user benefits, (4) short code snippets only if they add clarity, \"\n\n   \"(5) a 'How to upgrade' section, and (6) links to official docs/changelog. Keep it accurate and avoid speculation.\"\n\n)\n\n...\n\ndef run_newsletter() -> str:\n\n   load_dotenv()\n\n   agent = create_agent(\n\n       model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o\"),\n\n       tools=[fetch_python_whatsnew],\n\n       system_prompt=SYSTEM_PROMPT,\n\n       # response_format=ToolStrategy(ResponseFormat),\n\n   )\n\n...\nAs before, we provide a system prompt and the API key for OpenAI to the agent.\nFor the full scripts and project, see here.\nAdvanced LangChain concepts\nLangChain’s more advanced features can be extremely useful when you’re building a more sophisticated AI agent. Not all AI agents require these extra elements, but they are commonly used in production. Let’s look at some of them.\nMCP adapter\nThe MCP (Model Context Protocol) allows you to add extra tools or functionalities to an AI agent, making it increasingly popular among active AI agent users and AI enthusiasts alike. \nLangChain’s Client module provides a MultiServerMCPClient class that allows the AI agent to accept MCP server connections. For example:\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\n\nclient = MultiServerMCPClient(\n\n   {\n\n       \"postman-server\": {\n\n          \"type\": \"http\",\n\n          \"url\": \"https://mcp.eu.postman.com\",\n\n           \"headers\": {\n\n               \"Authorization\": \"Bearer ${input:postman-api-key}\"\n\n           }\n\n       }\n\n   }\n\n)\n\nall_tools = await client.get_tools()\nThe above connects to the Postman MCP server in the EU with an API key.\nGuardrails\nAs with many AI technologies, since the logic is not pre-determined, the behavior of an AI agent is non-deterministic. Guardrails are necessary for managing AI behavior and ensuring that it is policy-compliant.\nLangChain middleware can be used to set up specific guardrails. For example, you can use PII detection middleware to protect personal information or human-in-the-loop middleware for human verification. You can even create custom middleware for more specific guardrail policies. \nFor instance, you can use the `@before_agent` or `@after_agent` decorators to declare guardrails for the agent’s input or output. Below is an example of a code snippet that checks for banned keywords:\nfrom typing import Any\n\nfrom langchain.agents.middleware import before_agent\n\nbanned_keywords = [\"kill\", \"shoot\", \"genocide\", \"bomb\"]\n\n@before_agent(can_jump_to=[\"end\"])\n\ndef content_filter() -> dict[str, Any] | None:\n\n  \"\"\"Block requests containing banned keywords.\"\"\"\n\n  content = first_message.content.lower()\n\n# Check for banned keywords\n\n  for keyword in banned_keywords:\n\n      if keyword in content:\n\n          return {\n\n              \"messages\": [{\n\n                  \"role\": \"assistant\",\n\n                  \"content\": \"I cannot process your requests due to inappropriate content.\"\n\n              }],\n\n              \"jump_to\": \"end\"\n\n          }\n\n  return None\n\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n\n  model=\"gpt-4o\",\n\n  tools=[search_tool],\n\n  middleware=[content_filter],\n\n)\n\n# This request will be blocked\n\nresult = agent.invoke({\n\n  \"messages\": [{\"role\": \"user\", \"content\": \"How to make a bomb?\"}]\n\n})\nFor more details, check out the documentation here.\nTesting\nJust like in other software development cycles, testing needs to be performed before we can start rolling out AI agent products. LangChain provides testing tools for both unit tests and integration tests. \nUnit tests\nJust like in other applications, unit tests are used to test out each part of the AI agent and make sure it works individually. The most helpful tools used in unit tests are mock objects and mock responses, which help isolate the specific part of the application you’re testing. \nLangChain provides GenericFakeChatModel, which mimics response texts. A response iterator is set in the mock object, and when invoked, it returns the set of responses one by one. For example:\nfrom langchain_core.language_models.fake_chat_models import GenericFakeChatModel\n\ndef respond(msgs, **kwargs):\n\n   text = msgs[-1].content if msgs else \"\"\n\n   examples = {\"Hello\": \"Hi there!\", \"Ping\": \"Pong.\", \"Bye\": \"Goodbye!\"}\n\n   return examples.get(text, \"OK.\")\n\nmodel = GenericFakeChatModel(respond=respond)\n\nprint(model.invoke(\"Hello\").content)\nIntegration tests\nOnce we’re sure that all parts of the agent work individually, we have to test whether they work together. For an AI agent, this means testing the trajectory of its actions. To do so, LangChain provides another package: AgentEvals.\nAgentEvals provides two main evaluators to choose from:\nTrajectory match – A reference trajectory is required and will be compared to the trajectory of the result. For this comparison, you have 4 different models to choose from.\nLLM judge – An LLM judge can be used with or without a reference trajectory. An LLM judge evaluates whether the resulting trajectory is on the right path.\nLangChain support in PyCharm\nWith LangChain, you can develop an AI agent that suits your needs in no time. However, to be able to effectively use LangChain in your application, you need an effective debugger. In PyCharm, we have the AI Agents Debugger plugin, which allows you to power up your experience with LangChain.\nIf you don’t yet have PyCharm, you can download it here.\nUsing the AI Agents Debugger is very straightforward. Once you install the plug-in, it will appear as an icon on the right-hand side of the IDE.\n\n\n\n\nWhen you click on this icon, a side window will open with text saying that no extra code is needed – just run your agent and traces will be shown automatically.\nAs an example, we will run the content generation agent that we built above. If you need a custom run configuration, you will have to set it up now by following this guide on custom run configurations in PyCharm.\n\n\n\n\nOnce it is done, you can review all the input prompts and output responses at a glance. To inspect the LangGraph, click on the Graph button in the top-right corner.\n\n\n\n\nThe LangGraph view is especially useful if you have an agent that has complicated steps or a customized workflow.\nSumming up\nLangChain is a powerful tool for building AI agents that work for many use cases and scenarios. It’s built on LangGraph, which provides low-level orchestration and runtime customization, as well as compatibility with a vast variety of LLMs on the market. Together, LangChain and LangGraph set a new industry standard for developing AI agents.",
        "dc:creator": "Cheuk Ting Ho",
        "content": "If you’ve read the blog post How to Build Chatbots With LangChain, you may want to know more about LangChain. This blog post will dive deeper into what LangChain offers and guide you through a few more real-world use cases. And even if you haven’t read the first post, you might still find the info [&#8230;]",
        "contentSnippet": "If you’ve read the blog post How to Build Chatbots With LangChain, you may want to know more about LangChain. This blog post will dive deeper into what LangChain offers and guide you through a few more real-world use cases. And even if you haven’t read the first post, you might still find the info […]",
        "guid": "https://blog.jetbrains.com/?post_type=pycharm&p=681664",
        "categories": [
          "data-science",
          "tutorials",
          "ai",
          "ai-agents",
          "chatbots",
          "langchain"
        ],
        "isoDate": "2026-02-19T10:40:15.000Z"
      },
      {
        "creator": "Kodee",
        "title": "Kodee’s Kotlin Roundup: KotlinConf ’26 Updates, New Releases, and More",
        "link": "https://blog.jetbrains.com/kotlin/2026/02/kodees-kotlin-roundup-kotlinconf-26-updates-new-releases-and-more/",
        "pubDate": "Tue, 17 Feb 2026 21:28:42 +0000",
        "content:encodedSnippet": "KotlinConf 2026 is starting to take shape, and there’s a lot happening across the Kotlin ecosystem right now. From the first conference speakers and community awards to new releases, tools, and real-world Kotlin stories at serious scale, I’ve gathered all the highlights you won’t want to miss. Let’s dive in!\nKodee-Approved Spotlight\nThe full KotlinConf’26 schedule is ready!\nThe full KotlinConf’26 schedule is finally live on our website! Talks, workshops, amazing speakers – it’s all there. I’ve already started mapping out my agenda (and yes, there are some tough choices). Take a look and start shaping your own conference experience!\nExplore the schedule\n                                                    \nJoin the Kotlin Ecosystem Mentorship Program\nThe Kotlin Foundation is launching a new mentorship program to help newcomers make their first meaningful open-source contribution to Kotlin. Guided by experienced maintainers, mentees will go through the whole contribution process, from onboarding to getting a real change merged, with a limited number of mentor–mentee pairs, Kotlin-branded swag, and even a chance to win a trip to KotlinConf 2026.\nLearn more\n                                                    \nWhat’s New in Kotlin 2.3\nI took Kotlin 2.3 for a spin, and it brings smarter checks, cleaner property patterns, stabilized language features, and improved time and UUID APIs. Kotlin Multiplatform also gets faster builds, smaller binaries, and smoother interop across Native, Web, and JavaScript.\nSee what’s new\n                                                    \nKtor 3.4.0 is now available\nKtor 3.4.0 is out, bringing updates that make server-side Kotlin more flexible and robust. This release adds duplex streaming support, better control over the request lifecycle, and expanded compression options to help you handle modern workloads more efficiently.\nExplore Ktor 3.4.0\n                                                    \nCompose Hot Reload reaches 1.0.0\nCompose Hot Reload has reached version 1.0.0, marking a significant milestone for faster UI development. The post walks through the journey to stability and explains how Compose Hot Reload can speed up iteration when working with Compose by applying UI changes instantly, without restarting your app. Compose Hot Reload is bundled with Compose Multiplatform starting from version 1.10.\nSee how it works\n                                                    \nHow Amazon Fashion uses Kotlin for backend development\nI always like seeing Kotlin used at a serious scale, and this example definitely qualifies. In this talk, Katie Levy from Amazon Fashion shares how her team migrated a large backend service from Java to Kotlin, gaining cleaner, more testable code, faster delivery, and far fewer nullability issues.\nWatch how they did it\n                                                    \nLast chance to nominate yourself or a community member for the Golden Kodee Community Awards\nKotlinConf 2026 is also introducing something new, and I couldn’t be more excited about it: the Golden Kodee Community Awards. Nominations are open until February 22, inviting you to spotlight the people who make the Kotlin community thrive across creativity, education, and community building. Finalists will be announced in April, with winners revealed live at KotlinConf. Travel costs for all finalists are covered.\nNominate someone awesome\n                                                    \nHelp us understand how you use Exposed!\nIf you’re using the Exposed library in your Kotlin projects, I’d love to hear how it’s working for you. There’s a short survey, which should only take five minutes to complete, focused on real-world usage and where Exposed can be improved next. Your feedback helps guide future improvements – and as a small thank-you, you can enter a draw for a prize of your choice. If you have a moment, your input can make a real difference.\nTake the survey\n                                                    \nThe Ultimate Guide to successfully adopting Kotlin in a Java-dominated environment\nI know that adopting Kotlin in a Java-heavy codebase isn’t about flipping a switch or rewriting everything overnight. This Ultimate Guide lays out a practical, step-by-step path – from safe experiments in tests to production use and scaling Kotlin across a team or organization, with concrete migration patterns you can actually apply.\nRead the guide\n                                                    \nQodana for Android Kotlin\nQodana now supports Android projects written in Kotlin, bringing static analysis and code quality checks tailored for Android workflows. It helps teams catch issues early and keep Kotlin codebases healthy as they scale.\nCheck it out\n                                                    \nKoog x ACP: Connect an agent to your IDE and more\nKoog now integrates with the Agent Client Protocol (ACP), enabling AI agents to connect directly to JetBrains IDEs. This post explains how the integration works and what it enables for Kotlin-based AI tooling.\nRead more\n                                                    \nFrom certifed Kotlin trainers\nHow Backend Development Teams Use Kotlin in 2025\nHow Mobile Development Teams Use Kotlin in 2025\nWhere you can learn more\nIndustry Leaders on the KotlinConf’25 Stage: What Global Brands Built With Kotlin\nAdvent of Code 2025 in Kotlin: Puzzles, Prizes, and Community\nPick Your KotlinConf Workshop by What You Want to Learn\nExposed 1.0 Is Now Available\nKubernetes Made Simple: A Guide for JVM Developers\nA Better Way to Explore kotlinx-benchmark Results with Kotlin Notebooks\nYouTube highlights\nAPI Design at Google: Building Android Libraries\nTalking Kotlin #144 | Kotlin 2.3 Release Special (Audio Only)\nMaking Apps Accessible With Kotlin and Compose\nSell or Buy? Custom Financial Data Visualisation With Kotlin\nLanguage Design in the Age of AI\nWhy iOS Devs Struggle With KMP (and How to Fix It)\nA Better Way to Explore and Solve Programming Puzzles: Kotlin Notebooks\nWhy ING Chooses Kotlin for Server-Side",
        "dc:creator": "Kodee",
        "content": "KotlinConf 2026 is starting to take shape, and there’s a lot happening across the Kotlin ecosystem right now. From the first conference speakers and community awards to new releases, tools, and real-world Kotlin stories at serious scale, I’ve gathered all the highlights you won’t want to miss. Let’s dive in! From certifed Kotlin trainers Where [&#8230;]",
        "contentSnippet": "KotlinConf 2026 is starting to take shape, and there’s a lot happening across the Kotlin ecosystem right now. From the first conference speakers and community awards to new releases, tools, and real-world Kotlin stories at serious scale, I’ve gathered all the highlights you won’t want to miss. Let’s dive in! From certifed Kotlin trainers Where […]",
        "guid": "https://blog.jetbrains.com/?post_type=kotlin&p=681662",
        "categories": [
          "news",
          "kotlin-roundup"
        ],
        "isoDate": "2026-02-17T21:28:42.000Z"
      },
      {
        "creator": "Bulat Davletov",
        "title": "Editor Improvements: Smooth Caret Animation and New Selection Behavior",
        "link": "https://blog.jetbrains.com/platform/2026/02/editor-improvements-smooth-caret-animation-and-new-selection-behavior/",
        "pubDate": "Tue, 17 Feb 2026 13:30:56 +0000",
        "content:encodedSnippet": "We’re continuing to modernize our IDEs, and in this update we’ve refreshed something you interact with constantly – the editor. These changes are designed to provide improved comfort, a cleaner look, and a more enjoyable experience during the hours you spend coding. We want to make the editor easier on the eyes, help you stay productive, and add a bit of fun variety.\nWhat’s new\nNew selection behavior: Selection now only highlights the actual text, not the blank space at the end of a line.\nNew smooth caret movement: A new animation makes caret jumps easier to follow. This is a long-awaited request from our users.\nSmooth blinking and rounded caret: The editor now has a more modern look and feel, matching the recently introduced Islands UI theme.\nNew selection behavior\nWhen working with code, you often have to select multiple lines at once, and it is important to be able to see what is included in the selection. The editor now highlights only the characters you actually selected, rather than the entirety of each line. This reduces the amount of blank blue space, making the selection itself clearer. It also removes ambiguity, letting you see exactly which characters are selected.\n\n\n\n\nNew smooth caret movement\nThis one has been on our wishlist for a long time. Like us, you work in the text editor every day, constantly navigating and modifying code. One way we’re making this experience more enjoyable is by introducing smooth, animated caret movement. You might not realize you need this until you try it. It enhances comfort and helps you stay oriented in your work, making the editor feel both more functional and visually pleasing.\nAt the same time, we recognize that excessive caret animations may give the impression of a delay when you’re typing and feel slow and unresponsive. We believe that typing must feel instantaneous, and we wanted to make sure to get it right. It is a significant challenge to introduce animations without breaking habits.\nThat’s why we built our own mode for caret movement – Snappy – which you won’t find in other editors. In this mode, the caret lands almost immediately where it should be and then settles with a smooth stop. The result feels quick yet smooth.\n\n\n\n\nIf you prefer clearly visible motion, there’s also Gliding mode, where the caret moves smoothly, making jumps easy to follow with your eyes. This option is similar to what you see in other popular text editors.\n\n\n\n\nYou can switch between movement modes using the Use smooth caret movement dropdown in Settings | Editor | General | Appearance. You can revert to the old behavior simply by disabling it.\nSmooth blinking and rounded caret\nWe’ve also given the caret a new look. Its smooth blinking feels calmer and more modern. As a final touch, we rounded it to match rounded elements throughout the IDE in the Islands UI.\n\n\n\n\n\n\n\n\nHow to try it\nThese new features are all available in the latest early access builds of JetBrains IDEs, meaning there’s still time for us to adjust them based on your feedback. Give them a try and let us know what you think!\nTry them in the IntelliJ IDEA EAP today",
        "dc:creator": "Bulat Davletov",
        "content": "We’re continuing to modernize our IDEs, and in this update we’ve refreshed something you interact with constantly – the editor. These changes are designed to provide improved comfort, a cleaner look, and a more enjoyable experience during the hours you spend coding. We want to make the editor easier on the eyes, help you stay [&#8230;]",
        "contentSnippet": "We’re continuing to modernize our IDEs, and in this update we’ve refreshed something you interact with constantly – the editor. These changes are designed to provide improved comfort, a cleaner look, and a more enjoyable experience during the hours you spend coding. We want to make the editor easier on the eyes, help you stay […]",
        "guid": "https://blog.jetbrains.com/?post_type=platform&p=681585",
        "categories": [
          "intellij-platform",
          "news"
        ],
        "isoDate": "2026-02-17T13:30:56.000Z"
      },
      {
        "creator": "Tatiana Parshutkina",
        "title": "The Evolution of Async Rust: From Tokio to High-Level Applications",
        "link": "https://blog.jetbrains.com/rust/2026/02/17/the-evolution-of-async-rust-from-tokio-to-high-level-applications/",
        "pubDate": "Tue, 17 Feb 2026 13:06:27 +0000",
        "content:encodedSnippet": "Disclaimer: This article was created using AI-based writing and communication companions. With its help, the core topics of this rich and nuanced livestream were conveniently distilled into a compact blog post format.\nIn our yet another JetBrains livestream, Vitaly Bragilevsky was joined by Carl Lerche, the creator of Tokio, for an in-depth conversation about the evolution of async Rust. Tokio has become the de facto asynchronous runtime for high-performance networking in Rust, powering everything from backend services to databases. During the discussion, they explored how async Rust matured over the years, the architectural decisions behind Tokio, common challenges developers face today, and where the ecosystem is heading next. If you missed the live session, you can watch the full recording on JetBrains TV. Below, you’ll find a structured recap of the key questions and insights from the conversation.\n\n\n\n\n\n\nQ1. What is TokioConf and why did you decide to organize it?\nTokioConf is the first conference dedicated to the Tokio ecosystem, taking place in Portland, Oregon. This year marks ten years since Tokio was first announced, making it a natural moment to bring the community together. Use the code jetbrains10 for 10% off the general admission ticket (excluding any add-ons).\nBuy TokioConf ticket\n                                                    \nTokio and Rust have become foundational technologies for infrastructure-level networking software, including databases and proxies. The conference is meant to reflect that maturity and growth. While the name highlights Tokio, the scope includes broader async and networking topics in Rust.\n\n\n\n    \n“Tokio and Rust have become one of the default ways companies build infrastructure-level networking software these days.”\n\n            \nQ2. When people hear “Async Rust,” what should they picture?\nAsync Rust is about more than performance. While handling high concurrency is a key advantage, async programming also improves how developers structure event-driven systems.\nTimeouts, cancellation, and managing multiple in-flight tasks become significantly easier in async Rust compared to traditional threaded approaches. Async in Rust leverages the ownership model and Drop, enabling safe and clean cancellation patterns.\n“Async is both performance, but also a way of managing lots of in-flight threads of logic well.”\n\n            \nQ3. How did Tokio begin? Why did Rust need it?\nTokio evolved from earlier experimentation with non-blocking I/O in Rust. Initially, Rust only had blocking socket APIs, and building efficient network systems required low-level abstractions. The journey went from Mio (epoll bindings), to the Future trait, and to async/await. Async/await was a major milestone in making async programming ergonomic in Rust.\n“The way async/await ended up being designed is actually quite impressive.”\n\n            \nThe language team managed to deliver memory safety and zero-cost abstractions in a way that wasn’t obvious at the time.\nQ4. Could Rust have something like Java’s virtual threads?\nRust originally had green threads and coroutines before version 1.0, but they were removed to preserve zero-cost abstractions and C-level performance characteristics. The overhead and complexity of stack management for green threads conflicted with Rust’s design goals at the time.\n“Rust actually started with lightweight virtual threads and coroutines.”\n\n            \nWhether such a feature could return is an open question, but today’s Rust async model is fundamentally different.\nQ5. How does cancellation work in Async Rust?\nCancellation in Rust is implemented through Drop. When you drop a future, its cleanup logic runs automatically.\nIf the future directly owns a socket, it closes immediately. If the socket is owned by another task (for example in Hyper), cancellation signals cascade through channels and trigger cleanup.\nHowever, async functions can be dropped at any point, and developers must write defensively to handle that reality correctly.\nQ6. Why did Tokio become the dominant async runtime?\nTokio became the de facto standard largely due to ecosystem momentum. Early crates like Hyper built on Tokio, and once that foundation solidified, switching runtimes required compelling reasons.\nOther runtimes exist (especially for embedded or specialized contexts) but for general server-side development, Tokio’s ecosystem depth made it the default.\n“There just wasn’t a good reason to not use Tokio.”\n\n            \nQ7. What about io_uring? Is it the future?\nio_uring can provide benefits, especially for batching filesystem operations. However, for networking workloads, real-world gains are often limited. It is more complex than epoll and has historically had more security issues. That said, Tokio allows mixing in io_uring-specific crates when you have a clear use case.\n“I’ve not seen real performance benefits with swapping out io_uring for sockets under the hood in Tokio.”\n\n            \nQ8. What were the most important design decisions in Tokio?\nTokio intentionally avoided reinventing scheduling patterns. Instead, it adopted proven strategies from Go and Erlang, including work-stealing schedulers.\nThe philosophy was to provide:\nGood defaults,\nStrong performance,\nEscape hatches for advanced tuning.\n\n\n\n\n\nThe goal was to make Tokio easy enough for most developers while still enabling performance optimization when needed.\nQ9. What are common mistakes in Async Rust?\nThe biggest issue comes from cooperative scheduling. Tasks only yield at .await, so long CPU-heavy work without awaiting can stall the runtime. Tokio provides runtime metrics to help detect such problems. Understanding how the scheduler works is crucial to avoiding tail-latency problems.\n“Because async is cooperative scheduling, you have to make sure you’re yielding back to the runtime regularly enough.”\n\n            \nQ10. What’s the best way to debug Async Rust?\nDebugging async systems often involves:\nTracing,\nRuntime metrics,\nAsync backtraces,\nTraditional debuggers.\n\n\n\n\n\nStuck tasks and high tail latency remain the hardest issues to diagnose. Better static analysis and linting tools could significantly improve this area in the future.\n“The biggest pitfall stems down to developers accidentally canceling something and not handling the cancellation appropriately.”\n\n            \nQ11. What is Toasty, and why are you building it?\nRust has matured as a systems and infrastructure language, but higher-level web application tooling remains underdeveloped. Toasty aims to explore that space by building a productive, ergonomic data modeling and query layer. The goal is not just performance, but developer ergonomics – while still preserving escape hatches for advanced use cases.\nQ12. Can Rust move into high-level web frameworks?\nRust already has a foothold in many organizations thanks to its infrastructure strengths. As internal Rust ecosystems grow, the demand for higher-level tooling increases. The missing piece is ergonomic, opinionated frameworks that prioritize productivity. The long-term vision is not to replace existing ecosystems, but to expand Rust’s reach upward into full-stack development.\n“I do think there’s a way to build productive and ergonomic libraries with Rust that focus on ease of use.”\n\n            \n\n\n\n\nClosing Thoughts\nRust has firmly established itself as the best choice for many infrastructure-level systems. The next frontier is higher-level application development. Tokio solved async infrastructure and now the ecosystem is evolving toward productivity and full-stack capability.\nIf you’re interested:\nExplore Toasty on the Tokio GitHub\nJoin the Tokio Discord\nAttend TokioConf in Portland, Oregon\nWatch our previous lifestream with Herbert Wolverson and explore everything you wanted to ask about Rust",
        "dc:creator": "Tatiana Parshutkina",
        "content": "Disclaimer: This article was created using AI-based writing and communication companions. With its help, the core topics of this rich and nuanced livestream were conveniently distilled into a compact blog post format. In our yet another JetBrains livestream, Vitaly Bragilevsky was joined by Carl Lerche, the creator of Tokio, for an in-depth conversation about the [&#8230;]",
        "contentSnippet": "Disclaimer: This article was created using AI-based writing and communication companions. With its help, the core topics of this rich and nuanced livestream were conveniently distilled into a compact blog post format. In our yet another JetBrains livestream, Vitaly Bragilevsky was joined by Carl Lerche, the creator of Tokio, for an in-depth conversation about the […]",
        "guid": "https://blog.jetbrains.com/?post_type=rust&p=681725",
        "categories": [
          "rustrover",
          "async-rust",
          "rust",
          "tokio"
        ],
        "isoDate": "2026-02-17T13:06:27.000Z"
      },
      {
        "creator": "Claire Amaouche",
        "title": "Databao Becomes a Partner of the Open Semantic Interchange Initiative led by Snowflake and other industry leaders",
        "link": "https://blog.jetbrains.com/databao/2026/02/databao-becomes-osi-partner/",
        "pubDate": "Tue, 17 Feb 2026 11:00:00 +0000",
        "content:encodedSnippet": "Modern data teams need flexibility and scalability as workflows evolve and AI becomes central to analytics. They are increasingly relying on AI to enable self-service analytics and accelerate data workflows, and it has become essential to establish shared business logic and context that both humans and AI systems can reliably understand and use. That shared context is the semantic layer.\nThat’s why we are thrilled to announce that Databao, a recent JetBrains product bringing semantic context and local data agents to data teams, is joining the Open Semantic Interchange (OSI), the open-source initiative led by Snowflake and other industry leaders to advance interoperable and governed semantic models.\nWhy open semantics matter\nThe Open Semantic Interchange (OSI) is an open-source initiative led by Snowflake and a broad ecosystem of partners across multiple domains and industries including BI, data engineering, governance, and AI. Its goal is to define a shared, vendor-neutral standard for semantic metadata, enabling it to move seamlessly across tools and platforms.\nBy making semantic context portable and interoperable, OSI reduces complexity, accelerates the adoption of AI and analytics tools, and helps organizations align on consistent data definitions, laying the foundation for more reliable insights and scalable AI innovation.\nWhat Databao brings to data teams\nOSI reflects the principles that have guided Databao from the start.\nFirst, trust at scale depends on semantics. Databao treats the semantic layer as living context, shared, governed, and continuously refined, so business logic remains clear and reliable as data usage grows.\nSecond, adoption requires flexibility. Teams shouldn’t have to choose between strong governance and usability. Databao already works with existing tools and logic. Teams stay in control of where their source of truth lives and how it’s maintained.\nBy joining OSI, Databao reinforces its commitment to an open, community-driven approach to sharing semantic models, so business definitions remain consistent and usable across every team’s workflows.\nAbout Databao\nDatabao is a new data product from JetBrains that helps data teams create and maintain a shared semantic context and build their own data agents on top of it. Our goal is to provide an AI-native analytics experience that business users can trust, enabling them to query and analyze data in plain language.\nDatabao’s modular components, the context engine and data agent can run independently, either locally or within your existing infrastructure using your own API keys.\nWe are also inviting data teams to build a proof of concept with us: we’ll explore your use case, define a context-building process, and grant agent access to a selected group of business users. Together, we will then evaluate the quality of responses and the overall value.\n      \n      TALK TO THE TEAM\n    \n\n\n\n\nLearn more about the Open Semantic Interchange on Snowflake’s blog, or explore both modules at databao.app .",
        "dc:creator": "Claire Amaouche",
        "content": "Modern data teams need flexibility and scalability as workflows evolve and AI becomes central to analytics. They are increasingly relying on AI to enable self-service analytics and accelerate data workflows, and it has become essential to establish shared business logic and context that both humans and AI systems can reliably understand and use. That shared [&#8230;]",
        "contentSnippet": "Modern data teams need flexibility and scalability as workflows evolve and AI becomes central to analytics. They are increasingly relying on AI to enable self-service analytics and accelerate data workflows, and it has become essential to establish shared business logic and context that both humans and AI systems can reliably understand and use. That shared […]",
        "guid": "https://blog.jetbrains.com/?post_type=databao&p=677516",
        "isoDate": "2026-02-17T11:00:00.000Z"
      },
      {
        "creator": "Conrad Schwellnus",
        "title": "The Most Popular AI Tools: What Developers Use and Why",
        "link": "https://blog.jetbrains.com/ai/2026/02/the-most-popular-ai-tools-what-developers-use-and-why/",
        "pubDate": "Tue, 17 Feb 2026 06:26:28 +0000",
        "content:encodedSnippet": "AI tools have become a core part of modern software development. Developers rely on them throughout the life cycle, from writing and refactoring code to testing, documentation, and analysis. \nOnce experimental add-ons, these tools now function as everyday assistants and are firmly embedded in routine workflows. But why have AI tools become so essential – and how are developers actually using them?\nThe insights in this article draw on findings from the JetBrains State of Developer Ecosystem Report 2025, which tracks how developers use tools, languages, and technologies, including AI tools, in real-world environments. Shifting the focus from technical model performance, this article looks at usage patterns, developer preferences, and adoption trends across tools, regions, and workflows.\nBefore we work through which AI tools developers use most, why they choose them, and how these tools fit into everyday work, let’s first clarify what AI tools are and why they matter so much right now.\nDisclaimer: Please note that the findings in the article reflect data collected during the specific research period set out in the report.\n\n\n\n\nTable of Contents\n·       What AI tools are and why they matter now\n·       Most popular AI tools among developers\n·       What makes developers choose one AI tool over another\n·       How developers use AI tools in daily workflows\n·       Global snapshot: How AI tool adoption differs across regions\n·       Barriers to adopting AI tools\n·       Future of AI tools: What developers want next\n·       FAQ\n·       Conclusion\nWhat AI tools are and why they matter now\nToday’s AI tools for developers span several categories. They include code assistants that suggest or generate code, as well as tools that review code autonomously. Many come as IDE integrations that understand project context.\nThere are also AI-powered search and navigation tools, refactoring helpers, and documentation generators. In addition, teams now use testing assistants and autonomous or semi-autonomous agents to support more complex workflows.\nUnderstanding today’s AI tools list for developers matters because these tools directly address growing pressures in modern development. They shorten development cycles, reduce manual tasks, and help teams maintain quality, which is especially important as codebases grow.\nThis growing reliance makes it important to understand which tools developers actually use most. In the next section, we will see what these AI tools are.\nMost popular AI tools among developers\nDevelopers rarely rely on a single AI tool. Instead, they combine multiple tools depending on their IDE, workflow style, and project requirements. According to the AI usage insights in the the JetBrains State of Developer Ecosystem Report 2025, adoption clusters around three main categories: IDE-native assistants, standalone AI-powered development environments, and browser-based or cloud chat tools.\nAcross these categories, the most popular AI assistants are GitHub Copilot, JetBrains AI Assistant, Cursor, Windsurf, and Tabnine. Adoption of these top AI tools varies based on ecosystem, IDE choice, and workflow style.\nIDE-native assistants, such as GitHub Copilot and JetBrains AI Assistant, remain among the most popular AI tools because they operate inside the editor and integrate directly into existing workflows, making them more context-aware.\nStandalone AI-focused editors and assistants, such as Cursor and Windsurf, often emphasise more experimental or agent-style workflows. This is an area that is evolving across the ecosystem, with increasing convergence between IDE-native tools and more agent-driven capabilities.\nOther tools focus on specific priorities. For example, Tabnine attracts teams that prioritize privacy and local inference. Region-specific tools also play an important role in areas with strong domestic AI ecosystems or regulatory constraints.\nThis diversity becomes clearer when comparing the best AI tools for developers side by side.\nComparison table: AI tools overview\n\nAI toolTypical use caseUnderlying modelsDistinct featuresIntegration type\nGitHub CopilotCode generation and completionGPT familyTight GitHub + VS Code workflowsIDE / Cloud\nJetBrains AI AssistantContext-aware help, refactoringClaude / GPT / GeminiDeep IDE context + privacy focusIn-IDE\nCursorInline edits, debugging, chatClaude / GeminiFast UI, multi-step editsIDE plugin\nWindsurfAutonomous task execution and code changesClaude / GPTAgent-like capabilitiesStandalone\nTabninePrivacy-oriented code suggestionsProprietary / DeepSeekLocal inference optionsIDE plugin\n\nDisclaimer: Please note that the findings reflect data collected during the specific research period set out in the report.\n\n\n\nWhat makes developers choose one AI tool over another\nDevelopers are not choosing AI tools solely on novelty. They evaluate how well a tool fits existing workflows, how reliable the output feels, and whether the tool aligns with team constraints. The JetBrains State of Developer Ecosystem Report 2025 identifies several of these practical considerations that shape decision-making.\nIntegration quality ranks among the most important factors. Developers prefer AI coding tools that work seamlessly inside their preferred IDE. A tool that interrupts flow or requires constant context switching often fails to gain long-term adoption.\nAccuracy and code quality are equally crucial. Developers expect AI coding tools to produce reliable results that they can trust. When outputs require extensive correction, confidence drops quickly.\nPrivacy and data security also influence developer AI preferences. This is especially true in enterprise environments. Tools that offer local processing or clear privacy guarantees often see stronger uptake in regulated industries.\nFinally, pricing, transparency, and vendor reputation affect adoption. Developers value clear pricing models, flexible access, and vendors with a track record of supporting developer tools. Trust builds over time through consistency and ongoing communication.\nLet’s see how developers evaluate each of these factors in this AI assistant comparison.\nKey factors influencing tool choice\n\nFactorWhy it mattersHow developers evaluate it\nIDE integrationSupports smooth workflowsWorks natively in their preferred IDE\nCode accuracy and qualityAffects trust and usabilityProduces correct, clear, and maintainable code\nPrivacy and securityProtects source code and IPProvides clear data handling and local mode options\nPricing and accessImpacts adoption at scaleOffers flexible tiers and predictable costs\nTransparencyBuilds confidenceDiscloses model provider and data policies\nVendor reputationSignals long-term reliabilityDemonstrates a history of dev tools and quality support\n\nDisclaimer: please note that the findings reflect data collected during the specific research period set out in the report.\n\n\n\nHow developers use AI tools in daily workflows\nDevelopers integrate AI tool usage throughout the development life cycle rather than limiting it to a single task. Most workflows combine several forms of AI access depending on the problem at hand.\nWhen coding with AI tools, developers may use in-IDE assistants for context-aware code help and chat-based interfaces for problem-solving and prototyping. In addition, developer AI assistant usage may combine browser tools for quick inline answers, APIs for automation and CI/CD tasks, and local models for privacy-restricted environments.\nAcross these use cases, developers are clearly no longer relying on a single tool. AI workflows increasingly involve choosing the right tool for the task at hand, be it writing code, refactoring, debugging, generating documentation, testing, or understanding unfamiliar code.\nThe JetBrains State of Developer Ecosystem Report 2025 indicates that developers frequently switch between AI access points in this way. They choose the interface that best fits the task rather than expecting one tool to handle everything.\n\nWorkflow types and examples\n\nWorkflow typeTypical use caseExample toolsIntegration contextDeveloper benefit\nIn-IDE assistanceCode suggestions, refactoringJetBrains AI Assistant, GitHub CopilotIDEImmediate, context-aware help\nChat-based interactionExplanations, brainstorming, regex, prototypingChatGPT, ClaudeBrowser / CloudFast iteration and reasoning\nAPI integrationAutomation, CI tasks, documentationOpenAI API, Anthropic APIBackend / DevOpsScalable automation\nBrowser extensionsQuick inline code insightsCodeium, AIXWebLightweight access\nLocal/private modelsSecure, offline codingTabnine, DeepSeek (self-hosted models)On-premises / EnterpriseHigh privacy and control\n\nDisclaimer: please note that the findings reflect data collected during the specific research period set out in the report.\n\n\n\nWith AI firmly established in daily workflows, the next section looks at regional differences in AI tool adoption.\nGlobal snapshot: How AI tool adoption differs across regions\nGlobal AI adoption patterns do not look the same everywhere. Regional ecosystems, regulations, and developer communities shape which tools gain traction. The JetBrains State of Developer Ecosystem Report 2025 highlights clear regional AI trends.\nIn North America, developers commonly adopt mainstream tools such as GitHub Copilot, JetBrains AI Assistant, and Claude-based assistants. Strong cloud infrastructure and rapid LLM innovation encourage experimentation with multiple tools.\nEuropean developers balance adoption with privacy considerations. Data residency and compliance requirements influence tool selection, leading to broader interest in solutions that offer transparency and local processing options.\nIn the Asia-Pacific region, developers often combine global tools with regional offerings. Mobile-first development cultures and fast-growing ecosystems drive rapid experimentation, particularly with cloud-based assistants.\nMainland China stands out due to its strong domestic AI ecosystem. Developers there frequently rely on local tools and models such as DeepSeek, Qwen, and Hunyuan, which align better with infrastructure and regulatory realities.\n\nRegional highlights and local leaders\n\nRegionMost used toolsLocal ecosystem driversNotable observations\nNorth AmericaGitHub Copilot, JetBrains AI Assistant, ClaudeStrong cloud and LLM innovationHigh multi-tool adoption\nEuropeJetBrains AI Assistant, GitHub CopilotPrivacy regulations, data residencyBalanced adoption across tools\nAsia-PacificGitHub Copilot, GeminiMobile/cloud-first development culturesRapid experimentation and growth\nMainland ChinaDeepSeek, Qwen, HunyuanStrong domestic AI ecosystemPreference for locally hosted models\n\nDisclaimer: please note that the findings reflect data collected during the specific research period set out in the report.\n\n\n\n\nWhile AI tool usage worldwide is undoubtedly gaining momentum, barriers to AI adoption also exist, which we explore in the next section.\nBarriers to adopting AI tools\nDespite growing interest, not all developers or teams adopt AI tools easily. The JetBrains State of Developer Ecosystem Report 2025 shows that such AI adoption challenges often stem from uncertainty rather than opposition.\nPrivacy and security concerns remain the most common AI coding tool barriers. Teams worry about exposing sensitive code or intellectual property, especially when tools rely on cloud processing. Without clear guarantees, organizations may restrict or ban usage.\nLegal and ownership questions are other reasons why developers avoid AI tools. Developers and managers want clarity about who owns AI-generated code and how licensing applies. Uncertainty leads many teams to limit AI use to non-critical tasks.\nIndividual barriers matter as well. Some developers lack confidence in using AI tools effectively or struggle to evaluate output quality. Others distrust AI suggestions due to past inaccuracies.\nCost, licensing, and infrastructure constraints can also limit adoption, particularly for larger teams. Per-seat pricing and usage caps further complicate budgeting and rollout decisions.\n\nObstacles and evaluation criteria\n\nBarrierWhy it mattersTypical impact\nPrivacy and security concernsIncreases the risk of exposing sensitive codeUsage blocked or restricted\nIP and code ownership concernsCreates legal uncertaintyHesitation to rely on AI for core code\nLack of knowledge or trainingReduces confidence in using toolsSlower individual adoption\nAccuracy and reliability issuesImpacts trust in outputsMore manual review required\nInternal policies and processesRequires compliance and complex approval workflowsDelayed tool rollout\nCost and licensingExceeds budget or per-seat limitsPartial or limited deployment\n\nDisclaimer: please note that the findings reflect data collected during the specific research period set out in the report.\n\n\n\n\nIn the next section, we move from the barriers of today to developers’ hopes for the future.\nFuture of AI tools: What developers want next\nDevelopers do not simply want more AI features. They want better ones. The JetBrains State of Developer Ecosystem Report 2025 not only indicates greater adoption but also shows that developers are hopeful about the future of AI. Their expectations focus on reliability, integration depth, and control rather than novelty.\nHigher code quality tops developer AI expectations. Developers want fewer hallucinations, cleaner outputs, and suggestions that respect project conventions. Trust grows when AI behaves predictably.\nDeeper IDE integration also ranks high. Developers expect future AI tools to understand entire projects, not just individual files. Context retention across sessions and multi-file awareness are increasingly important.\nPrivacy remains central. Many developers want local or on-device options that allow them to use AI without sharing code externally. Transparent data handling builds confidence.\nPricing clarity and explainability also influence future AI assistant trends. Developers want predictable costs and better insight into why tools suggest certain changes.\nBut most significantly, as AI tools evolve, developers want support for complex workflows and architecture reasoning. The goalpost is also shifting. Developers now expect future AI tools to move beyond basic autocomplete and act as collaborative partners.\n\nDeveloper expectations and trends\n\nExpectationWhy developers want itExample improvements\nHigher code qualityTrust and reliabilityFewer hallucinations, cleaner output\nDeeper IDE integrationSeamless workflowsContext retention, multi-file awareness\nPrivacy and controlSecure code handlingOn-device or local LLM options\nTransparent pricingPredictable team adoptionUsage-based models, clearer tiers\nExplainability and reasoningTrust in decisionsClearer chain-of-thought summaries\nContext awarenessHandling real projectsLarger context windows, project-wide understanding\n\nDisclaimer: please note that the findings reflect data collected during the specific research period set out in the report.\n\n\n\n\nThe following FAQ addresses some of the most common questions developers ask when evaluating and using AI tools.\nFAQ\nWhat are the most popular AI tools among developers today?\nAccording to the report’s findings, developers commonly use tools such as GitHub Copilot, JetBrains AI Assistant, Cursor, and Tabnine, often combining them rather than using a single tool.\nAre AI tools safe for use with private or proprietary code?\nSafety depends on the tool. Developers increasingly prefer tools that provide clear privacy policies or local processing options.\nWhich AI tools work best inside IDEs?\nIDE-native tools tend to perform best for daily coding tasks because they understand project context and workflows.\nDo developers prefer local AI models or cloud-based solutions?\nPreferences vary. Some developers value cloud flexibility, while others prioritize local models for privacy and compliance.\nHow do AI tools help with debugging and documentation?\nThey explain code, identify errors, suggest fixes, and generate comments or documentation drafts.\nAre AI tools suitable for enterprise teams with strict security requirements?\nMany are, especially when they offer strong privacy guarantees, administrative controls, and predictable pricing.\nCan AI tools speed up development without reducing code quality?\nYes, when developers use them intentionally. AI tools speed up repetitive tasks such as code generation, refactoring, testing, and documentation, while reviews, IDE checks, and automated tests help maintain quality.\nConclusion\nAI tools have evolved from optional add-ons into essential components of modern software development. Developers now rely on them for coding, refactoring, documentation, testing, and learning, integrating AI assistance throughout daily workflows.\nCurrent adoption trends show that developers value accuracy, deep integration, and privacy above experimental features. The JetBrains State of Developer Ecosystem Report 2025 reflects broad and growing use across regions, tools, and development styles.\nAs AI tools continue to evolve, they move toward deeper context awareness, stronger reasoning, and more secure deployment options. \nFor developers, AI no longer represents a future possibility. It has become a practical, everyday partner in building software.",
        "dc:creator": "Conrad Schwellnus",
        "content": "AI tools have become a core part of modern software development. Developers rely on them throughout the life cycle, from writing and refactoring code to testing, documentation, and analysis. Once experimental add-ons, these tools now function as everyday assistants and are firmly embedded in routine workflows. But why have AI tools become so essential – [&#8230;]",
        "contentSnippet": "AI tools have become a core part of modern software development. Developers rely on them throughout the life cycle, from writing and refactoring code to testing, documentation, and analysis. Once experimental add-ons, these tools now function as everyday assistants and are firmly embedded in routine workflows. But why have AI tools become so essential – […]",
        "guid": "https://blog.jetbrains.com/?post_type=ai&p=678908",
        "categories": [
          "ai",
          "deveco",
          "research",
          "data-analysis",
          "jetbrains-research"
        ],
        "isoDate": "2026-02-17T06:26:28.000Z"
      },
      {
        "creator": "Dmitrii Korovin",
        "title": "Say Goodbye to “It Works on My Machine”: A Look at TeamCity’s Pretested Commits",
        "link": "https://blog.jetbrains.com/teamcity/2026/02/say-goodbye-to-works-on-my-machine/",
        "pubDate": "Fri, 13 Feb 2026 12:45:10 +0000",
        "content:encodedSnippet": "This article was brought to you by Adeyinka Adegbenro, draft.dev.\nDevelopers know the frustration well. Code works perfectly on their laptop, then breaks the moment it hits staging or production. It’s easy to overlook slight environmental differences that hide latent bugs like race conditions, platform-specific library loading failures, data inconsistencies (large data sets can expose hidden edge cases like timeouts), network variations, flaky tests, and memory issues.\nEven with careful testing, some issues only surface in shared environments. You can’t catch every edge case with local checks alone, and sometimes, pushing unverified changes is unavoidable. That’s why it’s important to catch test failures before code reaches the shared repository. It prevents integration issues and ensures only green commits make it to the shared branch.\nIn this article, you’ll learn how TeamCity’s pretested commits feature stops broken code from reaching your repository. We’ll explain what pretested (gated) commits are and walk through TeamCity’s workflow using remote runs and IDE integration.\nThe problem: Broken builds from unverified commits\nIn software development, unverified commits are common. They speed up individual workflows yet also increase the risk of failed builds.\nTypically, developers run local tests, commit their changes, and push to a shared repository before peer review or validation by the continuous integration server. If there’s an error (especially one caused by differences between local and production environments), it can disrupt the entire team’s workflow.\nTake database connections. Locally, you might connect one service to your DB and stay well within the database’s max connection limits. But in production, several worker processes connect to the same database, quickly hitting the maximum number of connection limits and triggering timeouts.\nWhen these differences go unnoticed, the result is often a cascading chain of failures. Anyone who pulled that branch as their base now has bad code. Other developers who depend on that branch might also have to spend time debugging the code, especially if the original developer who introduced the bug is not available. This is a massive waste of time and resources, and it could have been avoided by enforcing a pretested commit workflow that uses the validation of a powerful CI server like TeamCity.\nOver time, if the main branch is frequently broken, developers become hesitant to pull their latest changes for fear that they could be unstable. This loss of confidence can lead to developers working in isolation and eventually result in multiple merge conflicts, which defeats the whole purpose of using version control as a tool for collaborative development.\nWhat are pretested commits in TeamCity?\nTeamCity’s pretested commits, also known as gated or delayed commits, reverse the build-after-commit workflow. Instead of the typical edit, commit, push, and build flow where you hope that the build passes on the continuous integration platform’s server, it’s flipped. You edit the code and then build the change on the TeamCity servers before committing it.\nThe CI build includes code compilation, tests, linting, and any other predetermined checks defined in your build configuration. If that build fails, the code is not committed, and the developer can fix the issue without affecting the entire team’s process. But if the code build passes the tests, TeamCity or the developer can automatically commit the changes to version control.\n\n\n\n\nThe pretested commit workflow\nThe pretested commit workflow guarantees code quality by running a full build and test cycle before changes hit the main branch. The implementation varies significantly depending on the type of version control system (VCS) being used.\nFor distributed systems like Git, pretested commits are built around feature branches, so there’s no need to apply patches directly to the main branch. This keeps parallel development safe through local, isolated testing and committing. TeamCity can test against a temporary patch of staged changes you made locally, but stops short of performing the final automatic commit to avoid race conditions. Instead, it uses dedicated validation branches through what is known as the branch remote run.\nThe workflow described below is built around Git.\nCreate a project\nOnce you have a TeamCity instance, you need to create a project either manually or by entering the repository URL (e.g., git@github.com:your-org/your-repo.git) of an existing project. If you select the repository option, you’ll be prompted to log in to the version control host (e.g. GitHub, GitLab, or Bitbucket), and you’ll need to provide the necessary authentication credentials:\n\n\n\n\nConfigure the project\nNext, you’ll need to enter some preliminary settings: the build name, the default branch name, and the branch specifications. For most Git projects, the default branch is either refs/heads/main or refs/heads/master.\nIn the branch specification option, make sure you enter at least one branch, with each branch on a new line. This tells TeamCity which branches to monitor for changes. Here’s a sample branch specification:\n// Default branch\n\nrefs/heads/main\n\n// Regular feature branches\n\nrefs/heads/feature/*\n\n\n\n\nClick Proceed to continue to the build step.\nAdd your build steps\nAfter clicking Proceed, you have to add build steps by clicking the Build Steps tab on your build page. The build steps define the actual sequence of commands required to validate the code. These steps run regardless of the branch type (main or feature/*). A minimal command-line build configuration for a hypothetical Node.js project might look like this:\nset -e\n\nnpm install # Download and install required packages for the project.\n\nnpm run build # Compile source code (e.g., TypeScript, Babel, webpack) into deployable artifacts.\n\nnpm run test # Execute all unit and integration tests. The build will fail if any test fails.\n\nnpm run lint # run linting checks for files\n\n\n\n\nDon’t forget to save your changes and run the build to make sure it works on the default branch.\nPretest validation\nAfter you’ve added build steps, developers need to work on their isolated feature branch locally, making and committing changes frequently (e.g. a branch named feature/login-flow). To initiate the pretest, the developer pushes their local feature branch to the remote repository remote-run/ prefix. TeamCity automatically watches any branches with the prefix remote-run and will run an automatic build once code is pushed there.\n# Pushes feature/login-flow to the remote as remote-run/login-flow\n\ngit push origin feature/login-flow:remote-run/login-flow\n\n\n\n\nIntegration\nOnce the remote-run/login-flow build completes, the status dictates the next step. If it fails, the developer reviews the build log, fixes the issues locally on their feature branch, and repeats the push to the temporary remote-run/login-flow branch.\n\n\n\n\nIf the build is successful, the developer deletes the temporary remote branch. The feature branch (feature/login-flow) is now proven stable and is ready for the final action:\n\n\n\n\nThe developer can now commit and merge with the main branch or create a pull request from their pretested feature branch.\nIn centralized version control systems like SVN or Perforce, TeamCity’s remote run feature allows developers to validate uncommitted local changes using a patch (a bundle of uncommitted changes). A developer uses an IDE like IntelliJ IDEA and the TeamCity plugin to send a patch to the build server, then TeamCity builds and tests the patch. If that’s successful, TeamCity automatically commits the changes to the main repository, completing the pretested commit.\n\n\n\n\nThe benefits of using pretested commits\nPretested commits shift the verification from the developer’s machine to the team’s agreed CI environments. Code only gets added to the main branch after passing the specified checks, so failed builds never disrupt other people’s work.\nThis keeps integration clean and catches regressions early. Everyone gets a stable base to branch from. You know the latest version actually works, and you won’t have to spend hours chasing errors introduced by someone else’s build.\nIt also cuts down on frustration. When teams aren’t wasting time fixing someone else’s mistakes, they can focus on their own features. And because you get immediate feedback during pretesting, you catch your own issues before they become someone else’s problem.\nThese benefits add up. Your commit history stays focused on real progress instead of getting cluttered with commit messages like “fixed typo”, “fixed linting issue”, “added missing dependency that caused build failure”, or “added type checks”. Reviewers can focus on meaningful code changes instead of other noise. Your project history tells the story of how the code evolved, not how often it broke.\nUltimately, pretested commits support continuous delivery goals, especially for agile teams that ship frequently and rely on stable releases. Teams can rest easy knowing that their production code has gone through automated, enforced checks.\nVCS and configuration considerations\nTo get pretested commits running smoothly in TeamCity, there are a few version control and configuration details you should pay attention to:\nExtensive VCS integrations: TeamCity supports all major version control platforms. Centralized systems such as Subversion, Perforce, and TFVC can use remote run in the IDE, while distributed systems like Git (GitHub, GitLab, Bitbucket, or Azure DevOps) and Mercurial use the branch remote run.\nIDE plugin setup: Using the pretested commit feature within an IDE (remote run) depends on the installation of the TeamCity IDE plugin. The plugin lets you select local, uncommitted changes and send them directly to the TeamCity server for verification.\nBranch specifications: Your build configurations in the TeamCity UI need proper branch specifications (e.g. +:refs/heads/*) so that TeamCity knows which branches to monitor and test automatically.\nParameters and secrets: Define all build parameters (especially secure secrets) at the project or build configuration level in the TeamCity UI. TeamCity will securely insert them during the personal build. This separation ensures the code remains clean of sensitive configuration details. Parameter settings can be found in the project and build settings, after enabling the Settings mode in the upper-right corner of your dashboard.\nMatching repository URLs: If you’re using remote run in the IDE, make sure the repository URL configured in IntelliJ IDEA (or your IDE) matches exactly what is defined in the TeamCity server site. Even small differences (e.g. https://github.com/acct/repo.git vs. https://github.com/acct/repo) can prevent TeamCity from recognizing the patch as belonging to the right VCS root.\nBuild triggers: Triggers let you control when your builds run under the specific circumstances/events that you have configured in the settings. For example, you can skip triggering a build if a certain user commits changes or if a phrase is present in a commit message. Configure this in the Triggers tab of your build settings.\nBuild configuration: Make sure you match your build configuration as close as possible to your branch/commit workflow for consistency. This helps make sure that the logic used to test a developer’s changes is similar to that used for the final merge made to the main branch. For example, if your main branch runs database migrations, your personal build should include the same setup.\nWhen to use pretested commits vs. alternatives\nPretested commits are powerful, but they’re not always the right tool for every project. You need to consider project size, branch stability, and how long your tests take to run before incorporating them in your workflow.\nPretested commits work best for teams with a single stable branch where stability is important. They’re also a good fit when you have solid automated tests, and you’re pushing toward continuous integration and delivery.\nIf your test suites and checks are large, take a long time (i.e. 15 minutes), take up memory, or use production-grade data, running pretested commits remotely frees up developers’ machines and keeps them productive.\nBut if your team relies heavily on feature branches and long-lived branch workflows, pull requests and merge gates may be a better fit. And if your test suite is incomplete or flaky, pretested commits won’t help much; they’re only as good as the tests backing them up.\nCode reviews and staging environments, used along with pretested commits, may be helpful for exploratory testing of the kinks that the flaky test suites cannot cover. Manual commits with quick feedback may be simpler for small teams, solo developers, or teams with tiny codebases.\nIt’s not always a question of choosing one or the other. Pretested commits can be layered on top of existing workflows. For example, a feature branch might have multiple developers contributing. Each developer uses pretested commits to ensure that only passing commits reach the shared feature branch. Once the feature is complete, the team still opens a PR to merge into main (or master). At that stage, the PR process provides an additional layer of code review and CI checks before the final merge.\nConclusion\nPretested commits give teams a way to guarantee that only tested, working code enters the main branch. This shifts the responsibility of integration checks onto the CI server, allowing developers to focus on writing features and trust that the system enforces quality.\nWhile this workflow isn’t the best fit for every team, it can be transformative for environments where stability and continuous delivery are priorities. Keep in mind that a pretested commit workflow is only as good as its tests and checks. If your tests are unreliable, errors can slip through the cracks and cause problems.\nJetBrains TeamCity gives teams everything they need to automatically enforce quality checks, from IDE plugins that let you trigger remote runs directly to flexible branch remote runs. If you’re currently using Jenkins and want to explore how to switch to TeamCity, check out our migration planning kit. For a deeper dive into platform capabilities, JetBrains also has detailed resources you can explore.",
        "dc:creator": "Dmitrii Korovin",
        "content": "This article was brought to you by Adeyinka Adegbenro, draft.dev. Developers know the frustration well. Code works perfectly on their laptop, then breaks the moment it hits staging or production. It&#8217;s easy to overlook slight environmental differences that hide latent bugs like race conditions, platform-specific library loading failures, data inconsistencies (large data sets can expose [&#8230;]",
        "contentSnippet": "This article was brought to you by Adeyinka Adegbenro, draft.dev. Developers know the frustration well. Code works perfectly on their laptop, then breaks the moment it hits staging or production. It’s easy to overlook slight environmental differences that hide latent bugs like race conditions, platform-specific library loading failures, data inconsistencies (large data sets can expose […]",
        "guid": "https://blog.jetbrains.com/?post_type=teamcity&p=680592",
        "categories": [
          "best-practices",
          "teamcity-2",
          "devopspains",
          "jenkins"
        ],
        "isoDate": "2026-02-13T12:45:10.000Z"
      },
      {
        "creator": "Artem Pronichev",
        "title": "Moving Your Codebase to Go 1.26 With GoLand Syntax Updates",
        "link": "https://blog.jetbrains.com/go/2026/02/13/moving-your-codebase-to-go-1-26-with-goland-syntax-updates/",
        "pubDate": "Fri, 13 Feb 2026 12:33:36 +0000",
        "content:encodedSnippet": "Working on an existing Go project rarely starts with a plan to modernize it. More often, you open a file to make a small change, add a field, or adjust some logic. The code compiles, tests pass, and everything looks fine, but the language has moved forward, and your code hasn’t kept up.\nAs you bump your project’s Go version, you may start noticing small patterns from older code that stay around for years. A helper variable here. A brittle error check there. You fix them by hand, but as the work spreads across files, you lose context fast.\nIn GoLand, Go 1.26 syntax updates now show up as focused inspections with quick-fixes. You see the change where you are already working, and you can apply the same change throughout the project when you are ready.\nDownload GoLand\n                                                    \nApplying syntax updates\nAs soon as you switch the language version of your project to 1.26, GoLand treats that as a signal: It can now look for patterns that suit Go 1.26 better.\n\n\n\n\nThe first thing you’ll notice is subtle. A blue underline appears under code that is safe to modernize. The underline uses a dedicated severity level named Syntax updates, with a language updates icon (). It is not an error. It is instead an indication that the code can be updated without changing its behavior.\nGoLand adds two Go 1.26 syntax update inspections:\nPointer creation with new()\nType-safe error unwrapping with errors.AsType\nWe started with the latest Go 1.26 changes, and we plan to add more inspections for important language and standard library updates from recent years.\nType-safe error unwrapping with errors.AsType\nGo 1.26 adds errors.AsType, which gives you a typed result. It avoids the pointer setup that errors.As needs and prevents type-mismatch panics. GoLand suggests the safer form and offers the Replace with errors.AsType quick-fix. You can read more about errors.AsType in the GoLand or official documentation.\nBefore\n\n\n\n\nAfter\n\n\n\n\nPointer creation with new()\nGo 1.26 lets new() accept expressions. This removes temporary variables that exist only so you can take their address. GoLand highlights the pattern and offers the Replace with new()  quick-fix. You can read more about new()in the GoLand or official documentation.\nBefore\n\n\n\n\nAfter\n\n\n\n\nExpanding from one fix to the whole project\nOnce you apply the first quick-fix, you can move from a single change to a project-wide update. GoLand gives you several entry points, depending on how you work:\nRight after a quick-fix: Just click Analyze code for other syntax updates.\n\n\n\n\n\nFrom Search Everywhere: Open Search Everywhere (press Shift twice) and select the Update Syntax action.\n\n\n\n\n\nFrom go.mod: Open the module file containing the go 1.26 directive and click Analyze code for syntax updates.\n\n\n\n\n\nFrom the Refactor menu: Click Refactor and select Update Syntax.\n\n\n\n\nGoLand collects the results in a separate tab under the Syntax updates node in the Problems tool window. You can review the updates one by one or apply them in bulk.\n\n\n\n\nGoLand shows a before-and-after diff for each suggested update, so you can review the exact rewrite before you apply it.\n\n\n\n\nWhat this approach to syntax updates changes in practice\nMigration to a new Go version is rarely one big rewrite. It usually happens over the course of dozens of small, safe modernizations mixed into daily work.\nGoLand supports this workflow in a few connected steps:\nIt helps you notice update candidates early. When you edit code that can be modernized, GoLand highlights it in the editor.\nIt offers a safe rewrite. You can apply a quick-fix that rewrites the code to the Go 1.26 form without changing its behavior.\nIt scales to the whole project. When you are ready, run Analyze code for other syntax updates on a wider scope and review the suggested updates before you apply them.\nIt lets you apply updates in bulk. From the list of results in the Problems tool window, you can apply fixes one by one or apply a grouped fix to update many occurrences at once.\nThis combination lets you move your codebase forward without turning the migration into a separate project. You update a line, you see a better form, you apply it, and you keep going.\nHappy coding!\nThe GoLand team",
        "dc:creator": "Artem Pronichev",
        "content": "Working on an existing Go project rarely starts with a plan to modernize it. More often, you open a file to make a small change, add a field, or adjust some logic. The code compiles, tests pass, and everything looks fine, but the language has moved forward, and your code hasn’t kept up. As you [&#8230;]",
        "contentSnippet": "Working on an existing Go project rarely starts with a plan to modernize it. More often, you open a file to make a small change, add a field, or adjust some logic. The code compiles, tests pass, and everything looks fine, but the language has moved forward, and your code hasn’t kept up. As you […]",
        "guid": "https://blog.jetbrains.com/?post_type=go&p=680704",
        "categories": [
          "goland",
          "news",
          "tutorials",
          "update"
        ],
        "isoDate": "2026-02-13T12:33:36.000Z"
      },
      {
        "creator": "Alina Dolgikh",
        "title": "Building Modular Monoliths With Kotlin and Spring",
        "link": "https://blog.jetbrains.com/kotlin/2026/02/building-modular-monoliths-with-kotlin-and-spring/",
        "pubDate": "Fri, 13 Feb 2026 12:27:12 +0000",
        "content:encodedSnippet": "This tutorial was written by an external contributor.\nVivek Kumar Maskara\nVivek Kumar Maskara is an Associate Software Engineer at JP Morgan. He loves writing code, developing apps, creating websites, and writing technical blogs about his experiences. His profile and contact information can be found at maskaravivek.com.\nWebsite | Twitter\nOver a decade ago, Netflix became one of the early adopters of microservice architecture, showcasing its potential at a large scale. Since then, many companies have jumped on the microservices bandwagon, building their backends this way from day one. While microservices offer isolation and independent scaling, their distributed nature requires managing multiple deployments, monitoring interservice communication, and handling network failures across service boundaries.\nAs teams have gained real-world experience with this complexity, there’s been a shift back toward monoliths, but not the tightly coupled monoliths of the past. Instead, developers are embracing modular monoliths: an architectural pattern where single deployable applications are organized into well-defined modules based on logical boundaries or business domains. Think of an e-commerce platform where users, products, and orders live in separate modules that interact through clear contracts, such as APIs for synchronous calls and events for async communication. This separation lets teams work in parallel for faster development and better maintainability, while single-unit deployment keeps releases simple and avoids microservice operational complexity.\nIn this guide, we explore how modular monoliths differ from traditional monoliths, why they’re gaining traction, and how to build them using Spring Modulith and Kotlin.\nThe Need for Modular Monoliths\nThe growing modular monolith countertrend makes more sense when viewed against the shortcomings of traditional approaches.\nTraditional Monoliths\nTraditional monoliths bundle the entire backend into a single codebase with tight coupling between user interfaces, business logic, and data access patterns. In an e-commerce platform, for example, the product catalog, checkout, payments, and order history services are in a single codebase and are deployed together. A monolith uses function calls for internal communication, and often, the call patterns and interdependencies become messy or difficult to maintain.\nMicroservices\nMicroservices emerged to solve these maintainability challenges by splitting backends into loosely coupled services, each handling a specific domain. A cab-hailing platform may separate users, drivers, ride matching, payments, and notifications into independent services. However, this introduces distributed system challenges, including complex service discovery, coordinating deployments across dependent services, and debugging interservice communication issues. Without proper expertise, tooling, and observability, this can slow down development.\nBenefits of Modular Monoliths\nModular monoliths strike a balance by keeping everything in a single codebase and deploying it as one artifact, while structuring the application into logical modules with well-defined interfaces. This addresses the challenges of distributed systems while maintaining the structural benefits of well-defined interfaces and independent development workflows. Some benefits of a modular monolith include:\nSimplified deployment: A single deployment artifact simplifies the release process because you don’t need to coordinate multiple service rollouts, manage service meshes, or handle distributed database migrations and rollbacks.\nReliable testing: As modules in a monolith communicate in-process rather than over a network, integration tests are faster and more stable. You can use mocks where needed, avoid brittle network dependencies, and run end-to-end (E2E) and performance tests in a controlled environment.\nStronger domain modeling: Modular monoliths group related business logic into modules, with clear ownership and communication boundaries between modules. It enforces communication only through well-defined interfaces and enables domain objects to be shared directly without serialization or cross-service APIs. This makes the system easier to maintain and improves development velocity.\nIn-process communication: Since modules communicate through direct method invocations instead of network calls, it reduces latency and points of failure.\nDesigning a Modular Monolith\nWhen you’re building a modular monolith, you first identify the business domains and split the application into multiple loosely coupled modules with clear boundaries and dependencies. Unlike the tightly interwoven code of a traditional monolith, the modular design ensures that the modules can be developed and maintained independently while still being deployed as a single unit. For example, you can break down an e-commerce platform into separate modules such as users, product catalog, shopping cart, payments, and orders:\n\n\n\n\nEach module encapsulates a specific entity or capability. A product catalog module would manage product details and categories, and an order processing module would handle order and payment entities. \nUnlike traditional monoliths, where internal calls often use ad hoc dependencies, in a modular monolith, the interactions with other modules are performed using explicit interfaces and well-defined contracts. This ensures that the intermodule dependencies remain clear and intentional. The communication between the modules uses in-process function calls, so it’s faster and less error-prone compared to network-based interservice calls in microservices.\nThe modular structure allows logical separation between the modules and enforces fixed boundaries, increasing development speed, improving maintainability, and making testing more reliable. Each module defines its user interface, business logic, and data access layers separately:\n\n\n\n\nThese boundaries also lay the groundwork for extracting a particular module as a microservice based on the scaling requirements.\nIntegrate Spring Modulith\nSpring Modulith is a Spring Boot framework that’s based on modular monolith architectural principles. It helps identify, structure, and enforce application modules. It also includes tools for verifying module boundaries and observing their behavior, along with module-level testing capabilities, making Spring Boot applications easier to build and maintain.\nHere’s how to integrate Spring Modulith into a Kotlin-based Spring Boot application.\nAll the code samples are drawn from a fully working Kotlin example, which you can find in this GitHub repository.\nRepository with the companion code for the tutorial\nGo to GitHub\n                                                    \nQuick Start Kotlin Example\nSpring Modulith can be added to any Spring Boot application by including its dependencies in the project’s build.gradle.kts:\n// build.gradle.kts\ndependencies {\n    implementation(\"org.springframework.boot:spring-boot-starter\")\n    implementation(\"org.springframework.modulith:spring-modulith-starter-core:1.4.3\")\n}\nNote: If your project uses Maven, you can add these dependencies to the pom.xml file.\nTo define the modules, you need to add relevant package directories to the src directory. This code snippet illustrates order and product packages added to the application, each handling its own business logic, data, and services:\nSpringModulithExample\n└── src/main/java\n    ├── example\n    │   └── SpringmonolithApplication.kt\n    └── example.order\n        └── …\n    └── example.product\n        └── …\n    └── example.payment\n        └── …\nWithin each of the modules, you can define the business logic, service, and data access layers based on your application’s requirements. The following code snippet shows a ProductService within the example.product package that returns a static greeting message:\npackage com.example.springmonolith.product\n\nimport org.springframework.stereotype.Service\n\n@Service\nclass ProductService {\n\n    fun getGreeting(): String {\n        return \"Hello from Product Module!\"\n    }\n}\nSimilarly, define an OrderService within the example.order package that invokes ProductService::getGreeting() and returns a combined greeting message:\npackage com.example.springmonolith.order\n\nimport com.example.springmonolith.product.ProductService\nimport org.springframework.stereotype.Service\n\n@Service\nclass OrderService(\n    private val productService: ProductService\n) {\n\n    fun getGreeting(): String {\n        return \"Hello from Order Module!\"\n    }\n    \n    fun getCombinedGreeting(): String {\n        return \"Hello from Order Module and: ${productService.getGreeting()}\"\n    }\n}\nAfter adding similar business logic for each of the modules (eg ProductService, PaymentService), you also need to add the @Modulithic annotation to the Spring Boot Application class to mark it as modular. \nThe annotation tells Spring Modulith to automatically detect modules based on package structure and enable the tooling for verification, testing, and observability:\npackage com.example.springmonolith\n\nimport org.springframework.boot.autoconfigure.SpringBootApplication\nimport org.springframework.boot.runApplication\nimport org.springframework.modulith.Modulithic\n\n// add this annotation to the application\n@Modulithic\n@SpringBootApplication\nclass SpringmonolithApplication\n\nfun main(args: Array<String>) {\n    runApplication<SpringmonolithApplication>(*args)\n}\nDefining allowed module dependencies\nNext, you can update the package info file for each of the modules to define the allowed module dependencies. Since the order module depends on the methods defined in the product module, you’ll need to add the following annotation in the order/package-info.java file:\n// add this annotation\n@org.springframework.modulith.ApplicationModule(allowedDependencies = {\"product\"})\n@org.springframework.lang.NonNullApi\npackage com.example.springmonolith.order;\nFinally, you can update the product/package-info.java file to set an empty dependency list for the product module:\n// add this annotation\n@org.springframework.modulith.ApplicationModule(allowedDependencies = {})\n@org.springframework.lang.NonNullApi\npackage com.example.springmonolith.product;\nThe above annotation ensures that if a class or object defined in the product module tries to invoke a method defined outside the module, Spring Modulith verification tests will flag the violation. You will see an example of this scenario in later sections.\nSpring Modulith Features\nSpring Modulith supports various tools for working with modules, including module verification, documentation, and runtime observability. With @Modulithic, the application automatically recognizes its modules (based on package structure) and enables the modulith tooling. Let’s look at how these work.\nModular Structure Checks\nSpring Modulith provides built-in tooling to verify that the module boundaries adhere to the constraints. It checks for cyclic dependencies, validates that modules access other modules only through their public API packages (not internal code), and enforces explicit dependency rules. In your tests, you can use the ApplicationModules.verify() to verify the modular structure:\nApplicationModules.of(Application::class.java).verify()\nRefer to the source code on GitHub for a complete example of the ModularityTest. With the above test configured, if the ProductService tries to invoke an order module method, the module verification test will fail. You can test the behavior by extending the ProductService to call getGreeting as shown below:\n// add import\nimport com.example.springmonolith.order.OrderService\n\n@Service\nclass ProductService(\n    private val orderService: OrderService\n) {\n    \n    // add this after getGreeting()\n    fun getCombinedGreeting(): String {\n        return \"Hello from Product Module and: ${orderService.getGreeting()}\"\n    }\n}\nSince product module is configured to disallow all intermodule dependencies, when you run unit tests (./gradlew test), you would get a module violation error as shown below:\n— TRUNCATED OUTPUT —\nModularityTests > verifiesModularStructure() FAILED\n    org.springframework.modulith.core.Violations at ModularityTests.kt:20\nYou can replace direct intermodule calls with application events that let one module publish a domain event and another module listens to it. This preserves boundaries and avoids compile-time coupling between modules. For example, the order module can publish an event when an order is created, as shown below:\n// order module\nimport org.springframework.context.ApplicationEventPublisher\n\n@Service\nclass OrderService(private val events: ApplicationEventPublisher) {\n\n    fun completeOrder(orderId: String) {\n        events.publishEvent(OrderCompleted(orderId))\n    }\n}\n\ndata class OrderCompleted(val orderId: String)\nNotice that the completeOrder method publishes the OrderCompleted event, and other modules (eg, InventoryPolicy) could react to the event using @ApplicationModuleListener, as shown below:\n// product module\nimport org.springframework.modulith.events.ApplicationModuleListener\nimport org.springframework.stereotype.Component\n\n@Component\nclass InventoryPolicy {\n\n    @ApplicationModuleListener\n    fun on(event: OrderCompleted) {\n        println(\"Updating inventory for order: ${event.orderId}\")\n    }\n}\nNotice that the InventoryPolicy component has a listener configured for the OrderCompleted event that prints the order ID when it receives the event. Refer to the refactor branch on GitHub for a complete example on domain events.\nModular Level Testing\nModulith supports writing integration tests scoped to a single module. You can annotate a test class with @ApplicationModuleTests to test the module and its dependencies in isolation. This avoids the need to spin up the entire application, reducing setup overhead and making tests more reliable. For example, this code snippet shows a bare-bones integration test for the product module:\nimport org.junit.jupiter.api.Test\nimport org.springframework.modulith.test.ApplicationModuleTests\n\n@ApplicationModuleTests\nclass ProductModuleTests {\n\n    @Test\n    fun testProductServiceGreeting() {\n        val greeting = productService.getGreeting()\n        assertTrue(greeting.contains(\"Product Module\"))\n    }\n}\nFor the OrderService test, since it depends on the product module, you need to set the extraIncludes parameter in the @ApplicationModuleTests annotation to include it as shown below:\npackage com.example.springmonolith.order\n\nimport org.junit.jupiter.api.Test\nimport org.springframework.beans.factory.annotation.Autowired\nimport org.springframework.modulith.test.ApplicationModuleTest\nimport org.springframework.test.context.junit.jupiter.SpringJUnitConfig\nimport kotlin.test.assertTrue\n\n@ApplicationModuleTest(extraIncludes = [\"product\"])\n@SpringJUnitConfig\nclass OrderModuleTests {\n\n    @Autowired\n    private lateinit var orderService: OrderService\n\n    @Test\n    fun testOrderServiceGreeting() {\n        val greeting = orderService.getGreeting()\n        assertTrue(greeting.contains(\"Order Module\"))\n    }\n}\nObservability into Module Interactions\nSpring Modulith helps generate developer documentation using the Documenter abstraction. This tool can generate Unified Modeling Language (UML) component diagrams describing the relationship between modules and can also generate a tabular view of the key elements of a module. \nThis code snippet generates an application module component diagram using Documenter:\nclass DocumentationTests {\n    private val modules = ApplicationModules.of(SpringmonolithApplication::class.java)\n\n    @Test\n    fun writeDocumentationSnippets() {\n        Documenter(modules)\n            .writeModulesAsPlantUml()\n            .writeIndividualModulesAsPlantUml()\n    }\n}\nSpring Modulith also integrates with Micrometer to capture spans for module interactions. These spans can be sent to tracing tools such as Zipkin to generate runtime visualizations, making it easier to inspect which modules depend on each other, see how events flow across modules, and monitor interactions in production.\nDeciding When to Use a Modular Monolith\nAlthough a modular monolith can be the ideal balance between simplicity and structure in many cases, it isn’t universally the right choice.\nModular Monolith Use Cases\nEarly-stage development or limited resources: In the early stages of a product or when working with small teams, a modular monolith reduces operational overhead. Developers can focus on delivering features quickly without the complexity of distributed systems. The modular design still enforces boundaries between business capabilities, so if the system grows, you can gradually migrate high-demand modules into separate microservices.\n\nExample: A food delivery platform can start with modules for restaurants, menu, and orders inside a single deployable unit, but it can later extract and deploy one of the modules as a microservice.\nComplex business domains: Applications that involve complex business logic, workflows, or dependencies can benefit from a modular structure. By encapsulating each business capability in its own module, the system becomes easier to develop, test, and maintain.\n\nExample: An insurance platform can split policy management, claims processing, and customer support into separate modules to avoid creating interdependencies that can become difficult to maintain.\nWhen Modular Monoliths Aren’t Always the Right Choice\nSystems with independent scaling needs: Some systems have uneven load patterns where certain components handle millions of requests daily, while others are rarely used. Because modular monoliths deploy as one unit, you can’t scale individual parts independently. A microservice-based approach can more easily scale components that expect a higher load than others.\n\nExample: In an e-commerce platform, the product catalog or recommendation services may experience higher request volumes than order or payment services.\nSystems that use diverse tech stacks: In some organizations, different teams rely on different programming languages, runtimes, or specialized infrastructure for different parts of the system. A modular monolith requires the entire application to use the same stack, which can limit flexibility. In these cases, a microservice-based architecture can provide the isolation needed to mix and match technologies.\n\nExample: Machine learning or analytics teams may want to use Python or Go for their services, while client-facing or internal services can be based on Kotlin or Java.\nConclusion\nModular monolith architecture allows you to split the application logic into isolated modules with their own business logic, while still being deployed as a single artifact. It combines the benefits of a modular design while maintaining the development and release-related benefits of a monolithic architecture. Additionally, modern programming languages such as Kotlin provide tools that can help you achieve monolith stability without giving up the productivity that draws people to microservices.\nSpring Modulith and Kotlin provide the tools to design and enforce clear module boundaries, test modules independently, and monitor their interactions. Try out Spring Modulith to build modular Kotlin applications, while keeping the flexibility to evolve into microservices if your scaling needs change.",
        "dc:creator": "Alina Dolgikh",
        "content": "This tutorial was written by an external contributor. Over a decade ago, Netflix became one of the early adopters of microservice architecture, showcasing its potential at a large scale. Since then, many companies have jumped on the microservices bandwagon, building their backends this way from day one. While microservices offer isolation and independent scaling, their [&#8230;]",
        "contentSnippet": "This tutorial was written by an external contributor. Over a decade ago, Netflix became one of the early adopters of microservice architecture, showcasing its potential at a large scale. Since then, many companies have jumped on the microservices bandwagon, building their backends this way from day one. While microservices offer isolation and independent scaling, their […]",
        "guid": "https://blog.jetbrains.com/?post_type=kotlin&p=679360",
        "categories": [
          "news",
          "tutorials",
          "architecture",
          "backend",
          "kotlin",
          "spring",
          "tutorial"
        ],
        "isoDate": "2026-02-13T12:27:12.000Z"
      }
    ]
  },
  {
    "name": "Airbnb Engineering & Data Science",
    "category": "기업",
    "posts": [
      {
        "creator": "Cosmo W. Q",
        "title": "Safeguarding Dynamic Configuration Changes at Scale",
        "link": "https://medium.com/airbnb-engineering/safeguarding-dynamic-configuration-changes-at-scale-5aca5222ed68?source=rss----53c7c27702d5---4",
        "pubDate": "Wed, 18 Feb 2026 17:01:01 GMT",
        "content:encodedSnippet": "How Airbnb ships dynamic config changes safely and reliably\n\nBy Cosmo Qiu, Bo Teng, Siyuan Zhou, Ankur Soni, Willis Harvey\nDynamic configuration is a core infrastructure capability in modern systems. It allows developers to change runtime behavior without restarting or redeploying services, even as the number of services and requests grows. In practice, that might mean rolling out a new address form for a region launch, tightening an authorization rule, or adjusting timeouts when a dependency is slow.\nLike any powerful tool, dynamic configuration is a double-edged sword. While it enables fast iteration and rapid incident response, a bad change can cause regressions or even outages. This is a common challenge across the industry: balancing developer flexibility with system reliability.\nIn this post, we will outline the expectations of a modern dynamic configuration platform, then walk through the high-level architecture of Airbnb’s dynamic config platform and how its core components work together to enable safe, flexible config changes.\nModern config platform essentials\nAs Airbnb’s business grows, our expectations for the dynamic config platform have evolved over time through our own learnings as well as industry best practices. These shape our view of what a good dynamic config platform should provide, including:\n\nA coherent experience for config management: The platform provides a streamlined, end-to-end experience for defining, reviewing, testing, and rolling out config changes. It covers the most common needs out of the box with rich built-in features, while still offering escape hatches for edge cases.\nStrong reliability, availability and safety guarantees: All config changes are validated, reviewed, and rolled out progressively, with clear ownership and well-defined access control. Treating config as code is a key focus: config changes are versioned, reviewed, and auditable like service code, but remain dynamic at runtime. The platform itself must be highly available so that services can reliably fetch and apply configs. Changes should be observable, with support for fast rollbacks when needed.\nSafe testing in isolated environments: Developers can validate config changes in isolated local or canary environments before they reach production.\nFlexible multi-tenant support: In a multi-tenant platform, different tenants have different risk profiles. The platform should allow config owners to customize how their configs behave per tenant, including deployment triggers, guardrails, and rollout strategy (for example, AWS zone or Kubernetes pod percentage-based rollouts).\nFast and controlled incident response: During an incident, responders can ship emergency configs as needed with clear auditability. The platform also provides observability for config changes, so incident responders can tell what changed, who was affected, when the change was made, and who made the change. This enables them to effectively identify the culprit and take action.\n\nHigh-level architecture\nAt Airbnb, Sitar is the internal name for our dynamic config platform. It provides a common way for teams to manage runtime behavior safely. At a high level, Sitar has four main parts: a developer-facing layer, a control plane, a data plane, and the clients and agents that run alongside application code.\n\nThe developer-facing layer is where config changes are created and reviewed. By default, configs are managed through a Git-based workflow, while a few exceptions are managed in the web interface (sitar-portal), which is also used for admin operations such as emergency deployments.\nThe control plane is responsible for orchestrating config changes. It enforces schema validation, ownership, and access control, and decides how each change should be rolled out: for example, which environments or AWS zone to target, what percentage of Kubernetes pods to start with, and how to progress the rollout over time. The control plane also specifies how to roll back the changes when needed, and supports routing in-flight configs to specific environments or slices of subscribers for fast testing.\nThe data plane provides scalable storage and efficient distribution of configs. It acts as the source of truth for config values and versions, and propagates updates to services reliably, consistently, and quickly.\nOn the product services side, an agent sidecar running alongside each service fetches the subscribed configs from the data plane and maintains a local cache. Client libraries inside the service then read from this cache and expose configs to application logic with fast, in-process access and optional fallbacks.\nPutting these together, a typical change starts from a Git flow, proceeds through control-plane validation and rollout decisions, into the data plane for distribution, and finally to agents and client libraries that apply the config updates to application logic.\nKey design choices\nIn this section, we highlight a few key design choices that shape how the platform looks and is operated.\nConfigs as code with a Git-based workflow\nConfig changes are by default managed by a Git-centric workflow. We use GitHub as the primary interface for managing configs, because we have an established and responsive internal team to manage GitHub Enterprise. GitHub integrates naturally with our existing CI/CD tooling, so we can reuse rich validation and deployment pipelines without re-inventing the wheel. This approach gives developers a consistent experience to make code changes: open a pull request, get reviews, merge, and deploy. GitHub also brings additional benefits such as mandatory reviewers, review and approval flows, and a change history. Configs under the same theme are grouped into tenants, with clear owners, customizable tests, and a dedicated CD pipeline.\nWhile the Git-based flow is the default, we keep a UI portal for teams that prefer a portal-based experience and as a shortcut for specific operational needs, such as fast emergency config updates that can bypass the normal CI/CD pipeline.\nStaged rollouts and fast rollbacks\nWhen a change is proposed, schema validation (checking that the config matches the expected structure and types) and other automated checks run in CI. The change is always reviewed and approved before rollout.\nOnce merged in the main branch, the control plane performs a staged rollout where the change is first deployed to a limited scope, then gradually expanded to a larger scope if things look good. At each stage of this rollout, the change is evaluated, the author and the stakeholders are notified if regressions are detected, and a fast rollback can be triggered if needed. Staged rollouts can greatly reduce the blast radius of bad changes and improve the overall reliability of the platform.\nSeparated control and data planes\nWe separate the “decide” and “deliver” responsibilities. The control plane focuses on validation, authorization, and rollout decisions, while the data plane focuses on storing configs and distributing them reliably at scale. This separation allows us to evolve rollout strategies and policies without disrupting the underlying storage and delivery mechanisms, and vice versa.\nLocal caching and resilient clients\nOn the product services side, we introduced a local caching layer between the agent sidecar and the client library to improve resilience and availability. The agent sidecar runs alongside the main service container, regardless of which language the service is written in, and periodically fetches subscribed configs from the backend and persists them locally. The client libraries then read from this local cache. Even if the backend is temporarily unavailable or degraded, services can continue operating on the last known good configs from the local cache.\nImpact on product teams\nIt is essential for the Sitar system to make life easier for product teams. In practice, its architecture changes how teams ship and operate in a few ways:\n\nRollouts become safer and more predictable. New behaviors, such as refined authorization rules, can be introduced gradually, verified on a small slice of traffic or in a specific environment, and rolled back quickly if needed. Teams spend less time worrying about “big bang” releases and more time iterating on behavior.\nTeams get more flexibility in how their configs are managed and rolled out. Each team can tailor a config flow to its own risk profile and release schedules. For example, teams can choose between automatic, manual, or cron rollouts, select the rollout strategy, and add extra checks. This lets teams keep their existing ways of working while still benefiting from a common platform and shared guardrails.\nIncident mitigation becomes faster and more controlled. When something goes wrong in production, incident responders can use observability tools that integrate config events to quickly locate the culprit change, then take quick action using the portal’s emergency flow. These emergency updates are fully auditable for future review.\n\nBesides these examples, the platform includes other improvements in usability, safety, and observability that we will not cover in detail here. Together, they contribute to a smoother day-to-day experience for teams that rely on dynamic configuration.\nConclusions and next steps\nDynamic configuration is a foundational capability of modern infrastructure. It enables fast iteration and rapid incident response, but only when it is equipped with strong safety features and provides a good developer experience. In this post, we shared how we think about a modern dynamic config platform at Airbnb, and how we developed Sitar’s architecture to meet those expectations.\nThe work is ongoing. As Airbnb’s business grows, we are continuing to refine rollout strategies, improve config testing, invest in observability and smart incident response tooling, and evolve other platform components.\nIn future posts, we plan to dive deeper into specific areas of the platform, such as how we optimize the Kubernetes sidecar that delivers config updates and how we design the developer experience around config management.\nIf this type of work interests you check out our open roles.\nAcknowledgments\nOur progress with Sitar would not have been possible without the support and contributions of many people. We’d like to thank Craig Sosin, Nikolaj Nielsen, Daniel Fagnan, Alex Edwards, Xian Gao, Nick Morgan, Carolina Calderon, Hanfei Lin, Joyce Li, Yunong Liu, Alex Berghage, Brian Wolfe, Yann Ramin, Denis Sheahan, Richa Khandelwal, Swetha Vaidy, Abhishek Parmar, Adam Kocoloski, Adam Miskiewicz, and all the other engineers and teams at Airbnb who joined design reviews and offered valuable feedback, as this work would not have been possible without them.\nAll product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.\n\nSafeguarding Dynamic Configuration Changes at Scale was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Cosmo W. Q",
        "guid": "https://medium.com/p/5aca5222ed68",
        "categories": [
          "engineering",
          "software-development",
          "infrastructure",
          "distributed-systems",
          "software-architecture"
        ],
        "isoDate": "2026-02-18T17:01:01.000Z"
      }
    ]
  },
  {
    "name": "PayPal Engineering",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Visual Studio Blog",
    "category": "기업",
    "posts": [
      {
        "creator": "Rhea Patel, Kelly Fam",
        "title": "Custom Agents in Visual Studio: Built in and Build-Your-Own agents",
        "link": "https://devblogs.microsoft.com/visualstudio/custom-agents-in-visual-studio-built-in-and-build-your-own-agents/",
        "pubDate": "Thu, 19 Feb 2026 21:02:13 +0000",
        "content:encodedSnippet": "Agents in Visual Studio now go beyond a single general-purpose assistant. We’re shipping a set of curated preset agents that tap into deep IDE capabilities; debugging, profiling, testing alongside a framework for building your own custom agents tailored to how your team works.\nBuilt in agents\nEach preset agent is designed around a specific developer workflow and integrates with Visual Studio’s native tooling in ways that a generic assistant can’t.\nDebugger – Goes beyond “read the error message.” Uses your call stacks, variable state, and diagnostic tools to walk through error diagnosis systematically across your solution.\nProfiler – Connects to Visual Studio’s profiling infrastructure to identify bottlenecks and suggest targeted optimizations grounded in your codebase, not generic advice.\nTest – (when solution is loaded) Generates unit tests tuned to your project’s framework and patterns, not boilerplate that your CI will reject.\nModernize (.NET and C++ only) -Framework and dependency upgrades with awareness of your actual project graph. Flags breaking changes, generates migration code, and follows your existing patterns.\nAccess them through the agent picker in the chat panel or using ‘@’ in chat.\nBring your own: custom agents (preview)\nThe presets cover workflows we think matter most, but your team knows your workflow better than we do. Custom agents let you build your own using the same foundation—workspace awareness, code understanding, tools accessed by your prompts, your preferred model, and your tools.\nWhere it gets powerful is MCP. You can connect custom agents to external knowledge sources internal documentation, design systems, APIs, and databases so the agent isn’t limited to what’s in your repo.\nA few patterns we’re seeing from teams:\nCode review that checks PRs against your actual conventions, connected via MCP to your style guide or ADR repository\nDesign system enforcement connected to your Figma files or component libraries to catch UI drift before it ships\nPlanning helps you think through a feature or task before any code is written. Gathers requirements, asks clarifying questions, and builds out a plan that you can hand off\nThe awesome-copilot repo has community-contributed agent configurations you can use as starting points.\nGet started\nCustom agents are defined as .agent.md files in your repository’s .github/agents/ folder:\nyour-repo/\r\n└── .github/\r\n    └── agents/\r\n        └── code-reviewer.agent.md\nA few things to note:\nThis is a preview feature; the format of these files may change over to support different capabilities\nIf you don’t specify a model, the agent uses whatever is selected in the model picker\nTool names vary across GitHub Copilot platforms- check the tools available in Visual Studio specifically to make sure your agent works as expected\nConfigurations from the awesome-copilot repo are a great starting point, but verify tool names before using them in VS\nTell us what you’re building\nShare your configurations in the awesome-copilot repo or file feedback here.\nThe post Custom Agents in Visual Studio: Built in and Build-Your-Own agents appeared first on Visual Studio Blog.",
        "dc:creator": "Rhea Patel, Kelly Fam",
        "comments": "https://devblogs.microsoft.com/visualstudio/custom-agents-in-visual-studio-built-in-and-build-your-own-agents/#respond",
        "content": "<p>Agents in Visual Studio now go beyond a single general-purpose assistant. We&#8217;re shipping a set of curated preset agents that tap into deep IDE capabilities; debugging, profiling, testing alongside a framework for building your own custom agents tailored to how your team works. Built in agents Each preset agent is designed around a specific developer [&#8230;]</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/visualstudio/custom-agents-in-visual-studio-built-in-and-build-your-own-agents/\">Custom Agents in Visual Studio: Built in and Build-Your-Own agents</a> appeared first on <a href=\"https://devblogs.microsoft.com/visualstudio\">Visual Studio Blog</a>.</p>\n",
        "contentSnippet": "Agents in Visual Studio now go beyond a single general-purpose assistant. We’re shipping a set of curated preset agents that tap into deep IDE capabilities; debugging, profiling, testing alongside a framework for building your own custom agents tailored to how your team works. Built in agents Each preset agent is designed around a specific developer […]\nThe post Custom Agents in Visual Studio: Built in and Build-Your-Own agents appeared first on Visual Studio Blog.",
        "guid": "https://devblogs.microsoft.com/visualstudio/?p=255658",
        "categories": [
          "Copilot",
          "GitHub Copilot",
          "agents",
          "custom agents",
          "Visual Studio"
        ],
        "isoDate": "2026-02-19T21:02:13.000Z"
      }
    ]
  },
  {
    "name": "Joshua",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권재명",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김석기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김선영",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강대명",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권정혁",
    "category": "개인",
    "posts": []
  },
  {
    "name": "줌구",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수보",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김시은",
    "category": "개인",
    "posts": []
  },
  {
    "name": "곽민수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김범진",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민서",
    "category": "개인",
    "posts": []
  },
  {
    "name": "I am not Okay",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권창현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권기호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수민",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강태욱",
    "category": "개인",
    "posts": [
      {
        "title": "unsloth 기반 효율적이고 환각 없는 모델 파인튜닝 개발 방법 ",
        "link": "http://daddynkidsmakers.blogspot.com/2026/02/blog-post.html",
        "pubDate": "2026-02-18T01:08:00.000Z",
        "author": "Daddy Maker",
        "content": "<div style=\"text-align: left;\">이 글은&nbsp;신뢰성있는 모델 파인튜닝 개발 방법을 나눔한다.</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEhyTbYNMYJO8UEKbtlLWf0lOpWOUL7Mcds7Tao1VqyR88h33zbLYWfuwKnHqHPwNpuixeRzuKX5eldiMo8C063BfBbHmixrKRDX67sHjazoZ9xhbm8a3waGuzbByqQnLYxnzJWkFa_iFQ3Rb3jybxWjJMRsRRcj825MAljRJ3A18wkLVa_TdKiUTk6bNjvR\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"139\" data-original-width=\"786\" height=\"90\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEhyTbYNMYJO8UEKbtlLWf0lOpWOUL7Mcds7Tao1VqyR88h33zbLYWfuwKnHqHPwNpuixeRzuKX5eldiMo8C063BfBbHmixrKRDX67sHjazoZ9xhbm8a3waGuzbByqQnLYxnzJWkFa_iFQ3Rb3jybxWjJMRsRRcj825MAljRJ3A18wkLVa_TdKiUTk6bNjvR=w505-h90\" width=\"505\" /></a></div><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEi-1PKOFlEXdQJOfzHSqBME-_hc5-3A-F2MegQzy_jxpm060XNtDVxUAqr3-wFJEYSHgawiDoI2fs9RZtSxga1AoIY9Ws31yqYtGnNa2QdKH7HAiz0jfiTblVCZoFSyaiqPNPhLWL7Ku37nvvGg1Caj4-pdsK4rp8PprDtRJDrPL6FUMpfO7P-RcRGTkng0\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"231\" data-original-width=\"786\" height=\"160\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEi-1PKOFlEXdQJOfzHSqBME-_hc5-3A-F2MegQzy_jxpm060XNtDVxUAqr3-wFJEYSHgawiDoI2fs9RZtSxga1AoIY9Ws31yqYtGnNa2QdKH7HAiz0jfiTblVCZoFSyaiqPNPhLWL7Ku37nvvGg1Caj4-pdsK4rp8PprDtRJDrPL6FUMpfO7P-RcRGTkng0=w542-h160\" width=\"542\" /></a></div><br /></div><div style=\"text-align: left;\">pip install \"unsloth @ git+https://github.com/unslothai/unsloth.git\"</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b>레퍼런스</b></div><div style=\"text-align: left;\"><ul style=\"text-align: left;\"><li><a href=\"https://medium.com/@nomannayeem/finetuning-phi-3-for-rag-why-small-models-are-best-for-production-c2f6a5f74b51\">FineTuning PHI-3 for RAG: Why Small Models Are Best for Production | by Nayeem Islam | Medium</a></li><li><a href=\"https://firastlili.medium.com/fine-tuning-phi-3-with-unsloth-a-beginner-friendly-guide-12913d7ea506\">Fine-Tuning Phi-3 with Unsloth: A Beginner-Friendly Guide 🚀🤖 | by Firas Tlili | Medium</a></li><li><a href=\"https://medium.com/@liana.napalkova/fine-tuning-small-vision-language-models-phi-3-vision-b9d406e4e268\">Fine-tuning Small Vision Language Models: Phi-3-vision | by Liana Napalkova, PhD | Medium</a></li><li><a href=\"https://medium.com/@kondam.reddy/from-scratch-a-deep-dive-into-fine-tuning-phi-3-5-for-clinical-q-a-a49cb01b2414\">From Scratch: A Deep Dive into Fine-Tuning Phi-3.5 for Clinical Q&amp;A | by Sriman | Medium</a></li><li><a href=\"https://unsloth.ai/docs/get-started/unsloth-notebooks\">Unsloth Notebooks | Unsloth Documentation</a></li><li><a href=\"https://medium.com/data-science-at-microsoft/unlocking-the-power-of-data-synthesis-for-function-calling-with-fine-tuned-slms-a733df7a1d07\">Unlocking the power of data synthesis for function calling with fine-tuned SLMs | by Kosuke Fujimoto | Data Science + AI at Microsoft | Medium</a></li><li><a href=\"https://github.com/microsoft/PhiCookBook/blob/main/code%2F04.Finetuning%2FPhi-3-finetune-lora-python.ipynb\">PhiCookBook/code/04.Finetuning/Phi-3-finetune-lora-python.ipynb at main · microsoft/PhiCookBook</a></li><li><a href=\"https://medium.com/@fahey_james/fine-tuning-vs-function-calling-the-new-stack-for-customizing-foundation-models-bd1e48decf2c\">Fine-Tuning vs. Function Calling: The New Stack for Customizing Foundation Models | by James Fahey | Medium</a></li><li><a href=\"https://blog.naver.com/ryurime88/224184149751?trackingCode=rss\">오픈클로(OpenClaw) 윈도우 설치 방법,.. : 네이버블로그</a></li></ul></div>",
        "contentSnippet": "이 글은 신뢰성있는 모델 파인튜닝 개발 방법을 나눔한다.\n\n\n\n\n\n\npip install \"unsloth @ git+https://github.com/unslothai/unsloth.git\"\n\n\n레퍼런스\n\nFineTuning PHI-3 for RAG: Why Small Models Are Best for Production | by Nayeem Islam | Medium\nFine-Tuning Phi-3 with Unsloth: A Beginner-Friendly Guide 🚀🤖 | by Firas Tlili | Medium\nFine-tuning Small Vision Language Models: Phi-3-vision | by Liana Napalkova, PhD | Medium\nFrom Scratch: A Deep Dive into Fine-Tuning Phi-3.5 for Clinical Q&A | by Sriman | Medium\nUnsloth Notebooks | Unsloth Documentation\nUnlocking the power of data synthesis for function calling with fine-tuned SLMs | by Kosuke Fujimoto | Data Science + AI at Microsoft | Medium\nPhiCookBook/code/04.Finetuning/Phi-3-finetune-lora-python.ipynb at main · microsoft/PhiCookBook\nFine-Tuning vs. Function Calling: The New Stack for Customizing Foundation Models | by James Fahey | Medium\n오픈클로(OpenClaw) 윈도우 설치 방법,.. : 네이버블로그",
        "id": "tag:blogger.com,1999:blog-5201956450461596914.post-3481572627910625849",
        "isoDate": "2026-02-18T01:08:00.000Z"
      }
    ]
  },
  {
    "name": "권용진",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "RSS feed for boyism Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성준의 린스타트업과 디자인씽킹",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권혁우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김준형",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강동혁",
    "category": "개인",
    "posts": []
  },
  {
    "name": "고종범",
    "category": "개인",
    "posts": []
  },
  {
    "name": "cheese10yun",
    "category": "개인",
    "posts": [
      {
        "title": "필드 단위 변경 이력(History) 추적 시스템",
        "link": "https://cheese10yun.github.io/diff-history-part-1/",
        "pubDate": "Sun, 15 Feb 2026 15:00:00 GMT",
        "content:encodedSnippet": "들어가며#\n운영 환경에서 데이터 변경 이력을 추적해야 하는 경우가 자주 발생합니다. 특히 주문 정보 수정, 가맹점 수수료율 변경 등 중요한 데이터가 어떻게 변경되었는지 필드 단위로 명확하게 기록하고 확인할 수 있어야 합니다.\n예를 들어, 운영자가 주문 내역에서 배송 주소만 변경했을 때, 전체 주문 데이터를 다시 저장하는 것보다 “어떤 필드가 어떻게 변경되었는지”를 명확히 기록하면 다음과 같은 이점이 있습니다:\n\n변경 이력 추적이 명확해집니다\n승인 프로세스에서 변경 내용 검토가 용이합니다\n디버깅 및 감사(Audit) 목적으로 활용할 수 있습니다\n데이터 롤백 시 정확한 변경 지점을 파악할 수 있습니다\n\n이번 포스트에서는 Kotlin과 Jackson을 활용하여 복잡한 중첩 객체의 변경사항을 자동으로 추적하는 시스템을 구현하는 방법을 알아보겠습니다.\n문제 상황#\n다음과 같은 주문(Order) 데이터가 있다고 가정해봅시다.\n변경 전 데이터#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n{\n  \"order_id\": \"ORD123456\",\n  \"customer\": {\n    \"customer_id\": \"CUST7890\",\n    \"name\": \"홍길동\",\n    \"contact\": {\n      \"email\": \"hong@example.com\",\n      \"phone\": \"010-1234-5678\",\n      \"address\": {\n        \"street\": \"서울특별시 종로구\",\n        \"city\": \"서울\",\n        \"zip_code\": \"03000\",\n        \"country\": \"KR\"\n      }\n    }\n  },\n  \"items\": [\n    {\n      \"product\": {\n        \"product_id\": \"PROD001\",\n        \"product_name\": \"노트북\",\n        \"category\": {\n          \"main_category\": \"전자제품\",\n          \"sub_category\": \"컴퓨터\"\n        }\n      },\n      \"quantity\": 1,\n      \"price\": 1500000\n    }\n  ],\n  \"payment\": {\n    \"method\": \"신용카드\",\n    \"transaction_id\": \"TXN987654321\",\n    \"status\": \"완료\"\n  }\n}\n\n\n변경 후 데이터#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n{\n  \"order_id\": \"ORD123456\",\n  \"customer\": {\n    \"customer_id\": \"CUST7890\",\n    \"name\": \"홍길동\",\n    \"contact\": {\n      \"email\": \"hong@example.com\",\n      \"phone\": \"010-1234-5678\",\n      \"address\": {\n        \"street\": \"서울특별시 강남구\",\n        \"city\": \"서울\",\n        \"zip_code\": \"03000\",\n        \"country\": \"KR\"\n      }\n    }\n  },\n  \"items\": [\n    {\n      \"product\": {\n        \"product_id\": \"PROD001\",\n        \"product_name\": \"노트북\",\n        \"category\": {\n          \"main_category\": \"전자제품\",\n          \"sub_category\": \"컴퓨터\"\n        }\n      },\n      \"quantity\": 1,\n      \"price\": 1400000\n    }\n  ],\n  \"payment\": {\n    \"method\": \"신용카드\",\n    \"transaction_id\": \"TXN987654322\",\n    \"status\": \"완료\"\n  }\n}\n\n\n위 두 데이터를 비교하면 다음 필드들이 변경되었습니다:\n\ncustomer.contact.address.street: “서울특별시 종로구” → “서울특별시 강남구”\nitems[0].price: 1500000 → 1400000\npayment.transaction_id: “TXN987654321” → “TXN987654322”\n\n이러한 변경사항을 자동으로 감지하고 추적하려면 어떻게 해야 할까요?\nIntelliJ의 Diff 기능처럼#\nIntelliJ IDE를 사용해보신 분들은 아시겠지만, 두 파일을 비교할 때 매우 직관적으로 변경 사항을 표시해줍니다.\n\n우리가 구현하려는 시스템도 이와 유사하게 두 객체를 비교하여 변경된 필드만 추출하는 것입니다.\n구현 방법#\n1. 핵심 라이브러리: zjsonpatch#\nJSON 객체 간의 차이를 계산하기 위해 zjsonpatch 라이브러리를 사용합니다. 이 라이브러리는 RFC 6902 JSON Patch 표준을 구현하여 두 JSON 문서의 차이를 효과적으로 계산합니다.\n\n\n1\n2\n3\n4\n5\n\n// build.gradle.kts\ndependencies {\n    implementation(\"com.flipkart.zjsonpatch:zjsonpatch:0.4.14\")\n    implementation(\"com.fasterxml.jackson.module:jackson-module-kotlin\")\n}\n\n\n2. DiffComparisonManager 구현#\n전체 코드는 다음과 같습니다:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n\npackage com.example.boot3mongo\n\nimport com.fasterxml.jackson.core.JsonGenerator\nimport com.fasterxml.jackson.databind.JsonNode\nimport com.fasterxml.jackson.databind.JsonSerializer\nimport com.fasterxml.jackson.databind.PropertyNamingStrategies\nimport com.fasterxml.jackson.databind.SerializerProvider\nimport com.fasterxml.jackson.databind.module.SimpleModule\nimport com.fasterxml.jackson.module.kotlin.jacksonObjectMapper\nimport com.flipkart.zjsonpatch.JsonDiff\nimport org.bson.types.ObjectId\n\ntypealias DiffValueTracker = Map<String, DiffValue<String, String>>\ntypealias DiffTriple = Triple<String, String, String>\n\nobject DiffComparisonManager {\n\n    private val diffMapper = jacksonObjectMapper()\n        .apply {\n            registerModules(\n                SimpleModule().apply {\n                    propertyNamingStrategy = PropertyNamingStrategies.SNAKE_CASE\n                    addSerializer(ObjectId::class.java, ObjectIdSerializer())\n                }\n            )\n        }\n\n    fun <T> calculateDifference(\n        originItem: T,\n        newItem: T\n    ): DiffValueTracker {\n        val originalNode = diffMapper.valueToTree<JsonNode>(originItem)\n        val newNode = diffMapper.valueToTree<JsonNode>(newItem)\n        val diff = JsonDiff.asJson(originalNode, newNode)\n        return when {\n            diff.size() > 0 -> {\n                diff.mapNotNull { diffNode ->\n                    val (path, originValue, newValue) = extractDiffValue(diffNode, originalNode, newNode)\n                    Pair(\n                        first = path,\n                        second = DiffValue(originValue, newValue)\n                    )\n                }\n                    .toMap()\n            }\n            else -> emptyMap()\n        }\n    }\n\n    fun <T, K, S> calculateDifferences(\n        originItems: List<T>,\n        newItems: List<T>,\n        associateByKey: (T) -> K,\n        groupByKey: (T) -> S\n    ): Map<S, DiffValueTracker> {\n        val originalAssociate = originItems.associateBy(associateByKey)\n        val newAssociate = newItems.associateBy(associateByKey)\n        val changes = newAssociate.flatMap { (id, newItem) ->\n            val originalItem = originalAssociate[id]\n            when {\n                originalItem != null -> {\n                    val originalNode = diffMapper.valueToTree<JsonNode>(originalItem)\n                    val newNode = diffMapper.valueToTree<JsonNode>(newItem)\n                    val diffNode = JsonDiff.asJson(originalNode, newNode)\n\n                    when {\n                        diffNode.size() > 0 -> {\n                            diffNode.mapNotNull { node ->\n                                val (path, originValue, newValue) = extractDiffValue(node, originalNode, newNode)\n\n                                Triple(\n                                    first = groupByKey(newItem),\n                                    second = path,\n                                    third = DiffValue(origin = originValue, new = newValue)\n                                )\n                            }\n                        }\n                        else -> emptyList()\n                    }\n                }\n                else -> emptyList()\n            }\n        }\n\n        return changes\n            .groupBy({ it.first }, { it.second to it.third })\n            .mapValues { (_, value) -> value.toMap() }\n    }\n\n    private fun extractDiffValue(node: JsonNode, originalNode: JsonNode, newNode: JsonNode): DiffTriple {\n        val path = node.get(\"path\").asText().removePrefix(\"/\")\n        val originValue = originalNode.at(\"/$path\").asText()\n        val newValue = newNode.at(\"/$path\").asText()\n        return DiffTriple(path, originValue, newValue)\n    }\n}\n\ndata class DiffValue<out A, out B>(\n    val origin: A,\n    val new: B\n)\n\nclass ObjectIdSerializer : JsonSerializer<ObjectId>() {\n    override fun serialize(value: ObjectId, gen: JsonGenerator, serializers: SerializerProvider) {\n        gen.writeString(value.toString())\n    }\n}\n\n\n3. 코드 상세 설명#\n3.1 Jackson ObjectMapper 설정#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\nprivate val diffMapper = jacksonObjectMapper()\n    .apply {\n        registerModules(\n            SimpleModule().apply {\n                propertyNamingStrategy = PropertyNamingStrategies.SNAKE_CASE\n                addSerializer(ObjectId::class.java, ObjectIdSerializer())\n            }\n        )\n    }\n\n\nSnake Case 변환: Kotlin의 camelCase 필드명을 JSON의 snake_case로 자동 변환합니다\nObjectId 직렬화: MongoDB의 ObjectId를 문자열로 변환하는 커스텀 Serializer를 등록합니다\n이를 통해 productName → product_name으로 자동 변환되어 일관된 필드명으로 추적할 수 있습니다\n\n3.2 calculateDifference 함수#\n단일 객체 간의 차이를 계산하는 핵심 함수입니다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\nfun <T> calculateDifference(\n    originItem: T,\n    newItem: T\n): DiffValueTracker {\n    // 1. Kotlin 객체를 JsonNode로 변환\n    val originalNode = diffMapper.valueToTree<JsonNode>(originItem)\n    val newNode = diffMapper.valueToTree<JsonNode>(newItem)\n    \n    // 2. JsonDiff로 차이 계산\n    val diff = JsonDiff.asJson(originalNode, newNode)\n    \n    // 3. 차이가 있으면 변경 정보 추출\n    return when {\n        diff.size() > 0 -> {\n            diff.mapNotNull { diffNode ->\n                val (path, originValue, newValue) = extractDiffValue(diffNode, originalNode, newNode)\n                Pair(\n                    first = path,\n                    second = DiffValue(originValue, newValue)\n                )\n            }.toMap()\n        }\n        else -> emptyMap()\n    }\n}\n\n\n동작 과정:\n\n객체를 JsonNode로 변환: Kotlin 객체를 Jackson의 JsonNode로 변환하여 JSON 구조로 다룰 수 있게 합니다\nJsonDiff 계산: JsonDiff.asJson()을 사용하여 두 JsonNode 간의 차이를 계산합니다\n변경 정보 추출: 각 diff node에서 경로(path), 이전 값(originValue), 새 값(newValue)을 추출합니다\n결과 반환: Map<String, DiffValue> 형태로 반환합니다\n\nKey: 필드 경로 (예: customer/contact/address/street)\nValue: DiffValue(origin, new) 객체\n\n3.3 extractDiffValue 함수#\n\n\n1\n2\n3\n4\n5\n6\n\nprivate fun extractDiffValue(node: JsonNode, originalNode: JsonNode, newNode: JsonNode): DiffTriple {\n    val path = node.get(\"path\").asText().removePrefix(\"/\")\n    val originValue = originalNode.at(\"/$path\").asText()\n    val newValue = newNode.at(\"/$path\").asText()\n    return DiffTriple(path, originValue, newValue)\n}\n\n\npath 추출: diff node에서 변경된 필드의 경로를 추출합니다 (예: /customer/contact/address/street)\n슬래시 제거: 경로 앞의 /를 제거하여 깔끔한 key로 만듭니다\n값 추출: JsonNode의 at() 메서드로 해당 경로의 값을 추출합니다\nTriple 반환: (경로, 이전값, 새값)을 하나의 Triple로 반환합니다\n\n3.4 calculateDifferences 함수 (복수 객체 처리)#\n여러 객체를 비교할 때 사용하는 함수입니다.\n\n\n1\n2\n3\n4\n5\n6\n\nfun <T, K, S> calculateDifferences(\n    originItems: List<T>,\n    newItems: List<T>,\n    associateByKey: (T) -> K,      // 매칭용 키 (예: ID)\n    groupByKey: (T) -> S            // 그룹화용 키\n): Map<S, DiffValueTracker>\n\n\nassociateByKey: 원본과 새로운 데이터를 매칭하기 위한 키 (예: orderId, productId)\ngroupByKey: 결과를 그룹화하기 위한 키\n여러 객체를 한 번에 처리하고 각 객체별 변경사항을 그룹화하여 반환합니다\n\n테스트 코드로 검증하기#\n다양한 케이스를 테스트하여 구현이 올바르게 동작하는지 확인했습니다.\n1. 단일 필드 변경 감지#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n@Test\nfun `calculateDifference - 단일 객체의 변경 사항을 감지한다`() {\n    // Given\n    val originalProduct = Product(\n        productId = \"PROD001\",\n        productName = \"노트북\",\n        category = Category(\"전자제품\", \"컴퓨터\")\n    )\n    val newProduct = Product(\n        productId = \"PROD001\",\n        productName = \"울트라 노트북\",\n        category = Category(\"전자제품\", \"컴퓨터\")\n    )\n\n    // When\n    val result = DiffComparisonManager.calculateDifference(originalProduct, newProduct)\n\n    // Then\n    then(result).hasSize(1)\n    then(result[\"product_name\"]).isNotNull\n    then(result[\"product_name\"]?.origin).isEqualTo(\"노트북\")\n    then(result[\"product_name\"]?.new).isEqualTo(\"울트라 노트북\")\n}\n\n\n결과:\n\n\n1\n2\n3\n4\n5\n6\n\n{\n  \"product_name\": {\n    \"origin\": \"노트북\",\n    \"new\": \"울트라 노트북\"\n  }\n}\n\n\n2. 중첩된 객체의 변경 감지#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n@Test\nfun `calculateDifference - 중첩된 객체의 변경 사항을 감지한다`() {\n    // Given\n    val originalProduct = Product(\n        productId = \"PROD001\",\n        productName = \"노트북\",\n        category = Category(\"전자제품\", \"컴퓨터\")\n    )\n    val newProduct = Product(\n        productId = \"PROD001\",\n        productName = \"노트북\",\n        category = Category(\"전자제품\", \"노트북\")\n    )\n\n    // When\n    val result = DiffComparisonManager.calculateDifference(originalProduct, newProduct)\n\n    // Then\n    then(result).hasSize(1)\n    then(result[\"category/sub_category\"]).isNotNull\n    then(result[\"category/sub_category\"]?.origin).isEqualTo(\"컴퓨터\")\n    then(result[\"category/sub_category\"]?.new).isEqualTo(\"노트북\")\n}\n\n\n결과:\n\n\n1\n2\n3\n4\n5\n6\n\n{\n  \"category/sub_category\": {\n    \"origin\": \"컴퓨터\",\n    \"new\": \"노트북\"\n  }\n}\n\n\n중첩 객체의 필드는 category/sub_category 형태로 경로가 명확히 표시됩니다.\n슬래시(/)를 구분자로 사용하여 객체의 계층 구조를 표현하므로, 어떤 깊이의 중첩 객체라도 경로만으로 정확한 위치를 파악할 수 있습니다.\n3. 여러 필드 동시 변경 감지#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n@Test\nfun `calculateDifference - 여러 필드의 변경 사항을 감지한다`() {\n    // Given\n    val originalItem = Item(\n        product = Product(\"PROD001\", \"노트북\", Category(\"전자제품\", \"컴퓨터\")),\n        quantity = 2,\n        price = 1500000\n    )\n    val newItem = Item(\n        product = Product(\"PROD001\", \"울트라 노트북\", Category(\"전자제품\", \"컴퓨터\")),\n        quantity = 3,\n        price = 1400000\n    )\n\n    // When\n    val result = DiffComparisonManager.calculateDifference(originalItem, newItem)\n\n    // Then\n    then(result).hasSize(3)\n    then(result[\"product/product_name\"]?.origin).isEqualTo(\"노트북\")\n    then(result[\"product/product_name\"]?.new).isEqualTo(\"울트라 노트북\")\n    then(result[\"quantity\"]?.origin).isEqualTo(\"2\")\n    then(result[\"quantity\"]?.new).isEqualTo(\"3\")\n    then(result[\"price\"]?.origin).isEqualTo(\"1500000\")\n    then(result[\"price\"]?.new).isEqualTo(\"1400000\")\n}\n\n\n결과:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n{\n  \"product/product_name\": {\n    \"origin\": \"노트북\",\n    \"new\": \"울트라 노트북\"\n  },\n  \"quantity\": {\n    \"origin\": \"2\",\n    \"new\": \"3\"\n  },\n  \"price\": {\n    \"origin\": \"1500000\",\n    \"new\": \"1400000\"\n  }\n}\n\n\n한 번의 비교로 여러 필드의 변경사항을 모두 추적할 수 있으며, 각 필드별로 이전 값과 새로운 값이 명확하게 구분됩니다.\n4. 깊은 중첩 구조 변경 감지#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n@Test\nfun `calculateDifference - 깊게 중첩된 객체의 변경 사항을 감지한다`() {\n    // Given\n    val originalOrder = Order(\n        orderId = \"ORD123\",\n        customer = Customer(\n            customerId = \"CUST001\",\n            name = \"홍길동\",\n            contact = Contact(\n                email = \"hong@example.com\",\n                phone = \"010-1234-5678\",\n                address = Address(\"서울특별시 종로구\", \"서울\", \"03001\", \"대한민국\")\n            )\n        ),\n        items = emptyList(),\n        payment = Payment(\"신용카드\", \"TXN001\", \"완료\")\n    )\n    val newOrder = Order(\n        orderId = \"ORD123\",\n        customer = Customer(\n            customerId = \"CUST001\",\n            name = \"홍길동\",\n            contact = Contact(\n                email = \"hong@example.com\",\n                phone = \"010-1234-5678\",\n                address = Address(\"서울특별시 강남구\", \"서울\", \"06001\", \"대한민국\")\n            )\n        ),\n        items = emptyList(),\n        payment = Payment(\"신용카드\", \"TXN001\", \"완료\")\n    )\n\n    // When\n    val result = DiffComparisonManager.calculateDifference(originalOrder, newOrder)\n\n    // Then\n    then(result).hasSize(2)\n    then(result[\"customer/contact/address/street\"]?.origin).isEqualTo(\"서울특별시 종로구\")\n    then(result[\"customer/contact/address/street\"]?.new).isEqualTo(\"서울특별시 강남구\")\n    then(result[\"customer/contact/address/zip_code\"]?.origin).isEqualTo(\"03001\")\n    then(result[\"customer/contact/address/zip_code\"]?.new).isEqualTo(\"06001\")\n}\n\n\n결과:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n{\n  \"customer/contact/address/street\": {\n    \"origin\": \"서울특별시 종로구\",\n    \"new\": \"서울특별시 강남구\"\n  },\n  \"customer/contact/address/zip_code\": {\n    \"origin\": \"03001\",\n    \"new\": \"06001\"\n  }\n}\n\n\n4단계 깊이의 중첩 구조(customer/contact/address/street)도 정확히 추적합니다.\n5. Null 처리#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n@Test\nfun `calculateDifference - null에서 값으로 변경을 감지한다`() {\n    // Given\n    data class TestData(val name: String, val description: String?)\n    val original = TestData(\"테스트\", null)\n    val new = TestData(\"테스트\", \"설명 추가\")\n\n    // When\n    val result = DiffComparisonManager.calculateDifference(original, new)\n\n    // Then\n    then(result).hasSize(1)\n    then(result[\"description\"]?.origin).isEmpty()\n    then(result[\"description\"]?.new).isEqualTo(\"설명 추가\")\n}\n\n\n결과:\n\n\n1\n2\n3\n4\n5\n6\n\n{\n  \"description\": {\n    \"origin\": \"\",\n    \"new\": \"설명 추가\"\n  }\n}\n\n\n5-2. 값에서 null로 변경#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n@Test\nfun `calculateDifference - 값에서 null로 변경을 감지한다`() {\n    // Given\n    data class TestData(val name: String, val description: String?)\n    val original = TestData(\"테스트\", \"기존 설명\")\n    val new = TestData(\"테스트\", null)\n\n    // When\n    val result = DiffComparisonManager.calculateDifference(original, new)\n\n    // Then\n    then(result).hasSize(1)\n    then(result[\"description\"]?.origin).isEqualTo(\"기존 설명\")\n    then(result[\"description\"]?.new).isEmpty()\n}\n\n\n결과:\n\n\n1\n2\n3\n4\n5\n6\n\n{\n  \"description\": {\n    \"origin\": \"기존 설명\",\n    \"new\": \"\"\n  }\n}\n\n\nnull 값의 변경도 정확하게 추적되며, null은 빈 문자열로 표시됩니다.\n6. 변경 없는 경우#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n@Test\nfun `calculateDifference - 동일한 객체는 변경 사항이 없다`() {\n    // Given\n    val product = Product(\n        productId = \"PROD001\",\n        productName = \"노트북\",\n        category = Category(\"전자제품\", \"컴퓨터\")\n    )\n\n    // When\n    val result = DiffComparisonManager.calculateDifference(product, product)\n\n    // Then\n    then(result).isEmpty()\n}\n\n\n결과:\n\n\n1\n\n{}\n\n\n동일한 객체를 비교하면 빈 Map이 반환되어, 불필요한 변경 이력이 저장되지 않습니다.\n7. 실제 주문 데이터 변경 추적#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n@Test\nfun `주문 데이터의 필드 변경을 확인한다`() {\n    // Given\n    val originalOrder: Order = diffMapper.readValue(readFile(\"/diff-origin.json\"))\n    val newOrder: Order = diffMapper.readValue(readFile(\"/diff-new.json\"))\n\n    // When\n    val result = DiffComparisonManager.calculateDifferences(\n        originItems = listOf(originalOrder),\n        newItems = listOf(newOrder),\n        associateByKey = { it.orderId },\n        groupByKey = { it.orderId }\n    )\n\n    // Then\n    val differences = result[\"ORD123456\"]\n    then(differences).isNotNull\n    then(differences!!.size).isEqualTo(3)\n    then(differences[\"customer/contact/address/street\"]?.origin).isEqualTo(\"서울특별시 종로구\")\n    then(differences[\"customer/contact/address/street\"]?.new).isEqualTo(\"서울특별시 강남구\")\n    then(differences[\"items/0/price\"]?.origin).isEqualTo(\"1500000\")\n    then(differences[\"items/0/price\"]?.new).isEqualTo(\"1400000\")\n    then(differences[\"payment/transaction_id\"]?.origin).isEqualTo(\"TXN987654321\")\n    then(differences[\"payment/transaction_id\"]?.new).isEqualTo(\"TXN987654322\")\n}\n\n\n결과:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n{\n  \"ORD123456\": {\n    \"customer/contact/address/street\": {\n      \"origin\": \"서울특별시 종로구\",\n      \"new\": \"서울특별시 강남구\"\n    },\n    \"items/0/price\": {\n      \"origin\": \"1500000\",\n      \"new\": \"1400000\"\n    },\n    \"payment/transaction_id\": {\n      \"origin\": \"TXN987654321\",\n      \"new\": \"TXN987654322\"\n    }\n  }\n}\n\n\n실제 JSON 파일에서 읽어온 복잡한 주문 데이터도 정확하게 변경사항을 추적합니다. calculateDifferences 함수는 여러 객체를 처리하고 그룹화된 결과를 반환합니다.\n활용 방안#\n1. 승인 프로세스에 활용#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\ndata class ApprovalRequest(\n    @Id\n    val id: ObjectId = ObjectId(),\n    val requestType: String,\n    val targetId: String,\n    val changes: DiffValueTracker,       // 어떤 필드가 어떻게 변경될지\n    val requestedBy: String,\n    val status: ApprovalStatus = ApprovalStatus.PENDING\n)\n\n// 승인 요청 생성\nfun createFeeChangeApproval(merchantId: String, currentFee: MerchantFee, newFee: MerchantFee, userId: String) {\n    val changes = DiffComparisonManager.calculateDifference(currentFee, newFee)\n    \n    val approvalRequest = ApprovalRequest(\n        requestType = \"MERCHANT_FEE_CHANGE\",\n        targetId = merchantId,\n        changes = changes,\n        requestedBy = userId\n    )\n    approvalRequestRepository.save(approvalRequest)\n    \n    // 승인권자에게 알림 전송\n    notifyApprovers(approvalRequest)\n}\n\n\n2. 감사 로그 및 모니터링#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n// 중요 필드 변경 모니터링\nfun monitorCriticalChanges(changes: DiffValueTracker, entityType: String) {\n    val criticalFields = setOf(\n        \"payment/method\",\n        \"customer/contact/address/street\",\n        \"items/0/price\"\n    )\n    \n    changes.keys.filter { it in criticalFields }\n        .forEach { field ->\n            val change = changes[field]!!\n            logger.warn(\n                \"Critical field changed in $entityType: $field - \" +\n                \"from '${change.origin}' to '${change.new}'\"\n            )\n            // 알림 전송, 메트릭 기록 등\n        }\n}\n\n\n장점과 고려사항#\n장점#\n\n자동화: 수동으로 변경 필드를 비교할 필요 없이 자동으로 추적합니다\n타입 안정성: Kotlin의 제네릭을 활용하여 타입 안전하게 구현됩니다\n중첩 객체 지원: 깊은 중첩 구조도 경로로 명확히 표시합니다\n저장소 독립성: 특정 데이터베이스에 의존하지 않는 순수한 로직으로 구현되어, MongoDB, PostgreSQL, MySQL 등 어떤 저장소에도 저장할 수 있습니다\n가독성: 변경 내역이 명확한 key-value 형태로 저장됩니다\n\n고려사항#\n\n성능: 큰 객체나 대량의 데이터를 비교할 때는 성능을 고려해야 합니다\n배열 처리: 배열의 순서가 바뀌면 전체가 변경된 것으로 인식될 수 있습니다\n저장 공간: 모든 변경 이력을 저장하면 데이터가 빠르게 증가할 수 있습니다\n민감 정보: 비밀번호 등 민감한 정보는 이력에서 제외하는 로직이 필요합니다\n\n마치며#\n이번 포스트에서는 Kotlin과 Jackson, zjsonpatch를 활용하여 필드 단위 변경 이력 추적 시스템을 구현하는 방법을 알아보았습니다.\n복잡한 중첩 객체의 변경사항을 자동으로 추적하고 명확한 경로로 표시하는 이 시스템은 다음과 같은 상황에서 유용하게 활용할 수 있습니다:\n\n주문/결제 정보 변경 이력 추적\n가맹점 정보 변경 승인 프로세스\n감사(Audit) 로그 시스템\n데이터 동기화 및 충돌 감지\n\n실제 프로덕션 환경에 적용할 때는 성능과 저장 공간, 민감 정보 처리 등을 충분히 고려하여 상황에 맞게 커스터마이징하시기 바랍니다.",
        "comments": "https://cheese10yun.github.io/diff-history-part-1/#disqus_thread",
        "content": "객체의 필드 변경 이력을 자동으로 추적해, 무엇이 어떻게 바뀌었는지 명확히 기록합니다.",
        "contentSnippet": "객체의 필드 변경 이력을 자동으로 추적해, 무엇이 어떻게 바뀌었는지 명확히 기록합니다.",
        "guid": "https://cheese10yun.github.io/diff-history-part-1/",
        "categories": [
          {
            "_": "Kotlin",
            "$": {
              "domain": "https://cheese10yun.github.io/tags/Kotlin/"
            }
          },
          {
            "_": "Guide",
            "$": {
              "domain": "https://cheese10yun.github.io/tags/Guide/"
            }
          }
        ],
        "isoDate": "2026-02-15T15:00:00.000Z"
      }
    ]
  },
  {
    "name": "구자철",
    "category": "개인",
    "posts": []
  },
  {
    "name": "FSS",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권동준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김용일",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김도균",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민석",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권윤학",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강성훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김만수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "엘키",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김슬기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김광현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성빈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강명훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민장",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성중",
    "category": "개인",
    "posts": []
  },
  {
    "name": "구교준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김덕기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "고명환",
    "category": "개인",
    "posts": [
      {
        "creator": "고명환",
        "title": "2026 카페 창업 트렌드에 따른 창업 준비 전략 - 창업(소상공인)",
        "link": "https://brunch.co.kr/@@LOc/329",
        "pubDate": "Thu, 19 Feb 2026 09:26:51 GMT",
        "author": "고명환",
        "content": "1. 도입 : 커피가 맛있는 카페? 이제는 당연히 기본값입니다.  &quot;우리 동네에 카페가 벌써 5개인데 내가 차려도 될까?&quot; 예비 사장님들의 가장 큰 고민일 것입니다. 2026년 현재, 단순히 커피가 맛있는 것만으로는 고객을 부를 수 없습니다. 골목 깊숙이 있는 우리 매장까지 손님이 찾아오게 만들려면 '목적성 메뉴(디저트)'와 '압도적 운영 효율(스마트 시스<img src= \"https://img1.kakaocdn.net/thumb/R1280x0/?fname=http%3A%2F%2Ft1.kakaocdn.net%2Fbrunch%2Fservice%2Fuser%2FLOc%2Fimage%2FixPjP8D4MNrPAM90ANfeft3qV7k.jpg\" width=\"500\" />",
        "contentSnippet": "1. 도입 : 커피가 맛있는 카페? 이제는 당연히 기본값입니다.  \"우리 동네에 카페가 벌써 5개인데 내가 차려도 될까?\" 예비 사장님들의 가장 큰 고민일 것입니다. 2026년 현재, 단순히 커피가 맛있는 것만으로는 고객을 부를 수 없습니다. 골목 깊숙이 있는 우리 매장까지 손님이 찾아오게 만들려면 '목적성 메뉴(디저트)'와 '압도적 운영 효율(스마트 시스",
        "guid": "https://brunch.co.kr/@@LOc/329",
        "isoDate": "2026-02-19T09:26:51.000Z"
      }
    ]
  },
  {
    "name": "강성희",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강병수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김봉현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강형석",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수로",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강미경",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강진우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권민재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권태관",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김도곤",
    "category": "개인",
    "posts": []
  },
  {
    "name": "칡토스의 게임 개발",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김선철",
    "category": "개인",
    "posts": []
  },
  {
    "name": "프리웨어 이야기",
    "category": "개인",
    "posts": [
      {
        "creator": "어떤오후의 프리웨어 이야기",
        "title": "프롬프트의 시작과 끝, 이 3가지 앱으로 종결",
        "link": "https://muzbox.tistory.com/483709",
        "pubDate": "Sun, 15 Feb 2026 17:04:15 +0900",
        "author": "어떤오후의 프리웨어 이야기",
        "comments": "https://muzbox.tistory.com/483709#entry483709comment",
        "content": "<div style=\"font-family: 'Noto Sans KR', sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; font-size: 16px; box-sizing: border-box; color: #3c4043;\">\n<div style=\"background-color: #e8f4fd; padding: 15px; border-radius: 8px; font-style: italic; margin-bottom: 25px; font-size: 15px;\">AI와의 대화가 늘 엉뚱한 결과로 이어져 답답하셨나요? 혹은 완벽한 프롬프트를 만들고 싶지만 어디서부터 시작해야 할지 막막하셨다면, 이 글이 해답이 될 거예요. AI 활용의 새로운 기준을 제시할 세 가지 혁신적인 앱으로 프롬프트 작성의 모든 고민을 끝내고, 여러분도 진정한 AI 프롬프트 마스터가 되어 보세요!</div>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">여러분, 혹시 AI에게 질문했는데 영 엉뚱한 답변이 나와서 황당했던 경험, 저만 있는 건 아니겠죠? 아니면, 정말 완벽한 결과물을 기대하면서 프롬프트를 만들고 싶은데, 대체 어디서부터 손을 대야 할지 감조차 안 잡히셨던 적도 분명 있으실 거예요. 오늘은 이런 고민을 시원하게 해결해 줄 마법 같은 세 가지 도구를 소개해 드리려고 해요. 이 앱들만 제대로 활용하신다면, 프롬프트 전문가의 영역이 더 이상 꿈이 아니랍니다!</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"download.jpg\" data-origin-width=\"1200\" data-origin-height=\"1200\"><span data-url=\"https://blog.kakaocdn.net/dn/bKUejs/dJMcafle932/zm7oXgOk4nRfsKT5Gjvw10/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/bKUejs/dJMcafle932/zm7oXgOk4nRfsKT5Gjvw10/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/bKUejs/dJMcafle932/zm7oXgOk4nRfsKT5Gjvw10/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbKUejs%2FdJMcafle932%2Fzm7oXgOk4nRfsKT5Gjvw10%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"홀로그램 인터페이스로 AI 프롬프트 요소를 조작하는 손. AI Prompt Studio, Prompt Equalizer Pro, 프롬프트 마스터와 같은 혁신적인 도구로 프롬프트 엔지니어링을 간소화하는 미래 지향적인 컨셉을 시각적으로 표현.\" loading=\"lazy\" width=\"1200\" height=\"1200\" data-filename=\"download.jpg\" data-origin-width=\"1200\" data-origin-height=\"1200\"/></span></figure>\n\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">제가 직접 경험해 보니, AI는 정말 질문을 어떻게 하느냐에 따라 답변의 질이 하늘과 땅 차이로 달라져요. 단순히 \"블로그 글 써줘\"라고 짧게 물으면, 어디서 많이 본 듯한 평범한 결과물만 툭 튀어나오기 일쑤죠. 그런데 만약 \"당신은 10년 경력의 여행 작가입니다. 20대 여성을 위한 제주도 3박 4일 여행기를 감성적인 톤으로 작성해주세요\"라고 디테일하게 요청한다면? 와, 이건 정말 완전히 다른 수준의, 심지어 감동적인 결과물이 나올 때도 있더라고요.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">하지만 문제는 매번 이렇게 완벽하고 정교한 프롬프트를 직접 만드는 게 너무 어렵고 귀찮다는 점이에요. 특히 \"AI한테 어떤 역할을 부여해야 하지?\", \"어떤 조건을 추가해야 가장 좋은 결과가 나올까?\" 같은 고민을 하다 보면 시간이 정말이지 눈 깜짝할 사이에 흘러가 버리죠. 그래서 오늘 제가 소개해 드릴 이 세 가지 앱이 필요한 겁니다. 각각 다른 역할로 여러분의 프롬프트 작성 전 과정을 빈틈없이 도와줄 거예요. 마치 요리할 때 레시피를 짜주는 셰프, 맛을 미세하게 조절하는 조미료, 그리고 나만의 특별한 레시피를 정리해 주는 요리책 같다고나 할까요?</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>1. AI Prompt Studio: 메타 프롬프트 설계의 마법사  &zwj;♀️</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">여러분, 혹시 ChatGPT나 제미나이를 쓸 때 \"제주도 여행 블로그 글 하나 써줘\"라고 짧게 물어보고 나서 결과가 너무 뻔해서 실망하신 적 있으실 거예요. 사실 이건 AI의 성능 문제라기보다는, 질문하는 방식에 아쉬움이 있었을 가능성이 큽니다. AI는 질문을 어떻게 하느냐에 따라 답변의 품질이 정말이지 하늘과 땅 차이로 달라지거든요.</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"AI PROMPT STUDIO.jpg\" data-origin-width=\"940\" data-origin-height=\"843\"><span data-url=\"https://blog.kakaocdn.net/dn/TbB2b/dJMcachLbTM/hPG88iOLiC02aTJ31Dtxw1/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/TbB2b/dJMcachLbTM/hPG88iOLiC02aTJ31Dtxw1/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/TbB2b/dJMcachLbTM/hPG88iOLiC02aTJ31Dtxw1/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FTbB2b%2FdJMcachLbTM%2FhPG88iOLiC02aTJ31Dtxw1%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"AI Prompt Studio\" loading=\"lazy\" width=\"940\" height=\"843\" data-filename=\"AI PROMPT STUDIO.jpg\" data-origin-width=\"940\" data-origin-height=\"843\"/></span></figure>\n\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">하지만 문제는 매번 \"너는 10년 차 작가야, 타겟은 누구고, 형식은 마크다운으로 해줘\" 같은 복잡한 프롬프트를 직접 짜기가 너무 어렵고 번거롭다는 거죠. 바로 이 번거로움을 해결해 주는 도구가 첫 번째로 소개해 드릴 <b>'AI Prompt Studio'</b>입니다. 이 앱은 단순한 질문이 아닌, LLM(거대 언어 모델)의 성능을 100% 끌어낼 수 있는 '메타 프롬프트(Meta Prompt)'를 전문적으로 설계해주는 앱이에요.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; border-bottom: 2px solid #e8f0fe; padding-bottom: 5px;\" data-ke-size=\"size23\">주요 기능  </h3>\n<ul style=\"list-style-type: disc; margin-left: 20px; margin-bottom: 20px; color: #3c4043;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 10px;\"><b>빠른 프롬프트 생성:</b> 복잡한 설정 없이 목표만 입력하면 버튼 하나로 강력한 프롬프트를 즉시 만들어줍니다. 바쁠 때 정말 최고죠.</li>\n<li style=\"margin-bottom: 10px;\"><b>역질문 유도:</b> 사용자의 요청이 모호할 때, AI가 스스로 부족한 정보를 파악하여 \"더 나은 답변을 위해 몇 가지 질문을 해도 될까요?\" 하고 되묻도록 유도합니다. 덕분에 불필요한 시행착오를 줄일 수 있어요.</li>\n<li style=\"margin-bottom: 10px;\"><b>페르소나 부여:</b> 목표 달성에 가장 적합한 전문가 역할을 AI가 스스로 설정하고 연기하도록 지시합니다. 예를 들어, \"최고의 마케터\" 역할을 부여해서 결과물의 전문성을 확 끌어올릴 수 있어요.</li>\n<li style=\"margin-bottom: 10px;\"><b>비판적 개선:</b> 입력한 내용을 비판적으로 분석해서 논리적 허점이나 모호한 부분을 찾아내고, 더 완벽한 구조로 즉시 수정해 줍니다. 스스로 고쳐나가면서 프롬프트 실력이 확 늘 거예요.</li>\n</ul>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; border-bottom: 2px solid #e8f0fe; padding-bottom: 5px;\" data-ke-size=\"size23\">다섯 가지 전문 모드  ️</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">이 앱의 가장 큰 특징이자 핵심은 바로 목적에 따라 골라 쓸 수 있는 다섯 가지 전문 모드를 지원한다는 점이에요. 여러분의 상황에 맞춰 선택하기만 하면 됩니다.</p>\n<ul style=\"list-style-type: disc; margin-left: 20px; margin-bottom: 20px; color: #3c4043;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 10px;\"><b>RTSC 마스터 모드:</b> 역할(Role), 작업(Task), 구조(Structure), 체인(Chain)의 앞 글자를 딴 모드인데요. 예를 들어 \"유튜브 쇼츠 대본 생성\"이라고 목표만 입력해 보세요. AI가 알아서 \"숏폼 바이럴 마케팅 디렉터\"라는 역할을 부여하고, 쇼츠 성공 공식을 완벽하게 구조화한 프롬프트를 짜줍니다. 정교하고 복잡한 창작물을 만들 때 정말 최고입니다.</li>\n<li style=\"margin-bottom: 10px;\"><b>비즈니스 프레임워크 모드:</b> 직장인분들이라면 이 모드를 강력 추천합니다! 기획안이나 보고서 쓸 때 막막하셨던 경험 다들 있으시죠? 코스타(COSTAR)나 프렙(PREP) 같은 검증된 비즈니스 틀을 자동으로 적용해서 아주 깔끔하고 논리적인 업무용 문서를 만들어 줘요. 정말이지 퇴근 시간을 앞당겨 주는 효자 앱이라고 할 수 있죠.</li>\n<li style=\"margin-bottom: 10px;\"><b>프롬프트 아키텍트:</b> 정말 귀찮을 때, 이 모드가 마법을 부려줍니다. \"복잡한 법률 문서를 쉽게 요약해 주는 AI 에이전트를 설계해 줘\"라고 딱 한 줄만 써보세요. 그럼 AI가 필요한 제약 조건과 작업 프로세스를 알아서 추론하고 설계해 냅니다. 마치 옆에 유능한 비서가 있는 것 같아요.</li>\n<li style=\"margin-bottom: 10px;\"><b>버텍스 AI XML 모드:</b> 기업용 데이터 처리나 구조화된 데이터가 필요할 때 빛을 발하는 모드예요. 결과물을 깔끔한 XML 태그 구조로 만들어주기 때문에 복잡한 데이터를 분류하거나 추출할 때 아주 정확하고 표준화된 답변을 얻을 수 있습니다. 데이터 분석가나 개발자분들께 특히 유용할 거예요.</li>\n<li style=\"margin-bottom: 10px;\"><b>CoT(Chain-of-Thought) 모드:</b> 논리적인 사고가 필요할 때 활용해 보세요. AI가 사람처럼 단계별로 추론하며 문제를 풀도록 유도하는 방식인데요. 덕분에 수학 문제나 복잡한 논리 과제에서 AI가 엉뚱한 답을 내놓는 현상을 획기적으로 줄여줍니다. 이제는 그저 \"무엇을 할지\"만 결정하시면 됩니다!</li>\n</ul>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">솔직히 말하면, 이 앱은 거의 '사기 앱'이라고 불러도 될 정도예요. 왜냐하면 프롬프트 엔지니어들이 밤새워 연구하고 강의에서 수십만 원을 주고 배워야 하는 복잡한 기법들을, 버튼 클릭 몇 번만으로 누구나 완벽하게 구현할 수 있게 만들어줬거든요. 저도 처음에 써보고 정말 깜짝 놀랐습니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">많은 분들이 AI를 쓰면서 \"AI가 별로야\"라고 아쉬워하시는데, 제 경험상 사실은 질문을 제대로 못한 경우가 대부분입니다. 그런데 이 앱을 쓰면 그런 문제가 완전히 해결돼요. 시작하는 방법도 아주 간단합니다. 브라우저에서 접속한 뒤 우측 상단 톱니바퀴 아이콘을 클릭해서, 구글 AI 스튜디오에서 무료로 발급받은 '제미나이 API 키'만 넣어주면 준비 끝입니다. 자, 이제 '시작하기' 버튼을 누르고 여러분만의 완벽한 프롬프트를 직접 설계해 보시기 바랍니다!</p>\n<figure id=\"og_1771142603506\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"Meta Prompt Master\" data-og-description=\"\" data-og-host=\"ai-prompt-studio-v2-basic.vercel.app\" data-og-source-url=\"https://ai-prompt-studio-v2-basic.vercel.app/\" data-og-url=\"https://ai-prompt-studio-v2-basic.vercel.app/\" data-og-image=\"\"><a href=\"https://ai-prompt-studio-v2-basic.vercel.app/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://ai-prompt-studio-v2-basic.vercel.app/\">\n<div class=\"og-image\" style=\"background-image: url();\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">Meta Prompt Master</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">&nbsp;</p>\n<p class=\"og-host\" data-ke-size=\"size16\">ai-prompt-studio-v2-basic.vercel.app</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<div style=\"background-color: #e8f4fd; border-left: 4px solid #1a73e8; padding: 15px; margin: 20px 0; border-radius: 0 8px 8px 0;\">  <b>잠깐! API 키 설정 팁:</b> AI Prompt Studio를 시작하기 전에, 구글 AI 스튜디오(<a style=\"color: #1a73e8; text-decoration: none;\" href=\"https://aistudio.google.com/\" target=\"_blank\" rel=\"noopener\">aistudio.google.com</a>)에서 무료로 제미나이 API 키를 발급받으세요. 이 키만 있으면 AI Prompt Studio의 모든 기능을 막힘없이 활용할 수 있습니다. 설정 과정도 매우 간단하니 걱정 마세요!</div>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>2. Prompt Equalizer Pro: 글의 뉘앙스를 조율하는 이퀄라이저  </b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">자, 다음으로 소개해 드릴 앱은 정말이지 흥미로운 도구입니다. 바로 <b>'Prompt Equalizer Pro'</b>인데요. 여러분, 혹시 음악 들으실 때 취향에 맞춰 저음이나 고음을 조절하는 '이퀄라이저' 써보셨나요? 이 앱은 그 원리를 글쓰기에 그대로 가져왔습니다. 앞서 보신 'AI Prompt Studio'가 프롬프트를 '만드는' 도구였다면, 이 앱은 이미 써놓은 글의 뉘앙스를 아주 정밀하게 튜닝하고 조정하는 도구라고 보시면 됩니다. 개인적으로 써보고 정말 감탄했던 앱이에요.</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Prompt Equalizer Pro.jpg\" data-origin-width=\"1898\" data-origin-height=\"914\"><span data-url=\"https://blog.kakaocdn.net/dn/lrWkf/dJMcaaYwUxV/bhHS6KK87Aql61gBR0SIBk/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/lrWkf/dJMcaaYwUxV/bhHS6KK87Aql61gBR0SIBk/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/lrWkf/dJMcaaYwUxV/bhHS6KK87Aql61gBR0SIBk/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FlrWkf%2FdJMcaaYwUxV%2FbhHS6KK87Aql61gBR0SIBk%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"Prompt Equalizer Pro\" loading=\"lazy\" width=\"1898\" height=\"914\" data-filename=\"Prompt Equalizer Pro.jpg\" data-origin-width=\"1898\" data-origin-height=\"914\"/></span></figure>\n\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; border-bottom: 2px solid #e8f0fe; padding-bottom: 5px;\" data-ke-size=\"size23\">핵심 기능: 5가지 슬라이더 ✨</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">앱 화면을 보시면 오른쪽에 다섯 개의 슬라이더가 보일 거예요. 이게 이 앱의 핵심인데요, 마치 음향 이퀄라이저처럼 글의 다양한 요소를 미세하게 조절할 수 있습니다.</p>\n<ul style=\"list-style-type: disc; margin-left: 20px; margin-bottom: 20px; color: #3c4043;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 10px;\"><b>창의성:</b> 슬라이더를 오른쪽으로 쭉 밀면, 평범했던 문장이 아주 은유적이고 풍성하게 변합니다. 반대로 왼쪽으로 낮추면 군더더기 없이 아주 직설적인 문장이 되죠. 소설이나 시를 쓸 때 정말 유용해요.</li>\n<li style=\"margin-bottom: 10px;\"><b>명확성:</b> 글의 이해도를 높이거나 낮출 수 있습니다. 복잡한 내용을 쉽게 풀어 설명할 때 아주 좋고, 때로는 일부러 난해하게 표현할 수도 있습니다.</li>\n<li style=\"margin-bottom: 10px;\"><b>감정의 깊이:</b> 글에 담긴 감정의 농도를 조절합니다. 따뜻하고 감성적인 글부터 차갑고 이성적인 글까지, 원하는 감정 톤을 만들 수 있어요.</li>\n<li style=\"margin-bottom: 10px;\"><b>문장의 길이:</b> 길고 유려한 문장과 짧고 간결한 문장 사이를 오갈 수 있습니다. 저는 주로 짧은 문장으로 속도감을 주고 싶을 때 사용하곤 해요.</li>\n<li style=\"margin-bottom: 10px;\"><b>전문성:</b> 이 기능이 정말 압권인데요! 수치를 높이면 아주 격식 있는 공문서 느낌부터, 낮추면 친구에게 말하는 듯한 편안한 말투까지 자유자재로 오갈 수 있습니다. 독자층에 맞춰 말투를 순식간에 바꾸는 게 가능해요.</li>\n</ul>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">슬라이더 몇 번 움직이는 것만으로 글의 온도와 질감이 완전히 달라지는 게 정말 신기하지 않나요? 처음에 써봤을 때 솔직히 '이게 된다고?' 싶었어요.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; border-bottom: 2px solid #e8f0fe; padding-bottom: 5px;\" data-ke-size=\"size23\">AI 페르소나 기능  </h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">더 놀라운 건 <b>'AI 페르소나'</b> 기능입니다. 앱에서 미리 제공하는 10가지 캐릭터 외에도 여러분이 직접 '냉철한 탐정'이나 '비즈니스 전문가'라고만 입력하면, AI가 그 캐릭터에 딱 맞는 말투와 성격, 그리고 최적의 이퀄라이저 값까지 알아서 세팅해 줍니다. 정말 생각지도 못한 부분에서 감탄하게 되는 기능이었어요.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">사용법도 아주 간단합니다. 처음 설정에서 API 키만 넣고 저장 폴더를 지정해 주세요. 그다음 원하는 말투의 페르소나를 고르거나 직접 만든 페르소나를 선택한 뒤, 슬라이더를 슥슥 조절하고, 원본 글을 넣은 뒤 <b>'명작 생성'</b> 버튼만 누르면 끝입니다. 마음에 들면 바로 파일로 저장하시면 되고요.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">소설을 쓰실 때 캐릭터마다 말투를 다르게 고정하거나, 딱딱한 보고서를 부드럽게 다듬을 때 이만한 도구가 없습니다. 단순히 수치를 높이기보다, 이것저것 만져보면서 여러분만의 '황금 조합'을 한 번 찾아보세요. 저도 저만의 조합을 찾는 재미에 푹 빠져버렸답니다!</p>\n<figure id=\"og_1771142618154\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"Prompt Equalizer Pro\" data-og-description=\"\" data-og-host=\"prompt-equalizer.vercel.app\" data-og-source-url=\"https://prompt-equalizer.vercel.app/\" data-og-url=\"https://prompt-equalizer.vercel.app/\" data-og-image=\"\"><a href=\"https://prompt-equalizer.vercel.app/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://prompt-equalizer.vercel.app/\">\n<div class=\"og-image\" style=\"background-image: url();\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">Prompt Equalizer Pro</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">&nbsp;</p>\n<p class=\"og-host\" data-ke-size=\"size16\">prompt-equalizer.vercel.app</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>3. 프롬프트 마스터: 당신의 프롬프트 라이브러리  </b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">혹시 매번 똑같은 프롬프트를 메모장에 복사해서 쓰거나, 예전에 써둔 정말 좋은 프롬프트를 못 찾아서 헤맨 적 없으신가요? 첫 번째 앱이 프롬프트를 '만들고', 두 번째 앱이 뉘앙스를 '바꿨다면', <b>'프롬프트 마스터'</b>는 자주 쓰는 프롬프트를 완벽하게 '관리하고 재사용하는' 도구입니다. 이 앱 하나면 반복되는 질문을 일일이 타이핑할 필요가 완전히 사라져요. 정말 솔깃하지 않나요?</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Prompt Master.png\" data-origin-width=\"1808\" data-origin-height=\"911\"><span data-url=\"https://blog.kakaocdn.net/dn/U4A5a/dJMcaiIZ9LW/xcNUkOqeEztcaPnUoaTgmk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/U4A5a/dJMcaiIZ9LW/xcNUkOqeEztcaPnUoaTgmk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/U4A5a/dJMcaiIZ9LW/xcNUkOqeEztcaPnUoaTgmk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FU4A5a%2FdJMcaiIZ9LW%2FxcNUkOqeEztcaPnUoaTgmk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"프롬프트 마스터\" loading=\"lazy\" width=\"1808\" height=\"911\" data-filename=\"Prompt Master.png\" data-origin-width=\"1808\" data-origin-height=\"911\"/></span></figure>\n\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">개인적으로 저는 이 앱이 작업 효율성을 몇 배로 끌어올려 줬다고 생각해요. 예를 들어, 콘텐츠 제작자라면 유튜브 스크립트 틀을 템플릿으로 저장해두고 주제만 바꿔서 빠르게 기획할 수 있고요. 개발자라면 복잡한 코드 리뷰 프롬프트를 템플릿화해서 언어와 코드만 바꿔 끼우면 끝입니다. 비즈니스 이메일이나 마케팅 카피도 타겟과 키워드만 툭툭 던지면 전문적인 결과물이 쏟아져 나오는 경험을 하실 수 있을 거예요.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; border-bottom: 2px solid #e8f0fe; padding-bottom: 5px;\" data-ke-size=\"size23\">강력한 기능  </h3>\n<ul style=\"list-style-type: disc; margin-left: 20px; margin-bottom: 20px; color: #3c4043;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 10px;\"><b>템플릿 생성 및 AI 제안:</b> 첫 화면에 있는 미리 설정된 10개의 프롬프트 템플릿을 바로 사용할 수도 있지만, 여러분의 취향에 맞는 새로운 템플릿을 직접 만들 수도 있어요. 여기에 'AI 제안' 버튼까지 활용하면 더 대박입니다. 제목만 써놓고 버튼을 누르면 AI가 알아서 적절한 설명과 본문 내용을 채워주거든요. 초안 짜는 시간조차 아껴주는 거죠.</li>\n<li style=\"margin-bottom: 10px;\"><b>변수 값 채우기:</b> 프롬프트를 실행하면 화면이 둘로 나뉩니다. 왼쪽에서 미리 정해둔 변수 값들을 채워 넣고 버튼을 누르면, 오른쪽에서 실시간으로 AI의 답변이 생성됩니다. 마치 코드를 실행하는 것 같은 느낌이에요.</li>\n<li style=\"margin-bottom: 10px;\"><b>강력한 검색 및 카테고리 관리:</b> 프롬프트가 많아져도 걱정 마세요. 강력한 검색 기능과 카테고리 관리 기능이 있어서 언제든 원하는 프롬프트를 1초 만에 찾아낼 수 있습니다. 저처럼 잃어버린 프롬프트 때문에 속상했던 경험이 있으시다면 이 기능에 감사하게 될 거예요.</li>\n<li style=\"margin-bottom: 10px;\"><b>라이브러리 백업:</b> 여러분의 모든 프롬프트를 한데 모아 관리할 수 있는 라이브러리 백업 기능까지 갖췄으니, 이제 프롬프트 마스터와 함께 반복 작업에서 완전히 해방되어 보세요!</li>\n</ul>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">사용법도 직관적입니다. 처음 실행할 때 제미나이 API 키와 저장 폴더만 지정해 주시면 준비 끝이에요. 이제 이 세 가지 앱을 활용하면 AI와의 소통이 훨씬 더 즐겁고 효율적으로 바뀔 거예요. 정말 추천합니다!</p>\n<figure id=\"og_1771142634911\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"프롬프트 마스터\" data-og-description=\"\" data-og-host=\"prompt-master-ten-flax.vercel.app\" data-og-source-url=\"https://prompt-master-ten-flax.vercel.app/\" data-og-url=\"https://prompt-master-ten-flax.vercel.app/\" data-og-image=\"\"><a href=\"https://prompt-master-ten-flax.vercel.app/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://prompt-master-ten-flax.vercel.app/\">\n<div class=\"og-image\" style=\"background-image: url();\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">프롬프트 마스터</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">&nbsp;</p>\n<p class=\"og-host\" data-ke-size=\"size16\">prompt-master-ten-flax.vercel.app</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<div style=\"background-color: #f8f9fa; border: 1px solid #dadce0; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); padding: 25px; margin: 40px 0;\">\n<div style=\"font-size: 26px; color: #1a73e8; font-weight: bold; margin-bottom: 15px; padding-bottom: 10px; border-bottom: 2px solid #1a73e8;\">  핵심 요약</div>\n<div style=\"font-size: 17px; line-height: 1.8; color: #3c4043;\">\n<p style=\"margin-bottom: 10px;\" data-ke-size=\"size16\">✅ <b>AI Prompt Studio:</b> 복잡한 '메타 프롬프트'를 손쉽게 설계해 AI 성능을 극대화합니다. RTSC, 비즈니스 프레임워크 등 5가지 전문 모드가 인상적이에요.</p>\n<p style=\"margin-bottom: 10px;\" data-ke-size=\"size16\">✅ <b>Prompt Equalizer Pro:</b> 글의 창의성, 전문성, 감정 등 5가지 뉘앙스를 슬라이더로 정밀하게 조절하여 원하는 톤을 구현할 수 있습니다. AI 페르소나 기능도 정말 강력해요.</p>\n<p style=\"margin-bottom: 10px;\" data-ke-size=\"size16\">✅ <b>프롬프트 마스터:</b> 자주 사용하는 프롬프트를 템플릿으로 저장하고 관리하며, 변수를 활용해 빠르게 재사용할 수 있는 효율적인 프롬프트 관리 도구입니다.</p>\n<p style=\"margin-bottom: 0;\" data-ke-size=\"size16\">✅ <b>2026년, AI와의 대화:</b> 이 3가지 앱을 통해 AI를 단순한 챗봇이 아닌, 여러분의 가장 강력하고 유능한 파트너로 만들 수 있습니다. 이제 프롬프트 작성에 대한 고민은 끝이에요!</p>\n</div>\n<div style=\"font-size: 14px; color: #5f6368; margin-top: 20px; padding-top: 15px; border-top: 1px solid #dadce0;\">* 이 앱들은 프롬프트 작성의 번거로움을 줄이고, AI의 잠재력을 최대한 끌어낼 수 있도록 돕는 도구입니다. 각 앱의 상세 기능과 활용법을 익히면 더 큰 시너지를 얻을 수 있습니다.</div>\n</div>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>❓ 자주 묻는 질문 (FAQ)</b></h2>\n<div style=\"margin-bottom: 20px;\">\n<p style=\"margin-bottom: 5px;\" data-ke-size=\"size16\"><b>Q1: 이 세 가지 앱이 실제로 얼마나 도움이 되나요?</b></p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\"><b>A1:</b> 제 경험상 AI와의 소통에서 겪는 거의 모든 어려움을 해결해 준다고 생각해요. AI Prompt Studio로 처음부터 정교한 프롬프트를 만들고, Prompt Equalizer Pro로 결과물의 톤앤매너를 섬세하게 조절하며, 프롬프트 마스터로 이 모든 과정을 효율적으로 관리하고 재사용할 수 있습니다. AI 활용 능력 향상에 결정적인 역할을 해줄 겁니다.</p>\n</div>\n<div style=\"margin-bottom: 20px;\">\n<p style=\"margin-bottom: 5px;\" data-ke-size=\"size16\"><b>Q2: 각 앱의 베이직/프로 버전 차이점은 무엇인가요?</b></p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\"><b>A2:</b> AI Prompt Studio의 경우, 베이직 버전은 '빠른 프롬프트 3종'과 같은 입문용 기능에 초점을 맞추고, 프로 버전은 이 글에서 설명한 모든 전문 모드와 기능을 제공합니다. 반면 Prompt Equalizer Pro와 프롬프트 마스터는 비회원도 대부분의 기능을 사용할 수 있으며, 소스 코드는 정회원에게만 제공되는 차이가 있습니다.</p>\n</div>\n<div style=\"margin-bottom: 20px;\">\n<p style=\"margin-bottom: 5px;\" data-ke-size=\"size16\"><b>Q3: 앱 사용에 필요한 API 키는 어떻게 발급받나요?</b></p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\"><b>A3:</b> 세 앱 모두 구글 AI 스튜디오(<a style=\"color: #1a73e8; text-decoration: none;\" href=\"https://aistudio.google.com/\" target=\"_blank\" rel=\"noopener\">aistudio.google.com</a>)에서 무료로 발급받을 수 있는 제미나이 API 키를 사용합니다. 발급 절차가 간단하고 무료이니, AI 스튜디오에 접속하셔서 쉽게 발급받으실 수 있을 거예요.</p>\n</div>\n</div>\n<script type=\"application/ld+json\">\n{\n  \"@context\": \"https://schema.org\",\n  \"@type\": \"FAQPage\",\n  \"mainEntity\": [\n    {\n      \"@type\": \"Question\",\n      \"name\": \"이 세 가지 앱이 실제로 얼마나 도움이 되나요?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"제 경험상 AI와의 소통에서 겪는 거의 모든 어려움을 해결해 준다고 생각해요. AI Prompt Studio로 처음부터 정교한 프롬프트를 만들고, Prompt Equalizer Pro로 결과물의 톤앤매너를 섬세하게 조절하며, 프롬프트 마스터로 이 모든 과정을 효율적으로 관리하고 재사용할 수 있습니다. AI 활용 능력 향상에 결정적인 역할을 해줄 겁니다.\"\n      }\n    },\n    {\n      \"@type\": \"Question\",\n      \"name\": \"각 앱의 베이직/프로 버전 차이점은 무엇인가요?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"AI Prompt Studio의 경우, 베이직 버전은 '빠른 프롬프트 3종'과 같은 입문용 기능에 초점을 맞추고, 프로 버전은 이 글에서 설명한 모든 전문 모드와 기능을 제공합니다. 반면 Prompt Equalizer Pro와 프롬프트 마스터는 비회원도 대부분의 기능을 사용할 수 있으며, 소스 코드는 정회원에게만 제공되는 차이가 있습니다.\"\n      }\n    },\n    {\n      \"@type\": \"Question\",\n      \"name\": \"앱 사용에 필요한 API 키는 어떻게 발급받나요?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"세 앱 모두 구글 AI 스튜디오(aistudio.google.com)에서 무료로 발급받을 수 있는 제미나이 API 키를 사용합니다. 발급 절차가 간단하고 무료이니, AI 스튜디오에 접속하셔서 쉽게 발급받으실 수 있을 거예요.\"\n      }\n    }\n  ]\n}\n</script>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<figure data-ke-type=\"video\" data-ke-style=\"alignCenter\" data-video-host=\"youtube\" data-video-url=\"https://www.youtube.com/watch?v=QPjkmFFaN_E\" data-video-thumbnail=\"https://scrap.kakaocdn.net/dn/bswQtc/dJMb8UHLrOG/qBi7pu5UMJKuih785BbAr0/img.jpg?width=1280&amp;height=720&amp;face=1126_448_1214_544\" data-video-width=\"860\" data-video-height=\"484\" data-video-origin-width=\"860\" data-video-origin-height=\"484\" data-ke-mobilestyle=\"widthContent\" data-video-title=\"수십만 원짜리 프롬프트 강의? 이제 이 앱 3개로 끝내세요! (메타 프롬프트 설계법)\" data-original-url=\"\"><iframe src=\"https://www.youtube.com/embed/QPjkmFFaN_E\" width=\"860\" height=\"484\" frameborder=\"\" allowfullscreen=\"true\"></iframe>\n<figcaption style=\"display: none;\"></figcaption>\n</figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<div id=\"gtx-trans\" style=\"position: absolute; left: -29px; top: 8435.72px;\">\n<div class=\"gtx-trans-icon\">&nbsp;</div>\n</div>",
        "contentSnippet": "AI와의 대화가 늘 엉뚱한 결과로 이어져 답답하셨나요? 혹은 완벽한 프롬프트를 만들고 싶지만 어디서부터 시작해야 할지 막막하셨다면, 이 글이 해답이 될 거예요. AI 활용의 새로운 기준을 제시할 세 가지 혁신적인 앱으로 프롬프트 작성의 모든 고민을 끝내고, 여러분도 진정한 AI 프롬프트 마스터가 되어 보세요!\n여러분, 혹시 AI에게 질문했는데 영 엉뚱한 답변이 나와서 황당했던 경험, 저만 있는 건 아니겠죠? 아니면, 정말 완벽한 결과물을 기대하면서 프롬프트를 만들고 싶은데, 대체 어디서부터 손을 대야 할지 감조차 안 잡히셨던 적도 분명 있으실 거예요. 오늘은 이런 고민을 시원하게 해결해 줄 마법 같은 세 가지 도구를 소개해 드리려고 해요. 이 앱들만 제대로 활용하신다면, 프롬프트 전문가의 영역이 더 이상 꿈이 아니랍니다!\n\n\n제가 직접 경험해 보니, AI는 정말 질문을 어떻게 하느냐에 따라 답변의 질이 하늘과 땅 차이로 달라져요. 단순히 \"블로그 글 써줘\"라고 짧게 물으면, 어디서 많이 본 듯한 평범한 결과물만 툭 튀어나오기 일쑤죠. 그런데 만약 \"당신은 10년 경력의 여행 작가입니다. 20대 여성을 위한 제주도 3박 4일 여행기를 감성적인 톤으로 작성해주세요\"라고 디테일하게 요청한다면? 와, 이건 정말 완전히 다른 수준의, 심지어 감동적인 결과물이 나올 때도 있더라고요.\n하지만 문제는 매번 이렇게 완벽하고 정교한 프롬프트를 직접 만드는 게 너무 어렵고 귀찮다는 점이에요. 특히 \"AI한테 어떤 역할을 부여해야 하지?\", \"어떤 조건을 추가해야 가장 좋은 결과가 나올까?\" 같은 고민을 하다 보면 시간이 정말이지 눈 깜짝할 사이에 흘러가 버리죠. 그래서 오늘 제가 소개해 드릴 이 세 가지 앱이 필요한 겁니다. 각각 다른 역할로 여러분의 프롬프트 작성 전 과정을 빈틈없이 도와줄 거예요. 마치 요리할 때 레시피를 짜주는 셰프, 맛을 미세하게 조절하는 조미료, 그리고 나만의 특별한 레시피를 정리해 주는 요리책 같다고나 할까요?\n1. AI Prompt Studio: 메타 프롬프트 설계의 마법사  ‍♀️\n여러분, 혹시 ChatGPT나 제미나이를 쓸 때 \"제주도 여행 블로그 글 하나 써줘\"라고 짧게 물어보고 나서 결과가 너무 뻔해서 실망하신 적 있으실 거예요. 사실 이건 AI의 성능 문제라기보다는, 질문하는 방식에 아쉬움이 있었을 가능성이 큽니다. AI는 질문을 어떻게 하느냐에 따라 답변의 품질이 정말이지 하늘과 땅 차이로 달라지거든요.\n\n\n하지만 문제는 매번 \"너는 10년 차 작가야, 타겟은 누구고, 형식은 마크다운으로 해줘\" 같은 복잡한 프롬프트를 직접 짜기가 너무 어렵고 번거롭다는 거죠. 바로 이 번거로움을 해결해 주는 도구가 첫 번째로 소개해 드릴 'AI Prompt Studio'입니다. 이 앱은 단순한 질문이 아닌, LLM(거대 언어 모델)의 성능을 100% 끌어낼 수 있는 '메타 프롬프트(Meta Prompt)'를 전문적으로 설계해주는 앱이에요.\n주요 기능  \n빠른 프롬프트 생성: 복잡한 설정 없이 목표만 입력하면 버튼 하나로 강력한 프롬프트를 즉시 만들어줍니다. 바쁠 때 정말 최고죠.\n역질문 유도: 사용자의 요청이 모호할 때, AI가 스스로 부족한 정보를 파악하여 \"더 나은 답변을 위해 몇 가지 질문을 해도 될까요?\" 하고 되묻도록 유도합니다. 덕분에 불필요한 시행착오를 줄일 수 있어요.\n페르소나 부여: 목표 달성에 가장 적합한 전문가 역할을 AI가 스스로 설정하고 연기하도록 지시합니다. 예를 들어, \"최고의 마케터\" 역할을 부여해서 결과물의 전문성을 확 끌어올릴 수 있어요.\n비판적 개선: 입력한 내용을 비판적으로 분석해서 논리적 허점이나 모호한 부분을 찾아내고, 더 완벽한 구조로 즉시 수정해 줍니다. 스스로 고쳐나가면서 프롬프트 실력이 확 늘 거예요.\n다섯 가지 전문 모드  ️\n이 앱의 가장 큰 특징이자 핵심은 바로 목적에 따라 골라 쓸 수 있는 다섯 가지 전문 모드를 지원한다는 점이에요. 여러분의 상황에 맞춰 선택하기만 하면 됩니다.\nRTSC 마스터 모드: 역할(Role), 작업(Task), 구조(Structure), 체인(Chain)의 앞 글자를 딴 모드인데요. 예를 들어 \"유튜브 쇼츠 대본 생성\"이라고 목표만 입력해 보세요. AI가 알아서 \"숏폼 바이럴 마케팅 디렉터\"라는 역할을 부여하고, 쇼츠 성공 공식을 완벽하게 구조화한 프롬프트를 짜줍니다. 정교하고 복잡한 창작물을 만들 때 정말 최고입니다.\n비즈니스 프레임워크 모드: 직장인분들이라면 이 모드를 강력 추천합니다! 기획안이나 보고서 쓸 때 막막하셨던 경험 다들 있으시죠? 코스타(COSTAR)나 프렙(PREP) 같은 검증된 비즈니스 틀을 자동으로 적용해서 아주 깔끔하고 논리적인 업무용 문서를 만들어 줘요. 정말이지 퇴근 시간을 앞당겨 주는 효자 앱이라고 할 수 있죠.\n프롬프트 아키텍트: 정말 귀찮을 때, 이 모드가 마법을 부려줍니다. \"복잡한 법률 문서를 쉽게 요약해 주는 AI 에이전트를 설계해 줘\"라고 딱 한 줄만 써보세요. 그럼 AI가 필요한 제약 조건과 작업 프로세스를 알아서 추론하고 설계해 냅니다. 마치 옆에 유능한 비서가 있는 것 같아요.\n버텍스 AI XML 모드: 기업용 데이터 처리나 구조화된 데이터가 필요할 때 빛을 발하는 모드예요. 결과물을 깔끔한 XML 태그 구조로 만들어주기 때문에 복잡한 데이터를 분류하거나 추출할 때 아주 정확하고 표준화된 답변을 얻을 수 있습니다. 데이터 분석가나 개발자분들께 특히 유용할 거예요.\nCoT(Chain-of-Thought) 모드: 논리적인 사고가 필요할 때 활용해 보세요. AI가 사람처럼 단계별로 추론하며 문제를 풀도록 유도하는 방식인데요. 덕분에 수학 문제나 복잡한 논리 과제에서 AI가 엉뚱한 답을 내놓는 현상을 획기적으로 줄여줍니다. 이제는 그저 \"무엇을 할지\"만 결정하시면 됩니다!\n솔직히 말하면, 이 앱은 거의 '사기 앱'이라고 불러도 될 정도예요. 왜냐하면 프롬프트 엔지니어들이 밤새워 연구하고 강의에서 수십만 원을 주고 배워야 하는 복잡한 기법들을, 버튼 클릭 몇 번만으로 누구나 완벽하게 구현할 수 있게 만들어줬거든요. 저도 처음에 써보고 정말 깜짝 놀랐습니다.\n많은 분들이 AI를 쓰면서 \"AI가 별로야\"라고 아쉬워하시는데, 제 경험상 사실은 질문을 제대로 못한 경우가 대부분입니다. 그런데 이 앱을 쓰면 그런 문제가 완전히 해결돼요. 시작하는 방법도 아주 간단합니다. 브라우저에서 접속한 뒤 우측 상단 톱니바퀴 아이콘을 클릭해서, 구글 AI 스튜디오에서 무료로 발급받은 '제미나이 API 키'만 넣어주면 준비 끝입니다. 자, 이제 '시작하기' 버튼을 누르고 여러분만의 완벽한 프롬프트를 직접 설계해 보시기 바랍니다!\n\n \nMeta Prompt Master\n \nai-prompt-studio-v2-basic.vercel.app\n\n \n  잠깐! API 키 설정 팁: AI Prompt Studio를 시작하기 전에, 구글 AI 스튜디오(aistudio.google.com)에서 무료로 제미나이 API 키를 발급받으세요. 이 키만 있으면 AI Prompt Studio의 모든 기능을 막힘없이 활용할 수 있습니다. 설정 과정도 매우 간단하니 걱정 마세요!\n2. Prompt Equalizer Pro: 글의 뉘앙스를 조율하는 이퀄라이저  \n자, 다음으로 소개해 드릴 앱은 정말이지 흥미로운 도구입니다. 바로 'Prompt Equalizer Pro'인데요. 여러분, 혹시 음악 들으실 때 취향에 맞춰 저음이나 고음을 조절하는 '이퀄라이저' 써보셨나요? 이 앱은 그 원리를 글쓰기에 그대로 가져왔습니다. 앞서 보신 'AI Prompt Studio'가 프롬프트를 '만드는' 도구였다면, 이 앱은 이미 써놓은 글의 뉘앙스를 아주 정밀하게 튜닝하고 조정하는 도구라고 보시면 됩니다. 개인적으로 써보고 정말 감탄했던 앱이에요.\n\n\n핵심 기능: 5가지 슬라이더 ✨\n앱 화면을 보시면 오른쪽에 다섯 개의 슬라이더가 보일 거예요. 이게 이 앱의 핵심인데요, 마치 음향 이퀄라이저처럼 글의 다양한 요소를 미세하게 조절할 수 있습니다.\n창의성: 슬라이더를 오른쪽으로 쭉 밀면, 평범했던 문장이 아주 은유적이고 풍성하게 변합니다. 반대로 왼쪽으로 낮추면 군더더기 없이 아주 직설적인 문장이 되죠. 소설이나 시를 쓸 때 정말 유용해요.\n명확성: 글의 이해도를 높이거나 낮출 수 있습니다. 복잡한 내용을 쉽게 풀어 설명할 때 아주 좋고, 때로는 일부러 난해하게 표현할 수도 있습니다.\n감정의 깊이: 글에 담긴 감정의 농도를 조절합니다. 따뜻하고 감성적인 글부터 차갑고 이성적인 글까지, 원하는 감정 톤을 만들 수 있어요.\n문장의 길이: 길고 유려한 문장과 짧고 간결한 문장 사이를 오갈 수 있습니다. 저는 주로 짧은 문장으로 속도감을 주고 싶을 때 사용하곤 해요.\n전문성: 이 기능이 정말 압권인데요! 수치를 높이면 아주 격식 있는 공문서 느낌부터, 낮추면 친구에게 말하는 듯한 편안한 말투까지 자유자재로 오갈 수 있습니다. 독자층에 맞춰 말투를 순식간에 바꾸는 게 가능해요.\n슬라이더 몇 번 움직이는 것만으로 글의 온도와 질감이 완전히 달라지는 게 정말 신기하지 않나요? 처음에 써봤을 때 솔직히 '이게 된다고?' 싶었어요.\nAI 페르소나 기능  \n더 놀라운 건 'AI 페르소나' 기능입니다. 앱에서 미리 제공하는 10가지 캐릭터 외에도 여러분이 직접 '냉철한 탐정'이나 '비즈니스 전문가'라고만 입력하면, AI가 그 캐릭터에 딱 맞는 말투와 성격, 그리고 최적의 이퀄라이저 값까지 알아서 세팅해 줍니다. 정말 생각지도 못한 부분에서 감탄하게 되는 기능이었어요.\n사용법도 아주 간단합니다. 처음 설정에서 API 키만 넣고 저장 폴더를 지정해 주세요. 그다음 원하는 말투의 페르소나를 고르거나 직접 만든 페르소나를 선택한 뒤, 슬라이더를 슥슥 조절하고, 원본 글을 넣은 뒤 '명작 생성' 버튼만 누르면 끝입니다. 마음에 들면 바로 파일로 저장하시면 되고요.\n소설을 쓰실 때 캐릭터마다 말투를 다르게 고정하거나, 딱딱한 보고서를 부드럽게 다듬을 때 이만한 도구가 없습니다. 단순히 수치를 높이기보다, 이것저것 만져보면서 여러분만의 '황금 조합'을 한 번 찾아보세요. 저도 저만의 조합을 찾는 재미에 푹 빠져버렸답니다!\n\n \nPrompt Equalizer Pro\n \nprompt-equalizer.vercel.app\n\n \n3. 프롬프트 마스터: 당신의 프롬프트 라이브러리  \n혹시 매번 똑같은 프롬프트를 메모장에 복사해서 쓰거나, 예전에 써둔 정말 좋은 프롬프트를 못 찾아서 헤맨 적 없으신가요? 첫 번째 앱이 프롬프트를 '만들고', 두 번째 앱이 뉘앙스를 '바꿨다면', '프롬프트 마스터'는 자주 쓰는 프롬프트를 완벽하게 '관리하고 재사용하는' 도구입니다. 이 앱 하나면 반복되는 질문을 일일이 타이핑할 필요가 완전히 사라져요. 정말 솔깃하지 않나요?\n\n\n개인적으로 저는 이 앱이 작업 효율성을 몇 배로 끌어올려 줬다고 생각해요. 예를 들어, 콘텐츠 제작자라면 유튜브 스크립트 틀을 템플릿으로 저장해두고 주제만 바꿔서 빠르게 기획할 수 있고요. 개발자라면 복잡한 코드 리뷰 프롬프트를 템플릿화해서 언어와 코드만 바꿔 끼우면 끝입니다. 비즈니스 이메일이나 마케팅 카피도 타겟과 키워드만 툭툭 던지면 전문적인 결과물이 쏟아져 나오는 경험을 하실 수 있을 거예요.\n강력한 기능  \n템플릿 생성 및 AI 제안: 첫 화면에 있는 미리 설정된 10개의 프롬프트 템플릿을 바로 사용할 수도 있지만, 여러분의 취향에 맞는 새로운 템플릿을 직접 만들 수도 있어요. 여기에 'AI 제안' 버튼까지 활용하면 더 대박입니다. 제목만 써놓고 버튼을 누르면 AI가 알아서 적절한 설명과 본문 내용을 채워주거든요. 초안 짜는 시간조차 아껴주는 거죠.\n변수 값 채우기: 프롬프트를 실행하면 화면이 둘로 나뉩니다. 왼쪽에서 미리 정해둔 변수 값들을 채워 넣고 버튼을 누르면, 오른쪽에서 실시간으로 AI의 답변이 생성됩니다. 마치 코드를 실행하는 것 같은 느낌이에요.\n강력한 검색 및 카테고리 관리: 프롬프트가 많아져도 걱정 마세요. 강력한 검색 기능과 카테고리 관리 기능이 있어서 언제든 원하는 프롬프트를 1초 만에 찾아낼 수 있습니다. 저처럼 잃어버린 프롬프트 때문에 속상했던 경험이 있으시다면 이 기능에 감사하게 될 거예요.\n라이브러리 백업: 여러분의 모든 프롬프트를 한데 모아 관리할 수 있는 라이브러리 백업 기능까지 갖췄으니, 이제 프롬프트 마스터와 함께 반복 작업에서 완전히 해방되어 보세요!\n사용법도 직관적입니다. 처음 실행할 때 제미나이 API 키와 저장 폴더만 지정해 주시면 준비 끝이에요. 이제 이 세 가지 앱을 활용하면 AI와의 소통이 훨씬 더 즐겁고 효율적으로 바뀔 거예요. 정말 추천합니다!\n\n \n프롬프트 마스터\n \nprompt-master-ten-flax.vercel.app\n\n \n  핵심 요약\n✅ AI Prompt Studio: 복잡한 '메타 프롬프트'를 손쉽게 설계해 AI 성능을 극대화합니다. RTSC, 비즈니스 프레임워크 등 5가지 전문 모드가 인상적이에요.\n✅ Prompt Equalizer Pro: 글의 창의성, 전문성, 감정 등 5가지 뉘앙스를 슬라이더로 정밀하게 조절하여 원하는 톤을 구현할 수 있습니다. AI 페르소나 기능도 정말 강력해요.\n✅ 프롬프트 마스터: 자주 사용하는 프롬프트를 템플릿으로 저장하고 관리하며, 변수를 활용해 빠르게 재사용할 수 있는 효율적인 프롬프트 관리 도구입니다.\n✅ 2026년, AI와의 대화: 이 3가지 앱을 통해 AI를 단순한 챗봇이 아닌, 여러분의 가장 강력하고 유능한 파트너로 만들 수 있습니다. 이제 프롬프트 작성에 대한 고민은 끝이에요!\n* 이 앱들은 프롬프트 작성의 번거로움을 줄이고, AI의 잠재력을 최대한 끌어낼 수 있도록 돕는 도구입니다. 각 앱의 상세 기능과 활용법을 익히면 더 큰 시너지를 얻을 수 있습니다.\n❓ 자주 묻는 질문 (FAQ)\nQ1: 이 세 가지 앱이 실제로 얼마나 도움이 되나요?\nA1: 제 경험상 AI와의 소통에서 겪는 거의 모든 어려움을 해결해 준다고 생각해요. AI Prompt Studio로 처음부터 정교한 프롬프트를 만들고, Prompt Equalizer Pro로 결과물의 톤앤매너를 섬세하게 조절하며, 프롬프트 마스터로 이 모든 과정을 효율적으로 관리하고 재사용할 수 있습니다. AI 활용 능력 향상에 결정적인 역할을 해줄 겁니다.\nQ2: 각 앱의 베이직/프로 버전 차이점은 무엇인가요?\nA2: AI Prompt Studio의 경우, 베이직 버전은 '빠른 프롬프트 3종'과 같은 입문용 기능에 초점을 맞추고, 프로 버전은 이 글에서 설명한 모든 전문 모드와 기능을 제공합니다. 반면 Prompt Equalizer Pro와 프롬프트 마스터는 비회원도 대부분의 기능을 사용할 수 있으며, 소스 코드는 정회원에게만 제공되는 차이가 있습니다.\nQ3: 앱 사용에 필요한 API 키는 어떻게 발급받나요?\nA3: 세 앱 모두 구글 AI 스튜디오(aistudio.google.com)에서 무료로 발급받을 수 있는 제미나이 API 키를 사용합니다. 발급 절차가 간단하고 무료이니, AI 스튜디오에 접속하셔서 쉽게 발급받으실 수 있을 거예요.",
        "guid": "https://muzbox.tistory.com/483709",
        "categories": [
          "AI, 미래기술/AI 챗봇 및 지침 무료 배포",
          "AI Prompt Studio",
          "ai 글쓰기 도구",
          "ai 프롬프트",
          "Ai 활용",
          "llm 활용",
          "Prompt Equalizer Pro",
          "메타 프롬프트",
          "프롬프트 관리",
          "프롬프트 마스터",
          "프롬프트 엔지니어링"
        ],
        "isoDate": "2026-02-15T08:04:15.000Z"
      }
    ]
  },
  {
    "name": "동우리의 블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "하테나",
    "category": "개인",
    "posts": []
  },
  {
    "name": "늑돌이네 라지온",
    "category": "개인",
    "posts": []
  },
  {
    "name": "루리웹 - 루리웹 리뷰 게시판",
    "category": "게임",
    "posts": [
      {
        "creator": "｜RULIWEB｜",
        "title": "악역영애 4컷 만화는 한 주 쉬어갑니다.",
        "link": "https://bbs.ruliweb.com/news/board/11/read/2422",
        "pubDate": "Wed, 18 Feb 2026 15:04:52 +0900",
        "author": "｜RULIWEB｜",
        "content": "<img width=\"236\" height=\"177\" src=\"https://i3.ruliweb.com/thumb/26/02/18/19c6f59806751ad6b.webp\">",
        "contentSnippet": "",
        "categories": [
          "웹툰"
        ],
        "isoDate": "2026-02-18T06:04:52.000Z"
      },
      {
        "creator": "｜RULIWEB｜",
        "title": "힘차게 달려라 설국열차999, 프로스트레인 2",
        "link": "https://bbs.ruliweb.com/news/board/11/read/2421",
        "pubDate": "Sun, 15 Feb 2026 18:39:47 +0900",
        "author": "｜RULIWEB｜",
        "content": "<img width=\"236\" height=\"177\" src=\"https://i3.ruliweb.com/thumb/26/02/15/19c60a8cd8b51ad6b.webp\">",
        "contentSnippet": "",
        "categories": [
          "게임툰"
        ],
        "isoDate": "2026-02-15T09:39:47.000Z"
      }
    ]
  },
  {
    "name": "Reasontobe",
    "category": "개인",
    "posts": []
  },
  {
    "name": "에스티마의 인터넷이야기 EstimaStory.com",
    "category": "개인",
    "posts": []
  },
  {
    "name": "나긋한 개발 - 데비안 리눅스와 프로그램 언어",
    "category": "개인",
    "posts": [
      {
        "creator": "summerandwinter",
        "title": "LVM 파티션 사용하는 디스크에 용량 추가 후 작업",
        "link": "https://sacstory.tistory.com/entry/LVM-%ED%8C%8C%ED%8B%B0%EC%85%98-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%94%94%EC%8A%A4%ED%81%AC%EC%97%90-%EC%9A%A9%EB%9F%89-%EC%B6%94%EA%B0%80-%ED%9B%84-%EC%9E%91%EC%97%85",
        "pubDate": "Mon, 16 Feb 2026 15:46:03 +0900",
        "author": "summerandwinter",
        "comments": "https://sacstory.tistory.com/entry/LVM-%ED%8C%8C%ED%8B%B0%EC%85%98-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%94%94%EC%8A%A4%ED%81%AC%EC%97%90-%EC%9A%A9%EB%9F%89-%EC%B6%94%EA%B0%80-%ED%9B%84-%EC%9E%91%EC%97%85#entry398comment",
        "content": "추가된 디스크 용량 확인\nlsblk\n&nbsp;\n추가된 용량 할당\ncfdisk\n&nbsp;\n&nbsp;\n물리 사이즈 재인식토록 갱신\npvresize /dev/sda3\n&nbsp;\n논리 파티션에 용량 추가\nlvextend -r -l +100%FREE /dev/mapper/DBMS--vg-var",
        "contentSnippet": "추가된 디스크 용량 확인\nlsblk\n \n추가된 용량 할당\ncfdisk\n \n \n물리 사이즈 재인식토록 갱신\npvresize /dev/sda3\n \n논리 파티션에 용량 추가\nlvextend -r -l +100%FREE /dev/mapper/DBMS--vg-var",
        "guid": "https://sacstory.tistory.com/398",
        "categories": [
          "리눅스/리눅스 - 데비안 계열"
        ],
        "isoDate": "2026-02-16T06:46:03.000Z"
      }
    ]
  },
  {
    "name": "일상을 여행처럼...",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Just hack'em",
    "category": "개인",
    "posts": []
  },
  {
    "name": "C++ Truths",
    "category": "개인",
    "posts": []
  },
  {
    "name": "jacking75",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Joel on Software",
    "category": "개인",
    "posts": []
  },
  {
    "name": "벤자민로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "악보쓰는 프로그래머",
    "category": "개인",
    "posts": []
  },
  {
    "name": "쭌안아빠",
    "category": "개인",
    "posts": []
  },
  {
    "name": "A Gangster World",
    "category": "개인",
    "posts": []
  },
  {
    "name": "요우의 내맘대로 블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "개발자스럽다",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "Against All Odds.",
    "category": "개인",
    "posts": []
  },
  {
    "name": "움직이는 게임서버",
    "category": "개인",
    "posts": []
  },
  {
    "name": "이상욱",
    "category": "개인",
    "posts": []
  },
  {
    "name": "임철재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "어쩐지 오늘은",
    "category": "개인",
    "posts": []
  },
  {
    "name": "oddpoet’s étude",
    "category": "개인",
    "posts": []
  },
  {
    "name": "0x00 - NULL",
    "category": "개인",
    "posts": []
  },
  {
    "name": "퇴근 후 서버다운",
    "category": "개인",
    "posts": [
      {
        "creator": "SIDNFT",
        "title": "유니티 셰이더 그래프 사용해보자 / 강의시장을 어지럽힐 유튜브 영상",
        "link": "https://serverdown.tistory.com/1574",
        "pubDate": "Thu, 19 Feb 2026 15:26:33 +0900",
        "author": "SIDNFT",
        "comments": "https://serverdown.tistory.com/1574#entry1574comment",
        "content": "<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"245\" data-origin-height=\"223\"><span data-url=\"https://blog.kakaocdn.net/dn/GKQqy/dJMcafr2j65/OdP5kQEewPhMjPnXzIQEdK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/GKQqy/dJMcafr2j65/OdP5kQEewPhMjPnXzIQEdK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/GKQqy/dJMcafr2j65/OdP5kQEewPhMjPnXzIQEdK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FGKQqy%2FdJMcafr2j65%2FOdP5kQEewPhMjPnXzIQEdK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"245\" height=\"223\" data-origin-width=\"245\" data-origin-height=\"223\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">결국 선 넘어버렸군요</p>\n<p data-ke-size=\"size16\">유튜브에 그냥 올라와 버렸습니다.</p>\n<p data-ke-size=\"size16\">강의 시장이 치열하다는 증거 입니다.</p>\n<p data-ke-size=\"size16\">배우기 좋은 타이밍입니다.</p>\n<p data-ke-size=\"size16\">셰이더를 배워봅시다.</p>\n<p data-ke-size=\"size16\">영상: <a href=\"https://www.youtube.com/watch?v=KnueAgpUL3Y&amp;t=237s\" target=\"_blank\" rel=\"noopener&nbsp;noreferrer\">https://www.youtube.com/watch?v=KnueAgpUL3Y&amp;t=237s</a></p>\n<figure data-ke-type=\"video\" data-ke-style=\"alignCenter\" data-video-host=\"youtube\" data-video-url=\"https://www.youtube.com/watch?v=KnueAgpUL3Y\" data-video-thumbnail=\"https://scrap.kakaocdn.net/dn/cVgten/dJMb8T9VYpF/9i9HwfkVnUVWgV7VYQs1Ek/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720,https://scrap.kakaocdn.net/dn/Zw4Ce/dJMb83Sfl35/8WEeZybGHQfqmkqIODP5IK/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720,https://scrap.kakaocdn.net/dn/1kIPc/dJMb8YpRW4I/ibHbB3odbNnze8dWXqpV8k/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720\" data-video-width=\"860\" data-video-height=\"484\" data-video-origin-width=\"860\" data-video-origin-height=\"484\" data-ke-mobilestyle=\"widthContent\" data-video-title=\"기적의 셰이더 그래프 \" data-original-url=\"\"><iframe src=\"https://www.youtube.com/embed/KnueAgpUL3Y\" width=\"860\" height=\"484\" frameborder=\"\" allowfullscreen=\"true\"></iframe>\n<figcaption style=\"display: none;\"></figcaption>\n</figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">어렵다고 피하지 말고&nbsp;</p>\n<p data-ke-size=\"size16\">봐두세요&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">온갖 용여를 다 설명해줍니다.</p>\n<p data-ke-size=\"size16\">메탈릭, 에미션, 등등</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">당장은 이해가 안가도 언젠가 갑자기 머리속에서 정리될 것입니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "결국 선 넘어버렸군요\n유튜브에 그냥 올라와 버렸습니다.\n강의 시장이 치열하다는 증거 입니다.\n배우기 좋은 타이밍입니다.\n셰이더를 배워봅시다.\n영상: https://www.youtube.com/watch?v=KnueAgpUL3Y&t=237s\n\n\n\n \n어렵다고 피하지 말고 \n봐두세요 \n \n온갖 용여를 다 설명해줍니다.\n메탈릭, 에미션, 등등\n \n당장은 이해가 안가도 언젠가 갑자기 머리속에서 정리될 것입니다.",
        "guid": "https://serverdown.tistory.com/1574",
        "categories": [
          "프로그래밍/개발메모",
          "셰이더",
          "유니티"
        ],
        "isoDate": "2026-02-19T06:26:33.000Z"
      },
      {
        "creator": "SIDNFT",
        "title": "유니티 강좌 - 픽셀아트를 위한 2D 라이팅 같이 배우기 / 골드메탈",
        "link": "https://serverdown.tistory.com/1573",
        "pubDate": "Sun, 15 Feb 2026 23:08:53 +0900",
        "author": "SIDNFT",
        "comments": "https://serverdown.tistory.com/1573#entry1573comment",
        "content": "<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"184\" data-origin-height=\"156\"><span data-url=\"https://blog.kakaocdn.net/dn/kRPay/dJMcacWk5KF/vP5EMsruuu7j6RT4Alu8iK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/kRPay/dJMcacWk5KF/vP5EMsruuu7j6RT4Alu8iK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/kRPay/dJMcacWk5KF/vP5EMsruuu7j6RT4Alu8iK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FkRPay%2FdJMcacWk5KF%2FvP5EMsruuu7j6RT4Alu8iK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"184\" height=\"156\" data-origin-width=\"184\" data-origin-height=\"156\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">영상: <a href=\"https://www.youtube.com/watch?v=rR5SrfRX4qU\" target=\"_blank\" rel=\"noopener&nbsp;noreferrer\">https://www.youtube.com/watch?v=rR5SrfRX4qU</a></p>\n<figure data-ke-type=\"video\" data-ke-style=\"alignCenter\" data-video-host=\"youtube\" data-video-url=\"https://www.youtube.com/watch?v=rR5SrfRX4qU\" data-video-thumbnail=\"https://scrap.kakaocdn.net/dn/mxeFJ/dJMb8TB5R5q/PBz1tO70QsjIViHlMdAYd0/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720,https://scrap.kakaocdn.net/dn/b6GUZ3/dJMb8SXt5Qv/y2w5yj6e9K3Em1iugK8Kik/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720\" data-video-width=\"860\" data-video-height=\"484\" data-video-origin-width=\"860\" data-video-origin-height=\"484\" data-ke-mobilestyle=\"widthContent\" data-video-title=\"픽셀아트를 위한 2D 라이팅 같이 배우기\" data-original-url=\"\"><iframe src=\"https://www.youtube.com/embed/rR5SrfRX4qU\" width=\"860\" height=\"484\" frameborder=\"\" allowfullscreen=\"true\"></iframe>\n<figcaption style=\"display: none;\"></figcaption>\n</figure>\n<p data-ke-size=\"size16\">여러가지 몰랐던 UI 들을 사용하네요</p>\n<p data-ke-size=\"size16\">나중에 진지하게 봐야겠당</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">비주얼스크립팅</p>\n<p data-ke-size=\"size16\">글로벌 광원</p>\n<p data-ke-size=\"size16\">픽셀퍼팩트</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"236\" data-origin-height=\"217\"><span data-url=\"https://blog.kakaocdn.net/dn/cDLaMc/dJMcafMk5NS/ZroP3msJWidUQQapiKPXYk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/cDLaMc/dJMcafMk5NS/ZroP3msJWidUQQapiKPXYk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/cDLaMc/dJMcafMk5NS/ZroP3msJWidUQQapiKPXYk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcDLaMc%2FdJMcafMk5NS%2FZroP3msJWidUQQapiKPXYk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"236\" height=\"217\" data-origin-width=\"236\" data-origin-height=\"217\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">라이트</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"262\" data-origin-height=\"279\"><span data-url=\"https://blog.kakaocdn.net/dn/tpSen/dJMcad1YSDs/QBtrmymu9zKKMkyB0EChkK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/tpSen/dJMcad1YSDs/QBtrmymu9zKKMkyB0EChkK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/tpSen/dJMcad1YSDs/QBtrmymu9zKKMkyB0EChkK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FtpSen%2FdJMcad1YSDs%2FQBtrmymu9zKKMkyB0EChkK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"262\" height=\"279\" data-origin-width=\"262\" data-origin-height=\"279\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">크림자 영역</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"586\" data-origin-height=\"405\"><span data-url=\"https://blog.kakaocdn.net/dn/b22tMo/dJMcabiRUKC/xdxL6ezhwZyGWNJTH4otK0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/b22tMo/dJMcabiRUKC/xdxL6ezhwZyGWNJTH4otK0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/b22tMo/dJMcabiRUKC/xdxL6ezhwZyGWNJTH4otK0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb22tMo%2FdJMcabiRUKC%2FxdxL6ezhwZyGWNJTH4otK0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"586\" height=\"405\" data-origin-width=\"586\" data-origin-height=\"405\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">2D 셰이더</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"152\" data-origin-height=\"136\"><span data-url=\"https://blog.kakaocdn.net/dn/rqxTG/dJMcagLeQPi/L3CXkXnsnY82pK2qRf0Nz1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/rqxTG/dJMcagLeQPi/L3CXkXnsnY82pK2qRf0Nz1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/rqxTG/dJMcagLeQPi/L3CXkXnsnY82pK2qRf0Nz1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FrqxTG%2FdJMcagLeQPi%2FL3CXkXnsnY82pK2qRf0Nz1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"152\" height=\"136\" data-origin-width=\"152\" data-origin-height=\"136\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">빛나는 외곽선</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"286\" data-origin-height=\"225\"><span data-url=\"https://blog.kakaocdn.net/dn/7pOS8/dJMcahwCchr/77m8s6DzngSWTKU9KEgL10/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/7pOS8/dJMcahwCchr/77m8s6DzngSWTKU9KEgL10/img.png\"><img src=\"https://blog.kakaocdn.net/dn/7pOS8/dJMcahwCchr/77m8s6DzngSWTKU9KEgL10/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F7pOS8%2FdJMcahwCchr%2F77m8s6DzngSWTKU9KEgL10%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"286\" height=\"225\" data-origin-width=\"286\" data-origin-height=\"225\"/></span></figure>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"252\" data-origin-height=\"201\"><span data-url=\"https://blog.kakaocdn.net/dn/5c2ZK/dJMcadVdLV9/WkDBry2Tk7c8JBK6FQgjxK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/5c2ZK/dJMcadVdLV9/WkDBry2Tk7c8JBK6FQgjxK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/5c2ZK/dJMcadVdLV9/WkDBry2Tk7c8JBK6FQgjxK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F5c2ZK%2FdJMcadVdLV9%2FWkDBry2Tk7c8JBK6FQgjxK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"252\" height=\"201\" data-origin-width=\"252\" data-origin-height=\"201\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">하이트맵 뚝딱 (2D 노멀맵)</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"282\" data-origin-height=\"300\"><span data-url=\"https://blog.kakaocdn.net/dn/mpIIN/dJMcahi5IOr/A6ERPqwdtuG4wW1reuBClK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/mpIIN/dJMcahi5IOr/A6ERPqwdtuG4wW1reuBClK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/mpIIN/dJMcahi5IOr/A6ERPqwdtuG4wW1reuBClK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FmpIIN%2FdJMcahi5IOr%2FA6ERPqwdtuG4wW1reuBClK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"282\" height=\"300\" data-origin-width=\"282\" data-origin-height=\"300\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">2D 스프라이트 애니메이션 전부에 적용</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"215\" data-origin-height=\"211\"><span data-url=\"https://blog.kakaocdn.net/dn/ODbXF/dJMcac9P43N/RlpOdMH7NHgOjenK7h05h1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/ODbXF/dJMcac9P43N/RlpOdMH7NHgOjenK7h05h1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/ODbXF/dJMcac9P43N/RlpOdMH7NHgOjenK7h05h1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FODbXF%2FdJMcac9P43N%2FRlpOdMH7NHgOjenK7h05h1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"215\" height=\"211\" data-origin-width=\"215\" data-origin-height=\"211\"/></span></figure>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"210\" data-origin-height=\"186\"><span data-url=\"https://blog.kakaocdn.net/dn/dkJ2ut/dJMcabwmMOX/IkxLcPTJ7CbkamtSsrW5H0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/dkJ2ut/dJMcabwmMOX/IkxLcPTJ7CbkamtSsrW5H0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/dkJ2ut/dJMcabwmMOX/IkxLcPTJ7CbkamtSsrW5H0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdkJ2ut%2FdJMcabwmMOX%2FIkxLcPTJ7CbkamtSsrW5H0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"210\" height=\"186\" data-origin-width=\"210\" data-origin-height=\"186\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">환경광 먹은거</p>\n<p data-ke-size=\"size16\">하이트맵 먹은 부분의 밝기가 달라지나봅니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "영상: https://www.youtube.com/watch?v=rR5SrfRX4qU\n\n\n\n여러가지 몰랐던 UI 들을 사용하네요\n나중에 진지하게 봐야겠당\n \n비주얼스크립팅\n글로벌 광원\n픽셀퍼팩트\n\n\n라이트\n\n\n크림자 영역\n\n\n2D 셰이더\n\n\n빛나는 외곽선\n\n\n\n하이트맵 뚝딱 (2D 노멀맵)\n\n\n2D 스프라이트 애니메이션 전부에 적용\n\n\n\n \n환경광 먹은거\n하이트맵 먹은 부분의 밝기가 달라지나봅니다.",
        "guid": "https://serverdown.tistory.com/1573",
        "categories": [
          "프로그래밍/개발메모"
        ],
        "isoDate": "2026-02-15T14:08:53.000Z"
      }
    ]
  },
  {
    "name": "coolspeed",
    "category": "개인",
    "posts": []
  },
  {
    "name": "오늘도 끄적끄적",
    "category": "개인",
    "posts": []
  },
  {
    "name": "dx11 Vanica's Lifelog - 夢が夢で終わらないように",
    "category": "개인",
    "posts": []
  },
  {
    "name": "초코사랑",
    "category": "개인",
    "posts": []
  },
  {
    "name": "ZeroCho Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "imays게임엔진개발자",
    "category": "개인",
    "posts": []
  },
  {
    "name": "RSS feed for hurinmon Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "기억보단 기록을",
    "category": "개인",
    "posts": [
      {
        "creator": "향로 (기억보단 기록을)",
        "title": "AI 시대에는 어떤 글을 써야할까?",
        "link": "https://jojoldu.tistory.com/865",
        "pubDate": "Sun, 15 Feb 2026 23:01:24 +0900",
        "author": "향로 (기억보단 기록을)",
        "comments": "https://jojoldu.tistory.com/865#entry865comment",
        "content": "<p data-ke-size=\"size16\">최근에 BC 신용카드를 해지하려고 했다.<br />BC 카드는 페이북이 공식 앱이라 여기서 해지를 하려고 했다.<br />그런데 아무리 찾아봐도 카드 해지가 없었다.<br />분명 여러 블로그 글에는 페이북 앱에서 <code>해지</code> 검색 -&gt; <code>카드 해지</code> 를 하면 된다고 나오는데도 말이다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"1.jpg\" data-origin-width=\"1080\" data-origin-height=\"2400\"><span data-url=\"https://blog.kakaocdn.net/dn/dnqJPJ/dJMcagxFwh1/sX4JTKL8EWypHl1Ke8iDU0/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/dnqJPJ/dJMcagxFwh1/sX4JTKL8EWypHl1Ke8iDU0/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/dnqJPJ/dJMcagxFwh1/sX4JTKL8EWypHl1Ke8iDU0/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdnqJPJ%2FdJMcagxFwh1%2FsX4JTKL8EWypHl1Ke8iDU0%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"406\" height=\"902\" data-filename=\"1.jpg\" data-origin-width=\"1080\" data-origin-height=\"2400\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">주말이라 고객센터 연락이 안 될 것 같아 앱 내에서 지원하는 AI 챗봇에게 물어봤다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"2.jpg\" data-origin-width=\"1080\" data-origin-height=\"2400\"><span data-url=\"https://blog.kakaocdn.net/dn/RotIf/dJMb99L4MPZ/QIhyzxXoCFXOQCdC2enkA0/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/RotIf/dJMb99L4MPZ/QIhyzxXoCFXOQCdC2enkA0/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/RotIf/dJMb99L4MPZ/QIhyzxXoCFXOQCdC2enkA0/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FRotIf%2FdJMb99L4MPZ%2FQIhyzxXoCFXOQCdC2enkA0%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"405\" height=\"900\" data-filename=\"2.jpg\" data-origin-width=\"1080\" data-origin-height=\"2400\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">구체적인 해지 방법은 안내해주지 못하고 카드 해지 주의 사항에 대해서만 계속 안내했다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"3.jpg\" data-origin-width=\"1080\" data-origin-height=\"2400\"><span data-url=\"https://blog.kakaocdn.net/dn/cm1uyw/dJMcac25Yhq/yIwJeCkhrKfd9HYfpJC2F1/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/cm1uyw/dJMcac25Yhq/yIwJeCkhrKfd9HYfpJC2F1/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/cm1uyw/dJMcac25Yhq/yIwJeCkhrKfd9HYfpJC2F1/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcm1uyw%2FdJMcac25Yhq%2FyIwJeCkhrKfd9HYfpJC2F1%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"382\" height=\"849\" data-filename=\"3.jpg\" data-origin-width=\"1080\" data-origin-height=\"2400\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">답답한 마음에 습관처럼 여러 AI 도구들에게 해지 방법을 물어봤다.</p>\n<p data-ke-size=\"size16\">Claude에게도</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"claude.png\" data-origin-width=\"1668\" data-origin-height=\"1080\"><span data-url=\"https://blog.kakaocdn.net/dn/ca1pJJ/dJMcafev34t/ONQfmcaKKK1AEJG3cAikIk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/ca1pJJ/dJMcafev34t/ONQfmcaKKK1AEJG3cAikIk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/ca1pJJ/dJMcafev34t/ONQfmcaKKK1AEJG3cAikIk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fca1pJJ%2FdJMcafev34t%2FONQfmcaKKK1AEJG3cAikIk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1668\" height=\"1080\" data-filename=\"claude.png\" data-origin-width=\"1668\" data-origin-height=\"1080\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">GPT에게도</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"gpt.png\" data-origin-width=\"1930\" data-origin-height=\"1004\"><span data-url=\"https://blog.kakaocdn.net/dn/bB0FwB/dJMcac9P4SD/VDGKS3OtsT6lE9OsNlUuUk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bB0FwB/dJMcac9P4SD/VDGKS3OtsT6lE9OsNlUuUk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bB0FwB/dJMcac9P4SD/VDGKS3OtsT6lE9OsNlUuUk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbB0FwB%2FdJMcac9P4SD%2FVDGKS3OtsT6lE9OsNlUuUk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1930\" height=\"1004\" data-filename=\"gpt.png\" data-origin-width=\"1930\" data-origin-height=\"1004\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">Gemini에게도</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"gemini.png\" data-origin-width=\"1630\" data-origin-height=\"1172\"><span data-url=\"https://blog.kakaocdn.net/dn/cZLDJM/dJMcaihXG4U/0BnwQX1QupeY0IBb98rJRK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/cZLDJM/dJMcaihXG4U/0BnwQX1QupeY0IBb98rJRK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/cZLDJM/dJMcaihXG4U/0BnwQX1QupeY0IBb98rJRK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcZLDJM%2FdJMcaihXG4U%2F0BnwQX1QupeY0IBb98rJRK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1630\" height=\"1172\" data-filename=\"gemini.png\" data-origin-width=\"1630\" data-origin-height=\"1172\"/></span></figure>\n</p>\n<blockquote data-ke-style=\"style1\">\n<p data-ke-size=\"size16\">참고로 3개 AI 서비스 모두 가장 높은 모드로 질문했다.</p>\n</blockquote>\n<p data-ke-size=\"size16\"><b>모두가 페이북 앱 내에 해지가 있다고 안내했다</b>.</p>\n<p data-ke-size=\"size16\">좀 더 찾아보니 BC 카드는 예전에 BC카드 앱이 별도로 있다가 페이북 앱으로 전환이 되었다.<br />과거의 페이북 앱에는 직접 해지가 있었지만, 최근의 페이북 앱에는 해지 기능이 없어졌다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"4.png\" data-origin-width=\"1196\" data-origin-height=\"596\"><span data-url=\"https://blog.kakaocdn.net/dn/rfnFK/dJMcaihXG5y/EvLILR0zhnaJZJedwSKZhk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/rfnFK/dJMcaihXG5y/EvLILR0zhnaJZJedwSKZhk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/rfnFK/dJMcaihXG5y/EvLILR0zhnaJZJedwSKZhk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FrfnFK%2FdJMcaihXG5y%2FEvLILR0zhnaJZJedwSKZhk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1196\" height=\"596\" data-filename=\"4.png\" data-origin-width=\"1196\" data-origin-height=\"596\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">그래서 대부분의 블로그 글이 과거의 BC카드 앱이나 과거의 페이북 앱 기준으로 작성된 글이 많았다.<br />아무래도 AI는 과거의 데이터를 기반으로 답변을 하니 어쩔 수 없단 생각에 직접 찾아보기로 했다.</p>\n<p data-ke-size=\"size16\">가장 최신의 방법을 찾으면 되니, 최신 기준으로 네이버 검색을 다시 했다.</p>\n<p data-ke-size=\"size16\">그런데 <b>가장 최신의 글들도 대부분이 AI가 작성한 블로그 글이었고, 그 AI는 예전에 작성된 글을 참고해서 여전히 과거의 해지 방법을 소개</b>하고 있었다.</p>\n<p data-ke-size=\"size16\">최신순으로 검색해도 내가 원하는 정보를 못 찾는 상황이 된 것이다.</p>\n<p data-ke-size=\"size16\">Claude에게 내가 원하는 조건의 해지 방법을 갖고 있는 네이버 블로그 글을 탐색하도록 시키고, 혹시 몰라 나도 하나씩 읽어보면서 찾아봤다.</p>\n<p data-ke-size=\"size16\">결국엔 <a href=\"https://blog.naver.com/yuretomo9/224108014416\">디지털 ARS로는 해지가 가능하다</a>는 것을 알게 되어서 디지털 ARS로 해지를 신청했다.</p>\n<p data-ke-size=\"size16\">신용카드 하나 해지하는 데 2시간을 보냈다.</p>\n<hr data-ke-style=\"style1\" />\n<p data-ke-size=\"size16\">AI가 답변을 다 해주는데 블로그 글을 쓰는 게 의미가 있는 것인가, 무엇을 써야 하는 것인가에 대한 질문을 자주 받는다.</p>\n<p data-ke-size=\"size16\">이번 일을 겪으면서 오히려 그 반대라는 확신이 생겼다.<br /><b>직접 해본 경험기를 남기는 것</b>이 훨씬 더 중요해졌다.</p>\n<p data-ke-size=\"size16\">AI 시대가 되면서, 어떤 방법이든 해결만 하면 되는 문제는 확실히 예전보다 쉬워졌다.<br />하지만 내가 원하는 특정 방법을 찾는 것은 오히려 더 어려워졌다.<br />실제로 그 일을 하지 않고도 그 일을 한 것처럼 과정을 공유하는 것이 너무나 쉬워졌기 때문이다.</p>\n<p data-ke-size=\"size16\">예전에도 원하는 정보를 찾는 것은 어려웠다.<br />그런데 이젠 가장 최신의 글조차도 과거 글의 복제본이어서 무효한 경우가 다반사가 되었다.</p>\n<p data-ke-size=\"size16\">그래서 점점 이런 것이 중요해진다고 생각한다.</p>\n<p data-ke-size=\"size16\">\"<b>저 사람은 항상 자신이 직접 하는 사람이야, 실제로 본인이 해본 것을 기록하는 사람이야</b>.\"</p>\n<p data-ke-size=\"size16\">작성자보다는 콘텐츠가 중요하다고 하는 시기도 있었지만, 이제는 작성자가 훨씬 더 중요하다.<br /><b>작성자가 신뢰할 만한 사람이냐</b>가 콘텐츠의 가치를 결정하는 시대가 된 것이다.</p>\n<p data-ke-size=\"size16\">그러니 AI의 도움을 받아 더 많이 직접 실행하고 경험해봐야 한다.<br />그리고 그걸 기록으로 남겨야 한다.</p>\n<p data-ke-size=\"size16\">AI로 얻게 된 가장 큰 장점은 더 많은 경험을 더 빠르게 해볼 수 있다는 것이다.<br /><b>AI를 간접 경험의 도구가 아니라, 직접 경험을 더 쉽고 빠르게, 더 깊게 해볼 수 있는 도구로 사용하는 것.</b><br />그것이 앞으로의 시대를 준비하는 가장 좋은 방법이 아닐까 싶다.</p>",
        "contentSnippet": "최근에 BC 신용카드를 해지하려고 했다.\nBC 카드는 페이북이 공식 앱이라 여기서 해지를 하려고 했다.\n그런데 아무리 찾아봐도 카드 해지가 없었다.\n분명 여러 블로그 글에는 페이북 앱에서 해지 검색 -> 카드 해지 를 하면 된다고 나오는데도 말이다.\n\n\n주말이라 고객센터 연락이 안 될 것 같아 앱 내에서 지원하는 AI 챗봇에게 물어봤다.\n\n\n구체적인 해지 방법은 안내해주지 못하고 카드 해지 주의 사항에 대해서만 계속 안내했다.\n\n\n답답한 마음에 습관처럼 여러 AI 도구들에게 해지 방법을 물어봤다.\nClaude에게도\n\n\nGPT에게도\n\n\nGemini에게도\n\n\n\n참고로 3개 AI 서비스 모두 가장 높은 모드로 질문했다.\n모두가 페이북 앱 내에 해지가 있다고 안내했다.\n좀 더 찾아보니 BC 카드는 예전에 BC카드 앱이 별도로 있다가 페이북 앱으로 전환이 되었다.\n과거의 페이북 앱에는 직접 해지가 있었지만, 최근의 페이북 앱에는 해지 기능이 없어졌다.\n\n\n그래서 대부분의 블로그 글이 과거의 BC카드 앱이나 과거의 페이북 앱 기준으로 작성된 글이 많았다.\n아무래도 AI는 과거의 데이터를 기반으로 답변을 하니 어쩔 수 없단 생각에 직접 찾아보기로 했다.\n가장 최신의 방법을 찾으면 되니, 최신 기준으로 네이버 검색을 다시 했다.\n그런데 가장 최신의 글들도 대부분이 AI가 작성한 블로그 글이었고, 그 AI는 예전에 작성된 글을 참고해서 여전히 과거의 해지 방법을 소개하고 있었다.\n최신순으로 검색해도 내가 원하는 정보를 못 찾는 상황이 된 것이다.\nClaude에게 내가 원하는 조건의 해지 방법을 갖고 있는 네이버 블로그 글을 탐색하도록 시키고, 혹시 몰라 나도 하나씩 읽어보면서 찾아봤다.\n결국엔 디지털 ARS로는 해지가 가능하다는 것을 알게 되어서 디지털 ARS로 해지를 신청했다.\n신용카드 하나 해지하는 데 2시간을 보냈다.\nAI가 답변을 다 해주는데 블로그 글을 쓰는 게 의미가 있는 것인가, 무엇을 써야 하는 것인가에 대한 질문을 자주 받는다.\n이번 일을 겪으면서 오히려 그 반대라는 확신이 생겼다.\n직접 해본 경험기를 남기는 것이 훨씬 더 중요해졌다.\nAI 시대가 되면서, 어떤 방법이든 해결만 하면 되는 문제는 확실히 예전보다 쉬워졌다.\n하지만 내가 원하는 특정 방법을 찾는 것은 오히려 더 어려워졌다.\n실제로 그 일을 하지 않고도 그 일을 한 것처럼 과정을 공유하는 것이 너무나 쉬워졌기 때문이다.\n예전에도 원하는 정보를 찾는 것은 어려웠다.\n그런데 이젠 가장 최신의 글조차도 과거 글의 복제본이어서 무효한 경우가 다반사가 되었다.\n그래서 점점 이런 것이 중요해진다고 생각한다.\n\"저 사람은 항상 자신이 직접 하는 사람이야, 실제로 본인이 해본 것을 기록하는 사람이야.\"\n작성자보다는 콘텐츠가 중요하다고 하는 시기도 있었지만, 이제는 작성자가 훨씬 더 중요하다.\n작성자가 신뢰할 만한 사람이냐가 콘텐츠의 가치를 결정하는 시대가 된 것이다.\n그러니 AI의 도움을 받아 더 많이 직접 실행하고 경험해봐야 한다.\n그리고 그걸 기록으로 남겨야 한다.\nAI로 얻게 된 가장 큰 장점은 더 많은 경험을 더 빠르게 해볼 수 있다는 것이다.\nAI를 간접 경험의 도구가 아니라, 직접 경험을 더 쉽고 빠르게, 더 깊게 해볼 수 있는 도구로 사용하는 것.\n그것이 앞으로의 시대를 준비하는 가장 좋은 방법이 아닐까 싶다.",
        "guid": "https://jojoldu.tistory.com/865",
        "categories": [
          "생각정리",
          "AI",
          "AI 생성 콘텐츠",
          "BC카드 해지",
          "바로 클리어 카드 해지",
          "블로그",
          "페이북 카드 해지"
        ],
        "isoDate": "2026-02-15T14:01:24.000Z"
      },
      {
        "creator": "향로 (기억보단 기록을)",
        "title": "회사에서 AI 개발 도구를 지원하지 않는다면",
        "link": "https://jojoldu.tistory.com/864",
        "pubDate": "Sun, 15 Feb 2026 00:52:38 +0900",
        "author": "향로 (기억보단 기록을)",
        "comments": "https://jojoldu.tistory.com/864#entry864comment",
        "content": "<p>요즘 다들 AI 도구에 대한 FOMO를 이야기하지만,<br>이메일로 고민 상담을 들어보면 여전히 AI 도구에 대한 사내의 공식적인 지원이 없는 경우가 많다.<br>(예전에 JetBrains 도구의 지원이 없어서 STS, Eclipse를 쓰던 것처럼..)</p>\n<p>물론, 금융권과 같이 특수하게 높은 보안 레벨을 유지해야 하는 서비스의 특성상 AI 도구 자체가 금지되는 경우도 있다.<br>이런 경우는 당연히 어쩔 수 없다.  </p>\n<p>하지만 그런 도메인의 특성이 없는 경우에도 AI 도구를 지원하지 않는 경우가 많다.<br>그럴 땐 어떻게 해야 할까?  </p>\n<p>10년 전이나 지금이나 개인의 생산성과 성장을 위해 필요하면 개인 비용으로 당연히 사용할 수 있다.</p>\n<p>그렇지만, 개인에 따라 그 비용도 부담스러울 수 있다.<br>요즘의 환율로 계산하면 거의 월 30만원은 돼야 하나의 도구를 제대로 써볼 수 있는 시대에는 특히나 그렇다.  </p>\n<p>지금의 AI 시대를 그냥 회사 탓하면서 보낼 순 없다.<br>그럴 경우엔 개인적으로 Amazon Web Services (AWS) Kiro와 JetBrains Junie, 이 두 가지를 검토해보는 것을 추천한다.</p>\n<p>이유는 단순하다.<br>이 두 도구는 이미 회사에서 쓰고 있을 가능성이 높은 플랫폼 위에 있기 때문이다.  </p>\n<p>JetBrains의 Junie는 All Product 라이선스가 있을 경우 AI Pro 라이선스로 사용할 수 있다.<br>(물론, JetBrains All Product 라이선스를 지원하면 AI 도구도 충분히 지원하는 회사이지 않겠냐고 할 수 있다.<br>이미 JetBrains IDE에 대한 지원을 하고 거기에 추가로 AI 도구까지 지원하는 그 +α의 비용을 부담스러워해서 미지원하는 경우가 생각보다 많다.)  </p>\n<p>이 경우 JetBrains의 Junie는 충분히 AI 개발 경험을 쌓을 수 있는 도구다.</p>\n<p>다른 AI 도구들의 Max Plan처럼 미친 듯이 토큰을 사용할 수 있을 정도는 아니지만, 그래도 어느 정도는 에이전틱 프로그래밍에 대한 경험을 쌓아볼 수는 있다.  </p>\n<p>AWS는 국내에서는 거의 대부분의 서비스가 사용하고 있는 클라우드이다.</p>\n<p>AWS Kiro의 장점은 바로 그 AWS Billing에서 비용이 관리되어 별도의 계약이나 재무적 협의가 필요하지 않다는 점, 그리고 요즘 AWS에서 크레딧 지원을 많이 해준다는 점이다.  </p>\n<p>Claude Code, Codex, Cursor와 같은 도구들은 아무리 좋아도 회사에 도입하려면 새로운 벤더와의 계약, 신규 예산 품의, 보안 검토 등을 처음부터 시작해야 한다.  </p>\n<p>그런데 Kiro와 Junie는 이미 회사가 쓰고 있는 AWS, 젯브레인 생태계 안에 있다.<br>즉, 새로운 벤더를 찾아 계약하는 것이 아니라 큰 협의 과정 없이 시작할 수 있다.</p>\n<p>앞서 이야기한 예산 품의, 보안 심사 같은 산들이 한꺼번에 낮아진다.</p>\n<p>솔직히 AI 코딩 도구의 성능 차이는 몇 달 단위로 뒤집힌다.<br>오늘 A가 좋아도 내일 B가 더 좋아질 수 있고, 그 반대도 마찬가지다.</p>\n<p>그래서 나는 &quot;어떤 도구가 가장 뛰어난가&quot;보다 &quot;어떤 도구를 우리 팀이 가장 빨리 쓸 수 있는가&quot;가 더 중요하다고 생각한다.</p>\n<p>아무리 좋은 도구도 도입까지 6개월이 걸리면 의미가 없다.<br>지금 당장 쓸 수 있는 도구를 먼저 도입하고, 팀이 AI 코딩에 익숙해지는 것이 훨씬 더 가치 있다.</p>\n<p>그런 관점에서 AI 도구를 사용하기 위해 사내에서 협의해야 할 것들이 너무 많다면, AWS Kiro와 JetBrains Junie를 고민해보는 것을 추천한다.</p>\n<hr>\n<p>&quot;좋은 건 알겠는데, 리더에게 어떻게 말씀드리지?&quot; 라는 고민이 있을 수 있다.    </p>\n<p>특히 Kiro의 경우 AWS 위에 있다 보니, 기존 예산 체계나 계정 관리 측면에서 리더를 설득할 수 있는 구체적인 포인트들이 있다.<br>Yan So 님이 이런 부분을 잘 정리해주신 글이 있어서 공유한다.</p>\n<p><a href=\"https://yanlog.yanbert.com/ko/blog/dear-team-lead-lets-adopt-kiro-20260211/\">Kiro를 기업에서 쓰기 좋은 이유 (IAM Identity Center)</a></p>\n<p>&quot;왜 우리 리더들은 AI 도구를 지원하지 않는 것인가&quot;에 대해 리더의 입장에서의 시각과 그들을 설득하기 위한 충분한 근거를 자세히 설명해 주셨다.<br>예산과 비용 처리 관점에서 왜 도입이 지연되는지, 그리고 리더 입장에서 Kiro를 어떤 포인트로 검토하면 좋을지가 실무적으로 정리되어 있다.</p>\n<p>AI 코딩 도구 도입이 아직인 조직이 있다면, 한번 읽어보고 팀 리더에게 제안해보자.</p>",
        "contentSnippet": "요즘 다들 AI 도구에 대한 FOMO를 이야기하지만,\n이메일로 고민 상담을 들어보면 여전히 AI 도구에 대한 사내의 공식적인 지원이 없는 경우가 많다.\n(예전에 JetBrains 도구의 지원이 없어서 STS, Eclipse를 쓰던 것처럼..)\n물론, 금융권과 같이 특수하게 높은 보안 레벨을 유지해야 하는 서비스의 특성상 AI 도구 자체가 금지되는 경우도 있다.\n이런 경우는 당연히 어쩔 수 없다.  \n하지만 그런 도메인의 특성이 없는 경우에도 AI 도구를 지원하지 않는 경우가 많다.\n그럴 땐 어떻게 해야 할까?  \n10년 전이나 지금이나 개인의 생산성과 성장을 위해 필요하면 개인 비용으로 당연히 사용할 수 있다.\n그렇지만, 개인에 따라 그 비용도 부담스러울 수 있다.\n요즘의 환율로 계산하면 거의 월 30만원은 돼야 하나의 도구를 제대로 써볼 수 있는 시대에는 특히나 그렇다.  \n지금의 AI 시대를 그냥 회사 탓하면서 보낼 순 없다.\n그럴 경우엔 개인적으로 Amazon Web Services (AWS) Kiro와 JetBrains Junie, 이 두 가지를 검토해보는 것을 추천한다.\n이유는 단순하다.\n이 두 도구는 이미 회사에서 쓰고 있을 가능성이 높은 플랫폼 위에 있기 때문이다.  \nJetBrains의 Junie는 All Product 라이선스가 있을 경우 AI Pro 라이선스로 사용할 수 있다.\n(물론, JetBrains All Product 라이선스를 지원하면 AI 도구도 충분히 지원하는 회사이지 않겠냐고 할 수 있다.\n이미 JetBrains IDE에 대한 지원을 하고 거기에 추가로 AI 도구까지 지원하는 그 +α의 비용을 부담스러워해서 미지원하는 경우가 생각보다 많다.)  \n이 경우 JetBrains의 Junie는 충분히 AI 개발 경험을 쌓을 수 있는 도구다.\n다른 AI 도구들의 Max Plan처럼 미친 듯이 토큰을 사용할 수 있을 정도는 아니지만, 그래도 어느 정도는 에이전틱 프로그래밍에 대한 경험을 쌓아볼 수는 있다.  \nAWS는 국내에서는 거의 대부분의 서비스가 사용하고 있는 클라우드이다.\nAWS Kiro의 장점은 바로 그 AWS Billing에서 비용이 관리되어 별도의 계약이나 재무적 협의가 필요하지 않다는 점, 그리고 요즘 AWS에서 크레딧 지원을 많이 해준다는 점이다.  \nClaude Code, Codex, Cursor와 같은 도구들은 아무리 좋아도 회사에 도입하려면 새로운 벤더와의 계약, 신규 예산 품의, 보안 검토 등을 처음부터 시작해야 한다.  \n그런데 Kiro와 Junie는 이미 회사가 쓰고 있는 AWS, 젯브레인 생태계 안에 있다.\n즉, 새로운 벤더를 찾아 계약하는 것이 아니라 큰 협의 과정 없이 시작할 수 있다.\n앞서 이야기한 예산 품의, 보안 심사 같은 산들이 한꺼번에 낮아진다.\n솔직히 AI 코딩 도구의 성능 차이는 몇 달 단위로 뒤집힌다.\n오늘 A가 좋아도 내일 B가 더 좋아질 수 있고, 그 반대도 마찬가지다.\n그래서 나는 \"어떤 도구가 가장 뛰어난가\"보다 \"어떤 도구를 우리 팀이 가장 빨리 쓸 수 있는가\"가 더 중요하다고 생각한다.\n아무리 좋은 도구도 도입까지 6개월이 걸리면 의미가 없다.\n지금 당장 쓸 수 있는 도구를 먼저 도입하고, 팀이 AI 코딩에 익숙해지는 것이 훨씬 더 가치 있다.\n그런 관점에서 AI 도구를 사용하기 위해 사내에서 협의해야 할 것들이 너무 많다면, AWS Kiro와 JetBrains Junie를 고민해보는 것을 추천한다.\n\"좋은 건 알겠는데, 리더에게 어떻게 말씀드리지?\" 라는 고민이 있을 수 있다.    \n특히 Kiro의 경우 AWS 위에 있다 보니, 기존 예산 체계나 계정 관리 측면에서 리더를 설득할 수 있는 구체적인 포인트들이 있다.\nYan So 님이 이런 부분을 잘 정리해주신 글이 있어서 공유한다.\nKiro를 기업에서 쓰기 좋은 이유 (IAM Identity Center)\n\"왜 우리 리더들은 AI 도구를 지원하지 않는 것인가\"에 대해 리더의 입장에서의 시각과 그들을 설득하기 위한 충분한 근거를 자세히 설명해 주셨다.\n예산과 비용 처리 관점에서 왜 도입이 지연되는지, 그리고 리더 입장에서 Kiro를 어떤 포인트로 검토하면 좋을지가 실무적으로 정리되어 있다.\nAI 코딩 도구 도입이 아직인 조직이 있다면, 한번 읽어보고 팀 리더에게 제안해보자.",
        "guid": "https://jojoldu.tistory.com/864",
        "categories": [
          "생각정리",
          "AI 코딩",
          "aws kiro",
          "jetbrains junie",
          "kiro",
          "vibe coding",
          "바이브 코딩",
          "에이전틱 코딩",
          "젯브레인"
        ],
        "isoDate": "2026-02-14T15:52:38.000Z"
      }
    ]
  },
  {
    "name": "WestwoodForever's Dev Log",
    "category": "개인",
    "posts": []
  },
  {
    "name": "허니몬(Honeymon)의 자바guru",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Wolf Loves Fox :: 일상",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Game Programmer Life",
    "category": "개인",
    "posts": []
  },
  {
    "name": "yuchi's dev",
    "category": "개인",
    "posts": []
  },
  {
    "name": "만화로 나누는 자유/오픈소스 소프트웨어 이야기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "신현석(Hyeonseok Shin)",
    "category": "개인",
    "posts": []
  },
  {
    "name": "즐거운 개발자 :: 네이버  블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "황제펭귄의 게임개발이야기 [여기는 한국]",
    "category": "개인",
    "posts": []
  },
  {
    "name": "LINE ENGINEERING",
    "category": "기업",
    "posts": [
      {
        "title": "Claude Code Action: 조직 전반의 코드 품질을 지키는 AI 코드 리뷰 플랫폼화",
        "link": "https://techblog.lycorp.co.jp/ko/building-ai-code-review-platform-with-claude-code-action",
        "pubDate": "Fri, 13 Feb 2026 02:00:00 GMT",
        "content": "들어가며안녕하세요. LINE NEXT DevOps 팀에서 일하고 있는 이동원입니다. 저는 쿠버네티스 기반 인프라 운영과 CI/CD 구축, 모니터링 및 장애 대응 등 인프라 운영 관...",
        "contentSnippet": "들어가며안녕하세요. LINE NEXT DevOps 팀에서 일하고 있는 이동원입니다. 저는 쿠버네티스 기반 인프라 운영과 CI/CD 구축, 모니터링 및 장애 대응 등 인프라 운영 관...",
        "guid": "https://techblog.lycorp.co.jp/ko/building-ai-code-review-platform-with-claude-code-action",
        "isoDate": "2026-02-13T02:00:00.000Z"
      },
      {
        "title": "슬로우 쿼리 해결기: 함수형 인덱스로 비트 연산 쿼리 최적화하기",
        "link": "https://techblog.lycorp.co.jp/ko/solving-slow-queries-optimizing-bitwise-operation-queries-with-functional-indexes",
        "pubDate": "Fri, 13 Feb 2026 02:00:00 GMT",
        "content": "들어가며안녕하세요. LINE VOOM 서비스의 포스트 서버를 개발하고 있는 서용준입니다. 이번 글에서는 저희 팀이 약 7개월에 걸쳐 슬로우 쿼리 문제를 해결한 과정과 그 과정에서 ...",
        "contentSnippet": "들어가며안녕하세요. LINE VOOM 서비스의 포스트 서버를 개발하고 있는 서용준입니다. 이번 글에서는 저희 팀이 약 7개월에 걸쳐 슬로우 쿼리 문제를 해결한 과정과 그 과정에서 ...",
        "guid": "https://techblog.lycorp.co.jp/ko/solving-slow-queries-optimizing-bitwise-operation-queries-with-functional-indexes",
        "isoDate": "2026-02-13T02:00:00.000Z"
      }
    ]
  },
  {
    "name": "뱅크샐러드 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "우아한형제들 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "TOAST Meetup",
    "category": "기업",
    "posts": []
  },
  {
    "name": "ZUM 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "SK Planet",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Spoqa tech blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "팀 왈도 번역팀",
    "category": "게임",
    "posts": []
  },
  {
    "name": "근원님",
    "category": "개인",
    "posts": []
  },
  {
    "name": "황의윤",
    "category": "개인",
    "posts": []
  },
  {
    "name": "호돌맨",
    "category": "개인",
    "posts": []
  },
  {
    "name": "박우빈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "문다영",
    "category": "개인",
    "posts": []
  },
  {
    "name": "유수민",
    "category": "개인",
    "posts": []
  },
  {
    "name": "안건주",
    "category": "개인",
    "posts": []
  },
  {
    "name": "손현호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "STARTUP BIBLE",
    "category": "개인",
    "posts": [
      {
        "creator": "Kihong Bae",
        "title": "참 어려운 상장 시장",
        "link": "https://www.thestartupbible.com/2026/02/public-markets-are-hard-for-vcs.html",
        "pubDate": "Wed, 18 Feb 2026 21:29:00 +0000",
        "content:encodedSnippet": "우리는 쿠팡의 매우 작은 주주이다. 스트롱이 쿠팡에 직접 투자하진 않았지만, 스트로의 포트폴리오였던 Recomio라는 회사가 쿠팡에 주식교환으로 인수되면서 쿠팡의 주주가 됐고, 상장 주식을 대부분 팔았지만 아직 소량을 보유하고 있다. 우리 1호 펀드의 포트폴리오인데 이미 이 펀드의 나이는 14살이 되어 간다. 청산 시점이 지났기 때문에 우리의 LP들이 지난 몇 년 동안 꾸준히 물어봤던 질문이 남은 쿠팡 주식 처분 시점이다.\n이분들에게 나의 한결같은 답변은 “한국에 살면 직접 체감할 수 있는데, 쿠팡은 한국의 이커머스를 완전히 다 먹어버리고 있습니다. 제가 보기엔 현재 쿠팡 주가는(당시 $30 ~ $35 정도) 저평가되어 있기 때문에 가격이 더 오르면 팔 건데 그게 최소 $50 선이 되지 않을까 싶습니다.” 였다. 실은 매우 확신에 찬 자신 있는 답변이었다. 솔직히 겉으로는 $50라고 했지만, 내 마음속엔 쿠팡은 $100까지 간다고 믿고 있었다.\n그런데 이젠 솔직히 잘 모르겠다. 개인정보가 유출됐을 때만 해도 잘 해결하면 된다고 생각했는데, 이후에 쿠팡이 보여준 태도와 중요한 순간에 했던 결정들의 질이 상당히 실망스러웠고, 그 결과 때문에 현재 쿠팡 주식은 $20를 하회하고 있다. 역시 어떤 LP 분들은 나에게 과연 쿠팡 주식이 앞으로 $50 가 될 수 있을지 물어보고, 더 나아가서는 우리의 포트폴리오사가 상장하면, 스트롱은 어떤 기준으로 상장 주식을 처리할지 물어보는 분들도 있다.\n이 질문에 대해서 나는 몇 주 동안 시간 날 때마다 곰곰이 생각해 보고 있는데, 내 결론은 상장 시장은 참 어려운 시장이고, 비상장 회사에 투자하는 VC에겐 – 특히나 우리 같이 극초기 회사에 투자하는 – 제3외국어와 같이 이해하고 배우기 어렵다는 것이다. 엄밀히 말하면 쿠팡 사업 자체의 펀더멘탈이 무너져서 주가가 하락한다고 볼 순 없다. 쿠팡의 펀더멘탈은 실은 매우 강하다. 이 정도로 물류 인프라를 잘 운영하고, 이 정도로 세상 온갖 제품을 잘 판매하는 회사는 한국에 없고, 전 세계에도 몇 개 없을 정도로 사업은 잘 한다. 물론, 그 물류와 이커머스 사업의 인프라를 비인간적으로 운영하기도 하지만, 어쨌든 사업 자체만 봤을 땐 굉장히 탄탄한 회사다. 주가가 폭락하는 이유는 시장의 정서와 감정의 문제이고, 흔히 이 바닥에서 말하는 FUDGE(Fear, Uncertainty, Doubt, Greed, Emotion)가 크게 작용하고 있다.\n이번에 미국에서 만난 어떤 투자자는 중국의 VC에 오래전에 투자했는데, 이 VC가 초기 투자했던 중국 회사가 상장하면서 당시의 시총에 의하면 거의 2,000배의 돈을 (서류상으로) 벌었다고 했다. 하지만, 이 VC는 마치 내가 쿠팡에 대해서 확신했듯이, 그 회사의 펀더멘탈이 강하고 시장이 더 성장할 것이기 때문에 몇 년 후엔 더 많은 돈을 벌 수 있을 것이라는 자신감으로 상장 주식을 안 팔고 계속 보유했다고 한다. 그래서 4년을 더 보유했는데, 그동안 부침이 있었지만, 현재 가격은 4년 전과 똑같다고 한다. 서류상으로 돈을 크게 잃진 않았지만, 이 VC에 투자한 LP들은 4년 동안 실제 배분받은 건 한 푼도 없었고, 막대한 기회비용이 발생해서 굉장히 불만이 많은 것 같다. 실은 이 회사는 그동안 매출은 많이 증가했지만, 중국 정부의 규제와 방향에 대한 불확실성으로 인해서 주가는 제자리걸음을 하고 있었다.\n상장 시장은 비상장 시장과 많이 다르다. 남들은 비상장 시장이 비이성적이라고 하는데, 나는 오히려 상장 시장이 더 비이성적인 것 같다. 논리보단 감정, 두려움과 욕심이 주가를 움직이는 시장인데, 솔직히 생각해 보면 이 세상이 비이성적이라서 이게 고스란히 상장 시장에 전체적으로 반영되는 것 같다.\n쿠팡 사태로 내가 배운 점은 – 그리고 아직도 이 배움은 계속 되고 있다 – 우리가 투자한 회사가 상장하게 되면, 너무 욕심부리지 말고 적당한 시점에 파는 게 어쩌면 VC들에겐 더 맞는 전략일 수도 있다는 것이다. 계속 보유하면서 상장 시장을 공부하고 예측하는 방법도 있지만, 위에서 말한 여러 가지 요소 때문에 상장 시장을 분석하고 예측하는 건 우리 같은 초기 VC들이 잘할 수 있는 게임은 아닌 것 같다.",
        "dc:creator": "Kihong Bae",
        "comments": "https://www.thestartupbible.com/2026/02/public-markets-are-hard-for-vcs.html#respond",
        "content": "우리는 쿠팡의 매우 작은 주주이다. 스트롱이 쿠팡에 직접 투자하진 않았지만, 스트로의 포트폴리오였던 Recomio라는 회사가 쿠팡에 주식교환으로 인수되면서 쿠팡의 주주가 됐고, 상장 주식을 대부분 팔았지만 아직 소량을 보유하고 있다. 우리 1호 펀드의 포트폴리오인데 이미 이 펀드의 나이는 14살이 되어 간다. 청산 시점이 지났기 때문에 우리의 LP들이 지난 몇 년 동안 꾸준히 물어봤던 질문이 남은 쿠팡 주식(...)",
        "contentSnippet": "우리는 쿠팡의 매우 작은 주주이다. 스트롱이 쿠팡에 직접 투자하진 않았지만, 스트로의 포트폴리오였던 Recomio라는 회사가 쿠팡에 주식교환으로 인수되면서 쿠팡의 주주가 됐고, 상장 주식을 대부분 팔았지만 아직 소량을 보유하고 있다. 우리 1호 펀드의 포트폴리오인데 이미 이 펀드의 나이는 14살이 되어 간다. 청산 시점이 지났기 때문에 우리의 LP들이 지난 몇 년 동안 꾸준히 물어봤던 질문이 남은 쿠팡 주식(...)",
        "guid": "https://www.thestartupbible.com/?p=9690",
        "categories": [
          "Uncategorized",
          "exit",
          "general",
          "ipo",
          "korea",
          "strategy",
          "Strong"
        ],
        "isoDate": "2026-02-18T21:29:00.000Z"
      }
    ]
  },
  {
    "name": "매거진 입맛",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "요즘 IT",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "쿠팡 엔지니어링",
    "category": "기업",
    "posts": []
  },
  {
    "name": "지마켓 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "리멤버 (드라마 앤 컴퍼니)",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Kimchi hill",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "Hudi.blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "토스",
    "category": "기업",
    "posts": [
      {
        "title": "Can a single word from a customer change a service?",
        "link": "https://toss.im/tossfeed/article/usercentric",
        "pubDate": "Thu, 19 Feb 2026 10:11:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-of5acw{margin:24px 0 8px;text-align:center;}.css-1pgssrp{max-width:100%;border-radius:16px;}.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}.css-1kxrhf3{white-space:pre-wrap;}Toss listens to hundreds of VOCs every month and applies user feedback to the product in no time—from small texts, screen layouts to new features. The principle Toss has always upheld, “User Centric”, isn’t just some grand slogan, but something that sits at the core of our daily conversations and questions.\n.css-1feg9au{font-size:24px;letter-spacing:0em;line-height:1.6;font-weight:bold;color:var(--adaptiveGrey900);margin:24px 0 4px;}\n.css-q3ktjb{white-space:pre-wrap;font-weight:bold;}Numbers Don’t Tell the Full Story\nData tells us “what” happened, but not “why.” That’s why we always listen to the users. When we set a goal, verify a hypothesis or identify a problem, real insights come from real conversations, not just numbers and assumptions. That is why our UX researchers build hypotheses based on various sources then validate them through tests and user interviews. The result isn’t guesswork, but insights rooted in real context that bring depth to every product decision we make.\nTools That Connect Us with Users\nBack in 2020, all research had to be done by the UX researchers. But as Toss expanded into banking, securities, insurance and more, research became broader and more complex, which led us to build our own tools to hear the voices of our users more clearly.\nThe first tool we built was the “Toss Form,” a simple way for anyone to create surveys, send push notifications to users and collect responses. This allowed not only UX researchers but also designers, product owners, and developers to run surveys themselves and get direct results. Next came the “Ask Me Anything” program. User research originally required recruiting participants and juggling schedules. Now, this program makes research effortless. Teams can instantly run online interviews with random users anytime, and uncover what data alone can’t tell us: When do people use Toss the most? What do users make of this phrase? Are our screens too complicated?\nConducting research became much simpler thanks to our internally developed tools and processes, but restrictions still remained. Usability Testing (UT) required preparing app prototypes and questionnaires, which took at least an hour for even the simplest tests. This kept us from conducting larger and more frequent tests. We needed to find a completely new way to check usability much more frequently and easily. That’s how “Huri-bot,” our AI-powered usability testing tool, was born. Trained on Toss user patterns, the bot can evaluate early prototypes and help refine designs and products with ease.\nUX Researchers, Drawing the Bigger Picture\nOnce the environment was set up for anyone to run tests easily, UX researchers were able to focus on more complex, in-depth problems. Their roles expanded beyond product features, into areas like analyzing business domains such as commerce and ads, and reviewing Toss’ overall UX flow to suggest improvements. They also dived deep into specific topics, such as studying the financial lives of senior users, testing accessibility for users with visual impairment, and validating interaction and graphic designs across the app.\nOn top of that, they find entirely new approaches to solve problems that existing methods could never uncover. Toss, which began with a single simple transfer service, now offers over 100 services. With so many services, it takes much more than a usability test to check if users are easily accessing their desired services. For that reason, we created TNS(Toss Navigation Score). TNS measures how easily users reach a service, and the results are used to improve entry points and information architecture across the app. For example, we might ask a user, “You can file a hospital bill with your insurer through Toss. Where would you go to do that?” We then track how they navigate the app and give a score. TNS helps us refine the overall service structure by analyzing what confuses users and what doesn’t.\nWe are continuously improving the way we collect VOCs and reflect them in our services. But every journey starts with the same question: Is this service truly for the users? The tools may change, but Toss principles remain the same. We keep our ears open to users every single day, because the best answers always start with them.\nResearch Story 1 :\nFinding answers where users get stuck\nThe “Personalized Loan Finder” service, launched in 2019, allows users to compare loan products and apply for the most favorable option when they are in genuine need of a loan. The TNS score for the service is 66, which is higher than the average of 59, but it still leaves room for improvement.\nA closer look at the user journey* through TNS revealed that many users struggled to find the entry point to the service.The reason was obvious. The entry button was labeled, “My Loan Limit,” which wasn’t enough to show that the users could actually compare loan products there. Some users even went straight to the “My Assets” page, thinking that loans were also part of their assets, only to find no path to the service. In other words, the user journey was cut off mid-flow.\n.css-18442ym{font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}.css-jfs1hr{white-space:pre-wrap;font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}*The end-to-end experience of a user interacting with a product to achieve a certain goal.\nFirst, we changed the label of the button. We tested 3 variations to replace the “My Loan Limit” label, and changed the label to “Compare New Loans,” which had the highest conversion rate. Second, we reinforced the link between the service and the “My Assets” page. By adding new content that displayed estimated interest rates and limits, users in need of a loan were directed more seamlessly into the “Personalized Loan Finder” service.\nThe impact was immediate. The internal data showed that the number of preliminary loan applications coming through the “My Assets” page jumped 144% MoM and 124% WoW.* This experience was a clear reminder of one simple truth: every answer can be found in the user.\n*As of July 2025\nResearch Story 2 :\nThe Toss Identity in the Eyes of Users\nIn 2023, Toss expanded its simple payment service, Toss Pay, into the offline world. Users could now pay with Toss not only online, but also at convenience stores, cafés, and restaurants. But this sparked a new challenge: How can we make Toss and Toss Pay known in the offline world?\nUntil now, Toss had thrived entirely in the digital world. The challenge was to find a way to present Toss in a consistent way in the unfamiliar offline world. We realized that a consistent graphic language was necessary, and we turned to user perception for the answer. When asked, “What comes to mind when you think of Toss?,” most users gave similar answers. They described Toss as simple, practical and convenient. One person said, “Toss feels like an app built in Excel by a genius engineer.” Funny as it sounded, it also revealed our brand’s essence in the most intuitive way.\nTo bring that abstract image into life, we conducted research on fonts, colors and logos. For fonts, most users said that English text in black felt the most like Toss. This was because that was the look users had grown familiar with from seeing it over and over in the press and marketing. When it came to colors and logos, the image most recalled by users was a blue logo on a white square background. This shows that users associated the brand more with the app icon, which is what they encounter on a daily basis, rather than a graphic symbol.\n\nToss’ blue logo alongside the English text “toss” in black, often seen in Toss’s press releases.\n\nToss app icon. The blue logo on a white square background is what most users recall when asked to picture the Toss logo from memory.\n\nThe Toss Pay payment icon, updated in the UI based on research findings.\n\nThese insights were quickly reflected. We applied the white background, blue logo, and black text across the Toss Pay UI and offline payment touchpoints, to reflect the image most familiar to our users.\n\n.css-1ifza5r{border:0;margin-top:0;margin-bottom:0;height:1px;opacity:1;background:var(--tHairlineBackground);margin:0;}\nWriter Myeonghwa Jung, Toss User Research Team Leader",
        "guid": "https://toss.im/tossfeed/article/usercentric",
        "isoDate": "2026-02-19T10:11:00.000Z"
      }
    ]
  },
  {
    "name": "모나미",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김진홍",
    "category": "개인",
    "posts": []
  }
]