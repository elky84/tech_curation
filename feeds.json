[
  {
    "name": "C++ Team Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Facebook Engineering",
    "category": "기업",
    "posts": [
      {
        "creator": "",
        "title": "The Death of Traditional Testing: Agentic Development Broke a 50-Year-Old Field, JiTTesting Can Revive It",
        "link": "https://engineering.fb.com/2026/02/11/developer-tools/the-death-of-traditional-testing-agentic-development-jit-testing-revival/",
        "pubDate": "Wed, 11 Feb 2026 17:00:05 +0000",
        "content:encodedSnippet": "WHAT IT IS\nThe rise of agentic software development means code is being written, reviewed, and shipped faster than ever before across the entire industry. It also means that testing frameworks need to evolve for this rapidly changing landscape. Faster development demands faster testing that can catch bugs as they land in a codebase, without requiring regular updates and maintenance.\nJust-in‑Time Tests (JiTTests) are a fundamentally novel approach to testing where tests are automatically generated by large language models (LLMs) on the fly to catch bugs – even ones that traditional testing might not catch – just-in-time before the code lands into production.\nA Catching JiTTest focuses specifically on finding regressions introduced by a code change. This type of testing reimagines decades of software testing theory and practice. While traditional testing relies on static test suites, manual authoring, and ongoing maintenance, Catching JiTTests require no test maintenance and no test code review, meaning engineers can focus their expertise on real bugs, not false positives. Catching JiTTests use sophisticated techniques to maximize test signal value and minimize false positive drag, targeting test signals where they matter most: on serious failures.\nHOW TESTING TRADITIONALLY WORKS\nUnder the traditional paradigm, tests are manually built as new code lands in a codebase and continually executed, requiring regular updates and maintenance. The engineers building these tests face the challenge of needing to check the behavior, not only of the current code, but all possible future changes. Inherent uncertainty about future changes results in tests that don’t catch anything, or when they do, it’s a false positive. Agentic development dramatically increases the pace of code change, straining test development burden and scaling the cost of false positives and test maintenance to breaking point. \nHOW CATCHING JITTESTS WORK\nBroadly, JiTTests are bespoke tests, tailored to a specific code change, that give engineers simple, actionable feedback about unexpected behavior changes without the need to read or write test code. LLMs can generate JiTTests automatically the moment a pull request is submitted. And since the JiTTest itself is LLM-generated, it can often infer the plausible intention of a code change and simulate possible faults that may result from it.\nWith an understanding of intent, Catching JiTTests can significantly drive down instances of false positives.\nHere are the key steps of the Catching JiTTest process:\nNew code lands in the codebase.\nThe system infers the intention of the code change.\nIt creates mutants (code versions with faults deliberately inserted) to simulate what could go wrong.\nIt generates and runs tests to catch those faults.\nEnsembles of rule-based and LLM-based assessors focus the signal on true positive failures.\nEngineers receive clear, relevant reports about unexpected changes right when it matters most.\nWHY IT MATTERS\nCatching JiTTests are designed for the world of AI-powered agentic software development and accelerate testing by focusing on serious unexpected bugs. With them engineers no longer have to spend time writing, reviewing, and testing complex test code. Catching JiTTests, by design, kill many of the issues with traditional testing in one stroke:\nThey are generated on-the-fly for each code change and do not reside in the codebase, eliminating ongoing maintenance costs and shifting effort from humans to machines.\nThey are tailored to each change, making them more robust and less prone to breaking due to intended updates.\nThey automatically adapt as the code changes.\nThey only require human review when a bug is actually caught.\nThis all amounts to an important shift in testing infrastructure where the focus moves from generic code quality to whether a test actually finds faults in a specific change without raising a false positive. It helps improve testing overall while also allowing it to keep up with the pace of agentic coding.\nREAD THE PAPER\nJust-in-Time Catching Test Generation at Meta\nThe post The Death of Traditional Testing: Agentic Development Broke a 50-Year-Old Field, JiTTesting Can Revive It appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>WHAT IT IS The rise of agentic software development means code is being written, reviewed, and shipped faster than ever before across the entire industry. It also means that testing frameworks need to evolve for this rapidly changing landscape. Faster development demands faster testing that can catch bugs as they land in a codebase, without [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2026/02/11/developer-tools/the-death-of-traditional-testing-agentic-development-jit-testing-revival/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2026/02/11/developer-tools/the-death-of-traditional-testing-agentic-development-jit-testing-revival/\">The Death of Traditional Testing: Agentic Development Broke a 50-Year-Old Field, JiTTesting Can Revive It</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "WHAT IT IS The rise of agentic software development means code is being written, reviewed, and shipped faster than ever before across the entire industry. It also means that testing frameworks need to evolve for this rapidly changing landscape. Faster development demands faster testing that can catch bugs as they land in a codebase, without [...]\nRead More...\nThe post The Death of Traditional Testing: Agentic Development Broke a 50-Year-Old Field, JiTTesting Can Revive It appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=23650",
        "categories": [
          "DevInfra",
          "ML Applications"
        ],
        "isoDate": "2026-02-11T17:00:05.000Z"
      }
    ]
  },
  {
    "name": "eBay Tech Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Twitter Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Netflix TechBlog",
    "category": "기업",
    "posts": [
      {
        "creator": "Netflix Technology Blog",
        "title": "Scaling LLM Post-Training at Netflix",
        "link": "https://netflixtechblog.com/scaling-llm-post-training-at-netflix-0046f8790194?source=rss----2615bd06b42e---4",
        "pubDate": "Fri, 13 Feb 2026 08:05:33 GMT",
        "content:encodedSnippet": "Baolin Li, Lingyi Liu, Binh Tang, Shaojing Li\nIntroduction\nPre-training gives Large Language Models (LLMs) broad linguistic ability and general world knowledge, but post-training is the phase that actually aligns them to concrete intents, domain constraints, and the reliability requirements of production environments. At Netflix, we are exploring how LLMs can enable new member experiences across recommendation, personalization, and search, which requires adapting generic foundation models so they can better reflect our catalog and the nuances of member interaction histories. At Netflix scale, post-training quickly becomes an engineering problem as much as a modeling one: building and operating complex data pipelines, coordinating distributed state across multi-node GPU clusters, and orchestrating workflows that interleave training and inference. This blog describes the architecture and engineering philosophy of our internal Post-Training Framework, built by the AI Platform team to hide infrastructure complexity so researchers and model developers can focus on model innovation — not distributed systems plumbing.\nA Model Developer’s Post-Training Journey\nPost-training often starts deceptively simply: curate proprietary domain data, load an open-weight model from Hugging Face, and iterate batches through it. At the experimentation scale, that’s a few lines of code. But when fine-tuning production-grade LLMs at scale, the gap between “running a script” and “robust post-training” becomes an abyss of engineering edge cases.\nFigure 1. Simple steps to post-train an open-weight model.\nGetting the data right\nOn paper, post-training is straightforward: choose a tokenizer, preprocess the dataset, and build a dataloader. In practice, data preparation is where things break. High-quality post-training — instruction following, multi-turn dialogue, Chain-of-Thought — depends on precisely controlling which tokens contribute to the loss. Hugging Face chat templates serialize conversations, but don’t specify what to train on versus ignore. The pipeline must apply explicit loss masking so only assistant tokens are optimized; otherwise the model learns from prompts and other non-target text, degrading quality.\nVariable sequence length is another pitfall. Padding within a batch can waste compute, and uneven shapes across FSDP workers can cause GPU synchronization overhead. A more GPU-efficient approach is to pack multiple samples into fixed-length sequences and use a “document mask” to prevent cross-attention across samples, reducing padding and keeping shapes consistent.\nSetting up the model\nLoading an open-source checkpoint sounds simple until the model no longer fits on one GPU. At that point you need a sharding strategy (e.g., FSDP, TP) and must load partial weights directly onto the device mesh to avoid ever materializing the full model on a single device.\nAfter loading, you still need to make the model trainable: choose full fine-tuning vs. LoRA, and apply optimizations like activation checkpointing, compilation, and correct precision settings (often subtle for RL, where rollout and policy precision must align). Large vocabularies (>128k) add a further memory trap: logits are [batch, seq_len, vocab] and can spike peak memory. Common mitigations include dropping ignored tokens before projection and computing logits/loss in chunks along the sequence dimension.\nStarting the training\nEven with data and models ready, production training is not a simple “for loop”. The system must support everything from SFT’s forward/backward pass to on-policy RL workflows that interleave rollout generation, reward/reference inference, and policy updates.\nAt Netflix scale, training runs as a distributed job. We use Ray to orchestrate workflows via actors, decoupling modeling logic from hardware. Robust runs also require experiment tracking (model quality metrics like loss and efficiency metrics like MFU) and fault tolerance via standardized checkpoints to resume cleanly after failures.\nThese challenges motivate a post-training framework that lets developers focus on modeling rather than distributed systems and operational details.\nThe Netflix Post-Training Framework\nWe built Netflix’s LLM post-training framework so Netflix model developers can turn ideas like those in Figure 1 into scalable, robust training jobs. It addresses the engineering hurdles described above, and also constraints that are specific to the Netflix ecosystem. Existing tools (e.g., Thinking Machines’ Tinker) work well for standard chat and instruction-tuning, but their structure can limit deeper experimentation. In contrast, our internal use cases often require architectural variation (for example, customizing output projection heads for task-specific objectives), expanded or nonstandard vocabularies driven by semantic IDs or special tokens, and even transformer models pre-trained from scratch on domain-specific, non-natural-language sequences. Supporting this range requires a framework that prioritizes flexibility and extensibility over a fixed fine-tuning paradigm.\nFigure 2. The post-training library within Netflix stack\nFigure 2 shows the end-to-end stack from infrastructure to trained models. At the base is Mako, Netflix’s internal ML compute platform, which provisions GPUs on AWS. On top of Mako, we run robust open-source components — PyTorch, Ray, and vLLM — largely out of the box. Our post-training framework sits above these foundations as a library: it provides reusable utilities and standardized training recipes for common workflows such as Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), Reinforcement Learning (RL), and Knowledge Distillation. Users typically express jobs as configuration files that select a recipe and plug in task-specific components.\nFigure 3. Main components developed for the post-training framework\nFigure 3 summarizes the modular components we built to reduce complexity across four dimensions. As with most ML systems, training success hinges on three pillars — Data, Model, and Compute — and the rise of RL fine-tuning adds a fourth pillar: Workflow, to support multi-stage execution patterns that don’t fit a simple training loop. Below, we detail the specific abstractions and features the framework provides for each of these dimensions:\n\nData: Dataset abstractions for SFT, reward modeling, and RL; high-throughput streaming from cloud and disk for datasets that exceed local storage; and asynchronous, on-the-fly sequence packing to overlap CPU-heavy packing with GPU execution and reduce idle time.\nModel: Support for modern architectures (e.g., Qwen3, Gemma3) and Mixture-of-Experts variants (e.g., Qwen3 MoE, GPT-OSS); LoRA integrated into model definitions; and high-level sharding APIs so developers can distribute large models across device meshes without writing low-level distributed code.\nCompute: A unified job submission interface that scales from a single node to hundreds of GPUs; MFU (Model FLOPS Utilization) monitoring that remains accurate under custom architectures and LoRA; and comprehensive checkpointing (states of trained parameters, optimizer, dataloader, data mixer, etc.) to enable exact resumption after interruptions.\nWorkflow: Support for training paradigms beyond SFT, including complex online RL. In particular, we extend Single Program, Multiple Data (SPMD) style SFT workloads to run online RL with a hybrid single-controller + SPMD execution model, which we’ll describe next.\n\nToday, this framework supports research use cases ranging from post-training large-scale foundation models to fine-tuning specialized expert models. By standardizing these workflows, we’ve lowered the barrier for teams to experiment with advanced techniques and iterate more quickly.\nLearnings from Building the Post-Training Framework\nBuilding a system of this scope wasn’t a linear implementation exercise. It meant tracking a fast-moving open-source ecosystem, chasing down failure modes that only appear under distributed load, and repeatedly revisiting architectural decisions as the post-training frontier shifted. Below are three engineering learnings and best practices that shaped the framework.\nScaling from SFT to RL\nWe initially designed the library around Supervised Fine-Tuning (SFT): relatively static data flow, a single training loop, and a Single Program, Multiple Data (SPMD) execution model. That assumption stopped holding in 2025. With DeepSeek-R1 and the broader adoption of efficient on-policy RL methods like GRPO, SFT became table stakes rather than the finish line. Staying close to the frontier required infrastructure that could move from “offline training loop” to “multi-stage, on-policy orchestration.”\nSFT’s learning signal is dense and immediate: for each token position we compute logits over the full vocabulary and backpropagate a differentiable loss. Infrastructure-wise, this looks a lot like pre-training and maps cleanly to SPMD — every GPU worker runs the same step function over a different shard of data, synchronizing through Pytorch distributed primitives.\nOn-policy RL changes the shape of the system. The learning signal is typically sparse and delayed (e.g., a scalar reward at the end of an episode), and the training step depends on data generated by the current policy. Individual sub-stages — policy updates, rollout generation, reference model inference, reward model scoring — can each be implemented as SPMD workloads, but the end-to-end algorithm needs explicit coordination: you’re constantly handing off artifacts (prompts, sampled trajectories, rewards, advantages) across stages and synchronizing their lifecycle.\nIn our original SFT architecture, the driver node was intentionally “thin”: it launched N identical Ray actors, each encapsulating the full training loop, and scaling meant launching more identical workers. That model breaks down for RL. RL required us to decompose the system into distinct roles — Policy, Rollout Workers, Reward Model, Reference Model, etc. — and evolve the driver into an active controller that encodes the control plane: when to generate rollouts, how to batch and score them, when to trigger optimization, and how to manage cluster resources across phases.\nFigure 4. Architectural differences of SFT and RL framework\nFigure 4 highlights this shift. To add RL support without reinventing distributed orchestration from scratch, we integrated the core infrastructure from the open-source Verl library to manage Ray actor lifecycle and GPU resource allocation. Leveraging Verl’s backend let us focus on the “modeling surface area” — our Data/Model/Compute abstractions and internal optimizations — while keeping orchestration concerns decoupled. The result is a hybrid design: a unified user interface where developers can move between SFT and RL workflows without adopting an entirely different mental model or API set.\nHugging Face-Centric Experience\nThe Hugging Face Hub has effectively become the default distribution channel for open-weight LLMs, tokenizers, and configs. We designed the framework to stay close to that ecosystem rather than creating an isolated internal standard. Even when we use optimized internal model representations for speed, we load and save checkpoints in standard Hugging Face formats. This avoids “walled garden” friction and lets teams pull in new architectures, weights, and tokenizers quickly.\nThis philosophy also shaped our tokenizer story. Early on, we bound directly to low-level tokenization libraries (e.g., SentencePiece, tiktoken) to maximize control. In practice, that created a costly failure mode: silent training–serving skew. Our inference stack (vLLM) defaults to Hugging Face AutoTokenizer, and tiny differences in normalization, special token handling, or chat templating can yield different token boundaries — exactly the kind of mismatch that shows up later as inexplicable quality regressions. We fixed this by making Hugging Face AutoTokenizer the single source of truth. We then built a thin compatibility layer (BaseHFModelTokenizer) to handle post-training needs — setting padding tokens, injecting generation markers to support loss masking, and managing special tokens / semantic IDs — while ensuring the byte-level tokenization path matches production.\nWe do take a different approach for model implementations. Rather than training directly on transformers model classes, we maintain our own optimized, unified model definitions that can still load/save Hugging Face checkpoints. This layer is what enables framework-level optimizations — e.g., FlexAttention, memory-efficient chunked cross-entropy, consistent MFU accounting, and uniform LoRA extensibility — without re-implementing them separately for every model family. A unified module naming convention also makes it feasible to programmatically locate and swap components (Attention, MLP, output heads) across architectures, and provides a consistent surface for Tensor Parallelism and FSDP wrapping policies.\nThe trade-off is clear: supporting a new model family requires building a bridge between the Hugging Face reference implementation and our internal definition. To reduce that overhead, we use AI coding agents to automate much of the conversion work, with a strict logit verifier as the gate: given random inputs, our internal model must match the Hugging Face logits within tolerance. Because the acceptance criterion is mechanically checkable, agents can iterate autonomously until the implementation is correct, dramatically shortening the time-to-support for new architectures.\nToday, this design means we can only train architectures we explicitly support — an intentional constraint shared by other high-performance systems like vLLM, SGLang, and torchtitan. To broaden coverage, we plan to add a fallback Hugging Face backend, similar to the compatibility patterns these projects use: users will be able to run training directly on native transformers models for rapid exploration of novel architectures, with the understanding that some framework optimizations and features may not apply in that mode.\nProviding Differential Value\nA post-training framework is only worth owning if it delivers clear value beyond assembling OSS components. We build on open source for velocity, but we invest heavily where off-the-shelf tools tend to be weakest: performance tuned to our workload characteristics, and integration with Netflix-specific model and business requirements. Here are some concrete examples:\nFirst, we optimize training efficiency for our real use cases. A representative example is extreme variance in sequence length. In FSDP-style training, long-tail sequences create stragglers: faster workers end up waiting at synchronization points for the slowest batch, lowering utilization. Standard bin-packing approaches help, but doing them offline at our data scale can add substantial preprocessing latency and make it harder to keep datasets fresh. Instead, we built on-the-fly sequence packing that streams samples from storage and dynamically packs them in memory. Packing runs asynchronously, overlapping CPU work with GPU compute. Figure 5 shows the impact: for our most skewed dataset, on-the-fly packing improved the effective token throughput by up to 4.7x.\nFigure 5. Training throughput on two of our internal datasets on A100 and H200 GPUs\nWe also encountered subtler performance cliffs around vocabulary expansion. Our workloads frequently add custom tokens and semantic IDs. We found that certain vocabulary sizes could cause the language model head to fall back from a highly optimized cuBLAS kernel to a much slower CUTLASS path, tripling that layer’s execution time. The framework now automatically pads vocabulary sizes to multiples of 64 so the compiler selects the fast kernel, preserving throughput without requiring developers to know these low-level constraints.\nSecond, owning the framework lets us support “non-standard” transformer use cases that generic LLM tooling rarely targets. For example, some internal models are trained on member interaction event sequences rather than natural language, and may require bespoke RL loops that integrate with highly-customized inference engines and optimize business-defined metrics. These workflows demand custom environments, reward computation, and orchestration patterns — while still needing the same underlying guarantees around performance, tracking, and fault tolerance. The framework is built to accommodate these specialized requirements without fragmenting into one-off pipelines, enabling rapid iteration.\nWrap up\nBuilding the Netflix Post-Training Framework has been a continual exercise in balancing standardization with specialization. By staying anchored to the open-source ecosystem, we’ve avoided drifting into a proprietary stack that diverges from where the community is moving. At the same time, by owning the core abstractions around Data, Model, Compute, and Workflow, we’ve preserved the freedom to optimize for Netflix-scale training and Netflix-specific requirements.\nIn the process, we’ve moved post-training from a loose collection of scripts into a managed, scalable system. Whether the goal is maximizing SFT throughput, orchestrating multi-stage on-policy RL, or training transformers over member interaction sequences, the framework provides a consistent set of primitives to do so reliably and efficiently. As the field shifts toward more agentic, reasoning-heavy, and multimodal architectures, this foundation will help us translate new ideas into scalable GenAI prototypes — so experimentation is constrained by our imagination, not by operational complexity.\nAcknowledgements\nThis work builds on the momentum of the broader open-source ML community. We’re especially grateful to the teams and contributors behind Torchtune, Torchtitan, and Verl, whose reference implementations and design patterns informed many of our training framework choices — particularly around scalable training recipes, distributed execution, and RL-oriented orchestration. We also thank our partner teams in Netflix AI for Member Systems for close collaboration, feedback, and shared problem-solving throughout the development and rollout of the Post-Training Framework, and the Training Platform team for providing the robust infrastructure and operational foundation that makes large-scale post-training possible.\n\nScaling LLM Post-Training at Netflix was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Netflix Technology Blog",
        "guid": "https://medium.com/p/0046f8790194",
        "categories": [
          "ai-infrastructure",
          "llm",
          "reinforcement-learning"
        ],
        "isoDate": "2026-02-13T08:05:33.000Z"
      },
      {
        "creator": "Netflix Technology Blog",
        "title": "Automating RDS Postgres to Aurora Postgres Migration",
        "link": "https://netflixtechblog.com/automating-rds-postgres-to-aurora-postgres-migration-261ca045447f?source=rss----2615bd06b42e---4",
        "pubDate": "Thu, 12 Feb 2026 14:07:19 GMT",
        "content:encodedSnippet": "Ram Srivasta Kannan, Wale Akintayo, Jay Bharadwaj, John Crimmins, Shengwei Wang, Zhitao Zhu\nIntroduction\nIn 2024, the Online Data Stores team at Netflix conducted a comprehensive review of the relational database technologies used across the company. This evaluation examined functionality, performance, and total cost of ownership across our database ecosystem. Based on this analysis, we decided to standardize on Amazon Aurora PostgreSQL as the primary relational database offering for Netflix teams.\nSeveral key factors influenced this decision:\n\nPostgreSQL already underpinned the majority of our relational workloads, which made it a natural foundation for standardization. Internal evaluations revealed that Aurora PostgreSQL had supported over 95% of the applications and workloads running on other relational databases across our internal services.\nIndustry momentum had continued to shift toward PostgreSQL, driven by its open ecosystem, strong community support, and broad adoption across modern data platforms.\nAurora’s cloud-native, distributed architecture provided clear advantages in scalability, high availability, and elasticity compared to traditional single-node PostgreSQL deployments.\nAurora PostgreSQL offered a rich feature set, along with a strong, forward-looking roadmap aligned with the needs of large-scale, globally distributed applications.\n\nA Clear Migration Path Forward\nAs part of this strategic shift, one of our key initiatives for 2024/2025 was migrating existing users to Aurora PostgreSQL. This effort began with RDS PostgreSQL migrations and will expand to include migrations from other relational systems in subsequent phases.\nAs a data platform organization, our goal is to make this evolution predictable, well-supported, and minimally disruptive. This allows teams to adopt Aurora PostgreSQL at a pace that aligns with their product and operational roadmaps, while we move toward a unified and scalable relational data platform across the organization.\nDatabase Migration: More Than a Simple Transfer\nMigrating a database involves far more than copying rows from one system to another. It is a coordinated process of transitioning both data and database functionality while preserving correctness, availability, and performance. At scale, a well-designed migration must minimize disruption to applications and ensure a clean, deterministic handoff from the old system to the new one.\nMost database migrations follow a common set of high-level steps:\n\nData Replication: Data is first copied from the source database to the destination, typically using replication, so that ongoing changes are continuously captured and applied.\nQuiescence: Write traffic to the source database is halted, allowing the destination to fully catch up and eliminate any remaining divergence.\nValidation: The system verifies that the source and destination databases are fully synchronized and contain identical data.\nCutover: Client applications are reconfigured to point to the destination database, which becomes the new primary source of truth.\n\nChallenges\nOperational Challenges\nMigrating to a new relational database at Netflix scale presents substantial operational challenges. With a fleet approaching 400 PostgreSQL clusters, manually migrating each one is simply not scalable for the data platform team. Such an approach would require a significant amount of time, introduce the risk of human error, and necessitate considerable hands-on engineering effort. Compounding the problem, coordinating downtime across the many interconnected services that depend on each database is extremely cumbersome at this scale.\nTo address these challenges, we designed a self-service migration workflow that enables service owners to run their own RDS PostgreSQL to Aurora PostgreSQL migrations. The workflow automatically handles orchestration, safety checks, and correctness guarantees end-to-end, resulting in lower operational overhead and a predictable, reliable migration experience.\nTechnical challenges\n\nZero data loss — We must guarantee that all data from the source cluster is fully and safely migrated to the destination within a very tight window, with no possibility of data loss.\nMinimal downtime — Some downtime is unavoidable during migration, as applications must briefly pause write traffic while cutting over to Aurora PostgreSQL. For higher-tier services that power critical parts of the Netflix ecosystem, this window must be kept extremely short to prevent user-facing impact and maintain service reliability.\nNo control over client applications — As the platform team, we manage the databases, but application teams handle the read and write operations. We cannot assume that they have the ability to pause writes on demand, nor do we want to expose such controls to them, as mistakes could lead to data inconsistencies post migration. Therefore, building a self-service migration pipeline requires creative control-plane solutions to halt traffic, ensuring that no writes occur during the validation and cutover phases.\nNo direct access to RDS credentials — The migration automation must perform replication, quiescence, and validation without requesting database credentials from users or relying on manual authentication. Source databases are often tightly secured, allowing access only from client applications, but more importantly, requiring credential access — even if it were possible — would significantly increase operational overhead and risk. At the same time, the migration platform may operate in environments without direct access to the source database, making traditional verification or parity checks impossible.\nNo Degradation in Performance — The migration process must not impact the performance or stability of production databases once they are running in the Aurora PostgreSQL ecosystem.\nFull Ecosystem Parity — Beyond migrating the core database, associated components such as parameter groups, read replicas, and replication slots must also be migrated to ensure functional equivalence.\n\nMinimal User Effort — Since we rely on teams who are not database experts to perform migrations, the process must be simple, intuitive, and fully self-guided.\nAWS recommended migration techniques\nUsing a snapshot\nOne of the simplest AWS-recommended approaches for migrating from RDS PostgreSQL to Aurora PostgreSQL is based on snapshots. In this model, write traffic to the source PostgreSQL database is first stopped. A manual snapshot of the RDS PostgreSQL instance is then taken and migrated to Aurora, where AWS converts it into an Aurora-compatible format.\n Once the conversion completes, a new Aurora PostgreSQL cluster is created from the snapshot. After the cluster is brought online and validated, application traffic is redirected to the Aurora endpoint, completing the migration.\nReference\nUsing an Aurora read replica\nIn the read-replica–based approach, an Aurora PostgreSQL read replica is created from an existing RDS PostgreSQL instance. AWS establishes continuous, asynchronous replication from the RDS source to the Aurora replica, allowing ongoing changes to be streamed in near real time.\nBecause replication runs continuously, the Aurora replica remains closely synchronized with the source database. This enables teams to provision and validate the Aurora environment — including configuration, connectivity, and performance characteristics — while production traffic continues to flow to the source.\nWhen the replication lag is sufficiently low, write traffic is briefly paused to allow the replica to fully catch up. The Aurora read replica is then promoted to a standalone Aurora PostgreSQL cluster, and application traffic is redirected to the new Aurora endpoint. This approach significantly reduces downtime compared to snapshot-based migrations and is well-suited for production systems that require minimal disruption.\nMigration Strategy Trade-Offs\nThese differences represent the key considerations when choosing a migration strategy from RDS PostgreSQL to Aurora PostgreSQL. For our automation, we opted for the Aurora Read Replica approach, trading increased implementation complexity for a significantly shorter downtime window for client applications.\nNetflix RDS PostgreSQL Deployment Architecture\nIn Netflix’s RDS setup, a Data Access Layer (DAL) sits between applications and backend databases, acting as middleware that centralizes database connectivity, security, and traffic routing on behalf of client applications.\nOn the client side, applications connect through a forward proxy that manages mutual TLS (mTLS) authentication and establishes a secure tunnel to the Data Gateway service. The Data Gateway, acting as a reverse proxy for database servers, terminates client connections, enforces centralized authentication and authorization, and forwards traffic to the appropriate RDS PostgreSQL instance.\nThis layered design ensures that applications never handle raw database credentials, provides a consistent and secure access pattern across all datastore types, and delivers isolated, transparent connectivity to managed PostgreSQL clusters. While the primary goal of this architecture is to enforce strong security controls and standardize how applications access external AWS data stores, it also allows backend databases to be switched transparently via configuration, enabling controlled, low-downtime migrations.\nMigration Process\nThe Platform team’s goal is to deliver a fully automated, self-service workflow that helps with the migration of customer RDS PostgreSQL instances to Aurora PostgreSQL clusters. This migration tool orchestrates the entire process — from preparing the source environment, initializing the Aurora read replica, and maintaining continuous synchronization, all the way through to cutover — without requiring any database credentials or manual intervention from the customer.\nDesigned for minimal downtime and seamless user experience, the workflow ensures full ecosystem parity between RDS and Aurora, preserving performance characteristics and operational behavior while enabling customers to benefit from Aurora’s improved scalability, resilience, and cost efficiency.\nData Replication Phase\nEnable Automated Backups\nAutomated backups must be enabled on the source database because the Aurora read replica is initialized from a consistent snapshot of the source and then kept in sync through continuous replication. Automated backups provide the stable snapshot required to bootstrap the replica, along with the continuous streaming of write-ahead log (WAL) records needed to keep the read replica closely synchronized with the source.\nPort RDS parameters to an Aurora parameter group\nWe create a dedicated Aurora parameter group for each cluster and migrate all RDS-compatible parameters from the source RDS instance. This ensures that the Aurora cluster inherits the same configuration settings — such as memory configuration, connection limits, query planner behavior, and other PostgreSQL engine parameters that have equivalents in Aurora. Parameters that are unsupported or behave differently in Aurora are either omitted or adjusted according to Aurora best practices.\nCreate an Aurora read replica cluster and instance\nCreating an Aurora read replica cluster is a critical step in migrating from RDS PostgreSQL to Aurora PostgreSQL. At this stage, the Aurora cluster is created and attached to the RDS PostgreSQL primary as a replica, establishing continuous replication from the source RDS PostgreSQL instance. These Aurora read replicas stay nearly in sync with ongoing changes by streaming write-ahead logs (WAL) from the source, enabling minimal downtime during cutover. The cluster is fully operational for validation and performance testing, but it is not yet writable — RDS remains the authoritative primary.\n\nQuiescence Phase\nThe goal of the quiescence phase is to transition client applications from the source RDS PostgreSQL instance to the Aurora PostgreSQL cluster as the new primary database, while preserving data consistency during cutover.\nThe first step in this process is to stop all write traffic to the source RDS PostgreSQL instance to guarantee consistency. To achieve this, we instruct users to halt application-level traffic, which helps prevent issues such as retry storms, queue backlogs, or unnecessary resource consumption when connectivity changes during cutover. This coordination also gives teams time to prepare operationally, for example, by suppressing alerts, notifying downstream consumers, or communicating planned maintenance to their customers.\nHowever, relying solely on application-side controls is unreliable. Operational gaps, misconfigurations, or lingering connections can still modify the source database state, potentially resulting in changes that are not replicated to the destination and leading to data inconsistency or loss. To enforce a clean and deterministic cutover, we also block traffic at the infrastructure layer. This is done by detaching the RDS instance’s security groups to prevent new inbound connections, followed by a reboot of the instance. With security groups removed, no new SQL sessions can be established, and the reboot forcibly terminates any existing connections.\nThis approach intentionally avoids requiring database credentials or logging into the PostgreSQL server to manually terminate connections. While it may be slower than application- or database-level intervention, it provides a reliably automated and repeatable mechanism to fully quiesce the source RDS PostgreSQL instance before Aurora promotion, eliminating the risk of divergent writes or an inconsistent WAL state.\nValidation Phase\nTo determine whether the Aurora read replica has fully caught up with the source RDS PostgreSQL instance, we track replication progress using Aurora’s OldestReplicationSlotLag metric. This metric represents how far the Aurora replica is behind the source in applying write-ahead log (WAL) records.\nOnce client traffic is halted during quiescence, the source RDS PostgreSQL instance stops producing meaningful WAL entries. At that point, the replication lag should converge to zero, indicating that all WAL records corresponding to real writes have been fully replayed on Aurora.\nHowever, in practice, our experiments show that the metric never settles at a steady zero. Instead, it briefly drops to 0, then quickly returns to 64 MB, repeating this pattern every few minutes as shown in the figure below.\nOldestReplicationSlotLag\nThis behavior stems from how OldestReplicationSlotLag is calculated. Internally, the lag is derived using the following query:\nSELECT\n  slot_name,\n  pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn) AS slot_lag_bytes\nFROM pg_replication_slots;\nConceptually, this translates to:\nOldestReplicationSlotLag = current_WAL_position_on_RDS \n                           – restart_lsn \nSee AWS references here and here.\nThe restart_lsn represents the oldest write-ahead log (WAL) record that PostgreSQL must retain to ensure a replication consumer can safely resume replication.\nWhen PostgreSQL performs a WAL segment switch, Aurora typically catches up almost immediately. At that moment, the restart_lsn briefly matches the source’s current WAL position, causing the reported lag to drop to 0. During idle periods, PostgreSQL performs an empty WAL segment rotation approximately every five minutes, driven by the archive_timeout = 300s setting in the database parameter group.\nImmediately afterward, PostgreSQL begins writing to the new WAL segment. Since this new segment has not yet been fully flushed or consumed by Aurora, the WAL position in source RDS PostgreSQL advances ahead of the restart_lsn of Aurora PostgreSQL by exactly one segment. As a result, OldestReplicationSlotLag jumps to 64 MB, which corresponds to the configured WAL segment size at database initialization, and remains there until the next segment switch occurs.\nBecause idle PostgreSQL performs an empty WAL rotation approximately every five minutes, this zero-then-64 MB oscillation is expected. Importantly, the moment when the lag drops to 0 indicates that all meaningful WAL records have been fully replicated, and the Aurora read replica is fully caught up with the source.\nCutover Phase\nOnce the Aurora read replica has fully caught up with the source RDS PostgreSQL instance — as confirmed through replication lag analysis — the final step is to promote the replica and redirect application traffic. Promoting the Aurora read replica converts it into an independent, writable Aurora PostgreSQL cluster with its own writer and reader endpoints. At this point, the source RDS PostgreSQL instance is no longer the authoritative primary and is made inaccessible.\nBecause Netflix’s RDS ecosystem is fronted by a Data Access Layer (DAL), consisting of client-side forward proxies and a centralized Data Gateway, switching databases does not require application code changes or database credential access. Instead, traffic redirection is handled entirely through configuration updates in the reverse-proxy layer. Specifically, we update the runtime configuration of the Envoy-based Data Gateway to route traffic to the newly promoted Aurora cluster. Once this configuration change propagates, all client-initiated database connections are transparently routed through the DAL to the Aurora writer endpoint, completing the migration without requiring any application changes.\nThis proxy-level cutover, combined with Aurora promotion, enables a seamless transition for service owners, minimizes downtime, and preserves data consistency throughout the migration process.\nCustomer Experience: Migrating a Business-Critical Partner Platform\nOne of the critical teams to adopt the RDS PostgreSQL to Aurora PostgreSQL migration workflow was the Enablement Applications team. This team owns a set of databases that model Netflix’s entire ecosystem of partner integrations, including device manufacturers, discovery platforms, and distribution partners. These databases power a suite of enterprise applications that partners worldwide rely on to build, test, certify, and launch Netflix experiences on their devices and services.\nBecause these databases sit at the center of Netflix’s partner enablement and certification workflows, they are consumed by a diverse set of client applications across both internal and external organizations. Internally, reliability teams use this data to identify streaming failures for specific devices and configurations, supporting quality improvements across the device ecosystem. At the same time, these databases directly serve external partners operating across many regions. Device manufacturers rely on them to configure, test, and certify new hardware, while payment partners use them to set up and launch bundled offerings with Netflix.\nSimplified Enablement Applications Overview\nDevice Lifecycle Management\nNetflix works with a wide range of device partners to ensure Netflix streams seamlessly across a diverse ecosystem of consumer devices. A core responsibility of Device Lifecycle Management is to provide tools and workflows that allow partners to develop, test, and certify Netflix integrations on their devices.\nAs part of the device lifecycle, partners run Netflix-provided test suites against their NRDP implementation. We store signals that represent the current stage for each device in the certification process. This certification data forms the backbone of Netflix’s device enablement program, ensuring that only validated devices can launch Netflix experiences.\nPartner Billed Integrations\nIn addition to device enablement, the same partner metadata is also consumed by Netflix’s Partner Billed Integrations organization. This group enables external partners to offer Netflix as part of bundled subscription and billing experiences.\nAny disruption in these databases affects partner integration workflows. If the database is unavailable, partners may be unable to configure or launch service bundles with Netflix. Maintaining high availability and data correctness is essential to preserving smooth integration operations.\nThe global nature of these workflows makes it difficult to schedule downtime windows. Any disruption would impact partner productivity and risk eroding trust in Netflix’s integration and certification processes.\nPreparation\nGiven the criticality of the Enablement Applications databases, thorough preparation was essential before initiating the migration. The team invested significant effort upfront to understand traffic patterns, identify all consumers, and establish clear communication channels.\nUnderstand Client Fan-Out and Traffic Patterns\nThe first step was to gain a complete view of how the databases were being used in production. Using observability tools like CloudWatch metrics, the team analyzed PostgreSQL connection counts, read and write patterns, and overall load characteristics. This helped establish a baseline for normal behavior and ensured there were no unexpected traffic spikes or hidden dependencies that could complicate the migration.\nJust as importantly, this baseline gave the Enablement Applications team a rough idea of the post-migration behavior on Aurora. For example, they expected to see a similar number of active database connections and comparable traffic patterns after cutover, making it easier to validate that the migration had preserved operational characteristics.\nIdentify and Enumerate All Database Consumers\nUnlike most databases, where the set of consumers is well known to the owning team, these databases were accessed by a wide range of internal services and external-facing systems that were not fully enumerated upfront. To address this, we leveraged a tool called flowlogs, an eBPF-based network attribution tooling was used to capture TCP flow data to identify the services and applications establishing connections to the database(link).\n This approach allowed the team to enumerate active consumers, including those that were not previously documented, ensuring no clients were missed during migration planning.\nEstablish Dedicated Communication Channels\nOnce all consumers were identified, a dedicated communication channel was created to provide continuous updates throughout the migration process. This channel was used to share timelines, readiness checks, status updates, and cutover notifications, ensuring that all stakeholders remained aligned and could respond quickly if issues arose.\nMigration Process\nAfter completing application-side preparation, the Enablement Applications team initiated the data replication phase of the migration workflow. The automation successfully provisioned the Aurora read replica cluster and ported the RDS PostgreSQL parameter group to a corresponding Aurora parameter group, bringing the destination environment up with equivalent configuration.\nUnexpected Replication Slot Behavior\nHowever, shortly after replication began, we observed that the OldestReplicationSlotLag metric was unexpectedly high. This was counterintuitive, as Aurora read replicas are designed to remain closely synchronized with the source database by continuously streaming write-ahead logs (WAL).\nFurther investigation revealed the presence of an inactive logical replication slot on the source RDS PostgreSQL instance. An inactive replication slot can cause elevated OldestReplicationSlotLag because PostgreSQL must retain all WAL records required by the slot’s last known position (restart_lsn), even if no client is actively consuming data from it. Replication slots are intentionally designed to prevent data loss by ensuring that a consumer can resume replication from where it left off. As a result, PostgreSQL will not recycle or delete WAL segments needed by a replication slot until the slot advances. When a slot becomes inactive — such as when a client migration task is stopped or abandoned — the slot’s position no longer moves forward. Meanwhile, the database continues to generate WAL, forcing PostgreSQL to retain increasingly older WAL files. This growing gap between the current WAL position and the slot’s restart_lsn manifests as a high OldestReplicationSlotLag.\nIdentifying and addressing these inactive replication slots was a critical prerequisite to proceeding safely with the migration and ensuring accurate replication state during cutover.\nSuccessful Migration After Remediation\n After identifying the inactive logical replication slot, the team safely cleaned it up on the source RDS PostgreSQL instance and resumed the migration workflow. With the stale slot removed, replication progressed as expected, and the Aurora read replica quickly converged with the source. The migration then proceeded smoothly through the quiescence phase, with no unexpected behavior or replication anomalies observed.\nFollowing promotion, application traffic transitioned seamlessly to the newly writable Aurora PostgreSQL cluster. Through the Data Access Layer, new client connections were automatically routed to Aurora, and observability metrics confirmed healthy behavior — connection counts, read/write patterns, and overall load closely matched pre-migration baselines. From the application and partner perspective, the cutover was transparent, validating both the correctness of the migration workflow and the effectiveness of the preparation steps.\nOpen questions\nHow do we select target Aurora PostgreSQL instance types based on the existing production RDS PostgreSQL instance?\nWhen selecting the target Aurora PostgreSQL instance type for a production migration, our guidance is intentionally conservative. We prioritize stability and performance first, and optimize for cost only after observing real workload behavior on Aurora.\nIn practice, the recommended approach is to adopt Graviton2-based instances (particularly the r6g family) whenever possible, maintain the same instance family and size where feasible, and — at minimum — preserve the memory footprint of the existing RDS instance.\nUnlike RDS PostgreSQL, Aurora does not support the m-series, making a direct family match impossible for those instances. In such cases, simply keeping the same “size” (e.g., 2xlarge → 2xlarge) is not meaningful because the memory profiles differ across families. Instead, we map instances by memory equivalence. For example, an Aurora r6g.xlarge provides a memory footprint comparable to an RDS m5.2xlarge, making it a practical replacement. This memory-aligned strategy offers a safer and more predictable baseline for production migrations.\nDowntime During RDS → Aurora Cutover?\nTo achieve minimal downtime during an RDS PostgreSQL → Aurora PostgreSQL migration, we front-load as much work as possible into the preparation phase. By the time we reach cutover, the Aurora read replica is already provisioned and continuously replicating WAL from the source RDS instance. Before initiating downtime, we ensure that the replication lag between Aurora and RDS has stabilized within an acceptable threshold. If the lag is large or fluctuating significantly, forcing a cutover will only inflate downtime.\nDowntime begins the moment we remove the security groups from the source RDS instance, blocking all inbound traffic. We then reboot the instance to forcibly terminate existing connections, which typically takes up to a minute. From this point forward, no writes can be performed.\nAfter traffic is halted, the next objective is to verify that Aurora has fully replayed all meaningful WAL records from RDS. We track this using OldestReplicationSlotLag. We first wait for the metric to drop to 0, indicating that Aurora has consumed all WAL with real writes. Under normal idle behavior, PostgreSQL triggers an empty WAL switch every five minutes. After observing one data point at 0, we wait for an additional idle WAL rotation and confirm that the lag oscillates within the expected 0 → 64 MB pattern — signifying that the only remaining WAL segments are empty ones produced during idle time. At this point, we know the Aurora replica is fully caught up and can be safely promoted.\nWhile these validation steps run, we perform the configuration updates on the Envoy reverse proxy in parallel. Once promotion completes and Envoy is restarted with the new runtime configuration, all client-initiated connections begin routing to the Aurora cluster. In practice, the total write-downtime observed across services averages around 10 minutes, dominated largely by the RDS reboot and the idle WAL switch interval.\nOptimization: Reducing Idle-Time Wait\nFor services requiring stricter downtime budgets, waiting the full five minutes for an idle WAL switch can be prohibitively expensive. In such cases, we can force a WAL rotation immediately after traffic is cut off by issuing:\nSELECT pg_switch_wal();\nOnce the switch occurs, OldestReplicationSlotLag will drop to 0 again as Aurora consumes the new (empty) WAL segment. This approach eliminates the need to wait for the default archive_timeout interval, which can significantly reduce overall downtime.\nHow do we migrate CDC consumers?\nAs part of the data platform organization in Netflix, we provide a managed Change Data Capture (CDC) service across a variety of datastores. For PostgreSQL, logical replication slots is the way of implementing change data capture. At Netflix, we build a managed abstraction on top of these replication slots called datamesh to manage customers who are leveraging them (link).\nEach logical replication slot tracks a consumer’s position in the write-ahead log (WAL), ensuring that WAL records are retained until the consumer has successfully processed them. This guarantees ordered and reliable delivery of row-level changes to downstream systems. At the same time, it tightly couples the lifecycle of replication slots to database operations, making their management a critical consideration during database migrations.\nA key challenge in migrating from RDS PostgreSQL to Aurora PostgreSQL is transitioning these CDC consumers safely — without data loss, stalled replication, or extended downtime — while ensuring that replication slots are correctly managed throughout the cutover process.\nEach row-level change in PostgreSQL is emitted as a CDC event with an operation type of INSERT, UPDATE, DELETE, or REFRESH. REFRESH events are generated during backfills by querying the database directly and emitting the current state of rows in chunks. Downstream consumers are designed to be idempotent and eventually consistent, allowing them to safely process retries, replays, and backfills.\nHandling Replication Slots During Migration\nBefore initiating database cutover, we temporarily pause CDC consumption by stopping the infrastructure responsible for consuming from PostgreSQL replication slots and writing into datamesh source. This also drops the replication slot from the database and cleans up our internal state around replication slot offsets. This essentially resets the state of the connector to one of a brand new one.\nThis step is critical for two reasons. First, it prevents replication slots from blocking WAL recycling during migration. Second, it ensures that no CDC consumers are left pointing at the source database once traffic is quiesced and cutover begins. While CDC consumers are paused, downstream systems temporarily stop receiving new change events, but remain stable. Once CDC consumers are paused, we proceed with stopping other client traffic and executing the RDS-to-Aurora cutover.\nReinitializing CDC After Cutover\nAfter the Aurora PostgreSQL cluster has been promoted and traffic has been redirected, CDC consumers are reconfigured to point to the Aurora endpoint and restarted. Because their previous state was intentionally cleared, consumers initialize as if they are starting fresh.\nOn startup, new logical replication slots are created on Aurora, and a full backfill is performed by querying the database and emitting REFRESH events for all existing rows. These events let the consumer know that a manual refresh was done from Aurora and to treat this as an upsert operation. This establishes a clean and consistent baseline from which ongoing CDC can resume. Consumers are expected to handle these refresh events correctly as part of normal operation.\nBy explicitly managing PostgreSQL replication slots as part of the migration workflow, we are able to migrate CDC consumers safely and predictably, without leaving behind stalled slots, retained WAL, or consumers pointing to the wrong database. This approach allows CDC pipelines to be cleanly re-established on Aurora while preserving correctness and operational simplicity.\nHow do we roll back in the middle of the process?\nPre-quiescence\nRolling back before the pre-quienscence phase is quite easy. Your primary RDS database is still the source. Rolling back before the quiescence phase is straightforward. At this stage, the primary RDS PostgreSQL instance continues to serve as the sole source of truth, and no client traffic has been redirected.\nIf a rollback is required, the migration can be safely aborted by deleting the newly created Aurora PostgreSQL cluster along with its associated parameter groups. No changes are needed on the application side, and normal operations on RDS PostgreSQL can continue without impact.\nDuring-quiescence\nRolling back during the quiescence phase is more involved. At this point, client traffic to the source RDS PostgreSQL instance has already been stopped by detaching its security groups. To roll back safely, access must first be restored by reattaching the original security groups to the RDS instance, allowing client connections to resume. In addition, any logical replication slots removed during the migration must be recreated so that CDC consumers can continue processing changes from the source database.\nOnce connectivity and replication slots are restored, the RDS PostgreSQL instance can safely resume its role as the primary source of truth.\nPost-quiescence \nRolling back after cutover, once the Aurora PostgreSQL cluster is serving production traffic, is significantly more complex. At this stage, Aurora has become the primary source of truth, and client applications may already have written new data to it.\nIn this scenario, rollback requires setting up replication in the opposite direction, with Aurora as the source and RDS PostgreSQL as the destination. This can be achieved using a service such as AWS Database Migration Service (DMS). AWS provides detailed guidance for setting up this reverse replication flow, which can be followed to migrate data back to RDS if necessary.\nConclusion\nStandardizing and reducing the surface area of data technologies is crucial for any large-scale platform. For the Netflix platform team, this strategy allows us to concentrate engineering effort, deliver deeper value on a smaller set of well-understood systems, and significantly cut the operational overhead of running multiple database technologies that serve similar purposes. Within the relational database ecosystem, Aurora PostgreSQL has become the paved-path datastore — offering strong scalability, resilience, and consistent operational patterns across the fleet.\nMigrations of this scale demand solutions that are reliable, low-touch, and minimally disruptive for service owners. Our automated RDS PostgreSQL → Aurora PostgreSQL workflow represents a major step forward, providing predictable cutovers, strong correctness guarantees, and a migration experience that works uniformly across diverse workloads.\nAs we continue this journey, the Relational Data Platform team is building higher-level abstractions and capabilities on top of Aurora, enabling service owners to focus less on the complexities of database internals and more on delivering product value. More to come — stay tuned.\nAcknowledgements\nSpecial thanks to our other stunning colleagues/customers who contributed to the success of the RDS PostgreSQL to Aurora PostgreSQL migration. Sumanth Pasupuleti, Cole Perez, Ammar Khaku\n\nAutomating RDS Postgres to Aurora Postgres Migration was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Netflix Technology Blog",
        "guid": "https://medium.com/p/261ca045447f",
        "categories": [
          "postgresql",
          "aurora",
          "netflix",
          "postgres",
          "database-migration"
        ],
        "isoDate": "2026-02-12T14:07:19.000Z"
      }
    ]
  },
  {
    "name": "JetBrains: Developer Tools for Professionals and Teams – Company Blog | JetBrains",
    "category": "기업",
    "posts": [
      {
        "creator": "Viliam Sedliak",
        "title": "Kodee’s Kotlin Roundup: KotlinConf ’26 Updates, New Releases, and More",
        "link": "https://blog.jetbrains.com/kotlin/2026/02/kodees-kotlin-roundup-kotlinconf-26-updates-new-releases-and-more/",
        "pubDate": "Tue, 17 Feb 2026 21:28:42 +0000",
        "content:encodedSnippet": "KotlinConf 2026 is starting to take shape, and there’s a lot happening across the Kotlin ecosystem right now. From the first conference speakers and community awards to new releases, tools, and real-world Kotlin stories at serious scale, I’ve gathered all the highlights you won’t want to miss. Let’s dive in!\nKodee-Approved Spotlight\nMeet the first speakers heading to KotlinConf 2026\nI’ve just spotted the first speakers confirmed for KotlinConf 2026. This initial lineup already brings together people shaping Kotlin and its real-world use – and it’s only the beginning. Expect in-depth talks, hands-on workshops, and plenty of opportunities to connect with other Kotlin developers. If you’re planning to attend, now’s a good time to reserve your spot – more speakers and the full schedule are on the way.\nMeet the first speakers\n                                                    \nJoin the Kotlin Ecosystem Mentorship Program\nThe Kotlin Foundation is launching a new mentorship program to help newcomers make their first meaningful open-source contribution to Kotlin. Guided by experienced maintainers, mentees will go through the whole contribution process, from onboarding to getting a real change merged, with a limited number of mentor–mentee pairs, Kotlin-branded swag, and even a chance to win a trip to KotlinConf 2026.\nLearn more\n                                                    \nWhat’s New in Kotlin 2.3\nI took Kotlin 2.3 for a spin, and it brings smarter checks, cleaner property patterns, stabilized language features, and improved time and UUID APIs. Kotlin Multiplatform also gets faster builds, smaller binaries, and smoother interop across Native, Web, and JavaScript.\nSee what’s new\n                                                    \nKtor 3.4.0 is now available\nKtor 3.4.0 is out, bringing updates that make server-side Kotlin more flexible and robust. This release adds duplex streaming support, better control over the request lifecycle, and expanded compression options to help you handle modern workloads more efficiently.\nExplore Ktor 3.4.0\n                                                    \nCompose Hot Reload reaches 1.0.0\nCompose Hot Reload has reached version 1.0.0, marking a significant milestone for faster UI development. The post walks through the journey to stability and explains how Compose Hot Reload can speed up iteration when working with Compose by applying UI changes instantly, without restarting your app. Compose Hot Reload is bundled with Compose Multiplatform starting from version 1.10.\nSee how it works\n                                                    \nHow Amazon Fashion uses Kotlin for backend development\nI always like seeing Kotlin used at a serious scale, and this example definitely qualifies. In this talk, Katie Levy from Amazon Fashion shares how her team migrated a large backend service from Java to Kotlin, gaining cleaner, more testable code, faster delivery, and far fewer nullability issues.\nWatch how they did it\n                                                    \nLast chance to nominate yourself or a community member for the Golden Kodee Community Awards\nKotlinConf 2026 is also introducing something new, and I couldn’t be more excited about it: the Golden Kodee Community Awards. Nominations are open until February 22, inviting you to spotlight the people who make the Kotlin community thrive across creativity, education, and community building. Finalists will be announced in April, with winners revealed live at KotlinConf. Travel costs for all finalists are covered.\nNominate someone awesome\n                                                    \nHelp us understand how you use Exposed!\nIf you’re using the Exposed library in your Kotlin projects, I’d love to hear how it’s working for you. There’s a short survey, which should only take five minutes to complete, focused on real-world usage and where Exposed can be improved next. Your feedback helps guide future improvements – and as a small thank-you, you can enter a draw for a prize of your choice. If you have a moment, your input can make a real difference.\nTake the survey\n                                                    \nThe Ultimate Guide to successfully adopting Kotlin in a Java-dominated environment\nI know that adopting Kotlin in a Java-heavy codebase isn’t about flipping a switch or rewriting everything overnight. This Ultimate Guide lays out a practical, step-by-step path – from safe experiments in tests to production use and scaling Kotlin across a team or organization, with concrete migration patterns you can actually apply.\nRead the guide\n                                                    \nQodana for Android Kotlin\nQodana now supports Android projects written in Kotlin, bringing static analysis and code quality checks tailored for Android workflows. It helps teams catch issues early and keep Kotlin codebases healthy as they scale.\nCheck it out\n                                                    \nKoog x ACP: Connect an agent to your IDE and more\nKoog now integrates with the Agent Client Protocol (ACP), enabling AI agents to connect directly to JetBrains IDEs. This post explains how the integration works and what it enables for Kotlin-based AI tooling.\nRead more\n                                                    \nFrom certifed Kotlin trainers\nHow Backend Development Teams Use Kotlin in 2025\nHow Mobile Development Teams Use Kotlin in 2025\nWhere you can learn more\nIndustry Leaders on the KotlinConf’25 Stage: What Global Brands Built With Kotlin\nAdvent of Code 2025 in Kotlin: Puzzles, Prizes, and Community\nPick Your KotlinConf Workshop by What You Want to Learn\nExposed 1.0 Is Now Available\nKubernetes Made Simple: A Guide for JVM Developers\nA Better Way to Explore kotlinx-benchmark Results with Kotlin Notebooks\nYouTube highlights\nAPI Design at Google: Building Android Libraries\nTalking Kotlin #144 | Kotlin 2.3 Release Special (Audio Only)\nMaking Apps Accessible With Kotlin and Compose\nSell or Buy? Custom Financial Data Visualisation With Kotlin\nLanguage Design in the Age of AI\nWhy iOS Devs Struggle With KMP (and How to Fix It)\nA Better Way to Explore and Solve Programming Puzzles: Kotlin Notebooks\nWhy ING Chooses Kotlin for Server-Side",
        "dc:creator": "Viliam Sedliak",
        "content": "KotlinConf 2026 is starting to take shape, and there’s a lot happening across the Kotlin ecosystem right now. From the first conference speakers and community awards to new releases, tools, and real-world Kotlin stories at serious scale, I’ve gathered all the highlights you won’t want to miss. Let’s dive in! From certifed Kotlin trainers Where [&#8230;]",
        "contentSnippet": "KotlinConf 2026 is starting to take shape, and there’s a lot happening across the Kotlin ecosystem right now. From the first conference speakers and community awards to new releases, tools, and real-world Kotlin stories at serious scale, I’ve gathered all the highlights you won’t want to miss. Let’s dive in! From certifed Kotlin trainers Where […]",
        "guid": "https://blog.jetbrains.com/?post_type=kotlin&p=681662",
        "categories": [
          "news",
          "kotlin-roundup"
        ],
        "isoDate": "2026-02-17T21:28:42.000Z"
      },
      {
        "creator": "Bulat Davletov",
        "title": "Editor Improvements: Smooth Caret Animation and New Selection Behavior",
        "link": "https://blog.jetbrains.com/platform/2026/02/editor-improvements-smooth-caret-animation-and-new-selection-behavior/",
        "pubDate": "Tue, 17 Feb 2026 13:30:56 +0000",
        "content:encodedSnippet": "We’re continuing to modernize our IDEs, and in this update we’ve refreshed something you interact with constantly – the editor. These changes are designed to provide improved comfort, a cleaner look, and a more enjoyable experience during the hours you spend coding. We want to make the editor easier on the eyes, help you stay productive, and add a bit of fun variety.\nWhat’s new\nNew selection behavior: Selection now only highlights the actual text, not the blank space at the end of a line.\nNew smooth caret movement: A new animation makes caret jumps easier to follow. This is a long-awaited request from our users.\nSmooth blinking and rounded caret: The editor now has a more modern look and feel, matching the recently introduced Islands UI theme.\nNew selection behavior\nWhen working with code, you often have to select multiple lines at once, and it is important to be able to see what is included in the selection. The editor now highlights only the characters you actually selected, rather than the entirety of each line. This reduces the amount of blank blue space, making the selection itself clearer. It also removes ambiguity, letting you see exactly which characters are selected.\n\n\n\n\nNew smooth caret movement\nThis one has been on our wishlist for a long time. Like us, you work in the text editor every day, constantly navigating and modifying code. One way we’re making this experience more enjoyable is by introducing smooth, animated caret movement. You might not realize you need this until you try it. It enhances comfort and helps you stay oriented in your work, making the editor feel both more functional and visually pleasing.\nAt the same time, we recognize that excessive caret animations may give the impression of a delay when you’re typing and feel slow and unresponsive. We believe that typing must feel instantaneous, and we wanted to make sure to get it right. It is a significant challenge to introduce animations without breaking habits.\nThat’s why we built our own mode for caret movement – Snappy – which you won’t find in other editors. In this mode, the caret lands almost immediately where it should be and then settles with a smooth stop. The result feels quick yet smooth.\n\n\n\n\nIf you prefer clearly visible motion, there’s also Gliding mode, where the caret moves smoothly, making jumps easy to follow with your eyes. This option is similar to what you see in other popular text editors.\n\n\n\n\nYou can switch between movement modes using the Use smooth caret movement dropdown in Settings | Editor | General | Appearance. You can revert to the old behavior simply by disabling it.\nSmooth blinking and rounded caret\nWe’ve also given the caret a new look. Its smooth blinking feels calmer and more modern. As a final touch, we rounded it to match rounded elements throughout the IDE in the Islands UI.\n\n\n\n\n\n\n\n\nHow to try it\nThese new features are all available in the latest early access builds of JetBrains IDEs, meaning there’s still time for us to adjust them based on your feedback. Give them a try and let us know what you think!\nTry them in the IntelliJ IDEA EAP today",
        "dc:creator": "Bulat Davletov",
        "content": "We’re continuing to modernize our IDEs, and in this update we’ve refreshed something you interact with constantly – the editor. These changes are designed to provide improved comfort, a cleaner look, and a more enjoyable experience during the hours you spend coding. We want to make the editor easier on the eyes, help you stay [&#8230;]",
        "contentSnippet": "We’re continuing to modernize our IDEs, and in this update we’ve refreshed something you interact with constantly – the editor. These changes are designed to provide improved comfort, a cleaner look, and a more enjoyable experience during the hours you spend coding. We want to make the editor easier on the eyes, help you stay […]",
        "guid": "https://blog.jetbrains.com/?post_type=platform&p=681585",
        "categories": [
          "intellij-platform",
          "news"
        ],
        "isoDate": "2026-02-17T13:30:56.000Z"
      },
      {
        "creator": "Tatiana Parshutkina",
        "title": "The Evolution of Async Rust: From Tokio to High-Level Applications",
        "link": "https://blog.jetbrains.com/rust/2026/02/17/the-evolution-of-async-rust-from-tokio-to-high-level-applications/",
        "pubDate": "Tue, 17 Feb 2026 13:06:27 +0000",
        "content:encodedSnippet": "Disclaimer: This article was created using AI-based writing and communication companions. With its help, the core topics of this rich and nuanced livestream were conveniently distilled into a compact blog post format.\nIn our yet another JetBrains livestream, Vitaly Bragilevsky was joined by Carl Lerche, the creator of Tokio, for an in-depth conversation about the evolution of async Rust. Tokio has become the de facto asynchronous runtime for high-performance networking in Rust, powering everything from backend services to databases. During the discussion, they explored how async Rust matured over the years, the architectural decisions behind Tokio, common challenges developers face today, and where the ecosystem is heading next. If you missed the live session, you can watch the full recording on JetBrains TV. Below, you’ll find a structured recap of the key questions and insights from the conversation.\n\n\n\n\n\n\nQ1. What is TokioConf and why did you decide to organize it?\nTokioConf is the first conference dedicated to the Tokio ecosystem, taking place in Portland, Oregon. This year marks ten years since Tokio was first announced, making it a natural moment to bring the community together. Use the code jetbrains10 for 10% off the general admission ticket (excluding any add-ons).\nBuy TokioConf ticket\n                                                    \nTokio and Rust have become foundational technologies for infrastructure-level networking software, including databases and proxies. The conference is meant to reflect that maturity and growth. While the name highlights Tokio, the scope includes broader async and networking topics in Rust.\n\n\n\n    \n“Tokio and Rust have become one of the default ways companies build infrastructure-level networking software these days.”\n\n            \nQ2. When people hear “Async Rust,” what should they picture?\nAsync Rust is about more than performance. While handling high concurrency is a key advantage, async programming also improves how developers structure event-driven systems.\nTimeouts, cancellation, and managing multiple in-flight tasks become significantly easier in async Rust compared to traditional threaded approaches. Async in Rust leverages the ownership model and Drop, enabling safe and clean cancellation patterns.\n“Async is both performance, but also a way of managing lots of in-flight threads of logic well.”\n\n            \nQ3. How did Tokio begin? Why did Rust need it?\nTokio evolved from earlier experimentation with non-blocking I/O in Rust. Initially, Rust only had blocking socket APIs, and building efficient network systems required low-level abstractions. The journey went from Mio (epoll bindings), to the Future trait, and to async/await. Async/await was a major milestone in making async programming ergonomic in Rust.\n“The way async/await ended up being designed is actually quite impressive.”\n\n            \nThe language team managed to deliver memory safety and zero-cost abstractions in a way that wasn’t obvious at the time.\nQ4. Could Rust have something like Java’s virtual threads?\nRust originally had green threads and coroutines before version 1.0, but they were removed to preserve zero-cost abstractions and C-level performance characteristics. The overhead and complexity of stack management for green threads conflicted with Rust’s design goals at the time.\n“Rust actually started with lightweight virtual threads and coroutines.”\n\n            \nWhether such a feature could return is an open question, but today’s Rust async model is fundamentally different.\nQ5. How does cancellation work in Async Rust?\nCancellation in Rust is implemented through Drop. When you drop a future, its cleanup logic runs automatically.\nIf the future directly owns a socket, it closes immediately. If the socket is owned by another task (for example in Hyper), cancellation signals cascade through channels and trigger cleanup.\nHowever, async functions can be dropped at any point, and developers must write defensively to handle that reality correctly.\nQ6. Why did Tokio become the dominant async runtime?\nTokio became the de facto standard largely due to ecosystem momentum. Early crates like Hyper built on Tokio, and once that foundation solidified, switching runtimes required compelling reasons.\nOther runtimes exist (especially for embedded or specialized contexts) but for general server-side development, Tokio’s ecosystem depth made it the default.\n“There just wasn’t a good reason to not use Tokio.”\n\n            \nQ7. What about io_uring? Is it the future?\nio_uring can provide benefits, especially for batching filesystem operations. However, for networking workloads, real-world gains are often limited. It is more complex than epoll and has historically had more security issues. That said, Tokio allows mixing in io_uring-specific crates when you have a clear use case.\n“I’ve not seen real performance benefits with swapping out io_uring for sockets under the hood in Tokio.”\n\n            \nQ8. What were the most important design decisions in Tokio?\nTokio intentionally avoided reinventing scheduling patterns. Instead, it adopted proven strategies from Go and Erlang, including work-stealing schedulers.\nThe philosophy was to provide:\nGood defaults,\nStrong performance,\nEscape hatches for advanced tuning.\n\n\n\n\n\nThe goal was to make Tokio easy enough for most developers while still enabling performance optimization when needed.\nQ9. What are common mistakes in Async Rust?\nThe biggest issue comes from cooperative scheduling. Tasks only yield at .await, so long CPU-heavy work without awaiting can stall the runtime. Tokio provides runtime metrics to help detect such problems. Understanding how the scheduler works is crucial to avoiding tail-latency problems.\n“Because async is cooperative scheduling, you have to make sure you’re yielding back to the runtime regularly enough.”\n\n            \nQ10. What’s the best way to debug Async Rust?\nDebugging async systems often involves:\nTracing,\nRuntime metrics,\nAsync backtraces,\nTraditional debuggers.\n\n\n\n\n\nStuck tasks and high tail latency remain the hardest issues to diagnose. Better static analysis and linting tools could significantly improve this area in the future.\n“The biggest pitfall stems down to developers accidentally canceling something and not handling the cancellation appropriately.”\n\n            \nQ11. What is Toasty, and why are you building it?\nRust has matured as a systems and infrastructure language, but higher-level web application tooling remains underdeveloped. Toasty aims to explore that space by building a productive, ergonomic data modeling and query layer. The goal is not just performance, but developer ergonomics – while still preserving escape hatches for advanced use cases.\nQ12. Can Rust move into high-level web frameworks?\nRust already has a foothold in many organizations thanks to its infrastructure strengths. As internal Rust ecosystems grow, the demand for higher-level tooling increases. The missing piece is ergonomic, opinionated frameworks that prioritize productivity. The long-term vision is not to replace existing ecosystems, but to expand Rust’s reach upward into full-stack development.\n“I do think there’s a way to build productive and ergonomic libraries with Rust that focus on ease of use.”\n\n            \n\n\n\n\nClosing Thoughts\nRust has firmly established itself as the best choice for many infrastructure-level systems. The next frontier is higher-level application development. Tokio solved async infrastructure and now the ecosystem is evolving toward productivity and full-stack capability.\nIf you’re interested:\nExplore Toasty on the Tokio GitHub\nJoin the Tokio Discord\nAttend TokioConf in Portland, Oregon\nWatch our previous lifestream with Herbert Wolverson and explore everything you wanted to ask about Rust",
        "dc:creator": "Tatiana Parshutkina",
        "content": "Disclaimer: This article was created using AI-based writing and communication companions. With its help, the core topics of this rich and nuanced livestream were conveniently distilled into a compact blog post format. In our yet another JetBrains livestream, Vitaly Bragilevsky was joined by Carl Lerche, the creator of Tokio, for an in-depth conversation about the [&#8230;]",
        "contentSnippet": "Disclaimer: This article was created using AI-based writing and communication companions. With its help, the core topics of this rich and nuanced livestream were conveniently distilled into a compact blog post format. In our yet another JetBrains livestream, Vitaly Bragilevsky was joined by Carl Lerche, the creator of Tokio, for an in-depth conversation about the […]",
        "guid": "https://blog.jetbrains.com/?post_type=rust&p=681725",
        "categories": [
          "rustrover",
          "async-rust",
          "rust",
          "tokio"
        ],
        "isoDate": "2026-02-17T13:06:27.000Z"
      },
      {
        "creator": "Claire Amaouche",
        "title": "Databao Becomes a Partner of the Open Semantic Interchange Initiative led by Snowflake and other industry leaders",
        "link": "https://blog.jetbrains.com/databao/2026/02/databao-becomes-osi-partner/",
        "pubDate": "Tue, 17 Feb 2026 11:00:00 +0000",
        "content:encodedSnippet": "Modern data teams need flexibility and scalability as workflows evolve and AI becomes central to analytics. They are increasingly relying on AI to enable self-service analytics and accelerate data workflows, and it has become essential to establish shared business logic and context that both humans and AI systems can reliably understand and use. That shared context is the semantic layer.\nThat’s why we are thrilled to announce that Databao, a recent JetBrains product bringing semantic context and local data agents to data teams, is joining the Open Semantic Interchange (OSI), the open-source initiative led by Snowflake and other industry leaders to advance interoperable and governed semantic models.\nWhy open semantics matter\nThe Open Semantic Interchange (OSI) is an open-source initiative led by Snowflake and a broad ecosystem of partners across multiple domains and industries including BI, data engineering, governance, and AI. Its goal is to define a shared, vendor-neutral standard for semantic metadata, enabling it to move seamlessly across tools and platforms.\nBy making semantic context portable and interoperable, OSI reduces complexity, accelerates the adoption of AI and analytics tools, and helps organizations align on consistent data definitions, laying the foundation for more reliable insights and scalable AI innovation.\nWhat Databao brings to data teams\nOSI reflects the principles that have guided Databao from the start.\nFirst, trust at scale depends on semantics. Databao treats the semantic layer as living context, shared, governed, and continuously refined, so business logic remains clear and reliable as data usage grows.\nSecond, adoption requires flexibility. Teams shouldn’t have to choose between strong governance and usability. Databao already works with existing tools and logic. Teams stay in control of where their source of truth lives and how it’s maintained.\nBy joining OSI, Databao reinforces its commitment to an open, community-driven approach to sharing semantic models, so business definitions remain consistent and usable across every team’s workflows.\nAbout Databao\nDatabao is a new data product from JetBrains that helps data teams create and maintain a shared semantic context and build their own data agents on top of it. Our goal is to provide an AI-native analytics experience that business users can trust, enabling them to query and analyze data in plain language.\nDatabao’s modular components, the context engine and data agent can run independently, either locally or within your existing infrastructure using your own API keys.\nWe are also inviting data teams to build a proof of concept with us: we’ll explore your use case, define a context-building process, and grant agent access to a selected group of business users. Together, we will then evaluate the quality of responses and the overall value.\n      \n      TALK TO THE TEAM\n    \n\n\n\n\nLearn more about the Open Semantic Interchange on Snowflake’s blog, or explore both modules at databao.app .",
        "dc:creator": "Claire Amaouche",
        "content": "Modern data teams need flexibility and scalability as workflows evolve and AI becomes central to analytics. They are increasingly relying on AI to enable self-service analytics and accelerate data workflows, and it has become essential to establish shared business logic and context that both humans and AI systems can reliably understand and use. That shared [&#8230;]",
        "contentSnippet": "Modern data teams need flexibility and scalability as workflows evolve and AI becomes central to analytics. They are increasingly relying on AI to enable self-service analytics and accelerate data workflows, and it has become essential to establish shared business logic and context that both humans and AI systems can reliably understand and use. That shared […]",
        "guid": "https://blog.jetbrains.com/?post_type=databao&p=677516",
        "isoDate": "2026-02-17T11:00:00.000Z"
      },
      {
        "creator": "Conrad Schwellnus",
        "title": "The Most Popular AI Tools: What Developers Use and Why",
        "link": "https://blog.jetbrains.com/ai/2026/02/the-most-popular-ai-tools-what-developers-use-and-why/",
        "pubDate": "Tue, 17 Feb 2026 06:26:28 +0000",
        "content:encodedSnippet": "AI tools have become a core part of modern software development. Developers rely on them throughout the life cycle, from writing and refactoring code to testing, documentation, and analysis. \nOnce experimental add-ons, these tools now function as everyday assistants and are firmly embedded in routine workflows. But why have AI tools become so essential – and how are developers actually using them?\nThe insights in this article draw on findings from the JetBrains State of Developer Ecosystem Report 2025, which tracks how developers use tools, languages, and technologies, including AI tools, in real-world environments. Shifting the focus from technical model performance, this article looks at usage patterns, developer preferences, and adoption trends across tools, regions, and workflows.\nBefore we work through which AI tools developers use most, why they choose them, and how these tools fit into everyday work, let’s first clarify what AI tools are and why they matter so much right now.\nDisclaimer: Please note that the findings in the article reflect data collected during the specific research period set out in the report.\n\n\n\n\nTable of Contents\n·       What AI tools are and why they matter now\n·       Most popular AI tools among developers\n·       What makes developers choose one AI tool over another\n·       How developers use AI tools in daily workflows\n·       Global snapshot: How AI tool adoption differs across regions\n·       Barriers to adopting AI tools\n·       Future of AI tools: What developers want next\n·       FAQ\n·       Conclusion\nWhat AI tools are and why they matter now\nToday’s AI tools for developers span several categories. They include code assistants that suggest or generate code, as well as tools that review code autonomously. Many come as IDE integrations that understand project context.\nThere are also AI-powered search and navigation tools, refactoring helpers, and documentation generators. In addition, teams now use testing assistants and autonomous or semi-autonomous agents to support more complex workflows.\nUnderstanding today’s AI tools list for developers matters because these tools directly address growing pressures in modern development. They shorten development cycles, reduce manual tasks, and help teams maintain quality, which is especially important as codebases grow.\nThis growing reliance makes it important to understand which tools developers actually use most. In the next section, we will see what these AI tools are.\nMost popular AI tools among developers\nDevelopers rarely rely on a single AI tool. Instead, they combine multiple tools depending on their IDE, workflow style, and project requirements. According to the AI usage insights in the the JetBrains State of Developer Ecosystem Report 2025, adoption clusters around three main categories: IDE-native assistants, standalone AI-powered development environments, and browser-based or cloud chat tools.\nAcross these categories, the most popular AI assistants are GitHub Copilot, JetBrains AI Assistant, Cursor, Windsurf, and Tabnine. Adoption of these top AI tools varies based on ecosystem, IDE choice, and workflow style.\nIDE-native assistants, such as GitHub Copilot and JetBrains AI Assistant, remain among the most popular AI tools because they operate inside the editor and integrate directly into existing workflows, making them more context-aware.\nStandalone AI-focused editors and assistants, such as Cursor and Windsurf, often emphasise more experimental or agent-style workflows. This is an area that is evolving across the ecosystem, with increasing convergence between IDE-native tools and more agent-driven capabilities.\nOther tools focus on specific priorities. For example, Tabnine attracts teams that prioritize privacy and local inference. Region-specific tools also play an important role in areas with strong domestic AI ecosystems or regulatory constraints.\nThis diversity becomes clearer when comparing the best AI tools for developers side by side.\nComparison table: AI tools overview\n\nAI toolTypical use caseUnderlying modelsDistinct featuresIntegration type\nGitHub CopilotCode generation and completionGPT familyTight GitHub + VS Code workflowsIDE / Cloud\nJetBrains AI AssistantContext-aware help, refactoringClaude / GPT / GeminiDeep IDE context + privacy focusIn-IDE\nCursorInline edits, debugging, chatClaude / GeminiFast UI, multi-step editsIDE plugin\nWindsurfAutonomous task execution and code changesClaude / GPTAgent-like capabilitiesStandalone\nTabninePrivacy-oriented code suggestionsProprietary / DeepSeekLocal inference optionsIDE plugin\n\nDisclaimer: Please note that the findings reflect data collected during the specific research period set out in the report.\n\n\n\nWhat makes developers choose one AI tool over another\nDevelopers are not choosing AI tools solely on novelty. They evaluate how well a tool fits existing workflows, how reliable the output feels, and whether the tool aligns with team constraints. The JetBrains State of Developer Ecosystem Report 2025 identifies several of these practical considerations that shape decision-making.\nIntegration quality ranks among the most important factors. Developers prefer AI coding tools that work seamlessly inside their preferred IDE. A tool that interrupts flow or requires constant context switching often fails to gain long-term adoption.\nAccuracy and code quality are equally crucial. Developers expect AI coding tools to produce reliable results that they can trust. When outputs require extensive correction, confidence drops quickly.\nPrivacy and data security also influence developer AI preferences. This is especially true in enterprise environments. Tools that offer local processing or clear privacy guarantees often see stronger uptake in regulated industries.\nFinally, pricing, transparency, and vendor reputation affect adoption. Developers value clear pricing models, flexible access, and vendors with a track record of supporting developer tools. Trust builds over time through consistency and ongoing communication.\nLet’s see how developers evaluate each of these factors in this AI assistant comparison.\nKey factors influencing tool choice\n\nFactorWhy it mattersHow developers evaluate it\nIDE integrationSupports smooth workflowsWorks natively in their preferred IDE\nCode accuracy and qualityAffects trust and usabilityProduces correct, clear, and maintainable code\nPrivacy and securityProtects source code and IPProvides clear data handling and local mode options\nPricing and accessImpacts adoption at scaleOffers flexible tiers and predictable costs\nTransparencyBuilds confidenceDiscloses model provider and data policies\nVendor reputationSignals long-term reliabilityDemonstrates a history of dev tools and quality support\n\nDisclaimer: please note that the findings reflect data collected during the specific research period set out in the report.\n\n\n\nHow developers use AI tools in daily workflows\nDevelopers integrate AI tool usage throughout the development life cycle rather than limiting it to a single task. Most workflows combine several forms of AI access depending on the problem at hand.\nWhen coding with AI tools, developers may use in-IDE assistants for context-aware code help and chat-based interfaces for problem-solving and prototyping. In addition, developer AI assistant usage may combine browser tools for quick inline answers, APIs for automation and CI/CD tasks, and local models for privacy-restricted environments.\nAcross these use cases, developers are clearly no longer relying on a single tool. AI workflows increasingly involve choosing the right tool for the task at hand, be it writing code, refactoring, debugging, generating documentation, testing, or understanding unfamiliar code.\nThe JetBrains State of Developer Ecosystem Report 2025 indicates that developers frequently switch between AI access points in this way. They choose the interface that best fits the task rather than expecting one tool to handle everything.\n\nWorkflow types and examples\n\nWorkflow typeTypical use caseExample toolsIntegration contextDeveloper benefit\nIn-IDE assistanceCode suggestions, refactoringJetBrains AI Assistant, GitHub CopilotIDEImmediate, context-aware help\nChat-based interactionExplanations, brainstorming, regex, prototypingChatGPT, ClaudeBrowser / CloudFast iteration and reasoning\nAPI integrationAutomation, CI tasks, documentationOpenAI API, Anthropic APIBackend / DevOpsScalable automation\nBrowser extensionsQuick inline code insightsCodeium, AIXWebLightweight access\nLocal/private modelsSecure, offline codingTabnine, DeepSeek (self-hosted models)On-premises / EnterpriseHigh privacy and control\n\nDisclaimer: please note that the findings reflect data collected during the specific research period set out in the report.\n\n\n\nWith AI firmly established in daily workflows, the next section looks at regional differences in AI tool adoption.\nGlobal snapshot: How AI tool adoption differs across regions\nGlobal AI adoption patterns do not look the same everywhere. Regional ecosystems, regulations, and developer communities shape which tools gain traction. The JetBrains State of Developer Ecosystem Report 2025 highlights clear regional AI trends.\nIn North America, developers commonly adopt mainstream tools such as GitHub Copilot, JetBrains AI Assistant, and Claude-based assistants. Strong cloud infrastructure and rapid LLM innovation encourage experimentation with multiple tools.\nEuropean developers balance adoption with privacy considerations. Data residency and compliance requirements influence tool selection, leading to broader interest in solutions that offer transparency and local processing options.\nIn the Asia-Pacific region, developers often combine global tools with regional offerings. Mobile-first development cultures and fast-growing ecosystems drive rapid experimentation, particularly with cloud-based assistants.\nMainland China stands out due to its strong domestic AI ecosystem. Developers there frequently rely on local tools and models such as DeepSeek, Qwen, and Hunyuan, which align better with infrastructure and regulatory realities.\n\nRegional highlights and local leaders\n\nRegionMost used toolsLocal ecosystem driversNotable observations\nNorth AmericaGitHub Copilot, JetBrains AI Assistant, ClaudeStrong cloud and LLM innovationHigh multi-tool adoption\nEuropeJetBrains AI Assistant, GitHub CopilotPrivacy regulations, data residencyBalanced adoption across tools\nAsia-PacificGitHub Copilot, GeminiMobile/cloud-first development culturesRapid experimentation and growth\nMainland ChinaDeepSeek, Qwen, HunyuanStrong domestic AI ecosystemPreference for locally hosted models\n\nDisclaimer: please note that the findings reflect data collected during the specific research period set out in the report.\n\n\n\n\nWhile AI tool usage worldwide is undoubtedly gaining momentum, barriers to AI adoption also exist, which we explore in the next section.\nBarriers to adopting AI tools\nDespite growing interest, not all developers or teams adopt AI tools easily. The JetBrains State of Developer Ecosystem Report 2025 shows that such AI adoption challenges often stem from uncertainty rather than opposition.\nPrivacy and security concerns remain the most common AI coding tool barriers. Teams worry about exposing sensitive code or intellectual property, especially when tools rely on cloud processing. Without clear guarantees, organizations may restrict or ban usage.\nLegal and ownership questions are other reasons why developers avoid AI tools. Developers and managers want clarity about who owns AI-generated code and how licensing applies. Uncertainty leads many teams to limit AI use to non-critical tasks.\nIndividual barriers matter as well. Some developers lack confidence in using AI tools effectively or struggle to evaluate output quality. Others distrust AI suggestions due to past inaccuracies.\nCost, licensing, and infrastructure constraints can also limit adoption, particularly for larger teams. Per-seat pricing and usage caps further complicate budgeting and rollout decisions.\n\nObstacles and evaluation criteria\n\nBarrierWhy it mattersTypical impact\nPrivacy and security concernsIncreases the risk of exposing sensitive codeUsage blocked or restricted\nIP and code ownership concernsCreates legal uncertaintyHesitation to rely on AI for core code\nLack of knowledge or trainingReduces confidence in using toolsSlower individual adoption\nAccuracy and reliability issuesImpacts trust in outputsMore manual review required\nInternal policies and processesRequires compliance and complex approval workflowsDelayed tool rollout\nCost and licensingExceeds budget or per-seat limitsPartial or limited deployment\n\nDisclaimer: please note that the findings reflect data collected during the specific research period set out in the report.\n\n\n\n\nIn the next section, we move from the barriers of today to developers’ hopes for the future.\nFuture of AI tools: What developers want next\nDevelopers do not simply want more AI features. They want better ones. The JetBrains State of Developer Ecosystem Report 2025 not only indicates greater adoption but also shows that developers are hopeful about the future of AI. Their expectations focus on reliability, integration depth, and control rather than novelty.\nHigher code quality tops developer AI expectations. Developers want fewer hallucinations, cleaner outputs, and suggestions that respect project conventions. Trust grows when AI behaves predictably.\nDeeper IDE integration also ranks high. Developers expect future AI tools to understand entire projects, not just individual files. Context retention across sessions and multi-file awareness are increasingly important.\nPrivacy remains central. Many developers want local or on-device options that allow them to use AI without sharing code externally. Transparent data handling builds confidence.\nPricing clarity and explainability also influence future AI assistant trends. Developers want predictable costs and better insight into why tools suggest certain changes.\nBut most significantly, as AI tools evolve, developers want support for complex workflows and architecture reasoning. The goalpost is also shifting. Developers now expect future AI tools to move beyond basic autocomplete and act as collaborative partners.\n\nDeveloper expectations and trends\n\nExpectationWhy developers want itExample improvements\nHigher code qualityTrust and reliabilityFewer hallucinations, cleaner output\nDeeper IDE integrationSeamless workflowsContext retention, multi-file awareness\nPrivacy and controlSecure code handlingOn-device or local LLM options\nTransparent pricingPredictable team adoptionUsage-based models, clearer tiers\nExplainability and reasoningTrust in decisionsClearer chain-of-thought summaries\nContext awarenessHandling real projectsLarger context windows, project-wide understanding\n\nDisclaimer: please note that the findings reflect data collected during the specific research period set out in the report.\n\n\n\n\nThe following FAQ addresses some of the most common questions developers ask when evaluating and using AI tools.\nFAQ\nWhat are the most popular AI tools among developers today?\nAccording to the report’s findings, developers commonly use tools such as GitHub Copilot, JetBrains AI Assistant, Cursor, and Tabnine, often combining them rather than using a single tool.\nAre AI tools safe for use with private or proprietary code?\nSafety depends on the tool. Developers increasingly prefer tools that provide clear privacy policies or local processing options.\nWhich AI tools work best inside IDEs?\nIDE-native tools tend to perform best for daily coding tasks because they understand project context and workflows.\nDo developers prefer local AI models or cloud-based solutions?\nPreferences vary. Some developers value cloud flexibility, while others prioritize local models for privacy and compliance.\nHow do AI tools help with debugging and documentation?\nThey explain code, identify errors, suggest fixes, and generate comments or documentation drafts.\nAre AI tools suitable for enterprise teams with strict security requirements?\nMany are, especially when they offer strong privacy guarantees, administrative controls, and predictable pricing.\nCan AI tools speed up development without reducing code quality?\nYes, when developers use them intentionally. AI tools speed up repetitive tasks such as code generation, refactoring, testing, and documentation, while reviews, IDE checks, and automated tests help maintain quality.\nConclusion\nAI tools have evolved from optional add-ons into essential components of modern software development. Developers now rely on them for coding, refactoring, documentation, testing, and learning, integrating AI assistance throughout daily workflows.\nCurrent adoption trends show that developers value accuracy, deep integration, and privacy above experimental features. The JetBrains State of Developer Ecosystem Report 2025 reflects broad and growing use across regions, tools, and development styles.\nAs AI tools continue to evolve, they move toward deeper context awareness, stronger reasoning, and more secure deployment options. \nFor developers, AI no longer represents a future possibility. It has become a practical, everyday partner in building software.",
        "dc:creator": "Conrad Schwellnus",
        "content": "AI tools have become a core part of modern software development. Developers rely on them throughout the life cycle, from writing and refactoring code to testing, documentation, and analysis. Once experimental add-ons, these tools now function as everyday assistants and are firmly embedded in routine workflows. But why have AI tools become so essential – [&#8230;]",
        "contentSnippet": "AI tools have become a core part of modern software development. Developers rely on them throughout the life cycle, from writing and refactoring code to testing, documentation, and analysis. Once experimental add-ons, these tools now function as everyday assistants and are firmly embedded in routine workflows. But why have AI tools become so essential – […]",
        "guid": "https://blog.jetbrains.com/?post_type=ai&p=678908",
        "categories": [
          "ai",
          "deveco",
          "research",
          "data-analysis",
          "jetbrains-research"
        ],
        "isoDate": "2026-02-17T06:26:28.000Z"
      },
      {
        "creator": "Dmitrii Korovin",
        "title": "Say Goodbye to “It Works on My Machine”: A Look at TeamCity’s Pretested Commits",
        "link": "https://blog.jetbrains.com/teamcity/2026/02/say-goodbye-to-works-on-my-machine/",
        "pubDate": "Fri, 13 Feb 2026 12:45:10 +0000",
        "content:encodedSnippet": "This article was brought to you by Adeyinka Adegbenro, draft.dev.\nDevelopers know the frustration well. Code works perfectly on their laptop, then breaks the moment it hits staging or production. It’s easy to overlook slight environmental differences that hide latent bugs like race conditions, platform-specific library loading failures, data inconsistencies (large data sets can expose hidden edge cases like timeouts), network variations, flaky tests, and memory issues.\nEven with careful testing, some issues only surface in shared environments. You can’t catch every edge case with local checks alone, and sometimes, pushing unverified changes is unavoidable. That’s why it’s important to catch test failures before code reaches the shared repository. It prevents integration issues and ensures only green commits make it to the shared branch.\nIn this article, you’ll learn how TeamCity’s pretested commits feature stops broken code from reaching your repository. We’ll explain what pretested (gated) commits are and walk through TeamCity’s workflow using remote runs and IDE integration.\nThe problem: Broken builds from unverified commits\nIn software development, unverified commits are common. They speed up individual workflows yet also increase the risk of failed builds.\nTypically, developers run local tests, commit their changes, and push to a shared repository before peer review or validation by the continuous integration server. If there’s an error (especially one caused by differences between local and production environments), it can disrupt the entire team’s workflow.\nTake database connections. Locally, you might connect one service to your DB and stay well within the database’s max connection limits. But in production, several worker processes connect to the same database, quickly hitting the maximum number of connection limits and triggering timeouts.\nWhen these differences go unnoticed, the result is often a cascading chain of failures. Anyone who pulled that branch as their base now has bad code. Other developers who depend on that branch might also have to spend time debugging the code, especially if the original developer who introduced the bug is not available. This is a massive waste of time and resources, and it could have been avoided by enforcing a pretested commit workflow that uses the validation of a powerful CI server like TeamCity.\nOver time, if the main branch is frequently broken, developers become hesitant to pull their latest changes for fear that they could be unstable. This loss of confidence can lead to developers working in isolation and eventually result in multiple merge conflicts, which defeats the whole purpose of using version control as a tool for collaborative development.\nWhat are pretested commits in TeamCity?\nTeamCity’s pretested commits, also known as gated or delayed commits, reverse the build-after-commit workflow. Instead of the typical edit, commit, push, and build flow where you hope that the build passes on the continuous integration platform’s server, it’s flipped. You edit the code and then build the change on the TeamCity servers before committing it.\nThe CI build includes code compilation, tests, linting, and any other predetermined checks defined in your build configuration. If that build fails, the code is not committed, and the developer can fix the issue without affecting the entire team’s process. But if the code build passes the tests, TeamCity or the developer can automatically commit the changes to version control.\n\n\n\n\nThe pretested commit workflow\nThe pretested commit workflow guarantees code quality by running a full build and test cycle before changes hit the main branch. The implementation varies significantly depending on the type of version control system (VCS) being used.\nFor distributed systems like Git, pretested commits are built around feature branches, so there’s no need to apply patches directly to the main branch. This keeps parallel development safe through local, isolated testing and committing. TeamCity can test against a temporary patch of staged changes you made locally, but stops short of performing the final automatic commit to avoid race conditions. Instead, it uses dedicated validation branches through what is known as the branch remote run.\nThe workflow described below is built around Git.\nCreate a project\nOnce you have a TeamCity instance, you need to create a project either manually or by entering the repository URL (e.g., git@github.com:your-org/your-repo.git) of an existing project. If you select the repository option, you’ll be prompted to log in to the version control host (e.g. GitHub, GitLab, or Bitbucket), and you’ll need to provide the necessary authentication credentials:\n\n\n\n\nConfigure the project\nNext, you’ll need to enter some preliminary settings: the build name, the default branch name, and the branch specifications. For most Git projects, the default branch is either refs/heads/main or refs/heads/master.\nIn the branch specification option, make sure you enter at least one branch, with each branch on a new line. This tells TeamCity which branches to monitor for changes. Here’s a sample branch specification:\n// Default branch\n\nrefs/heads/main\n\n// Regular feature branches\n\nrefs/heads/feature/*\n\n\n\n\nClick Proceed to continue to the build step.\nAdd your build steps\nAfter clicking Proceed, you have to add build steps by clicking the Build Steps tab on your build page. The build steps define the actual sequence of commands required to validate the code. These steps run regardless of the branch type (main or feature/*). A minimal command-line build configuration for a hypothetical Node.js project might look like this:\nset -e\n\nnpm install # Download and install required packages for the project.\n\nnpm run build # Compile source code (e.g., TypeScript, Babel, webpack) into deployable artifacts.\n\nnpm run test # Execute all unit and integration tests. The build will fail if any test fails.\n\nnpm run lint # run linting checks for files\n\n\n\n\nDon’t forget to save your changes and run the build to make sure it works on the default branch.\nPretest validation\nAfter you’ve added build steps, developers need to work on their isolated feature branch locally, making and committing changes frequently (e.g. a branch named feature/login-flow). To initiate the pretest, the developer pushes their local feature branch to the remote repository remote-run/ prefix. TeamCity automatically watches any branches with the prefix remote-run and will run an automatic build once code is pushed there.\n# Pushes feature/login-flow to the remote as remote-run/login-flow\n\ngit push origin feature/login-flow:remote-run/login-flow\n\n\n\n\nIntegration\nOnce the remote-run/login-flow build completes, the status dictates the next step. If it fails, the developer reviews the build log, fixes the issues locally on their feature branch, and repeats the push to the temporary remote-run/login-flow branch.\n\n\n\n\nIf the build is successful, the developer deletes the temporary remote branch. The feature branch (feature/login-flow) is now proven stable and is ready for the final action:\n\n\n\n\nThe developer can now commit and merge with the main branch or create a pull request from their pretested feature branch.\nIn centralized version control systems like SVN or Perforce, TeamCity’s remote run feature allows developers to validate uncommitted local changes using a patch (a bundle of uncommitted changes). A developer uses an IDE like IntelliJ IDEA and the TeamCity plugin to send a patch to the build server, then TeamCity builds and tests the patch. If that’s successful, TeamCity automatically commits the changes to the main repository, completing the pretested commit.\n\n\n\n\nThe benefits of using pretested commits\nPretested commits shift the verification from the developer’s machine to the team’s agreed CI environments. Code only gets added to the main branch after passing the specified checks, so failed builds never disrupt other people’s work.\nThis keeps integration clean and catches regressions early. Everyone gets a stable base to branch from. You know the latest version actually works, and you won’t have to spend hours chasing errors introduced by someone else’s build.\nIt also cuts down on frustration. When teams aren’t wasting time fixing someone else’s mistakes, they can focus on their own features. And because you get immediate feedback during pretesting, you catch your own issues before they become someone else’s problem.\nThese benefits add up. Your commit history stays focused on real progress instead of getting cluttered with commit messages like “fixed typo”, “fixed linting issue”, “added missing dependency that caused build failure”, or “added type checks”. Reviewers can focus on meaningful code changes instead of other noise. Your project history tells the story of how the code evolved, not how often it broke.\nUltimately, pretested commits support continuous delivery goals, especially for agile teams that ship frequently and rely on stable releases. Teams can rest easy knowing that their production code has gone through automated, enforced checks.\nVCS and configuration considerations\nTo get pretested commits running smoothly in TeamCity, there are a few version control and configuration details you should pay attention to:\nExtensive VCS integrations: TeamCity supports all major version control platforms. Centralized systems such as Subversion, Perforce, and TFVC can use remote run in the IDE, while distributed systems like Git (GitHub, GitLab, Bitbucket, or Azure DevOps) and Mercurial use the branch remote run.\nIDE plugin setup: Using the pretested commit feature within an IDE (remote run) depends on the installation of the TeamCity IDE plugin. The plugin lets you select local, uncommitted changes and send them directly to the TeamCity server for verification.\nBranch specifications: Your build configurations in the TeamCity UI need proper branch specifications (e.g. +:refs/heads/*) so that TeamCity knows which branches to monitor and test automatically.\nParameters and secrets: Define all build parameters (especially secure secrets) at the project or build configuration level in the TeamCity UI. TeamCity will securely insert them during the personal build. This separation ensures the code remains clean of sensitive configuration details. Parameter settings can be found in the project and build settings, after enabling the Settings mode in the upper-right corner of your dashboard.\nMatching repository URLs: If you’re using remote run in the IDE, make sure the repository URL configured in IntelliJ IDEA (or your IDE) matches exactly what is defined in the TeamCity server site. Even small differences (e.g. https://github.com/acct/repo.git vs. https://github.com/acct/repo) can prevent TeamCity from recognizing the patch as belonging to the right VCS root.\nBuild triggers: Triggers let you control when your builds run under the specific circumstances/events that you have configured in the settings. For example, you can skip triggering a build if a certain user commits changes or if a phrase is present in a commit message. Configure this in the Triggers tab of your build settings.\nBuild configuration: Make sure you match your build configuration as close as possible to your branch/commit workflow for consistency. This helps make sure that the logic used to test a developer’s changes is similar to that used for the final merge made to the main branch. For example, if your main branch runs database migrations, your personal build should include the same setup.\nWhen to use pretested commits vs. alternatives\nPretested commits are powerful, but they’re not always the right tool for every project. You need to consider project size, branch stability, and how long your tests take to run before incorporating them in your workflow.\nPretested commits work best for teams with a single stable branch where stability is important. They’re also a good fit when you have solid automated tests, and you’re pushing toward continuous integration and delivery.\nIf your test suites and checks are large, take a long time (i.e. 15 minutes), take up memory, or use production-grade data, running pretested commits remotely frees up developers’ machines and keeps them productive.\nBut if your team relies heavily on feature branches and long-lived branch workflows, pull requests and merge gates may be a better fit. And if your test suite is incomplete or flaky, pretested commits won’t help much; they’re only as good as the tests backing them up.\nCode reviews and staging environments, used along with pretested commits, may be helpful for exploratory testing of the kinks that the flaky test suites cannot cover. Manual commits with quick feedback may be simpler for small teams, solo developers, or teams with tiny codebases.\nIt’s not always a question of choosing one or the other. Pretested commits can be layered on top of existing workflows. For example, a feature branch might have multiple developers contributing. Each developer uses pretested commits to ensure that only passing commits reach the shared feature branch. Once the feature is complete, the team still opens a PR to merge into main (or master). At that stage, the PR process provides an additional layer of code review and CI checks before the final merge.\nConclusion\nPretested commits give teams a way to guarantee that only tested, working code enters the main branch. This shifts the responsibility of integration checks onto the CI server, allowing developers to focus on writing features and trust that the system enforces quality.\nWhile this workflow isn’t the best fit for every team, it can be transformative for environments where stability and continuous delivery are priorities. Keep in mind that a pretested commit workflow is only as good as its tests and checks. If your tests are unreliable, errors can slip through the cracks and cause problems.\nJetBrains TeamCity gives teams everything they need to automatically enforce quality checks, from IDE plugins that let you trigger remote runs directly to flexible branch remote runs. If you’re currently using Jenkins and want to explore how to switch to TeamCity, check out our migration planning kit. For a deeper dive into platform capabilities, JetBrains also has detailed resources you can explore.",
        "dc:creator": "Dmitrii Korovin",
        "content": "This article was brought to you by Adeyinka Adegbenro, draft.dev. Developers know the frustration well. Code works perfectly on their laptop, then breaks the moment it hits staging or production. It&#8217;s easy to overlook slight environmental differences that hide latent bugs like race conditions, platform-specific library loading failures, data inconsistencies (large data sets can expose [&#8230;]",
        "contentSnippet": "This article was brought to you by Adeyinka Adegbenro, draft.dev. Developers know the frustration well. Code works perfectly on their laptop, then breaks the moment it hits staging or production. It’s easy to overlook slight environmental differences that hide latent bugs like race conditions, platform-specific library loading failures, data inconsistencies (large data sets can expose […]",
        "guid": "https://blog.jetbrains.com/?post_type=teamcity&p=680592",
        "categories": [
          "best-practices",
          "teamcity-2",
          "devopspains",
          "jenkins"
        ],
        "isoDate": "2026-02-13T12:45:10.000Z"
      },
      {
        "creator": "Artem Pronichev",
        "title": "Moving Your Codebase to Go 1.26 With GoLand Syntax Updates",
        "link": "https://blog.jetbrains.com/go/2026/02/13/moving-your-codebase-to-go-1-26-with-goland-syntax-updates/",
        "pubDate": "Fri, 13 Feb 2026 12:33:36 +0000",
        "content:encodedSnippet": "Working on an existing Go project rarely starts with a plan to modernize it. More often, you open a file to make a small change, add a field, or adjust some logic. The code compiles, tests pass, and everything looks fine, but the language has moved forward, and your code hasn’t kept up.\nAs you bump your project’s Go version, you may start noticing small patterns from older code that stay around for years. A helper variable here. A brittle error check there. You fix them by hand, but as the work spreads across files, you lose context fast.\nIn GoLand, Go 1.26 syntax updates now show up as focused inspections with quick-fixes. You see the change where you are already working, and you can apply the same change throughout the project when you are ready.\nDownload GoLand\n                                                    \nApplying syntax updates\nAs soon as you switch the language version of your project to 1.26, GoLand treats that as a signal: It can now look for patterns that suit Go 1.26 better.\n\n\n\n\nThe first thing you’ll notice is subtle. A blue underline appears under code that is safe to modernize. The underline uses a dedicated severity level named Syntax updates, with a language updates icon (). It is not an error. It is instead an indication that the code can be updated without changing its behavior.\nGoLand adds two Go 1.26 syntax update inspections:\nPointer creation with new()\nType-safe error unwrapping with errors.AsType\nWe started with the latest Go 1.26 changes, and we plan to add more inspections for important language and standard library updates from recent years.\nType-safe error unwrapping with errors.AsType\nGo 1.26 adds errors.AsType, which gives you a typed result. It avoids the pointer setup that errors.As needs and prevents type-mismatch panics. GoLand suggests the safer form and offers the Replace with errors.AsType quick-fix. You can read more about errors.AsType in the GoLand or official documentation.\nBefore\n\n\n\n\nAfter\n\n\n\n\nPointer creation with new()\nGo 1.26 lets new() accept expressions. This removes temporary variables that exist only so you can take their address. GoLand highlights the pattern and offers the Replace with new()  quick-fix. You can read more about new()in the GoLand or official documentation.\nBefore\n\n\n\n\nAfter\n\n\n\n\nExpanding from one fix to the whole project\nOnce you apply the first quick-fix, you can move from a single change to a project-wide update. GoLand gives you several entry points, depending on how you work:\nRight after a quick-fix: Just click Analyze code for other syntax updates.\n\n\n\n\n\nFrom Search Everywhere: Open Search Everywhere (press Shift twice) and select the Update Syntax action.\n\n\n\n\n\nFrom go.mod: Open the module file containing the go 1.26 directive and click Analyze code for syntax updates.\n\n\n\n\n\nFrom the Refactor menu: Click Refactor and select Update Syntax.\n\n\n\n\nGoLand collects the results in a separate tab under the Syntax updates node in the Problems tool window. You can review the updates one by one or apply them in bulk.\n\n\n\n\nGoLand shows a before-and-after diff for each suggested update, so you can review the exact rewrite before you apply it.\n\n\n\n\nWhat this approach to syntax updates changes in practice\nMigration to a new Go version is rarely one big rewrite. It usually happens over the course of dozens of small, safe modernizations mixed into daily work.\nGoLand supports this workflow in a few connected steps:\nIt helps you notice update candidates early. When you edit code that can be modernized, GoLand highlights it in the editor.\nIt offers a safe rewrite. You can apply a quick-fix that rewrites the code to the Go 1.26 form without changing its behavior.\nIt scales to the whole project. When you are ready, run Analyze code for other syntax updates on a wider scope and review the suggested updates before you apply them.\nIt lets you apply updates in bulk. From the list of results in the Problems tool window, you can apply fixes one by one or apply a grouped fix to update many occurrences at once.\nThis combination lets you move your codebase forward without turning the migration into a separate project. You update a line, you see a better form, you apply it, and you keep going.\nHappy coding!\nThe GoLand team",
        "dc:creator": "Artem Pronichev",
        "content": "Working on an existing Go project rarely starts with a plan to modernize it. More often, you open a file to make a small change, add a field, or adjust some logic. The code compiles, tests pass, and everything looks fine, but the language has moved forward, and your code hasn’t kept up. As you [&#8230;]",
        "contentSnippet": "Working on an existing Go project rarely starts with a plan to modernize it. More often, you open a file to make a small change, add a field, or adjust some logic. The code compiles, tests pass, and everything looks fine, but the language has moved forward, and your code hasn’t kept up. As you […]",
        "guid": "https://blog.jetbrains.com/?post_type=go&p=680704",
        "categories": [
          "goland",
          "news",
          "tutorials",
          "update"
        ],
        "isoDate": "2026-02-13T12:33:36.000Z"
      },
      {
        "creator": "Alina Dolgikh",
        "title": "Building Modular Monoliths With Kotlin and Spring",
        "link": "https://blog.jetbrains.com/kotlin/2026/02/building-modular-monoliths-with-kotlin-and-spring/",
        "pubDate": "Fri, 13 Feb 2026 12:27:12 +0000",
        "content:encodedSnippet": "This tutorial was written by an external contributor.\nVivek Kumar Maskara\nVivek Kumar Maskara is an Associate Software Engineer at JP Morgan. He loves writing code, developing apps, creating websites, and writing technical blogs about his experiences. His profile and contact information can be found at maskaravivek.com.\nWebsite | Twitter\nOver a decade ago, Netflix became one of the early adopters of microservice architecture, showcasing its potential at a large scale. Since then, many companies have jumped on the microservices bandwagon, building their backends this way from day one. While microservices offer isolation and independent scaling, their distributed nature requires managing multiple deployments, monitoring interservice communication, and handling network failures across service boundaries.\nAs teams have gained real-world experience with this complexity, there’s been a shift back toward monoliths, but not the tightly coupled monoliths of the past. Instead, developers are embracing modular monoliths: an architectural pattern where single deployable applications are organized into well-defined modules based on logical boundaries or business domains. Think of an e-commerce platform where users, products, and orders live in separate modules that interact through clear contracts, such as APIs for synchronous calls and events for async communication. This separation lets teams work in parallel for faster development and better maintainability, while single-unit deployment keeps releases simple and avoids microservice operational complexity.\nIn this guide, we explore how modular monoliths differ from traditional monoliths, why they’re gaining traction, and how to build them using Spring Modulith and Kotlin.\nThe Need for Modular Monoliths\nThe growing modular monolith countertrend makes more sense when viewed against the shortcomings of traditional approaches.\nTraditional Monoliths\nTraditional monoliths bundle the entire backend into a single codebase with tight coupling between user interfaces, business logic, and data access patterns. In an e-commerce platform, for example, the product catalog, checkout, payments, and order history services are in a single codebase and are deployed together. A monolith uses function calls for internal communication, and often, the call patterns and interdependencies become messy or difficult to maintain.\nMicroservices\nMicroservices emerged to solve these maintainability challenges by splitting backends into loosely coupled services, each handling a specific domain. A cab-hailing platform may separate users, drivers, ride matching, payments, and notifications into independent services. However, this introduces distributed system challenges, including complex service discovery, coordinating deployments across dependent services, and debugging interservice communication issues. Without proper expertise, tooling, and observability, this can slow down development.\nBenefits of Modular Monoliths\nModular monoliths strike a balance by keeping everything in a single codebase and deploying it as one artifact, while structuring the application into logical modules with well-defined interfaces. This addresses the challenges of distributed systems while maintaining the structural benefits of well-defined interfaces and independent development workflows. Some benefits of a modular monolith include:\nSimplified deployment: A single deployment artifact simplifies the release process because you don’t need to coordinate multiple service rollouts, manage service meshes, or handle distributed database migrations and rollbacks.\nReliable testing: As modules in a monolith communicate in-process rather than over a network, integration tests are faster and more stable. You can use mocks where needed, avoid brittle network dependencies, and run end-to-end (E2E) and performance tests in a controlled environment.\nStronger domain modeling: Modular monoliths group related business logic into modules, with clear ownership and communication boundaries between modules. It enforces communication only through well-defined interfaces and enables domain objects to be shared directly without serialization or cross-service APIs. This makes the system easier to maintain and improves development velocity.\nIn-process communication: Since modules communicate through direct method invocations instead of network calls, it reduces latency and points of failure.\nDesigning a Modular Monolith\nWhen you’re building a modular monolith, you first identify the business domains and split the application into multiple loosely coupled modules with clear boundaries and dependencies. Unlike the tightly interwoven code of a traditional monolith, the modular design ensures that the modules can be developed and maintained independently while still being deployed as a single unit. For example, you can break down an e-commerce platform into separate modules such as users, product catalog, shopping cart, payments, and orders:\n\n\n\n\nEach module encapsulates a specific entity or capability. A product catalog module would manage product details and categories, and an order processing module would handle order and payment entities. \nUnlike traditional monoliths, where internal calls often use ad hoc dependencies, in a modular monolith, the interactions with other modules are performed using explicit interfaces and well-defined contracts. This ensures that the intermodule dependencies remain clear and intentional. The communication between the modules uses in-process function calls, so it’s faster and less error-prone compared to network-based interservice calls in microservices.\nThe modular structure allows logical separation between the modules and enforces fixed boundaries, increasing development speed, improving maintainability, and making testing more reliable. Each module defines its user interface, business logic, and data access layers separately:\n\n\n\n\nThese boundaries also lay the groundwork for extracting a particular module as a microservice based on the scaling requirements.\nIntegrate Spring Modulith\nSpring Modulith is a Spring Boot framework that’s based on modular monolith architectural principles. It helps identify, structure, and enforce application modules. It also includes tools for verifying module boundaries and observing their behavior, along with module-level testing capabilities, making Spring Boot applications easier to build and maintain.\nHere’s how to integrate Spring Modulith into a Kotlin-based Spring Boot application.\nAll the code samples are drawn from a fully working Kotlin example, which you can find in this GitHub repository.\nRepository with the companion code for the tutorial\nGo to GitHub\n                                                    \nQuick Start Kotlin Example\nSpring Modulith can be added to any Spring Boot application by including its dependencies in the project’s build.gradle.kts:\n// build.gradle.kts\ndependencies {\n    implementation(\"org.springframework.boot:spring-boot-starter\")\n    implementation(\"org.springframework.modulith:spring-modulith-starter-core:1.4.3\")\n}\nNote: If your project uses Maven, you can add these dependencies to the pom.xml file.\nTo define the modules, you need to add relevant package directories to the src directory. This code snippet illustrates order and product packages added to the application, each handling its own business logic, data, and services:\nSpringModulithExample\n└── src/main/java\n    ├── example\n    │   └── SpringmonolithApplication.kt\n    └── example.order\n        └── …\n    └── example.product\n        └── …\n    └── example.payment\n        └── …\nWithin each of the modules, you can define the business logic, service, and data access layers based on your application’s requirements. The following code snippet shows a ProductService within the example.product package that returns a static greeting message:\npackage com.example.springmonolith.product\n\nimport org.springframework.stereotype.Service\n\n@Service\nclass ProductService {\n\n    fun getGreeting(): String {\n        return \"Hello from Product Module!\"\n    }\n}\nSimilarly, define an OrderService within the example.order package that invokes ProductService::getGreeting() and returns a combined greeting message:\npackage com.example.springmonolith.order\n\nimport com.example.springmonolith.product.ProductService\nimport org.springframework.stereotype.Service\n\n@Service\nclass OrderService(\n    private val productService: ProductService\n) {\n\n    fun getGreeting(): String {\n        return \"Hello from Order Module!\"\n    }\n    \n    fun getCombinedGreeting(): String {\n        return \"Hello from Order Module and: ${productService.getGreeting()}\"\n    }\n}\nAfter adding similar business logic for each of the modules (eg ProductService, PaymentService), you also need to add the @Modulithic annotation to the Spring Boot Application class to mark it as modular. \nThe annotation tells Spring Modulith to automatically detect modules based on package structure and enable the tooling for verification, testing, and observability:\npackage com.example.springmonolith\n\nimport org.springframework.boot.autoconfigure.SpringBootApplication\nimport org.springframework.boot.runApplication\nimport org.springframework.modulith.Modulithic\n\n// add this annotation to the application\n@Modulithic\n@SpringBootApplication\nclass SpringmonolithApplication\n\nfun main(args: Array<String>) {\n    runApplication<SpringmonolithApplication>(*args)\n}\nDefining allowed module dependencies\nNext, you can update the package info file for each of the modules to define the allowed module dependencies. Since the order module depends on the methods defined in the product module, you’ll need to add the following annotation in the order/package-info.java file:\n// add this annotation\n@org.springframework.modulith.ApplicationModule(allowedDependencies = {\"product\"})\n@org.springframework.lang.NonNullApi\npackage com.example.springmonolith.order;\nFinally, you can update the product/package-info.java file to set an empty dependency list for the product module:\n// add this annotation\n@org.springframework.modulith.ApplicationModule(allowedDependencies = {})\n@org.springframework.lang.NonNullApi\npackage com.example.springmonolith.product;\nThe above annotation ensures that if a class or object defined in the product module tries to invoke a method defined outside the module, Spring Modulith verification tests will flag the violation. You will see an example of this scenario in later sections.\nSpring Modulith Features\nSpring Modulith supports various tools for working with modules, including module verification, documentation, and runtime observability. With @Modulithic, the application automatically recognizes its modules (based on package structure) and enables the modulith tooling. Let’s look at how these work.\nModular Structure Checks\nSpring Modulith provides built-in tooling to verify that the module boundaries adhere to the constraints. It checks for cyclic dependencies, validates that modules access other modules only through their public API packages (not internal code), and enforces explicit dependency rules. In your tests, you can use the ApplicationModules.verify() to verify the modular structure:\nApplicationModules.of(Application::class.java).verify()\nRefer to the source code on GitHub for a complete example of the ModularityTest. With the above test configured, if the ProductService tries to invoke an order module method, the module verification test will fail. You can test the behavior by extending the ProductService to call getGreeting as shown below:\n// add import\nimport com.example.springmonolith.order.OrderService\n\n@Service\nclass ProductService(\n    private val orderService: OrderService\n) {\n    \n    // add this after getGreeting()\n    fun getCombinedGreeting(): String {\n        return \"Hello from Product Module and: ${orderService.getGreeting()}\"\n    }\n}\nSince product module is configured to disallow all intermodule dependencies, when you run unit tests (./gradlew test), you would get a module violation error as shown below:\n— TRUNCATED OUTPUT —\nModularityTests > verifiesModularStructure() FAILED\n    org.springframework.modulith.core.Violations at ModularityTests.kt:20\nYou can replace direct intermodule calls with application events that let one module publish a domain event and another module listens to it. This preserves boundaries and avoids compile-time coupling between modules. For example, the order module can publish an event when an order is created, as shown below:\n// order module\nimport org.springframework.context.ApplicationEventPublisher\n\n@Service\nclass OrderService(private val events: ApplicationEventPublisher) {\n\n    fun completeOrder(orderId: String) {\n        events.publishEvent(OrderCompleted(orderId))\n    }\n}\n\ndata class OrderCompleted(val orderId: String)\nNotice that the completeOrder method publishes the OrderCompleted event, and other modules (eg, InventoryPolicy) could react to the event using @ApplicationModuleListener, as shown below:\n// product module\nimport org.springframework.modulith.events.ApplicationModuleListener\nimport org.springframework.stereotype.Component\n\n@Component\nclass InventoryPolicy {\n\n    @ApplicationModuleListener\n    fun on(event: OrderCompleted) {\n        println(\"Updating inventory for order: ${event.orderId}\")\n    }\n}\nNotice that the InventoryPolicy component has a listener configured for the OrderCompleted event that prints the order ID when it receives the event. Refer to the refactor branch on GitHub for a complete example on domain events.\nModular Level Testing\nModulith supports writing integration tests scoped to a single module. You can annotate a test class with @ApplicationModuleTests to test the module and its dependencies in isolation. This avoids the need to spin up the entire application, reducing setup overhead and making tests more reliable. For example, this code snippet shows a bare-bones integration test for the product module:\nimport org.junit.jupiter.api.Test\nimport org.springframework.modulith.test.ApplicationModuleTests\n\n@ApplicationModuleTests\nclass ProductModuleTests {\n\n    @Test\n    fun testProductServiceGreeting() {\n        val greeting = productService.getGreeting()\n        assertTrue(greeting.contains(\"Product Module\"))\n    }\n}\nFor the OrderService test, since it depends on the product module, you need to set the extraIncludes parameter in the @ApplicationModuleTests annotation to include it as shown below:\npackage com.example.springmonolith.order\n\nimport org.junit.jupiter.api.Test\nimport org.springframework.beans.factory.annotation.Autowired\nimport org.springframework.modulith.test.ApplicationModuleTest\nimport org.springframework.test.context.junit.jupiter.SpringJUnitConfig\nimport kotlin.test.assertTrue\n\n@ApplicationModuleTest(extraIncludes = [\"product\"])\n@SpringJUnitConfig\nclass OrderModuleTests {\n\n    @Autowired\n    private lateinit var orderService: OrderService\n\n    @Test\n    fun testOrderServiceGreeting() {\n        val greeting = orderService.getGreeting()\n        assertTrue(greeting.contains(\"Order Module\"))\n    }\n}\nObservability into Module Interactions\nSpring Modulith helps generate developer documentation using the Documenter abstraction. This tool can generate Unified Modeling Language (UML) component diagrams describing the relationship between modules and can also generate a tabular view of the key elements of a module. \nThis code snippet generates an application module component diagram using Documenter:\nclass DocumentationTests {\n    private val modules = ApplicationModules.of(SpringmonolithApplication::class.java)\n\n    @Test\n    fun writeDocumentationSnippets() {\n        Documenter(modules)\n            .writeModulesAsPlantUml()\n            .writeIndividualModulesAsPlantUml()\n    }\n}\nSpring Modulith also integrates with Micrometer to capture spans for module interactions. These spans can be sent to tracing tools such as Zipkin to generate runtime visualizations, making it easier to inspect which modules depend on each other, see how events flow across modules, and monitor interactions in production.\nDeciding When to Use a Modular Monolith\nAlthough a modular monolith can be the ideal balance between simplicity and structure in many cases, it isn’t universally the right choice.\nModular Monolith Use Cases\nEarly-stage development or limited resources: In the early stages of a product or when working with small teams, a modular monolith reduces operational overhead. Developers can focus on delivering features quickly without the complexity of distributed systems. The modular design still enforces boundaries between business capabilities, so if the system grows, you can gradually migrate high-demand modules into separate microservices.\n\nExample: A food delivery platform can start with modules for restaurants, menu, and orders inside a single deployable unit, but it can later extract and deploy one of the modules as a microservice.\nComplex business domains: Applications that involve complex business logic, workflows, or dependencies can benefit from a modular structure. By encapsulating each business capability in its own module, the system becomes easier to develop, test, and maintain.\n\nExample: An insurance platform can split policy management, claims processing, and customer support into separate modules to avoid creating interdependencies that can become difficult to maintain.\nWhen Modular Monoliths Aren’t Always the Right Choice\nSystems with independent scaling needs: Some systems have uneven load patterns where certain components handle millions of requests daily, while others are rarely used. Because modular monoliths deploy as one unit, you can’t scale individual parts independently. A microservice-based approach can more easily scale components that expect a higher load than others.\n\nExample: In an e-commerce platform, the product catalog or recommendation services may experience higher request volumes than order or payment services.\nSystems that use diverse tech stacks: In some organizations, different teams rely on different programming languages, runtimes, or specialized infrastructure for different parts of the system. A modular monolith requires the entire application to use the same stack, which can limit flexibility. In these cases, a microservice-based architecture can provide the isolation needed to mix and match technologies.\n\nExample: Machine learning or analytics teams may want to use Python or Go for their services, while client-facing or internal services can be based on Kotlin or Java.\nConclusion\nModular monolith architecture allows you to split the application logic into isolated modules with their own business logic, while still being deployed as a single artifact. It combines the benefits of a modular design while maintaining the development and release-related benefits of a monolithic architecture. Additionally, modern programming languages such as Kotlin provide tools that can help you achieve monolith stability without giving up the productivity that draws people to microservices.\nSpring Modulith and Kotlin provide the tools to design and enforce clear module boundaries, test modules independently, and monitor their interactions. Try out Spring Modulith to build modular Kotlin applications, while keeping the flexibility to evolve into microservices if your scaling needs change.",
        "dc:creator": "Alina Dolgikh",
        "content": "This tutorial was written by an external contributor. Over a decade ago, Netflix became one of the early adopters of microservice architecture, showcasing its potential at a large scale. Since then, many companies have jumped on the microservices bandwagon, building their backends this way from day one. While microservices offer isolation and independent scaling, their [&#8230;]",
        "contentSnippet": "This tutorial was written by an external contributor. Over a decade ago, Netflix became one of the early adopters of microservice architecture, showcasing its potential at a large scale. Since then, many companies have jumped on the microservices bandwagon, building their backends this way from day one. While microservices offer isolation and independent scaling, their […]",
        "guid": "https://blog.jetbrains.com/?post_type=kotlin&p=679360",
        "categories": [
          "news",
          "tutorials",
          "architecture",
          "backend",
          "kotlin",
          "spring",
          "tutorial"
        ],
        "isoDate": "2026-02-13T12:27:12.000Z"
      },
      {
        "creator": "Maria Sharobaeva",
        "title": "JetBrains Academy – February Digest",
        "link": "https://blog.jetbrains.com/education/2026/02/12/jetbrains-academy-february-2026/",
        "pubDate": "Thu, 12 Feb 2026 22:16:37 +0000",
        "content:encodedSnippet": "Hi there 👋\nFebruary is in full swing, and we’ve packed this digest with things you don’t want to miss.\nInside: upcoming events, hands-on learning, competitions, scholarships, and fresh updates to keep you moving forward in 2026 🚀\nEvents\nIntelliJ IDEA Conf 2026\nJoin a free virtual conference for developers across the JVM ecosystem on March 26–27, 2026. Two full days of technical talks on modern Java and Kotlin, the evolution of the JVM, tooling, and real-world development workflows.\nRegister now\n                                                    \nPython Unplugged on PyTV\nGet ready for a full day of live Python goodness! Join us on March 4 at 11 am CET on YouTube. Hear from Carol Willing (JupyterLab core developer), Paul Everitt (JetBrains Developer Advocate), Sheena O’Connell (PSF Board Member), and other brilliant minds behind the tools, libraries, and communities you love.\nSave the date\n                                                    \nJetBrains Youth Math Challenge\nEnjoy math, teamwork, and a good challenge? Take part in this team-based olympiad for school students and put your problem-solving skills to the test. \nSecure your spot\n                                                    \nBlackbox Cup\nNo docs. No hints. Just an unknown system waiting to be explored. Prove your problem-solving skills in the Blackbox Cup and stand out in the JetBrains Foundation Scholarship selection.\nRegister now\n                                                    \n Learning highlights\nBest Programming Courses in 2025\nOur annual roundup of JetBrains Academy courses, including community favorites and releases from the past year.\nRead more\n                                                    \nGet your CSAI Bachelor’s in Cyprus \nThe JetBrains Foundation is offering up to 40 full scholarships for the Computer Science and Artificial Intelligence BSc program at Neapolis University Pafos, starting this year.\nApply now\n                                                    \nJetBrains Academy courses on Coursera\nOur new Python From Scratch: Learn by Coding course is now live on Coursera, featuring video lessons and full PyCharm integration. \nDiscover more\n                                                    \nLearn to develop with AI for free\nDeveloped by JetBrains and Nebius engineers, this new AI-assisted programming course shows you how to apply AI in practice through short, hands-on modules you can complete in under an hour. Try it now!\nStart learning\n                                                    \nTech insights\nHow to Prepare for the Future of Programming\nThe final post in our How to Learn to Program in an AI World series looks at how programming is changing and what skills will matter going forward.\nJoin the club\n                                                    \nPlugin updates\nHyperskill plugin\nAs of December 11, Hyperskill has moved to its own dedicated space and now offers a free standalone plugin for JetBrains IDEs, designed to ensure a smooth learning experience.\nLearn more",
        "dc:creator": "Maria Sharobaeva",
        "content": "Hi there 👋 February is in full swing, and we’ve packed this digest with things you don’t want to miss.Inside: upcoming events, hands-on learning, competitions, scholarships, and fresh updates to keep you moving forward in 2026 🚀",
        "contentSnippet": "Hi there 👋 February is in full swing, and we’ve packed this digest with things you don’t want to miss.Inside: upcoming events, hands-on learning, competitions, scholarships, and fresh updates to keep you moving forward in 2026 🚀",
        "guid": "https://blog.jetbrains.com/?post_type=education&p=680303",
        "categories": [
          "digest",
          "jetbrains-academy",
          "newsletter"
        ],
        "isoDate": "2026-02-12T22:16:37.000Z"
      },
      {
        "creator": "Maciej Gorywoda",
        "title": "Markdown in Scaladoc is now supported by IntelliJ IDEA!",
        "link": "https://blog.jetbrains.com/scala/2026/02/12/markdown-in-scaladoc/",
        "pubDate": "Thu, 12 Feb 2026 12:17:55 +0000",
        "content:encodedSnippet": "This blog post is available as well as a YouTube video.\nIn Scala 2, Scaladoc used Wikidoc syntax, which has advanced features like templates, categories, and metadata. These are useful for organizing and structuring large volumes of documentation, but they are rarely used in code comments.  On the other hand, Markdown offers a simpler, more readable syntax; in many cases, you can read it as raw text without any issues. Markdown is also much more popular and already used in README files and other documentation files, so it simply makes sense to use it in Scaladoc as well.\nThe best example of this is probably how tables are written in Markdown. \n\n\n\n\nAs you can see, the raw version of the table is simply a table made in ASCII which, at least in case the data is short and simple, can be read without rendering.\nIn Scala 3, both Wikidoc and Markdown are supported, but Markdown has been chosen as the default syntax. In other words, if you add a documentation comment (i.e. one starting with a slash and two asterisks, /** … */) to a class, method, or field declaration, by default we will assume its formatted with Markdown.\nMarkdown syntax\nIn the new release, IntelliJ Scala Plugin provides support for the new Scaladoc. That means all the following syntax features are supported:\nHeaders starting with a hash (#) in front of it, as well as headers made by underlining the text with the equality signs (=) in the next line.\nSmaller, “sub-chapter”, headers with multiple hashes or with the hyphens (-) in the next line.\nOrdered and unordered lists – on top of rendering them, the Scala plugin also automatically generates new indices and bullet points when you write them. You can make sub-lists too: When rendered, the ordered list will have deeper indentation for its sub-lists, while a sub-list of the unordered one will have different bullet points.\nBlock quotes with the “greater than” sign (>). If you start writing multiline quotes, the Scala Plugin will automatically generate signs for the next lines. Just as with lists, you can also write nested block quotes.\n\n  \n\n\n\n\n\n\nBold, italic, and bold italic font, made with either  asterisks (*) or underscore (_) signs. If you type just one asterisk or one underscore sign, the Scala Plugin will generate the other, closing one, and whatever you type between them will be rendered in italics. If you use two asterisks or two underscores on one side, the text in the middle will become bold. If you use three, the text will be rendered both italic and bold.\nSame for strike-through text, made with two tildes (~~).\nLinks with [link](text). If you render it and click it, IntelliJ IDEA will open your default web browser and direct you to the given webpage.\nCode anchors with [[name]]. The text within double square brackets should be a name of an existing class, trait, etc. While you write it, the Scala Plugin will display a code completion popup to help you find it. After rendering, you will see the anchor highlighted as if it was a link, but when you click on it, instead of redirecting you to that element, the Scala Plugin will display the documentation comment attached to it.\n\n\n\n\n\nHorizontal lines, made with an empty line followed by three hyphens (—). The empty line is important because otherwise the line made of hyphens will be understood as a sub-chapter header.\nInline code – both started with upper ticks and the <code> tag. \nMultiline code blocks – both started with curly braces {{{ … }}} and backticks “` … “`.\n\n\n\n\n\nAnd tables, which we already discussed above, but there’s one more thing to add – with the following syntax you can control the text alignment within the table’s columns:\n\n\n\n\nBackward compatibility\nIf you want to switch back to the old wikidoc syntax, put the @syntax wiki directive in the directives section at the end of the comment block. This is important because the @syntax directive is treated similarly to other directives like @param and @return and should be put together with them. If you make a mistake and put it at the top or inside the comment, everything below the directive will fail to render.\nHow to render\nThere are three ways to render Scaladoc in IntelliJ IDEA with the Scala Plugin:\nClick the gutter icon to the left of the first Scaladoc line. The documentation will render directly in the editor.\nHover the cursor over the comment block. A popup with rendered content will appear.\nIf the comment block is attached to a code element, such as a class or a method, you can hover the cursor over its name, regardless if it’s its definition or a call to it from another place in the code. This will also display a popup with rendered content.\n\n\n\n\nFor more information, you can refer to the following sources:\nScaladoc | Style Guide | Scala Documentation\nGuide to Scaladoc | Baeldung on Scala\nScaladoc for Library Authors | Scaladoc | Scala Documentation\n\n\n\n\nHappy developing!\nThe Scala team at JetBrains",
        "dc:creator": "Maciej Gorywoda",
        "content": "This blog post is available as well as a YouTube video. In Scala 2, Scaladoc used Wikidoc syntax, which has advanced features like templates, categories, and metadata. These are useful for organizing and structuring large volumes of documentation, but they are rarely used in code comments. &#160;On the other hand, Markdown offers a simpler, more [&#8230;]",
        "contentSnippet": "This blog post is available as well as a YouTube video. In Scala 2, Scaladoc used Wikidoc syntax, which has advanced features like templates, categories, and metadata. These are useful for organizing and structuring large volumes of documentation, but they are rarely used in code comments.  On the other hand, Markdown offers a simpler, more […]",
        "guid": "https://blog.jetbrains.com/?post_type=scala&p=679470",
        "categories": [
          "scala",
          "scala-programming",
          "intellij-idea",
          "scaladoc"
        ],
        "isoDate": "2026-02-12T12:17:55.000Z"
      },
      {
        "creator": "Anton Bragin",
        "title": "Are We Having the Wrong AI Dreams?",
        "link": "https://blog.jetbrains.com/ai/2026/02/are-we-having-the-wrong-ai-dreams/",
        "pubDate": "Thu, 12 Feb 2026 11:33:40 +0000",
        "content:encodedSnippet": "(This opinion piece by JetBrains’ Team Lead in AI Development Experience reflects on key takeaways from NeurIPS 2025, a major AI research conference. It explains why these insights matter and considers related signals emerging from other recent research.)\nMass layoffs, robots taking control of the planet, a post-truth world. Which of these comes to mind first when we talk about the disruptive innovation AI brings?\nIn her NeurIPS talk, “Are We Having the Wrong Nightmares About AI?”, Zeynep Tufekci argues that societies systematically misread the impact of major technological transformations in their early stages. We prepare for risks we already understand, like generals organizing for the previous war, while missing the challenges that actually matter.\nA growing theme in the research community is that AI’s intelligence is categorically different from human intelligence. That directly challenges the mental model of linear AI progress – the assumption that AI will “grow up” as a person does. \nNot less capable, but differently capable\nLLMs can beat humans on many benchmarks and tests, yet they still struggle with basic tasks beyond their generative capabilities. A simple physical task makes this gap tangible. \nThe image below illustrates the simple task of how to open a glass beer bottle with a metal house key; the model fails, despite the task being familiar and rather straightforward for most people. \n\n\n\n\nPrompt: You need to open a glass bottle of beer, but you don’t have a bottle opener handy. However, you have a metal house key. Illustrate how to use the key to open the bottle. Model: gpt-image-1.5.\nThis contrast shows a recurring pattern. LLMs can perform well above a professional level on some structured, text-based problems, then fail on others that even children can handle with ease. The issue is not a question of capability; rather, it is a matter of frame of reference. These systems do not sit higher or lower on a human scale. They operate on a different scale altogether.\nAI as another form of intelligence\nIn 2025, a widespread view was that AI in its current form represents another kind of intelligence, one that cannot be directly projected onto a human talent scale. Despite rapid progress, an old research goal remains unresolved: How do we help large language models to perform the kinds of practical tasks that every human can?\nLLMs can match or outperform humans in structured, text-based evaluations, yet they continue to lag behind when it comes to working out genuinely novel solutions and adapting when faced with complex, non-stationary settings.\nSeveral researchers argue that this gap reflects a deeper mismatch between human and model learning. Zeynep Tufekci stresses that generative AI is not a form of human intelligence, while Blake Lemoine puts it more bluntly: “Only one thing is clear: LLMs are not human.”\nStudies comparing children and models show that young children can infer causal structures from only a handful of observations. In contrast, large language models struggle to identify the relevant causal relationships at all.\nOther experiments demonstrate that in more complex, non-stationary environments, LLMs fail to match human adaptability, particularly when effective directed exploration matters.\nStrong evaluations don’t translate into impact (yet?)\nThis disconnect may help explain what Ilya Sutskever described as one of the confusing aspects of current models. They perform extremely well on evaluations, yet the economic impact trails far behind. \nStrong benchmark results do not translate directly into robust performance in open-ended, real-world settings.\nIn a software development context, this has direct implications. We should not align LLMs with humans, neither in the requirements we impose on development processes nor in the outputs we expect them to produce. \nAs we involve LLMs more deeply, with their distinct strengths and limitations, the surrounding processes will need to change accordingly. Effective use will come less from forcing models into human-shaped roles and more from reshaping workflows to fit the kind of intelligence they actually provide.\nLLMs will transform ecosystems\nWhen we talk about technology ecosystems, we often focus on tools. Biological ecosystems remind us that this view is incomplete. An ecosystem includes not only the organisms, but also the environment they live in, and that environment is neither static nor passive. \nOrganisms actively shape it, and in doing so, they create conditions that favour their own survival and reproduction, while sometimes destroying environments that no longer serve them.\nSoftware development has followed a similar pattern. Codebases, programming languages, build systems, and deployment practices have repeatedly reshaped not only the code itself, but also collaboration and development processes. These elements form the environment in which development tools operate, and they co-evolve with those tools.\nGiven the pace of LLM adoption, we should expect a comparable shift. LLMs are unlikely to remain passive inhabitants of today’s development environment. Instead, the ecosystem itself will change to better suit their strengths. \nLanguages, best practices, and workflows will emerge, evolve, or disappear based on how compatible they are with an LLM-dominated environment and how effectively they enable AI-driven work.\nThe sweetness of the bitter lesson\nRichard Sutton, one of the pioneers of reinforcement learning, formulated what he called the “bitter lesson” after decades of AI research. \nHis observation was that many apparent breakthroughs come from injecting human knowledge into systems, for example, by hand-crafting rules, heuristics, or domain-specific structures. \nThese approaches often deliver quick wins. Over time, however, they tend to lose to more general methods that rely on learning and search capabilities, and that scale with increases in computation and data.\nSutton’s point was not that human knowledge is useless, but that it becomes a limiting factor. Systems built around general methods continue to improve as computing grows, while systems constrained by human-designed shortcuts eventually hit a ceiling.\nApplied to software development, the implication is significant. If we treat development processes, tools, and workflows as methods, then approaches that maximize effective AI utilization are likely to win over time. \nIn contrast, approaches that restrict AI involvement or introduce friction, including heavy human-in-the-loop dependencies, risk becoming bottlenecks as models and infrastructure continue to scale.\nA view towards the future\nPredictions in 2026 are hard. Still, the research points in a consistent direction. LLMs are a different beast, and we should stop treating them as junior humans who will replace us one by one. \nThey will reshape the software development environment to suit their particular kind of intelligence. \nAlongside incremental improvements to today’s workflows, we should explore more radical shifts, deliberately reshaping codebases and processes to maximize effective AI utilization.",
        "dc:creator": "Anton Bragin",
        "content": "(This opinion piece by JetBrains’ Team Lead in AI Development Experience reflects on key takeaways from NeurIPS 2025, a major AI research conference. It explains why these insights matter and considers related signals emerging from other recent research.) Mass layoffs, robots taking control of the planet, a post-truth world. Which of these comes to mind [&#8230;]",
        "contentSnippet": "(This opinion piece by JetBrains’ Team Lead in AI Development Experience reflects on key takeaways from NeurIPS 2025, a major AI research conference. It explains why these insights matter and considers related signals emerging from other recent research.) Mass layoffs, robots taking control of the planet, a post-truth world. Which of these comes to mind […]",
        "guid": "https://blog.jetbrains.com/?post_type=ai&p=678934",
        "categories": [
          "industry-trends",
          "insights",
          "jetbrains-ai",
          "opinion"
        ],
        "isoDate": "2026-02-12T11:33:40.000Z"
      },
      {
        "creator": "Cheuk Ting Ho",
        "title": "Python Unplugged on PyTV – A Free Online Python Conference for Everyone ",
        "link": "https://blog.jetbrains.com/pycharm/2026/02/python-unplugged-on-pytv/",
        "pubDate": "Wed, 11 Feb 2026 16:37:44 +0000",
        "content:encodedSnippet": "The PyCharm team loves being part of the global Python community. From PyCon US to EuroPython to every PyCon in between, we enjoy the atmosphere at conferences, as well as meeting people who are as passionate about Python as we are. This includes everyone: professional Python developers, data scientists, Python hobbyists and students.\n\n\n\n\nHowever, we know that being able to attend a Python conference in person is not something that everyone can do, either because they don’t have a local conference, or cannot travel to one. So within the PyCharm team we started thinking: what if we could bring the five-star experience of Python conferences to everyone? What if everyone could have the experience of learning from professional speakers, accessing great networking opportunities, hearing from various voices from across the community, and – most importantly – having fun, no matter where they are in the world?\nPython is for Everyone – Announcing Python Unplugged on PyTV!\n\n\n\n\n\n\nAfter almost a year of planning, we’re proud to announce we’ll be hosting the first ever PyTV – a free online conference for everyone!\n\n\n\n\nJoin us on March 4th 2026, for an unforgettable, non-stop event, streamed from our studio in Amsterdam. We’ll be joined live by 15 well-known and beloved speakers from Python communities around the globe, including Carol Willing, Deb Nicholson, Sheena O’Connell, Paul Everitt, Marlene Mhangami, and Carlton Gibson. They’ll be speaking about topics such as core Python, AI, community, web development and data science. \n\n\n\n\nYou can get involved in the fun as well! Throughout the livestream, you can join our chat on Discord, where you can interact with other participants and our speakers. We’ve also prepared games and quizzes, with fabulous prizes up for grabs! You might even be able to get your hands on some of the super cool conference swag that we designed specifically for this event.\nWhat are you waiting for? Sign up here. \nIf you are local to Amsterdam, you can also sign up for the PyLadies Amsterdam meetup. It will be held on the same day as the conference, and will give you a chance to meet some of the PyTV speakers in person.",
        "dc:creator": "Cheuk Ting Ho",
        "content": "The PyCharm team loves being part of the global Python community. From PyCon US to EuroPython to every PyCon in between, we enjoy the atmosphere at conferences, as well as meeting people who are as passionate about Python as we are. This includes everyone: professional Python developers, data scientists, Python hobbyists and students. However, we [&#8230;]",
        "contentSnippet": "The PyCharm team loves being part of the global Python community. From PyCon US to EuroPython to every PyCon in between, we enjoy the atmosphere at conferences, as well as meeting people who are as passionate about Python as we are. This includes everyone: professional Python developers, data scientists, Python hobbyists and students. However, we […]",
        "guid": "https://blog.jetbrains.com/?post_type=pycharm&p=679943",
        "categories": [
          "ai",
          "conference",
          "ml",
          "python"
        ],
        "isoDate": "2026-02-11T16:37:44.000Z"
      }
    ]
  },
  {
    "name": "Airbnb Engineering & Data Science",
    "category": "기업",
    "posts": [
      {
        "creator": "Lauren Mackevich",
        "title": "My Journey to Airbnb — Anna Sulkina",
        "link": "https://medium.com/airbnb-engineering/my-journey-to-airbnb-anna-sulkina-85216183d094?source=rss----53c7c27702d5---4",
        "pubDate": "Wed, 11 Feb 2026 17:02:19 GMT",
        "content:encodedSnippet": "Anna Sulkina has always been a traveler, and we’re lucky her travels have brought her to Airbnb. Anna is a Senior Director of Engineering, and she’s responsible for Application & Cloud infrastructure. She brings over two decades of industry experience to Airbnb, including work spanning the stack from the frontend to the backend to the plumbing that makes everything come together. Anna is a mother, a passionate trail runner, and an accomplished leader. Here’s Anna’s story in her own words.\n\nDiscovering a passion after the Soviet Union\nI grew up in Eastern Ukraine, and the year I was graduating from high school, the Soviet Union collapsed. Despite the political turmoil, it was an interesting time to get into technology, and I have my brother to thank for that.\nI was always a nerdy kid, at school and at home, and my older brother really stoked that curiosity. He was studying computer hardware in Moscow, and he’d bring home computer parts to play with. I still remember the first computer he’d assembled, which required a cassette player to load programs. Only after many minutes of buzzing and clicking would the computer finally whirr to life.\nThinking back, that was really my first inspiration to work in technology. Seeing the inner workings of this new thing, a computer, and watching how the parts came together to form a whole — that’s what made me realize I wanted to work with computers, too.\nOf course, I didn’t know the Soviet Union would end, which made studying in Moscow impossible. But technology was still my future.\nEnglish: Harder than any programming language\nI got my start learning programming in a local Ukrainian university, and after four years of studying, I immigrated to America.\nWhen I arrived, I knew how to program, and I knew how to write and read English, but I couldn’t communicate well. I took ESL classes at a community college and, in parallel, enrolled in Berkeley Extension classes to advance my C++ knowledge and learn Java, which was still very new at the time.\nThroughout my first couple of jobs, I was more likely to run into challenges with the English language rather than with programming languages.\nMy first job was in computer hardware diagnostics at a tiny company with only five engineers, where we communicated directly with hardware manufacturers. This was right before the dot-com bubble burst.\nI almost didn’t get the job, though. The interview process for this job included a written portion that tested my knowledge of key computer science terms before getting to solve the coding problems. Given my prior education, I knew all the terms, but I ran out of time because the language gap slowed me down. Luckily, my interviewer happened to be taking the same Java class at Berkeley, and when I explained what happened, he gave me the chance to come back. I finished the test, got the job, and the rest is history.\nIn subsequent jobs, I transitioned fully from C++ to Java, which became my primary programming language for many years. I eventually got the hang of speaking English more confidently, but for a while, it still felt like Russian was my first language, Java was my second, and English was only my third.\nGoing deeper in the stack and taking on leadership roles\nAt various times, my career often felt all over the place. But looking back, I see a trajectory I wasn’t aware of at the time. I started with a brief stint in hardware diagnostics, but after that, I worked in the frontend and, over time, descended the software stack from frontend to backend to the deeper infrastructure I work with today.\nParallel to this trajectory down the stack was an upward trajectory in responsibility. Leadership wasn’t an obvious path for me at first — I had to be pitched multiple times — but the more I tried it, the more interesting and enjoyable it felt.\nWhen I worked at Caymas Systems, a telecom startup, my manager was quick to recognize my leadership potential. He was really encouraging, but even more encouraging was witnessing the difference between teams with good leaders and those without.\nAfter Caymas Systems, I worked at Comcast, where I eventually switched from an IC to an engineering manager. Once I experienced the joys of coaching people, building cool software together, and developing high-performing teams, I knew this was the path I wanted to take.\nFail whales and distributed systems\nThis path took me through a formative time in my career: the almost nine years I spent working at Twitter. I began as a first-line manager and, over time, worked through some of Twitter’s biggest events, including the “fail whale” era and the Ellen DeGeneres “selfie that broke Twitter” moment.\nThis was an exciting time. I was working at the heart of Twitter’s tech stack, supporting teams that powered its consumer and revenue verticals. This is where I grew into a senior manager and, eventually, a director. Looking back over nearly a decade of work, two major lessons stand out: one technical and one cultural.\nThe technical lesson was about failure — namely, its inevitability.\nOver my tenure, the Twitter stack transitioned from a monolith to a microservices architecture. This resulted in a set of robust, high-scale, low-latency distributed systems, and it was here that I learned that, when building resilient distributed systems, you need to design for failure, not hope to avoid it.\nI often think back to How Complex Systems Fail — the more complex a system, the more likely it is to fail. I remembered that lesson every time we were called to the Twitter command center to deal with an incident; it was all hands on deck until everything was back online.\nThe cultural lesson was about adoption and what it takes to fuel great ideas.\nToday, almost two-thirds of enterprises use GraphQL in production, but in its early days, it was a new, largely untested idea. During a hack week, a couple of engineers laid the groundwork for using this technology at Twitter. I worked closely with them and bootstrapped the team that eventually built Twitter’s GraphQL API, replacing the legacy REST services.\nI still think about this experience today. It required convincing leadership and building consensus across numerous teams and stakeholders, but once we did, the payoff was significant: this one technical choice accelerated the velocity of product feature teams across the company.\nWhy I picked Airbnb\nWhen Airbnb reached out in 2022, I realized my time at Twitter was coming to a close. By that point, my organization was well-run and high-performing — a success, but also a sign that I was ready for my next adventure.\nAirbnb immediately stood out because the company offered, for the first time in my career, a true alignment between my personal and technical interests. I love traveling, and I have been a long-time Airbnb guest since 2013. I had always wanted to work for a company that built a product I truly cared about, and this was my chance.\nI only got more excited when I learned about the people and teams I’d be working with. The Developer Platform organization, which was responsible for supporting all of Airbnb’s engineers, faced challenges I’d seen before. There was a lot of good work happening in silos, and folks were longing for a clear strategy and direction. Also, I saw an opportunity to not only improve developer experience but also build trust with the rest of the engineers and stakeholders.\nSo, I started at the beginning. We focused on setting up the organization, coaching leadership, and building internal alignment within the team, as well as external alignment across all the teams we supported. Fundamental questions like “Why are we here together?” and “Where are we going?” all had to be answered.\nAfter a year or two of this work, we had a high-performing team with a clear strategy and strong execution, consistently delivering business value and improving the developer experience and productivity at Airbnb. Even more importantly, we earned the rest of the engineers’ trust, and we enabled our technical teams to perform better.\nWe saw this reflected in the bi-annual DevX surveys (which we built out), and the results showed overall developer satisfaction increasing about 10% year over year during my time on the team.\nSolving new problems while working from anywhere\nToday, I’m Senior Director of Engineering for Application & Cloud Infrastructure, which includes compute, networking, core services, and the GraphQL application platform. Our mission is to deliver reliable, secure, and efficient platforms for building, operating, and scaling applications, services, and workloads at Airbnb.\nMy primary users are still the engineers at Airbnb. When they need to compute, they don’t wrangle AWS themselves — we provide a layer of abstraction that helps them use low-level infrastructure. Similarly, if they need authorization, authentication, configuration management, and a host of other services, they come to us rather than starting from scratch.\nI’m excited to come to work every day because of the people I get to work with and the opportunities we face together. The culture is excellent, the people are smart and collaborative, and the engineers we support appreciate the work we do.\nThe setup is empowering, too, and as you solve problems, you can grow and expand to tackle bigger problems that span teams and organizations. Add in the ability to work from anywhere, and for me, it feels like the sky is the limit.\nAs I look back on my career, and really, my entire life, I tend to see it now through the lens of long-distance trail running — a major hobby of mine.\nAfter working at a startup, having twins, and running my first marathon, I felt like I could do anything. At work and on the trails, I think about how to prepare for the journeys ahead and how to maintain a pace that allows me and the people around me to thrive in the long run. Recovery is necessary, but so is strategy, drive, discipline, and finding the people who will go with you as well as cheer you on along the way.\nI’m happy this path, as unpredictable as it has been, has taken me to Airbnb. Airbnb is in that ideal position between a startup and a long-established company. The systems and workflows are mature, but there are still many interesting problems to solve and opportunities to pursue. If that’s of interest to you, I encourage you to check out openings at Airbnb.\n\nMy Journey to Airbnb — Anna Sulkina was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Lauren Mackevich",
        "guid": "https://medium.com/p/85216183d094",
        "categories": [
          "engineering",
          "people",
          "leadership",
          "technology",
          "software-development"
        ],
        "isoDate": "2026-02-11T17:02:19.000Z"
      }
    ]
  },
  {
    "name": "PayPal Engineering",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Visual Studio Blog",
    "category": "기업",
    "posts": [
      {
        "creator": "Sinem Akinci, Hannah Hong (SHE/HER)",
        "title": "Unlock language-specific rich symbol context using new find_symbol tool",
        "link": "https://devblogs.microsoft.com/visualstudio/unlock-language-specific-rich-symbol-context-using-new-find_symbol-tool/",
        "pubDate": "Wed, 11 Feb 2026 15:29:15 +0000",
        "content:encodedSnippet": "Refactoring at scale is a time-consuming and error-prone process for developers. In large codebases, developers have relied on manual searches and incremental edits across multiple files to accomplish these tasks.\nModern development workflows depend on fast and accurate code navigation to avoid these pitfalls. When developers refactor existing code, explore unfamiliar areas of a large codebase, or make targeted changes, they naturally rely on IDE language service features such as Find All References, Go to Definition, and Go to Implementation to understand how code is structured and connected.\nAgent mode now has access to these same language-aware capabilities through the new find_symbol tool. This goes beyond traditional text or file search traditionally available in agent mode by enabling symbol-level reasoning powered by enterprise-grade language services.\n\nWhat is find_symbol?\nFind_symbol exposes rich, language-specific symbol information to Copilot Agent Mode, allowing the agent to reason about symbols (including functions, classes, interfaces, and variables).\nSpecifically, this tool allows Copilot agent mode to:\nView all references of a given symbol across the entire codebase\nUnderstand symbol metadata such as type, declaration, implementation, and scope\nThe find_symbol tool is available today in the latest Visual Studio 2026 Insiders version 18.4. Supported languages include: C++, C#, Razor, TypeScript, and any other language for which you have a supported Language Server Protocol (LSP) extension installed.\nFor best results, write clear prompts and use AI models that support tool-calling. Learn more at AI model comparison – GitHub Docs\nExample scenarios\nAll examples below were showcased using bullet3, an open-source C++ physics simulation engine.\nAdding additional functionality to existing code\nAs applications evolve, you often need to enhance existing functions without breaking current behavior. This can include adding logging or performance metrics.\nThese tools help the agent quickly identify all relevant references, ensuring complete and accurate updates for feature additions.\n\nAPI Refactoring\nRefactoring an API, such as hardening it, requires deep understanding of how the API is consumed across a codebase. With symbol-level insight, the agent can discover all usages, distinguish between call paths, and propose safe refactors with minimal breakage.\n\nTell us what you think\nWe appreciate the time you’ve spent reporting issues/suggestions and hope you continue to give us feedback when using Visual Studio on what you like and what we can improve. You can share feedback with us via Developer Community: report any bugs or issues via report a problem and share your suggestions for new features or improvements to existing ones.\nStay connected with the Visual Studio team by following us on YouTube, X, LinkedIn, Twitch and on Microsoft Learn\n \nThe post Unlock language-specific rich symbol context using new find_symbol tool appeared first on Visual Studio Blog.",
        "dc:creator": "Sinem Akinci, Hannah Hong (SHE/HER)",
        "comments": "https://devblogs.microsoft.com/visualstudio/unlock-language-specific-rich-symbol-context-using-new-find_symbol-tool/#comments",
        "content": "<p>Refactoring at scale is a time-consuming and error-prone process for developers. In large codebases, developers have relied on manual searches and incremental edits across multiple files to accomplish these tasks. Modern development workflows depend on fast and accurate code navigation to avoid these pitfalls. When developers refactor existing code, explore unfamiliar areas of a large [&#8230;]</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/visualstudio/unlock-language-specific-rich-symbol-context-using-new-find_symbol-tool/\">Unlock language-specific rich symbol context using new find_symbol tool</a> appeared first on <a href=\"https://devblogs.microsoft.com/visualstudio\">Visual Studio Blog</a>.</p>\n",
        "contentSnippet": "Refactoring at scale is a time-consuming and error-prone process for developers. In large codebases, developers have relied on manual searches and incremental edits across multiple files to accomplish these tasks. Modern development workflows depend on fast and accurate code navigation to avoid these pitfalls. When developers refactor existing code, explore unfamiliar areas of a large […]\nThe post Unlock language-specific rich symbol context using new find_symbol tool appeared first on Visual Studio Blog.",
        "guid": "https://devblogs.microsoft.com/visualstudio/?p=255598",
        "categories": [
          "GitHub Copilot"
        ],
        "isoDate": "2026-02-11T15:29:15.000Z"
      }
    ]
  },
  {
    "name": "Joshua",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권재명",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김석기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김선영",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강대명",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권정혁",
    "category": "개인",
    "posts": []
  },
  {
    "name": "줌구",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수보",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김시은",
    "category": "개인",
    "posts": []
  },
  {
    "name": "곽민수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김범진",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민서",
    "category": "개인",
    "posts": []
  },
  {
    "name": "I am not Okay",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권창현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권기호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수민",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강태욱",
    "category": "개인",
    "posts": [
      {
        "title": "신뢰성있는 모델 파인튜닝 개발 방법 ",
        "link": "http://daddynkidsmakers.blogspot.com/2026/02/blog-post.html",
        "pubDate": "2026-02-18T01:08:00.000Z",
        "author": "Daddy Maker",
        "content": "<div style=\"text-align: left;\">이 글은&nbsp;신뢰성있는 모델 파인튜닝 개발 방법을 나눔한다.<br /><br /></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b>레퍼런스</b></div><div style=\"text-align: left;\"><ul style=\"text-align: left;\"><li><a href=\"https://medium.com/@nomannayeem/finetuning-phi-3-for-rag-why-small-models-are-best-for-production-c2f6a5f74b51\">FineTuning PHI-3 for RAG: Why Small Models Are Best for Production | by Nayeem Islam | Medium</a></li><li><a href=\"https://firastlili.medium.com/fine-tuning-phi-3-with-unsloth-a-beginner-friendly-guide-12913d7ea506\">Fine-Tuning Phi-3 with Unsloth: A Beginner-Friendly Guide 🚀🤖 | by Firas Tlili | Medium</a></li><li><a href=\"https://medium.com/@liana.napalkova/fine-tuning-small-vision-language-models-phi-3-vision-b9d406e4e268\">Fine-tuning Small Vision Language Models: Phi-3-vision | by Liana Napalkova, PhD | Medium</a></li><li><a href=\"https://medium.com/@kondam.reddy/from-scratch-a-deep-dive-into-fine-tuning-phi-3-5-for-clinical-q-a-a49cb01b2414\">From Scratch: A Deep Dive into Fine-Tuning Phi-3.5 for Clinical Q&amp;A | by Sriman | Medium</a></li><li><a href=\"https://unsloth.ai/docs/get-started/unsloth-notebooks\">Unsloth Notebooks | Unsloth Documentation</a></li><li><a href=\"https://medium.com/data-science-at-microsoft/unlocking-the-power-of-data-synthesis-for-function-calling-with-fine-tuned-slms-a733df7a1d07\">Unlocking the power of data synthesis for function calling with fine-tuned SLMs | by Kosuke Fujimoto | Data Science + AI at Microsoft | Medium</a></li><li><a href=\"https://github.com/microsoft/PhiCookBook/blob/main/code%2F04.Finetuning%2FPhi-3-finetune-lora-python.ipynb\">PhiCookBook/code/04.Finetuning/Phi-3-finetune-lora-python.ipynb at main · microsoft/PhiCookBook</a></li><li><a href=\"https://medium.com/@fahey_james/fine-tuning-vs-function-calling-the-new-stack-for-customizing-foundation-models-bd1e48decf2c\">Fine-Tuning vs. Function Calling: The New Stack for Customizing Foundation Models | by James Fahey | Medium</a></li></ul></div>",
        "contentSnippet": "이 글은 신뢰성있는 모델 파인튜닝 개발 방법을 나눔한다.\n\n\n\n레퍼런스\n\nFineTuning PHI-3 for RAG: Why Small Models Are Best for Production | by Nayeem Islam | Medium\nFine-Tuning Phi-3 with Unsloth: A Beginner-Friendly Guide 🚀🤖 | by Firas Tlili | Medium\nFine-tuning Small Vision Language Models: Phi-3-vision | by Liana Napalkova, PhD | Medium\nFrom Scratch: A Deep Dive into Fine-Tuning Phi-3.5 for Clinical Q&A | by Sriman | Medium\nUnsloth Notebooks | Unsloth Documentation\nUnlocking the power of data synthesis for function calling with fine-tuned SLMs | by Kosuke Fujimoto | Data Science + AI at Microsoft | Medium\nPhiCookBook/code/04.Finetuning/Phi-3-finetune-lora-python.ipynb at main · microsoft/PhiCookBook\nFine-Tuning vs. Function Calling: The New Stack for Customizing Foundation Models | by James Fahey | Medium",
        "id": "tag:blogger.com,1999:blog-5201956450461596914.post-3481572627910625849",
        "isoDate": "2026-02-18T01:08:00.000Z"
      }
    ]
  },
  {
    "name": "권용진",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "RSS feed for boyism Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성준의 린스타트업과 디자인씽킹",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권혁우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김준형",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강동혁",
    "category": "개인",
    "posts": []
  },
  {
    "name": "고종범",
    "category": "개인",
    "posts": []
  },
  {
    "name": "cheese10yun",
    "category": "개인",
    "posts": [
      {
        "title": "필드 단위 변경 이력(History) 추적 시스템",
        "link": "https://cheese10yun.github.io/diff-history-part-1/",
        "pubDate": "Sun, 15 Feb 2026 15:00:00 GMT",
        "content:encodedSnippet": "들어가며#\n운영 환경에서 데이터 변경 이력을 추적해야 하는 경우가 자주 발생합니다. 특히 주문 정보 수정, 가맹점 수수료율 변경 등 중요한 데이터가 어떻게 변경되었는지 필드 단위로 명확하게 기록하고 확인할 수 있어야 합니다.\n예를 들어, 운영자가 주문 내역에서 배송 주소만 변경했을 때, 전체 주문 데이터를 다시 저장하는 것보다 “어떤 필드가 어떻게 변경되었는지”를 명확히 기록하면 다음과 같은 이점이 있습니다:\n\n변경 이력 추적이 명확해집니다\n승인 프로세스에서 변경 내용 검토가 용이합니다\n디버깅 및 감사(Audit) 목적으로 활용할 수 있습니다\n데이터 롤백 시 정확한 변경 지점을 파악할 수 있습니다\n\n이번 포스트에서는 Kotlin과 Jackson을 활용하여 복잡한 중첩 객체의 변경사항을 자동으로 추적하는 시스템을 구현하는 방법을 알아보겠습니다.\n문제 상황#\n다음과 같은 주문(Order) 데이터가 있다고 가정해봅시다.\n변경 전 데이터#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n{\n  \"order_id\": \"ORD123456\",\n  \"customer\": {\n    \"customer_id\": \"CUST7890\",\n    \"name\": \"홍길동\",\n    \"contact\": {\n      \"email\": \"hong@example.com\",\n      \"phone\": \"010-1234-5678\",\n      \"address\": {\n        \"street\": \"서울특별시 종로구\",\n        \"city\": \"서울\",\n        \"zip_code\": \"03000\",\n        \"country\": \"KR\"\n      }\n    }\n  },\n  \"items\": [\n    {\n      \"product\": {\n        \"product_id\": \"PROD001\",\n        \"product_name\": \"노트북\",\n        \"category\": {\n          \"main_category\": \"전자제품\",\n          \"sub_category\": \"컴퓨터\"\n        }\n      },\n      \"quantity\": 1,\n      \"price\": 1500000\n    }\n  ],\n  \"payment\": {\n    \"method\": \"신용카드\",\n    \"transaction_id\": \"TXN987654321\",\n    \"status\": \"완료\"\n  }\n}\n\n\n변경 후 데이터#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n{\n  \"order_id\": \"ORD123456\",\n  \"customer\": {\n    \"customer_id\": \"CUST7890\",\n    \"name\": \"홍길동\",\n    \"contact\": {\n      \"email\": \"hong@example.com\",\n      \"phone\": \"010-1234-5678\",\n      \"address\": {\n        \"street\": \"서울특별시 강남구\",\n        \"city\": \"서울\",\n        \"zip_code\": \"03000\",\n        \"country\": \"KR\"\n      }\n    }\n  },\n  \"items\": [\n    {\n      \"product\": {\n        \"product_id\": \"PROD001\",\n        \"product_name\": \"노트북\",\n        \"category\": {\n          \"main_category\": \"전자제품\",\n          \"sub_category\": \"컴퓨터\"\n        }\n      },\n      \"quantity\": 1,\n      \"price\": 1400000\n    }\n  ],\n  \"payment\": {\n    \"method\": \"신용카드\",\n    \"transaction_id\": \"TXN987654322\",\n    \"status\": \"완료\"\n  }\n}\n\n\n위 두 데이터를 비교하면 다음 필드들이 변경되었습니다:\n\ncustomer.contact.address.street: “서울특별시 종로구” → “서울특별시 강남구”\nitems[0].price: 1500000 → 1400000\npayment.transaction_id: “TXN987654321” → “TXN987654322”\n\n이러한 변경사항을 자동으로 감지하고 추적하려면 어떻게 해야 할까요?\nIntelliJ의 Diff 기능처럼#\nIntelliJ IDE를 사용해보신 분들은 아시겠지만, 두 파일을 비교할 때 매우 직관적으로 변경 사항을 표시해줍니다.\n\n우리가 구현하려는 시스템도 이와 유사하게 두 객체를 비교하여 변경된 필드만 추출하는 것입니다.\n구현 방법#\n1. 핵심 라이브러리: zjsonpatch#\nJSON 객체 간의 차이를 계산하기 위해 zjsonpatch 라이브러리를 사용합니다. 이 라이브러리는 RFC 6902 JSON Patch 표준을 구현하여 두 JSON 문서의 차이를 효과적으로 계산합니다.\n\n\n1\n2\n3\n4\n5\n\n// build.gradle.kts\ndependencies {\n    implementation(\"com.flipkart.zjsonpatch:zjsonpatch:0.4.14\")\n    implementation(\"com.fasterxml.jackson.module:jackson-module-kotlin\")\n}\n\n\n2. DiffComparisonManager 구현#\n전체 코드는 다음과 같습니다:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n\npackage com.example.boot3mongo\n\nimport com.fasterxml.jackson.core.JsonGenerator\nimport com.fasterxml.jackson.databind.JsonNode\nimport com.fasterxml.jackson.databind.JsonSerializer\nimport com.fasterxml.jackson.databind.PropertyNamingStrategies\nimport com.fasterxml.jackson.databind.SerializerProvider\nimport com.fasterxml.jackson.databind.module.SimpleModule\nimport com.fasterxml.jackson.module.kotlin.jacksonObjectMapper\nimport com.flipkart.zjsonpatch.JsonDiff\nimport org.bson.types.ObjectId\n\ntypealias DiffValueTracker = Map<String, DiffValue<String, String>>\ntypealias DiffTriple = Triple<String, String, String>\n\nobject DiffComparisonManager {\n\n    private val diffMapper = jacksonObjectMapper()\n        .apply {\n            registerModules(\n                SimpleModule().apply {\n                    propertyNamingStrategy = PropertyNamingStrategies.SNAKE_CASE\n                    addSerializer(ObjectId::class.java, ObjectIdSerializer())\n                }\n            )\n        }\n\n    fun <T> calculateDifference(\n        originItem: T,\n        newItem: T\n    ): DiffValueTracker {\n        val originalNode = diffMapper.valueToTree<JsonNode>(originItem)\n        val newNode = diffMapper.valueToTree<JsonNode>(newItem)\n        val diff = JsonDiff.asJson(originalNode, newNode)\n        return when {\n            diff.size() > 0 -> {\n                diff.mapNotNull { diffNode ->\n                    val (path, originValue, newValue) = extractDiffValue(diffNode, originalNode, newNode)\n                    Pair(\n                        first = path,\n                        second = DiffValue(originValue, newValue)\n                    )\n                }\n                    .toMap()\n            }\n            else -> emptyMap()\n        }\n    }\n\n    fun <T, K, S> calculateDifferences(\n        originItems: List<T>,\n        newItems: List<T>,\n        associateByKey: (T) -> K,\n        groupByKey: (T) -> S\n    ): Map<S, DiffValueTracker> {\n        val originalAssociate = originItems.associateBy(associateByKey)\n        val newAssociate = newItems.associateBy(associateByKey)\n        val changes = newAssociate.flatMap { (id, newItem) ->\n            val originalItem = originalAssociate[id]\n            when {\n                originalItem != null -> {\n                    val originalNode = diffMapper.valueToTree<JsonNode>(originalItem)\n                    val newNode = diffMapper.valueToTree<JsonNode>(newItem)\n                    val diffNode = JsonDiff.asJson(originalNode, newNode)\n\n                    when {\n                        diffNode.size() > 0 -> {\n                            diffNode.mapNotNull { node ->\n                                val (path, originValue, newValue) = extractDiffValue(node, originalNode, newNode)\n\n                                Triple(\n                                    first = groupByKey(newItem),\n                                    second = path,\n                                    third = DiffValue(origin = originValue, new = newValue)\n                                )\n                            }\n                        }\n                        else -> emptyList()\n                    }\n                }\n                else -> emptyList()\n            }\n        }\n\n        return changes\n            .groupBy({ it.first }, { it.second to it.third })\n            .mapValues { (_, value) -> value.toMap() }\n    }\n\n    private fun extractDiffValue(node: JsonNode, originalNode: JsonNode, newNode: JsonNode): DiffTriple {\n        val path = node.get(\"path\").asText().removePrefix(\"/\")\n        val originValue = originalNode.at(\"/$path\").asText()\n        val newValue = newNode.at(\"/$path\").asText()\n        return DiffTriple(path, originValue, newValue)\n    }\n}\n\ndata class DiffValue<out A, out B>(\n    val origin: A,\n    val new: B\n)\n\nclass ObjectIdSerializer : JsonSerializer<ObjectId>() {\n    override fun serialize(value: ObjectId, gen: JsonGenerator, serializers: SerializerProvider) {\n        gen.writeString(value.toString())\n    }\n}\n\n\n3. 코드 상세 설명#\n3.1 Jackson ObjectMapper 설정#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\nprivate val diffMapper = jacksonObjectMapper()\n    .apply {\n        registerModules(\n            SimpleModule().apply {\n                propertyNamingStrategy = PropertyNamingStrategies.SNAKE_CASE\n                addSerializer(ObjectId::class.java, ObjectIdSerializer())\n            }\n        )\n    }\n\n\nSnake Case 변환: Kotlin의 camelCase 필드명을 JSON의 snake_case로 자동 변환합니다\nObjectId 직렬화: MongoDB의 ObjectId를 문자열로 변환하는 커스텀 Serializer를 등록합니다\n이를 통해 productName → product_name으로 자동 변환되어 일관된 필드명으로 추적할 수 있습니다\n\n3.2 calculateDifference 함수#\n단일 객체 간의 차이를 계산하는 핵심 함수입니다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\nfun <T> calculateDifference(\n    originItem: T,\n    newItem: T\n): DiffValueTracker {\n    // 1. Kotlin 객체를 JsonNode로 변환\n    val originalNode = diffMapper.valueToTree<JsonNode>(originItem)\n    val newNode = diffMapper.valueToTree<JsonNode>(newItem)\n    \n    // 2. JsonDiff로 차이 계산\n    val diff = JsonDiff.asJson(originalNode, newNode)\n    \n    // 3. 차이가 있으면 변경 정보 추출\n    return when {\n        diff.size() > 0 -> {\n            diff.mapNotNull { diffNode ->\n                val (path, originValue, newValue) = extractDiffValue(diffNode, originalNode, newNode)\n                Pair(\n                    first = path,\n                    second = DiffValue(originValue, newValue)\n                )\n            }.toMap()\n        }\n        else -> emptyMap()\n    }\n}\n\n\n동작 과정:\n\n객체를 JsonNode로 변환: Kotlin 객체를 Jackson의 JsonNode로 변환하여 JSON 구조로 다룰 수 있게 합니다\nJsonDiff 계산: JsonDiff.asJson()을 사용하여 두 JsonNode 간의 차이를 계산합니다\n변경 정보 추출: 각 diff node에서 경로(path), 이전 값(originValue), 새 값(newValue)을 추출합니다\n결과 반환: Map<String, DiffValue> 형태로 반환합니다\n\nKey: 필드 경로 (예: customer/contact/address/street)\nValue: DiffValue(origin, new) 객체\n\n3.3 extractDiffValue 함수#\n\n\n1\n2\n3\n4\n5\n6\n\nprivate fun extractDiffValue(node: JsonNode, originalNode: JsonNode, newNode: JsonNode): DiffTriple {\n    val path = node.get(\"path\").asText().removePrefix(\"/\")\n    val originValue = originalNode.at(\"/$path\").asText()\n    val newValue = newNode.at(\"/$path\").asText()\n    return DiffTriple(path, originValue, newValue)\n}\n\n\npath 추출: diff node에서 변경된 필드의 경로를 추출합니다 (예: /customer/contact/address/street)\n슬래시 제거: 경로 앞의 /를 제거하여 깔끔한 key로 만듭니다\n값 추출: JsonNode의 at() 메서드로 해당 경로의 값을 추출합니다\nTriple 반환: (경로, 이전값, 새값)을 하나의 Triple로 반환합니다\n\n3.4 calculateDifferences 함수 (복수 객체 처리)#\n여러 객체를 비교할 때 사용하는 함수입니다.\n\n\n1\n2\n3\n4\n5\n6\n\nfun <T, K, S> calculateDifferences(\n    originItems: List<T>,\n    newItems: List<T>,\n    associateByKey: (T) -> K,      // 매칭용 키 (예: ID)\n    groupByKey: (T) -> S            // 그룹화용 키\n): Map<S, DiffValueTracker>\n\n\nassociateByKey: 원본과 새로운 데이터를 매칭하기 위한 키 (예: orderId, productId)\ngroupByKey: 결과를 그룹화하기 위한 키\n여러 객체를 한 번에 처리하고 각 객체별 변경사항을 그룹화하여 반환합니다\n\n테스트 코드로 검증하기#\n다양한 케이스를 테스트하여 구현이 올바르게 동작하는지 확인했습니다.\n1. 단일 필드 변경 감지#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n@Test\nfun `calculateDifference - 단일 객체의 변경 사항을 감지한다`() {\n    // Given\n    val originalProduct = Product(\n        productId = \"PROD001\",\n        productName = \"노트북\",\n        category = Category(\"전자제품\", \"컴퓨터\")\n    )\n    val newProduct = Product(\n        productId = \"PROD001\",\n        productName = \"울트라 노트북\",\n        category = Category(\"전자제품\", \"컴퓨터\")\n    )\n\n    // When\n    val result = DiffComparisonManager.calculateDifference(originalProduct, newProduct)\n\n    // Then\n    then(result).hasSize(1)\n    then(result[\"product_name\"]).isNotNull\n    then(result[\"product_name\"]?.origin).isEqualTo(\"노트북\")\n    then(result[\"product_name\"]?.new).isEqualTo(\"울트라 노트북\")\n}\n\n\n결과:\n\n\n1\n2\n3\n4\n5\n6\n\n{\n  \"product_name\": {\n    \"origin\": \"노트북\",\n    \"new\": \"울트라 노트북\"\n  }\n}\n\n\n2. 중첩된 객체의 변경 감지#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n@Test\nfun `calculateDifference - 중첩된 객체의 변경 사항을 감지한다`() {\n    // Given\n    val originalProduct = Product(\n        productId = \"PROD001\",\n        productName = \"노트북\",\n        category = Category(\"전자제품\", \"컴퓨터\")\n    )\n    val newProduct = Product(\n        productId = \"PROD001\",\n        productName = \"노트북\",\n        category = Category(\"전자제품\", \"노트북\")\n    )\n\n    // When\n    val result = DiffComparisonManager.calculateDifference(originalProduct, newProduct)\n\n    // Then\n    then(result).hasSize(1)\n    then(result[\"category/sub_category\"]).isNotNull\n    then(result[\"category/sub_category\"]?.origin).isEqualTo(\"컴퓨터\")\n    then(result[\"category/sub_category\"]?.new).isEqualTo(\"노트북\")\n}\n\n\n결과:\n\n\n1\n2\n3\n4\n5\n6\n\n{\n  \"category/sub_category\": {\n    \"origin\": \"컴퓨터\",\n    \"new\": \"노트북\"\n  }\n}\n\n\n중첩 객체의 필드는 category/sub_category 형태로 경로가 명확히 표시됩니다.\n슬래시(/)를 구분자로 사용하여 객체의 계층 구조를 표현하므로, 어떤 깊이의 중첩 객체라도 경로만으로 정확한 위치를 파악할 수 있습니다.\n3. 여러 필드 동시 변경 감지#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n@Test\nfun `calculateDifference - 여러 필드의 변경 사항을 감지한다`() {\n    // Given\n    val originalItem = Item(\n        product = Product(\"PROD001\", \"노트북\", Category(\"전자제품\", \"컴퓨터\")),\n        quantity = 2,\n        price = 1500000\n    )\n    val newItem = Item(\n        product = Product(\"PROD001\", \"울트라 노트북\", Category(\"전자제품\", \"컴퓨터\")),\n        quantity = 3,\n        price = 1400000\n    )\n\n    // When\n    val result = DiffComparisonManager.calculateDifference(originalItem, newItem)\n\n    // Then\n    then(result).hasSize(3)\n    then(result[\"product/product_name\"]?.origin).isEqualTo(\"노트북\")\n    then(result[\"product/product_name\"]?.new).isEqualTo(\"울트라 노트북\")\n    then(result[\"quantity\"]?.origin).isEqualTo(\"2\")\n    then(result[\"quantity\"]?.new).isEqualTo(\"3\")\n    then(result[\"price\"]?.origin).isEqualTo(\"1500000\")\n    then(result[\"price\"]?.new).isEqualTo(\"1400000\")\n}\n\n\n결과:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n{\n  \"product/product_name\": {\n    \"origin\": \"노트북\",\n    \"new\": \"울트라 노트북\"\n  },\n  \"quantity\": {\n    \"origin\": \"2\",\n    \"new\": \"3\"\n  },\n  \"price\": {\n    \"origin\": \"1500000\",\n    \"new\": \"1400000\"\n  }\n}\n\n\n한 번의 비교로 여러 필드의 변경사항을 모두 추적할 수 있으며, 각 필드별로 이전 값과 새로운 값이 명확하게 구분됩니다.\n4. 깊은 중첩 구조 변경 감지#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n@Test\nfun `calculateDifference - 깊게 중첩된 객체의 변경 사항을 감지한다`() {\n    // Given\n    val originalOrder = Order(\n        orderId = \"ORD123\",\n        customer = Customer(\n            customerId = \"CUST001\",\n            name = \"홍길동\",\n            contact = Contact(\n                email = \"hong@example.com\",\n                phone = \"010-1234-5678\",\n                address = Address(\"서울특별시 종로구\", \"서울\", \"03001\", \"대한민국\")\n            )\n        ),\n        items = emptyList(),\n        payment = Payment(\"신용카드\", \"TXN001\", \"완료\")\n    )\n    val newOrder = Order(\n        orderId = \"ORD123\",\n        customer = Customer(\n            customerId = \"CUST001\",\n            name = \"홍길동\",\n            contact = Contact(\n                email = \"hong@example.com\",\n                phone = \"010-1234-5678\",\n                address = Address(\"서울특별시 강남구\", \"서울\", \"06001\", \"대한민국\")\n            )\n        ),\n        items = emptyList(),\n        payment = Payment(\"신용카드\", \"TXN001\", \"완료\")\n    )\n\n    // When\n    val result = DiffComparisonManager.calculateDifference(originalOrder, newOrder)\n\n    // Then\n    then(result).hasSize(2)\n    then(result[\"customer/contact/address/street\"]?.origin).isEqualTo(\"서울특별시 종로구\")\n    then(result[\"customer/contact/address/street\"]?.new).isEqualTo(\"서울특별시 강남구\")\n    then(result[\"customer/contact/address/zip_code\"]?.origin).isEqualTo(\"03001\")\n    then(result[\"customer/contact/address/zip_code\"]?.new).isEqualTo(\"06001\")\n}\n\n\n결과:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n{\n  \"customer/contact/address/street\": {\n    \"origin\": \"서울특별시 종로구\",\n    \"new\": \"서울특별시 강남구\"\n  },\n  \"customer/contact/address/zip_code\": {\n    \"origin\": \"03001\",\n    \"new\": \"06001\"\n  }\n}\n\n\n4단계 깊이의 중첩 구조(customer/contact/address/street)도 정확히 추적합니다.\n5. Null 처리#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n@Test\nfun `calculateDifference - null에서 값으로 변경을 감지한다`() {\n    // Given\n    data class TestData(val name: String, val description: String?)\n    val original = TestData(\"테스트\", null)\n    val new = TestData(\"테스트\", \"설명 추가\")\n\n    // When\n    val result = DiffComparisonManager.calculateDifference(original, new)\n\n    // Then\n    then(result).hasSize(1)\n    then(result[\"description\"]?.origin).isEmpty()\n    then(result[\"description\"]?.new).isEqualTo(\"설명 추가\")\n}\n\n\n결과:\n\n\n1\n2\n3\n4\n5\n6\n\n{\n  \"description\": {\n    \"origin\": \"\",\n    \"new\": \"설명 추가\"\n  }\n}\n\n\n5-2. 값에서 null로 변경#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n@Test\nfun `calculateDifference - 값에서 null로 변경을 감지한다`() {\n    // Given\n    data class TestData(val name: String, val description: String?)\n    val original = TestData(\"테스트\", \"기존 설명\")\n    val new = TestData(\"테스트\", null)\n\n    // When\n    val result = DiffComparisonManager.calculateDifference(original, new)\n\n    // Then\n    then(result).hasSize(1)\n    then(result[\"description\"]?.origin).isEqualTo(\"기존 설명\")\n    then(result[\"description\"]?.new).isEmpty()\n}\n\n\n결과:\n\n\n1\n2\n3\n4\n5\n6\n\n{\n  \"description\": {\n    \"origin\": \"기존 설명\",\n    \"new\": \"\"\n  }\n}\n\n\nnull 값의 변경도 정확하게 추적되며, null은 빈 문자열로 표시됩니다.\n6. 변경 없는 경우#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n@Test\nfun `calculateDifference - 동일한 객체는 변경 사항이 없다`() {\n    // Given\n    val product = Product(\n        productId = \"PROD001\",\n        productName = \"노트북\",\n        category = Category(\"전자제품\", \"컴퓨터\")\n    )\n\n    // When\n    val result = DiffComparisonManager.calculateDifference(product, product)\n\n    // Then\n    then(result).isEmpty()\n}\n\n\n결과:\n\n\n1\n\n{}\n\n\n동일한 객체를 비교하면 빈 Map이 반환되어, 불필요한 변경 이력이 저장되지 않습니다.\n7. 실제 주문 데이터 변경 추적#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n@Test\nfun `주문 데이터의 필드 변경을 확인한다`() {\n    // Given\n    val originalOrder: Order = diffMapper.readValue(readFile(\"/diff-origin.json\"))\n    val newOrder: Order = diffMapper.readValue(readFile(\"/diff-new.json\"))\n\n    // When\n    val result = DiffComparisonManager.calculateDifferences(\n        originItems = listOf(originalOrder),\n        newItems = listOf(newOrder),\n        associateByKey = { it.orderId },\n        groupByKey = { it.orderId }\n    )\n\n    // Then\n    val differences = result[\"ORD123456\"]\n    then(differences).isNotNull\n    then(differences!!.size).isEqualTo(3)\n    then(differences[\"customer/contact/address/street\"]?.origin).isEqualTo(\"서울특별시 종로구\")\n    then(differences[\"customer/contact/address/street\"]?.new).isEqualTo(\"서울특별시 강남구\")\n    then(differences[\"items/0/price\"]?.origin).isEqualTo(\"1500000\")\n    then(differences[\"items/0/price\"]?.new).isEqualTo(\"1400000\")\n    then(differences[\"payment/transaction_id\"]?.origin).isEqualTo(\"TXN987654321\")\n    then(differences[\"payment/transaction_id\"]?.new).isEqualTo(\"TXN987654322\")\n}\n\n\n결과:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n{\n  \"ORD123456\": {\n    \"customer/contact/address/street\": {\n      \"origin\": \"서울특별시 종로구\",\n      \"new\": \"서울특별시 강남구\"\n    },\n    \"items/0/price\": {\n      \"origin\": \"1500000\",\n      \"new\": \"1400000\"\n    },\n    \"payment/transaction_id\": {\n      \"origin\": \"TXN987654321\",\n      \"new\": \"TXN987654322\"\n    }\n  }\n}\n\n\n실제 JSON 파일에서 읽어온 복잡한 주문 데이터도 정확하게 변경사항을 추적합니다. calculateDifferences 함수는 여러 객체를 처리하고 그룹화된 결과를 반환합니다.\n활용 방안#\n1. 승인 프로세스에 활용#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\ndata class ApprovalRequest(\n    @Id\n    val id: ObjectId = ObjectId(),\n    val requestType: String,\n    val targetId: String,\n    val changes: DiffValueTracker,       // 어떤 필드가 어떻게 변경될지\n    val requestedBy: String,\n    val status: ApprovalStatus = ApprovalStatus.PENDING\n)\n\n// 승인 요청 생성\nfun createFeeChangeApproval(merchantId: String, currentFee: MerchantFee, newFee: MerchantFee, userId: String) {\n    val changes = DiffComparisonManager.calculateDifference(currentFee, newFee)\n    \n    val approvalRequest = ApprovalRequest(\n        requestType = \"MERCHANT_FEE_CHANGE\",\n        targetId = merchantId,\n        changes = changes,\n        requestedBy = userId\n    )\n    approvalRequestRepository.save(approvalRequest)\n    \n    // 승인권자에게 알림 전송\n    notifyApprovers(approvalRequest)\n}\n\n\n2. 감사 로그 및 모니터링#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n// 중요 필드 변경 모니터링\nfun monitorCriticalChanges(changes: DiffValueTracker, entityType: String) {\n    val criticalFields = setOf(\n        \"payment/method\",\n        \"customer/contact/address/street\",\n        \"items/0/price\"\n    )\n    \n    changes.keys.filter { it in criticalFields }\n        .forEach { field ->\n            val change = changes[field]!!\n            logger.warn(\n                \"Critical field changed in $entityType: $field - \" +\n                \"from '${change.origin}' to '${change.new}'\"\n            )\n            // 알림 전송, 메트릭 기록 등\n        }\n}\n\n\n장점과 고려사항#\n장점#\n\n자동화: 수동으로 변경 필드를 비교할 필요 없이 자동으로 추적합니다\n타입 안정성: Kotlin의 제네릭을 활용하여 타입 안전하게 구현됩니다\n중첩 객체 지원: 깊은 중첩 구조도 경로로 명확히 표시합니다\n저장소 독립성: 특정 데이터베이스에 의존하지 않는 순수한 로직으로 구현되어, MongoDB, PostgreSQL, MySQL 등 어떤 저장소에도 저장할 수 있습니다\n가독성: 변경 내역이 명확한 key-value 형태로 저장됩니다\n\n고려사항#\n\n성능: 큰 객체나 대량의 데이터를 비교할 때는 성능을 고려해야 합니다\n배열 처리: 배열의 순서가 바뀌면 전체가 변경된 것으로 인식될 수 있습니다\n저장 공간: 모든 변경 이력을 저장하면 데이터가 빠르게 증가할 수 있습니다\n민감 정보: 비밀번호 등 민감한 정보는 이력에서 제외하는 로직이 필요합니다\n\n마치며#\n이번 포스트에서는 Kotlin과 Jackson, zjsonpatch를 활용하여 필드 단위 변경 이력 추적 시스템을 구현하는 방법을 알아보았습니다.\n복잡한 중첩 객체의 변경사항을 자동으로 추적하고 명확한 경로로 표시하는 이 시스템은 다음과 같은 상황에서 유용하게 활용할 수 있습니다:\n\n주문/결제 정보 변경 이력 추적\n가맹점 정보 변경 승인 프로세스\n감사(Audit) 로그 시스템\n데이터 동기화 및 충돌 감지\n\n실제 프로덕션 환경에 적용할 때는 성능과 저장 공간, 민감 정보 처리 등을 충분히 고려하여 상황에 맞게 커스터마이징하시기 바랍니다.",
        "comments": "https://cheese10yun.github.io/diff-history-part-1/#disqus_thread",
        "content": "객체의 필드 변경 이력을 자동으로 추적해, 무엇이 어떻게 바뀌었는지 명확히 기록합니다.",
        "contentSnippet": "객체의 필드 변경 이력을 자동으로 추적해, 무엇이 어떻게 바뀌었는지 명확히 기록합니다.",
        "guid": "https://cheese10yun.github.io/diff-history-part-1/",
        "categories": [
          {
            "_": "Kotlin",
            "$": {
              "domain": "https://cheese10yun.github.io/tags/Kotlin/"
            }
          },
          {
            "_": "Guide",
            "$": {
              "domain": "https://cheese10yun.github.io/tags/Guide/"
            }
          }
        ],
        "isoDate": "2026-02-15T15:00:00.000Z"
      }
    ]
  },
  {
    "name": "구자철",
    "category": "개인",
    "posts": []
  },
  {
    "name": "FSS",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권동준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김용일",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김도균",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민석",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권윤학",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강성훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김만수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "엘키",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김슬기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김광현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성빈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강명훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민장",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성중",
    "category": "개인",
    "posts": []
  },
  {
    "name": "구교준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김덕기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "고명환",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강성희",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강병수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김봉현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강형석",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수로",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강미경",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강진우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권민재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권태관",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김도곤",
    "category": "개인",
    "posts": []
  },
  {
    "name": "칡토스의 게임 개발",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김선철",
    "category": "개인",
    "posts": []
  },
  {
    "name": "프리웨어 이야기",
    "category": "개인",
    "posts": [
      {
        "creator": "어떤오후의 프리웨어 이야기",
        "title": "프롬프트의 시작과 끝, 이 3가지 앱으로 종결",
        "link": "https://muzbox.tistory.com/483709",
        "pubDate": "Sun, 15 Feb 2026 17:04:15 +0900",
        "author": "어떤오후의 프리웨어 이야기",
        "comments": "https://muzbox.tistory.com/483709#entry483709comment",
        "content": "<div style=\"font-family: 'Noto Sans KR', sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; font-size: 16px; box-sizing: border-box; color: #3c4043;\">\n<div style=\"background-color: #e8f4fd; padding: 15px; border-radius: 8px; font-style: italic; margin-bottom: 25px; font-size: 15px;\">AI와의 대화가 늘 엉뚱한 결과로 이어져 답답하셨나요? 혹은 완벽한 프롬프트를 만들고 싶지만 어디서부터 시작해야 할지 막막하셨다면, 이 글이 해답이 될 거예요. AI 활용의 새로운 기준을 제시할 세 가지 혁신적인 앱으로 프롬프트 작성의 모든 고민을 끝내고, 여러분도 진정한 AI 프롬프트 마스터가 되어 보세요!</div>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">여러분, 혹시 AI에게 질문했는데 영 엉뚱한 답변이 나와서 황당했던 경험, 저만 있는 건 아니겠죠? 아니면, 정말 완벽한 결과물을 기대하면서 프롬프트를 만들고 싶은데, 대체 어디서부터 손을 대야 할지 감조차 안 잡히셨던 적도 분명 있으실 거예요. 오늘은 이런 고민을 시원하게 해결해 줄 마법 같은 세 가지 도구를 소개해 드리려고 해요. 이 앱들만 제대로 활용하신다면, 프롬프트 전문가의 영역이 더 이상 꿈이 아니랍니다!</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"download.jpg\" data-origin-width=\"1200\" data-origin-height=\"1200\"><span data-url=\"https://blog.kakaocdn.net/dn/bKUejs/dJMcafle932/zm7oXgOk4nRfsKT5Gjvw10/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/bKUejs/dJMcafle932/zm7oXgOk4nRfsKT5Gjvw10/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/bKUejs/dJMcafle932/zm7oXgOk4nRfsKT5Gjvw10/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbKUejs%2FdJMcafle932%2Fzm7oXgOk4nRfsKT5Gjvw10%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"홀로그램 인터페이스로 AI 프롬프트 요소를 조작하는 손. AI Prompt Studio, Prompt Equalizer Pro, 프롬프트 마스터와 같은 혁신적인 도구로 프롬프트 엔지니어링을 간소화하는 미래 지향적인 컨셉을 시각적으로 표현.\" loading=\"lazy\" width=\"1200\" height=\"1200\" data-filename=\"download.jpg\" data-origin-width=\"1200\" data-origin-height=\"1200\"/></span></figure>\n\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">제가 직접 경험해 보니, AI는 정말 질문을 어떻게 하느냐에 따라 답변의 질이 하늘과 땅 차이로 달라져요. 단순히 \"블로그 글 써줘\"라고 짧게 물으면, 어디서 많이 본 듯한 평범한 결과물만 툭 튀어나오기 일쑤죠. 그런데 만약 \"당신은 10년 경력의 여행 작가입니다. 20대 여성을 위한 제주도 3박 4일 여행기를 감성적인 톤으로 작성해주세요\"라고 디테일하게 요청한다면? 와, 이건 정말 완전히 다른 수준의, 심지어 감동적인 결과물이 나올 때도 있더라고요.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">하지만 문제는 매번 이렇게 완벽하고 정교한 프롬프트를 직접 만드는 게 너무 어렵고 귀찮다는 점이에요. 특히 \"AI한테 어떤 역할을 부여해야 하지?\", \"어떤 조건을 추가해야 가장 좋은 결과가 나올까?\" 같은 고민을 하다 보면 시간이 정말이지 눈 깜짝할 사이에 흘러가 버리죠. 그래서 오늘 제가 소개해 드릴 이 세 가지 앱이 필요한 겁니다. 각각 다른 역할로 여러분의 프롬프트 작성 전 과정을 빈틈없이 도와줄 거예요. 마치 요리할 때 레시피를 짜주는 셰프, 맛을 미세하게 조절하는 조미료, 그리고 나만의 특별한 레시피를 정리해 주는 요리책 같다고나 할까요?</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>1. AI Prompt Studio: 메타 프롬프트 설계의 마법사  &zwj;♀️</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">여러분, 혹시 ChatGPT나 제미나이를 쓸 때 \"제주도 여행 블로그 글 하나 써줘\"라고 짧게 물어보고 나서 결과가 너무 뻔해서 실망하신 적 있으실 거예요. 사실 이건 AI의 성능 문제라기보다는, 질문하는 방식에 아쉬움이 있었을 가능성이 큽니다. AI는 질문을 어떻게 하느냐에 따라 답변의 품질이 정말이지 하늘과 땅 차이로 달라지거든요.</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"AI PROMPT STUDIO.jpg\" data-origin-width=\"940\" data-origin-height=\"843\"><span data-url=\"https://blog.kakaocdn.net/dn/TbB2b/dJMcachLbTM/hPG88iOLiC02aTJ31Dtxw1/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/TbB2b/dJMcachLbTM/hPG88iOLiC02aTJ31Dtxw1/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/TbB2b/dJMcachLbTM/hPG88iOLiC02aTJ31Dtxw1/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FTbB2b%2FdJMcachLbTM%2FhPG88iOLiC02aTJ31Dtxw1%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"AI Prompt Studio\" loading=\"lazy\" width=\"940\" height=\"843\" data-filename=\"AI PROMPT STUDIO.jpg\" data-origin-width=\"940\" data-origin-height=\"843\"/></span></figure>\n\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">하지만 문제는 매번 \"너는 10년 차 작가야, 타겟은 누구고, 형식은 마크다운으로 해줘\" 같은 복잡한 프롬프트를 직접 짜기가 너무 어렵고 번거롭다는 거죠. 바로 이 번거로움을 해결해 주는 도구가 첫 번째로 소개해 드릴 <b>'AI Prompt Studio'</b>입니다. 이 앱은 단순한 질문이 아닌, LLM(거대 언어 모델)의 성능을 100% 끌어낼 수 있는 '메타 프롬프트(Meta Prompt)'를 전문적으로 설계해주는 앱이에요.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; border-bottom: 2px solid #e8f0fe; padding-bottom: 5px;\" data-ke-size=\"size23\">주요 기능  </h3>\n<ul style=\"list-style-type: disc; margin-left: 20px; margin-bottom: 20px; color: #3c4043;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 10px;\"><b>빠른 프롬프트 생성:</b> 복잡한 설정 없이 목표만 입력하면 버튼 하나로 강력한 프롬프트를 즉시 만들어줍니다. 바쁠 때 정말 최고죠.</li>\n<li style=\"margin-bottom: 10px;\"><b>역질문 유도:</b> 사용자의 요청이 모호할 때, AI가 스스로 부족한 정보를 파악하여 \"더 나은 답변을 위해 몇 가지 질문을 해도 될까요?\" 하고 되묻도록 유도합니다. 덕분에 불필요한 시행착오를 줄일 수 있어요.</li>\n<li style=\"margin-bottom: 10px;\"><b>페르소나 부여:</b> 목표 달성에 가장 적합한 전문가 역할을 AI가 스스로 설정하고 연기하도록 지시합니다. 예를 들어, \"최고의 마케터\" 역할을 부여해서 결과물의 전문성을 확 끌어올릴 수 있어요.</li>\n<li style=\"margin-bottom: 10px;\"><b>비판적 개선:</b> 입력한 내용을 비판적으로 분석해서 논리적 허점이나 모호한 부분을 찾아내고, 더 완벽한 구조로 즉시 수정해 줍니다. 스스로 고쳐나가면서 프롬프트 실력이 확 늘 거예요.</li>\n</ul>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; border-bottom: 2px solid #e8f0fe; padding-bottom: 5px;\" data-ke-size=\"size23\">다섯 가지 전문 모드  ️</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">이 앱의 가장 큰 특징이자 핵심은 바로 목적에 따라 골라 쓸 수 있는 다섯 가지 전문 모드를 지원한다는 점이에요. 여러분의 상황에 맞춰 선택하기만 하면 됩니다.</p>\n<ul style=\"list-style-type: disc; margin-left: 20px; margin-bottom: 20px; color: #3c4043;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 10px;\"><b>RTSC 마스터 모드:</b> 역할(Role), 작업(Task), 구조(Structure), 체인(Chain)의 앞 글자를 딴 모드인데요. 예를 들어 \"유튜브 쇼츠 대본 생성\"이라고 목표만 입력해 보세요. AI가 알아서 \"숏폼 바이럴 마케팅 디렉터\"라는 역할을 부여하고, 쇼츠 성공 공식을 완벽하게 구조화한 프롬프트를 짜줍니다. 정교하고 복잡한 창작물을 만들 때 정말 최고입니다.</li>\n<li style=\"margin-bottom: 10px;\"><b>비즈니스 프레임워크 모드:</b> 직장인분들이라면 이 모드를 강력 추천합니다! 기획안이나 보고서 쓸 때 막막하셨던 경험 다들 있으시죠? 코스타(COSTAR)나 프렙(PREP) 같은 검증된 비즈니스 틀을 자동으로 적용해서 아주 깔끔하고 논리적인 업무용 문서를 만들어 줘요. 정말이지 퇴근 시간을 앞당겨 주는 효자 앱이라고 할 수 있죠.</li>\n<li style=\"margin-bottom: 10px;\"><b>프롬프트 아키텍트:</b> 정말 귀찮을 때, 이 모드가 마법을 부려줍니다. \"복잡한 법률 문서를 쉽게 요약해 주는 AI 에이전트를 설계해 줘\"라고 딱 한 줄만 써보세요. 그럼 AI가 필요한 제약 조건과 작업 프로세스를 알아서 추론하고 설계해 냅니다. 마치 옆에 유능한 비서가 있는 것 같아요.</li>\n<li style=\"margin-bottom: 10px;\"><b>버텍스 AI XML 모드:</b> 기업용 데이터 처리나 구조화된 데이터가 필요할 때 빛을 발하는 모드예요. 결과물을 깔끔한 XML 태그 구조로 만들어주기 때문에 복잡한 데이터를 분류하거나 추출할 때 아주 정확하고 표준화된 답변을 얻을 수 있습니다. 데이터 분석가나 개발자분들께 특히 유용할 거예요.</li>\n<li style=\"margin-bottom: 10px;\"><b>CoT(Chain-of-Thought) 모드:</b> 논리적인 사고가 필요할 때 활용해 보세요. AI가 사람처럼 단계별로 추론하며 문제를 풀도록 유도하는 방식인데요. 덕분에 수학 문제나 복잡한 논리 과제에서 AI가 엉뚱한 답을 내놓는 현상을 획기적으로 줄여줍니다. 이제는 그저 \"무엇을 할지\"만 결정하시면 됩니다!</li>\n</ul>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">솔직히 말하면, 이 앱은 거의 '사기 앱'이라고 불러도 될 정도예요. 왜냐하면 프롬프트 엔지니어들이 밤새워 연구하고 강의에서 수십만 원을 주고 배워야 하는 복잡한 기법들을, 버튼 클릭 몇 번만으로 누구나 완벽하게 구현할 수 있게 만들어줬거든요. 저도 처음에 써보고 정말 깜짝 놀랐습니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">많은 분들이 AI를 쓰면서 \"AI가 별로야\"라고 아쉬워하시는데, 제 경험상 사실은 질문을 제대로 못한 경우가 대부분입니다. 그런데 이 앱을 쓰면 그런 문제가 완전히 해결돼요. 시작하는 방법도 아주 간단합니다. 브라우저에서 접속한 뒤 우측 상단 톱니바퀴 아이콘을 클릭해서, 구글 AI 스튜디오에서 무료로 발급받은 '제미나이 API 키'만 넣어주면 준비 끝입니다. 자, 이제 '시작하기' 버튼을 누르고 여러분만의 완벽한 프롬프트를 직접 설계해 보시기 바랍니다!</p>\n<figure id=\"og_1771142603506\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"Meta Prompt Master\" data-og-description=\"\" data-og-host=\"ai-prompt-studio-v2-basic.vercel.app\" data-og-source-url=\"https://ai-prompt-studio-v2-basic.vercel.app/\" data-og-url=\"https://ai-prompt-studio-v2-basic.vercel.app/\" data-og-image=\"\"><a href=\"https://ai-prompt-studio-v2-basic.vercel.app/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://ai-prompt-studio-v2-basic.vercel.app/\">\n<div class=\"og-image\" style=\"background-image: url();\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">Meta Prompt Master</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">&nbsp;</p>\n<p class=\"og-host\" data-ke-size=\"size16\">ai-prompt-studio-v2-basic.vercel.app</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<div style=\"background-color: #e8f4fd; border-left: 4px solid #1a73e8; padding: 15px; margin: 20px 0; border-radius: 0 8px 8px 0;\">  <b>잠깐! API 키 설정 팁:</b> AI Prompt Studio를 시작하기 전에, 구글 AI 스튜디오(<a style=\"color: #1a73e8; text-decoration: none;\" href=\"https://aistudio.google.com/\" target=\"_blank\" rel=\"noopener\">aistudio.google.com</a>)에서 무료로 제미나이 API 키를 발급받으세요. 이 키만 있으면 AI Prompt Studio의 모든 기능을 막힘없이 활용할 수 있습니다. 설정 과정도 매우 간단하니 걱정 마세요!</div>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>2. Prompt Equalizer Pro: 글의 뉘앙스를 조율하는 이퀄라이저  </b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">자, 다음으로 소개해 드릴 앱은 정말이지 흥미로운 도구입니다. 바로 <b>'Prompt Equalizer Pro'</b>인데요. 여러분, 혹시 음악 들으실 때 취향에 맞춰 저음이나 고음을 조절하는 '이퀄라이저' 써보셨나요? 이 앱은 그 원리를 글쓰기에 그대로 가져왔습니다. 앞서 보신 'AI Prompt Studio'가 프롬프트를 '만드는' 도구였다면, 이 앱은 이미 써놓은 글의 뉘앙스를 아주 정밀하게 튜닝하고 조정하는 도구라고 보시면 됩니다. 개인적으로 써보고 정말 감탄했던 앱이에요.</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Prompt Equalizer Pro.jpg\" data-origin-width=\"1898\" data-origin-height=\"914\"><span data-url=\"https://blog.kakaocdn.net/dn/lrWkf/dJMcaaYwUxV/bhHS6KK87Aql61gBR0SIBk/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/lrWkf/dJMcaaYwUxV/bhHS6KK87Aql61gBR0SIBk/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/lrWkf/dJMcaaYwUxV/bhHS6KK87Aql61gBR0SIBk/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FlrWkf%2FdJMcaaYwUxV%2FbhHS6KK87Aql61gBR0SIBk%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"Prompt Equalizer Pro\" loading=\"lazy\" width=\"1898\" height=\"914\" data-filename=\"Prompt Equalizer Pro.jpg\" data-origin-width=\"1898\" data-origin-height=\"914\"/></span></figure>\n\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; border-bottom: 2px solid #e8f0fe; padding-bottom: 5px;\" data-ke-size=\"size23\">핵심 기능: 5가지 슬라이더 ✨</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">앱 화면을 보시면 오른쪽에 다섯 개의 슬라이더가 보일 거예요. 이게 이 앱의 핵심인데요, 마치 음향 이퀄라이저처럼 글의 다양한 요소를 미세하게 조절할 수 있습니다.</p>\n<ul style=\"list-style-type: disc; margin-left: 20px; margin-bottom: 20px; color: #3c4043;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 10px;\"><b>창의성:</b> 슬라이더를 오른쪽으로 쭉 밀면, 평범했던 문장이 아주 은유적이고 풍성하게 변합니다. 반대로 왼쪽으로 낮추면 군더더기 없이 아주 직설적인 문장이 되죠. 소설이나 시를 쓸 때 정말 유용해요.</li>\n<li style=\"margin-bottom: 10px;\"><b>명확성:</b> 글의 이해도를 높이거나 낮출 수 있습니다. 복잡한 내용을 쉽게 풀어 설명할 때 아주 좋고, 때로는 일부러 난해하게 표현할 수도 있습니다.</li>\n<li style=\"margin-bottom: 10px;\"><b>감정의 깊이:</b> 글에 담긴 감정의 농도를 조절합니다. 따뜻하고 감성적인 글부터 차갑고 이성적인 글까지, 원하는 감정 톤을 만들 수 있어요.</li>\n<li style=\"margin-bottom: 10px;\"><b>문장의 길이:</b> 길고 유려한 문장과 짧고 간결한 문장 사이를 오갈 수 있습니다. 저는 주로 짧은 문장으로 속도감을 주고 싶을 때 사용하곤 해요.</li>\n<li style=\"margin-bottom: 10px;\"><b>전문성:</b> 이 기능이 정말 압권인데요! 수치를 높이면 아주 격식 있는 공문서 느낌부터, 낮추면 친구에게 말하는 듯한 편안한 말투까지 자유자재로 오갈 수 있습니다. 독자층에 맞춰 말투를 순식간에 바꾸는 게 가능해요.</li>\n</ul>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">슬라이더 몇 번 움직이는 것만으로 글의 온도와 질감이 완전히 달라지는 게 정말 신기하지 않나요? 처음에 써봤을 때 솔직히 '이게 된다고?' 싶었어요.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; border-bottom: 2px solid #e8f0fe; padding-bottom: 5px;\" data-ke-size=\"size23\">AI 페르소나 기능  </h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">더 놀라운 건 <b>'AI 페르소나'</b> 기능입니다. 앱에서 미리 제공하는 10가지 캐릭터 외에도 여러분이 직접 '냉철한 탐정'이나 '비즈니스 전문가'라고만 입력하면, AI가 그 캐릭터에 딱 맞는 말투와 성격, 그리고 최적의 이퀄라이저 값까지 알아서 세팅해 줍니다. 정말 생각지도 못한 부분에서 감탄하게 되는 기능이었어요.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">사용법도 아주 간단합니다. 처음 설정에서 API 키만 넣고 저장 폴더를 지정해 주세요. 그다음 원하는 말투의 페르소나를 고르거나 직접 만든 페르소나를 선택한 뒤, 슬라이더를 슥슥 조절하고, 원본 글을 넣은 뒤 <b>'명작 생성'</b> 버튼만 누르면 끝입니다. 마음에 들면 바로 파일로 저장하시면 되고요.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">소설을 쓰실 때 캐릭터마다 말투를 다르게 고정하거나, 딱딱한 보고서를 부드럽게 다듬을 때 이만한 도구가 없습니다. 단순히 수치를 높이기보다, 이것저것 만져보면서 여러분만의 '황금 조합'을 한 번 찾아보세요. 저도 저만의 조합을 찾는 재미에 푹 빠져버렸답니다!</p>\n<figure id=\"og_1771142618154\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"Prompt Equalizer Pro\" data-og-description=\"\" data-og-host=\"prompt-equalizer.vercel.app\" data-og-source-url=\"https://prompt-equalizer.vercel.app/\" data-og-url=\"https://prompt-equalizer.vercel.app/\" data-og-image=\"\"><a href=\"https://prompt-equalizer.vercel.app/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://prompt-equalizer.vercel.app/\">\n<div class=\"og-image\" style=\"background-image: url();\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">Prompt Equalizer Pro</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">&nbsp;</p>\n<p class=\"og-host\" data-ke-size=\"size16\">prompt-equalizer.vercel.app</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>3. 프롬프트 마스터: 당신의 프롬프트 라이브러리  </b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">혹시 매번 똑같은 프롬프트를 메모장에 복사해서 쓰거나, 예전에 써둔 정말 좋은 프롬프트를 못 찾아서 헤맨 적 없으신가요? 첫 번째 앱이 프롬프트를 '만들고', 두 번째 앱이 뉘앙스를 '바꿨다면', <b>'프롬프트 마스터'</b>는 자주 쓰는 프롬프트를 완벽하게 '관리하고 재사용하는' 도구입니다. 이 앱 하나면 반복되는 질문을 일일이 타이핑할 필요가 완전히 사라져요. 정말 솔깃하지 않나요?</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"Prompt Master.png\" data-origin-width=\"1808\" data-origin-height=\"911\"><span data-url=\"https://blog.kakaocdn.net/dn/U4A5a/dJMcaiIZ9LW/xcNUkOqeEztcaPnUoaTgmk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/U4A5a/dJMcaiIZ9LW/xcNUkOqeEztcaPnUoaTgmk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/U4A5a/dJMcaiIZ9LW/xcNUkOqeEztcaPnUoaTgmk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FU4A5a%2FdJMcaiIZ9LW%2FxcNUkOqeEztcaPnUoaTgmk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"프롬프트 마스터\" loading=\"lazy\" width=\"1808\" height=\"911\" data-filename=\"Prompt Master.png\" data-origin-width=\"1808\" data-origin-height=\"911\"/></span></figure>\n\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">개인적으로 저는 이 앱이 작업 효율성을 몇 배로 끌어올려 줬다고 생각해요. 예를 들어, 콘텐츠 제작자라면 유튜브 스크립트 틀을 템플릿으로 저장해두고 주제만 바꿔서 빠르게 기획할 수 있고요. 개발자라면 복잡한 코드 리뷰 프롬프트를 템플릿화해서 언어와 코드만 바꿔 끼우면 끝입니다. 비즈니스 이메일이나 마케팅 카피도 타겟과 키워드만 툭툭 던지면 전문적인 결과물이 쏟아져 나오는 경험을 하실 수 있을 거예요.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; border-bottom: 2px solid #e8f0fe; padding-bottom: 5px;\" data-ke-size=\"size23\">강력한 기능  </h3>\n<ul style=\"list-style-type: disc; margin-left: 20px; margin-bottom: 20px; color: #3c4043;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 10px;\"><b>템플릿 생성 및 AI 제안:</b> 첫 화면에 있는 미리 설정된 10개의 프롬프트 템플릿을 바로 사용할 수도 있지만, 여러분의 취향에 맞는 새로운 템플릿을 직접 만들 수도 있어요. 여기에 'AI 제안' 버튼까지 활용하면 더 대박입니다. 제목만 써놓고 버튼을 누르면 AI가 알아서 적절한 설명과 본문 내용을 채워주거든요. 초안 짜는 시간조차 아껴주는 거죠.</li>\n<li style=\"margin-bottom: 10px;\"><b>변수 값 채우기:</b> 프롬프트를 실행하면 화면이 둘로 나뉩니다. 왼쪽에서 미리 정해둔 변수 값들을 채워 넣고 버튼을 누르면, 오른쪽에서 실시간으로 AI의 답변이 생성됩니다. 마치 코드를 실행하는 것 같은 느낌이에요.</li>\n<li style=\"margin-bottom: 10px;\"><b>강력한 검색 및 카테고리 관리:</b> 프롬프트가 많아져도 걱정 마세요. 강력한 검색 기능과 카테고리 관리 기능이 있어서 언제든 원하는 프롬프트를 1초 만에 찾아낼 수 있습니다. 저처럼 잃어버린 프롬프트 때문에 속상했던 경험이 있으시다면 이 기능에 감사하게 될 거예요.</li>\n<li style=\"margin-bottom: 10px;\"><b>라이브러리 백업:</b> 여러분의 모든 프롬프트를 한데 모아 관리할 수 있는 라이브러리 백업 기능까지 갖췄으니, 이제 프롬프트 마스터와 함께 반복 작업에서 완전히 해방되어 보세요!</li>\n</ul>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">사용법도 직관적입니다. 처음 실행할 때 제미나이 API 키와 저장 폴더만 지정해 주시면 준비 끝이에요. 이제 이 세 가지 앱을 활용하면 AI와의 소통이 훨씬 더 즐겁고 효율적으로 바뀔 거예요. 정말 추천합니다!</p>\n<figure id=\"og_1771142634911\" contenteditable=\"false\" data-ke-type=\"opengraph\" data-ke-align=\"alignCenter\" data-og-type=\"website\" data-og-title=\"프롬프트 마스터\" data-og-description=\"\" data-og-host=\"prompt-master-ten-flax.vercel.app\" data-og-source-url=\"https://prompt-master-ten-flax.vercel.app/\" data-og-url=\"https://prompt-master-ten-flax.vercel.app/\" data-og-image=\"\"><a href=\"https://prompt-master-ten-flax.vercel.app/\" target=\"_blank\" rel=\"noopener\" data-source-url=\"https://prompt-master-ten-flax.vercel.app/\">\n<div class=\"og-image\" style=\"background-image: url();\">&nbsp;</div>\n<div class=\"og-text\">\n<p class=\"og-title\" data-ke-size=\"size16\">프롬프트 마스터</p>\n<p class=\"og-desc\" data-ke-size=\"size16\">&nbsp;</p>\n<p class=\"og-host\" data-ke-size=\"size16\">prompt-master-ten-flax.vercel.app</p>\n</div>\n</a></figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<div style=\"background-color: #f8f9fa; border: 1px solid #dadce0; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); padding: 25px; margin: 40px 0;\">\n<div style=\"font-size: 26px; color: #1a73e8; font-weight: bold; margin-bottom: 15px; padding-bottom: 10px; border-bottom: 2px solid #1a73e8;\">  핵심 요약</div>\n<div style=\"font-size: 17px; line-height: 1.8; color: #3c4043;\">\n<p style=\"margin-bottom: 10px;\" data-ke-size=\"size16\">✅ <b>AI Prompt Studio:</b> 복잡한 '메타 프롬프트'를 손쉽게 설계해 AI 성능을 극대화합니다. RTSC, 비즈니스 프레임워크 등 5가지 전문 모드가 인상적이에요.</p>\n<p style=\"margin-bottom: 10px;\" data-ke-size=\"size16\">✅ <b>Prompt Equalizer Pro:</b> 글의 창의성, 전문성, 감정 등 5가지 뉘앙스를 슬라이더로 정밀하게 조절하여 원하는 톤을 구현할 수 있습니다. AI 페르소나 기능도 정말 강력해요.</p>\n<p style=\"margin-bottom: 10px;\" data-ke-size=\"size16\">✅ <b>프롬프트 마스터:</b> 자주 사용하는 프롬프트를 템플릿으로 저장하고 관리하며, 변수를 활용해 빠르게 재사용할 수 있는 효율적인 프롬프트 관리 도구입니다.</p>\n<p style=\"margin-bottom: 0;\" data-ke-size=\"size16\">✅ <b>2026년, AI와의 대화:</b> 이 3가지 앱을 통해 AI를 단순한 챗봇이 아닌, 여러분의 가장 강력하고 유능한 파트너로 만들 수 있습니다. 이제 프롬프트 작성에 대한 고민은 끝이에요!</p>\n</div>\n<div style=\"font-size: 14px; color: #5f6368; margin-top: 20px; padding-top: 15px; border-top: 1px solid #dadce0;\">* 이 앱들은 프롬프트 작성의 번거로움을 줄이고, AI의 잠재력을 최대한 끌어낼 수 있도록 돕는 도구입니다. 각 앱의 상세 기능과 활용법을 익히면 더 큰 시너지를 얻을 수 있습니다.</div>\n</div>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>❓ 자주 묻는 질문 (FAQ)</b></h2>\n<div style=\"margin-bottom: 20px;\">\n<p style=\"margin-bottom: 5px;\" data-ke-size=\"size16\"><b>Q1: 이 세 가지 앱이 실제로 얼마나 도움이 되나요?</b></p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\"><b>A1:</b> 제 경험상 AI와의 소통에서 겪는 거의 모든 어려움을 해결해 준다고 생각해요. AI Prompt Studio로 처음부터 정교한 프롬프트를 만들고, Prompt Equalizer Pro로 결과물의 톤앤매너를 섬세하게 조절하며, 프롬프트 마스터로 이 모든 과정을 효율적으로 관리하고 재사용할 수 있습니다. AI 활용 능력 향상에 결정적인 역할을 해줄 겁니다.</p>\n</div>\n<div style=\"margin-bottom: 20px;\">\n<p style=\"margin-bottom: 5px;\" data-ke-size=\"size16\"><b>Q2: 각 앱의 베이직/프로 버전 차이점은 무엇인가요?</b></p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\"><b>A2:</b> AI Prompt Studio의 경우, 베이직 버전은 '빠른 프롬프트 3종'과 같은 입문용 기능에 초점을 맞추고, 프로 버전은 이 글에서 설명한 모든 전문 모드와 기능을 제공합니다. 반면 Prompt Equalizer Pro와 프롬프트 마스터는 비회원도 대부분의 기능을 사용할 수 있으며, 소스 코드는 정회원에게만 제공되는 차이가 있습니다.</p>\n</div>\n<div style=\"margin-bottom: 20px;\">\n<p style=\"margin-bottom: 5px;\" data-ke-size=\"size16\"><b>Q3: 앱 사용에 필요한 API 키는 어떻게 발급받나요?</b></p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\"><b>A3:</b> 세 앱 모두 구글 AI 스튜디오(<a style=\"color: #1a73e8; text-decoration: none;\" href=\"https://aistudio.google.com/\" target=\"_blank\" rel=\"noopener\">aistudio.google.com</a>)에서 무료로 발급받을 수 있는 제미나이 API 키를 사용합니다. 발급 절차가 간단하고 무료이니, AI 스튜디오에 접속하셔서 쉽게 발급받으실 수 있을 거예요.</p>\n</div>\n</div>\n<script type=\"application/ld+json\">\n{\n  \"@context\": \"https://schema.org\",\n  \"@type\": \"FAQPage\",\n  \"mainEntity\": [\n    {\n      \"@type\": \"Question\",\n      \"name\": \"이 세 가지 앱이 실제로 얼마나 도움이 되나요?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"제 경험상 AI와의 소통에서 겪는 거의 모든 어려움을 해결해 준다고 생각해요. AI Prompt Studio로 처음부터 정교한 프롬프트를 만들고, Prompt Equalizer Pro로 결과물의 톤앤매너를 섬세하게 조절하며, 프롬프트 마스터로 이 모든 과정을 효율적으로 관리하고 재사용할 수 있습니다. AI 활용 능력 향상에 결정적인 역할을 해줄 겁니다.\"\n      }\n    },\n    {\n      \"@type\": \"Question\",\n      \"name\": \"각 앱의 베이직/프로 버전 차이점은 무엇인가요?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"AI Prompt Studio의 경우, 베이직 버전은 '빠른 프롬프트 3종'과 같은 입문용 기능에 초점을 맞추고, 프로 버전은 이 글에서 설명한 모든 전문 모드와 기능을 제공합니다. 반면 Prompt Equalizer Pro와 프롬프트 마스터는 비회원도 대부분의 기능을 사용할 수 있으며, 소스 코드는 정회원에게만 제공되는 차이가 있습니다.\"\n      }\n    },\n    {\n      \"@type\": \"Question\",\n      \"name\": \"앱 사용에 필요한 API 키는 어떻게 발급받나요?\",\n      \"acceptedAnswer\": {\n        \"@type\": \"Answer\",\n        \"text\": \"세 앱 모두 구글 AI 스튜디오(aistudio.google.com)에서 무료로 발급받을 수 있는 제미나이 API 키를 사용합니다. 발급 절차가 간단하고 무료이니, AI 스튜디오에 접속하셔서 쉽게 발급받으실 수 있을 거예요.\"\n      }\n    }\n  ]\n}\n</script>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<figure data-ke-type=\"video\" data-ke-style=\"alignCenter\" data-video-host=\"youtube\" data-video-url=\"https://www.youtube.com/watch?v=QPjkmFFaN_E\" data-video-thumbnail=\"https://scrap.kakaocdn.net/dn/bswQtc/dJMb8UHLrOG/qBi7pu5UMJKuih785BbAr0/img.jpg?width=1280&amp;height=720&amp;face=1126_448_1214_544\" data-video-width=\"860\" data-video-height=\"484\" data-video-origin-width=\"860\" data-video-origin-height=\"484\" data-ke-mobilestyle=\"widthContent\" data-video-title=\"수십만 원짜리 프롬프트 강의? 이제 이 앱 3개로 끝내세요! (메타 프롬프트 설계법)\" data-original-url=\"\"><iframe src=\"https://www.youtube.com/embed/QPjkmFFaN_E\" width=\"860\" height=\"484\" frameborder=\"\" allowfullscreen=\"true\"></iframe>\n<figcaption style=\"display: none;\"></figcaption>\n</figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<div id=\"gtx-trans\" style=\"position: absolute; left: -29px; top: 8435.72px;\">\n<div class=\"gtx-trans-icon\">&nbsp;</div>\n</div>",
        "contentSnippet": "AI와의 대화가 늘 엉뚱한 결과로 이어져 답답하셨나요? 혹은 완벽한 프롬프트를 만들고 싶지만 어디서부터 시작해야 할지 막막하셨다면, 이 글이 해답이 될 거예요. AI 활용의 새로운 기준을 제시할 세 가지 혁신적인 앱으로 프롬프트 작성의 모든 고민을 끝내고, 여러분도 진정한 AI 프롬프트 마스터가 되어 보세요!\n여러분, 혹시 AI에게 질문했는데 영 엉뚱한 답변이 나와서 황당했던 경험, 저만 있는 건 아니겠죠? 아니면, 정말 완벽한 결과물을 기대하면서 프롬프트를 만들고 싶은데, 대체 어디서부터 손을 대야 할지 감조차 안 잡히셨던 적도 분명 있으실 거예요. 오늘은 이런 고민을 시원하게 해결해 줄 마법 같은 세 가지 도구를 소개해 드리려고 해요. 이 앱들만 제대로 활용하신다면, 프롬프트 전문가의 영역이 더 이상 꿈이 아니랍니다!\n\n\n제가 직접 경험해 보니, AI는 정말 질문을 어떻게 하느냐에 따라 답변의 질이 하늘과 땅 차이로 달라져요. 단순히 \"블로그 글 써줘\"라고 짧게 물으면, 어디서 많이 본 듯한 평범한 결과물만 툭 튀어나오기 일쑤죠. 그런데 만약 \"당신은 10년 경력의 여행 작가입니다. 20대 여성을 위한 제주도 3박 4일 여행기를 감성적인 톤으로 작성해주세요\"라고 디테일하게 요청한다면? 와, 이건 정말 완전히 다른 수준의, 심지어 감동적인 결과물이 나올 때도 있더라고요.\n하지만 문제는 매번 이렇게 완벽하고 정교한 프롬프트를 직접 만드는 게 너무 어렵고 귀찮다는 점이에요. 특히 \"AI한테 어떤 역할을 부여해야 하지?\", \"어떤 조건을 추가해야 가장 좋은 결과가 나올까?\" 같은 고민을 하다 보면 시간이 정말이지 눈 깜짝할 사이에 흘러가 버리죠. 그래서 오늘 제가 소개해 드릴 이 세 가지 앱이 필요한 겁니다. 각각 다른 역할로 여러분의 프롬프트 작성 전 과정을 빈틈없이 도와줄 거예요. 마치 요리할 때 레시피를 짜주는 셰프, 맛을 미세하게 조절하는 조미료, 그리고 나만의 특별한 레시피를 정리해 주는 요리책 같다고나 할까요?\n1. AI Prompt Studio: 메타 프롬프트 설계의 마법사  ‍♀️\n여러분, 혹시 ChatGPT나 제미나이를 쓸 때 \"제주도 여행 블로그 글 하나 써줘\"라고 짧게 물어보고 나서 결과가 너무 뻔해서 실망하신 적 있으실 거예요. 사실 이건 AI의 성능 문제라기보다는, 질문하는 방식에 아쉬움이 있었을 가능성이 큽니다. AI는 질문을 어떻게 하느냐에 따라 답변의 품질이 정말이지 하늘과 땅 차이로 달라지거든요.\n\n\n하지만 문제는 매번 \"너는 10년 차 작가야, 타겟은 누구고, 형식은 마크다운으로 해줘\" 같은 복잡한 프롬프트를 직접 짜기가 너무 어렵고 번거롭다는 거죠. 바로 이 번거로움을 해결해 주는 도구가 첫 번째로 소개해 드릴 'AI Prompt Studio'입니다. 이 앱은 단순한 질문이 아닌, LLM(거대 언어 모델)의 성능을 100% 끌어낼 수 있는 '메타 프롬프트(Meta Prompt)'를 전문적으로 설계해주는 앱이에요.\n주요 기능  \n빠른 프롬프트 생성: 복잡한 설정 없이 목표만 입력하면 버튼 하나로 강력한 프롬프트를 즉시 만들어줍니다. 바쁠 때 정말 최고죠.\n역질문 유도: 사용자의 요청이 모호할 때, AI가 스스로 부족한 정보를 파악하여 \"더 나은 답변을 위해 몇 가지 질문을 해도 될까요?\" 하고 되묻도록 유도합니다. 덕분에 불필요한 시행착오를 줄일 수 있어요.\n페르소나 부여: 목표 달성에 가장 적합한 전문가 역할을 AI가 스스로 설정하고 연기하도록 지시합니다. 예를 들어, \"최고의 마케터\" 역할을 부여해서 결과물의 전문성을 확 끌어올릴 수 있어요.\n비판적 개선: 입력한 내용을 비판적으로 분석해서 논리적 허점이나 모호한 부분을 찾아내고, 더 완벽한 구조로 즉시 수정해 줍니다. 스스로 고쳐나가면서 프롬프트 실력이 확 늘 거예요.\n다섯 가지 전문 모드  ️\n이 앱의 가장 큰 특징이자 핵심은 바로 목적에 따라 골라 쓸 수 있는 다섯 가지 전문 모드를 지원한다는 점이에요. 여러분의 상황에 맞춰 선택하기만 하면 됩니다.\nRTSC 마스터 모드: 역할(Role), 작업(Task), 구조(Structure), 체인(Chain)의 앞 글자를 딴 모드인데요. 예를 들어 \"유튜브 쇼츠 대본 생성\"이라고 목표만 입력해 보세요. AI가 알아서 \"숏폼 바이럴 마케팅 디렉터\"라는 역할을 부여하고, 쇼츠 성공 공식을 완벽하게 구조화한 프롬프트를 짜줍니다. 정교하고 복잡한 창작물을 만들 때 정말 최고입니다.\n비즈니스 프레임워크 모드: 직장인분들이라면 이 모드를 강력 추천합니다! 기획안이나 보고서 쓸 때 막막하셨던 경험 다들 있으시죠? 코스타(COSTAR)나 프렙(PREP) 같은 검증된 비즈니스 틀을 자동으로 적용해서 아주 깔끔하고 논리적인 업무용 문서를 만들어 줘요. 정말이지 퇴근 시간을 앞당겨 주는 효자 앱이라고 할 수 있죠.\n프롬프트 아키텍트: 정말 귀찮을 때, 이 모드가 마법을 부려줍니다. \"복잡한 법률 문서를 쉽게 요약해 주는 AI 에이전트를 설계해 줘\"라고 딱 한 줄만 써보세요. 그럼 AI가 필요한 제약 조건과 작업 프로세스를 알아서 추론하고 설계해 냅니다. 마치 옆에 유능한 비서가 있는 것 같아요.\n버텍스 AI XML 모드: 기업용 데이터 처리나 구조화된 데이터가 필요할 때 빛을 발하는 모드예요. 결과물을 깔끔한 XML 태그 구조로 만들어주기 때문에 복잡한 데이터를 분류하거나 추출할 때 아주 정확하고 표준화된 답변을 얻을 수 있습니다. 데이터 분석가나 개발자분들께 특히 유용할 거예요.\nCoT(Chain-of-Thought) 모드: 논리적인 사고가 필요할 때 활용해 보세요. AI가 사람처럼 단계별로 추론하며 문제를 풀도록 유도하는 방식인데요. 덕분에 수학 문제나 복잡한 논리 과제에서 AI가 엉뚱한 답을 내놓는 현상을 획기적으로 줄여줍니다. 이제는 그저 \"무엇을 할지\"만 결정하시면 됩니다!\n솔직히 말하면, 이 앱은 거의 '사기 앱'이라고 불러도 될 정도예요. 왜냐하면 프롬프트 엔지니어들이 밤새워 연구하고 강의에서 수십만 원을 주고 배워야 하는 복잡한 기법들을, 버튼 클릭 몇 번만으로 누구나 완벽하게 구현할 수 있게 만들어줬거든요. 저도 처음에 써보고 정말 깜짝 놀랐습니다.\n많은 분들이 AI를 쓰면서 \"AI가 별로야\"라고 아쉬워하시는데, 제 경험상 사실은 질문을 제대로 못한 경우가 대부분입니다. 그런데 이 앱을 쓰면 그런 문제가 완전히 해결돼요. 시작하는 방법도 아주 간단합니다. 브라우저에서 접속한 뒤 우측 상단 톱니바퀴 아이콘을 클릭해서, 구글 AI 스튜디오에서 무료로 발급받은 '제미나이 API 키'만 넣어주면 준비 끝입니다. 자, 이제 '시작하기' 버튼을 누르고 여러분만의 완벽한 프롬프트를 직접 설계해 보시기 바랍니다!\n\n \nMeta Prompt Master\n \nai-prompt-studio-v2-basic.vercel.app\n\n \n  잠깐! API 키 설정 팁: AI Prompt Studio를 시작하기 전에, 구글 AI 스튜디오(aistudio.google.com)에서 무료로 제미나이 API 키를 발급받으세요. 이 키만 있으면 AI Prompt Studio의 모든 기능을 막힘없이 활용할 수 있습니다. 설정 과정도 매우 간단하니 걱정 마세요!\n2. Prompt Equalizer Pro: 글의 뉘앙스를 조율하는 이퀄라이저  \n자, 다음으로 소개해 드릴 앱은 정말이지 흥미로운 도구입니다. 바로 'Prompt Equalizer Pro'인데요. 여러분, 혹시 음악 들으실 때 취향에 맞춰 저음이나 고음을 조절하는 '이퀄라이저' 써보셨나요? 이 앱은 그 원리를 글쓰기에 그대로 가져왔습니다. 앞서 보신 'AI Prompt Studio'가 프롬프트를 '만드는' 도구였다면, 이 앱은 이미 써놓은 글의 뉘앙스를 아주 정밀하게 튜닝하고 조정하는 도구라고 보시면 됩니다. 개인적으로 써보고 정말 감탄했던 앱이에요.\n\n\n핵심 기능: 5가지 슬라이더 ✨\n앱 화면을 보시면 오른쪽에 다섯 개의 슬라이더가 보일 거예요. 이게 이 앱의 핵심인데요, 마치 음향 이퀄라이저처럼 글의 다양한 요소를 미세하게 조절할 수 있습니다.\n창의성: 슬라이더를 오른쪽으로 쭉 밀면, 평범했던 문장이 아주 은유적이고 풍성하게 변합니다. 반대로 왼쪽으로 낮추면 군더더기 없이 아주 직설적인 문장이 되죠. 소설이나 시를 쓸 때 정말 유용해요.\n명확성: 글의 이해도를 높이거나 낮출 수 있습니다. 복잡한 내용을 쉽게 풀어 설명할 때 아주 좋고, 때로는 일부러 난해하게 표현할 수도 있습니다.\n감정의 깊이: 글에 담긴 감정의 농도를 조절합니다. 따뜻하고 감성적인 글부터 차갑고 이성적인 글까지, 원하는 감정 톤을 만들 수 있어요.\n문장의 길이: 길고 유려한 문장과 짧고 간결한 문장 사이를 오갈 수 있습니다. 저는 주로 짧은 문장으로 속도감을 주고 싶을 때 사용하곤 해요.\n전문성: 이 기능이 정말 압권인데요! 수치를 높이면 아주 격식 있는 공문서 느낌부터, 낮추면 친구에게 말하는 듯한 편안한 말투까지 자유자재로 오갈 수 있습니다. 독자층에 맞춰 말투를 순식간에 바꾸는 게 가능해요.\n슬라이더 몇 번 움직이는 것만으로 글의 온도와 질감이 완전히 달라지는 게 정말 신기하지 않나요? 처음에 써봤을 때 솔직히 '이게 된다고?' 싶었어요.\nAI 페르소나 기능  \n더 놀라운 건 'AI 페르소나' 기능입니다. 앱에서 미리 제공하는 10가지 캐릭터 외에도 여러분이 직접 '냉철한 탐정'이나 '비즈니스 전문가'라고만 입력하면, AI가 그 캐릭터에 딱 맞는 말투와 성격, 그리고 최적의 이퀄라이저 값까지 알아서 세팅해 줍니다. 정말 생각지도 못한 부분에서 감탄하게 되는 기능이었어요.\n사용법도 아주 간단합니다. 처음 설정에서 API 키만 넣고 저장 폴더를 지정해 주세요. 그다음 원하는 말투의 페르소나를 고르거나 직접 만든 페르소나를 선택한 뒤, 슬라이더를 슥슥 조절하고, 원본 글을 넣은 뒤 '명작 생성' 버튼만 누르면 끝입니다. 마음에 들면 바로 파일로 저장하시면 되고요.\n소설을 쓰실 때 캐릭터마다 말투를 다르게 고정하거나, 딱딱한 보고서를 부드럽게 다듬을 때 이만한 도구가 없습니다. 단순히 수치를 높이기보다, 이것저것 만져보면서 여러분만의 '황금 조합'을 한 번 찾아보세요. 저도 저만의 조합을 찾는 재미에 푹 빠져버렸답니다!\n\n \nPrompt Equalizer Pro\n \nprompt-equalizer.vercel.app\n\n \n3. 프롬프트 마스터: 당신의 프롬프트 라이브러리  \n혹시 매번 똑같은 프롬프트를 메모장에 복사해서 쓰거나, 예전에 써둔 정말 좋은 프롬프트를 못 찾아서 헤맨 적 없으신가요? 첫 번째 앱이 프롬프트를 '만들고', 두 번째 앱이 뉘앙스를 '바꿨다면', '프롬프트 마스터'는 자주 쓰는 프롬프트를 완벽하게 '관리하고 재사용하는' 도구입니다. 이 앱 하나면 반복되는 질문을 일일이 타이핑할 필요가 완전히 사라져요. 정말 솔깃하지 않나요?\n\n\n개인적으로 저는 이 앱이 작업 효율성을 몇 배로 끌어올려 줬다고 생각해요. 예를 들어, 콘텐츠 제작자라면 유튜브 스크립트 틀을 템플릿으로 저장해두고 주제만 바꿔서 빠르게 기획할 수 있고요. 개발자라면 복잡한 코드 리뷰 프롬프트를 템플릿화해서 언어와 코드만 바꿔 끼우면 끝입니다. 비즈니스 이메일이나 마케팅 카피도 타겟과 키워드만 툭툭 던지면 전문적인 결과물이 쏟아져 나오는 경험을 하실 수 있을 거예요.\n강력한 기능  \n템플릿 생성 및 AI 제안: 첫 화면에 있는 미리 설정된 10개의 프롬프트 템플릿을 바로 사용할 수도 있지만, 여러분의 취향에 맞는 새로운 템플릿을 직접 만들 수도 있어요. 여기에 'AI 제안' 버튼까지 활용하면 더 대박입니다. 제목만 써놓고 버튼을 누르면 AI가 알아서 적절한 설명과 본문 내용을 채워주거든요. 초안 짜는 시간조차 아껴주는 거죠.\n변수 값 채우기: 프롬프트를 실행하면 화면이 둘로 나뉩니다. 왼쪽에서 미리 정해둔 변수 값들을 채워 넣고 버튼을 누르면, 오른쪽에서 실시간으로 AI의 답변이 생성됩니다. 마치 코드를 실행하는 것 같은 느낌이에요.\n강력한 검색 및 카테고리 관리: 프롬프트가 많아져도 걱정 마세요. 강력한 검색 기능과 카테고리 관리 기능이 있어서 언제든 원하는 프롬프트를 1초 만에 찾아낼 수 있습니다. 저처럼 잃어버린 프롬프트 때문에 속상했던 경험이 있으시다면 이 기능에 감사하게 될 거예요.\n라이브러리 백업: 여러분의 모든 프롬프트를 한데 모아 관리할 수 있는 라이브러리 백업 기능까지 갖췄으니, 이제 프롬프트 마스터와 함께 반복 작업에서 완전히 해방되어 보세요!\n사용법도 직관적입니다. 처음 실행할 때 제미나이 API 키와 저장 폴더만 지정해 주시면 준비 끝이에요. 이제 이 세 가지 앱을 활용하면 AI와의 소통이 훨씬 더 즐겁고 효율적으로 바뀔 거예요. 정말 추천합니다!\n\n \n프롬프트 마스터\n \nprompt-master-ten-flax.vercel.app\n\n \n  핵심 요약\n✅ AI Prompt Studio: 복잡한 '메타 프롬프트'를 손쉽게 설계해 AI 성능을 극대화합니다. RTSC, 비즈니스 프레임워크 등 5가지 전문 모드가 인상적이에요.\n✅ Prompt Equalizer Pro: 글의 창의성, 전문성, 감정 등 5가지 뉘앙스를 슬라이더로 정밀하게 조절하여 원하는 톤을 구현할 수 있습니다. AI 페르소나 기능도 정말 강력해요.\n✅ 프롬프트 마스터: 자주 사용하는 프롬프트를 템플릿으로 저장하고 관리하며, 변수를 활용해 빠르게 재사용할 수 있는 효율적인 프롬프트 관리 도구입니다.\n✅ 2026년, AI와의 대화: 이 3가지 앱을 통해 AI를 단순한 챗봇이 아닌, 여러분의 가장 강력하고 유능한 파트너로 만들 수 있습니다. 이제 프롬프트 작성에 대한 고민은 끝이에요!\n* 이 앱들은 프롬프트 작성의 번거로움을 줄이고, AI의 잠재력을 최대한 끌어낼 수 있도록 돕는 도구입니다. 각 앱의 상세 기능과 활용법을 익히면 더 큰 시너지를 얻을 수 있습니다.\n❓ 자주 묻는 질문 (FAQ)\nQ1: 이 세 가지 앱이 실제로 얼마나 도움이 되나요?\nA1: 제 경험상 AI와의 소통에서 겪는 거의 모든 어려움을 해결해 준다고 생각해요. AI Prompt Studio로 처음부터 정교한 프롬프트를 만들고, Prompt Equalizer Pro로 결과물의 톤앤매너를 섬세하게 조절하며, 프롬프트 마스터로 이 모든 과정을 효율적으로 관리하고 재사용할 수 있습니다. AI 활용 능력 향상에 결정적인 역할을 해줄 겁니다.\nQ2: 각 앱의 베이직/프로 버전 차이점은 무엇인가요?\nA2: AI Prompt Studio의 경우, 베이직 버전은 '빠른 프롬프트 3종'과 같은 입문용 기능에 초점을 맞추고, 프로 버전은 이 글에서 설명한 모든 전문 모드와 기능을 제공합니다. 반면 Prompt Equalizer Pro와 프롬프트 마스터는 비회원도 대부분의 기능을 사용할 수 있으며, 소스 코드는 정회원에게만 제공되는 차이가 있습니다.\nQ3: 앱 사용에 필요한 API 키는 어떻게 발급받나요?\nA3: 세 앱 모두 구글 AI 스튜디오(aistudio.google.com)에서 무료로 발급받을 수 있는 제미나이 API 키를 사용합니다. 발급 절차가 간단하고 무료이니, AI 스튜디오에 접속하셔서 쉽게 발급받으실 수 있을 거예요.",
        "guid": "https://muzbox.tistory.com/483709",
        "categories": [
          "AI, 미래기술/AI 챗봇 및 지침 무료 배포",
          "AI Prompt Studio",
          "ai 글쓰기 도구",
          "ai 프롬프트",
          "Ai 활용",
          "llm 활용",
          "Prompt Equalizer Pro",
          "메타 프롬프트",
          "프롬프트 관리",
          "프롬프트 마스터",
          "프롬프트 엔지니어링"
        ],
        "isoDate": "2026-02-15T08:04:15.000Z"
      },
      {
        "creator": "어떤오후의 프리웨어 이야기",
        "title": "2026년 유튜브 AI 콘텐츠 생존 전략: '슬롭' 피하고 고품질 인정받는 법",
        "link": "https://muzbox.tistory.com/483708",
        "pubDate": "Thu, 12 Feb 2026 08:12:24 +0900",
        "author": "어떤오후의 프리웨어 이야기",
        "comments": "https://muzbox.tistory.com/483708#entry483708comment",
        "content": "<div style=\"font-family: 'Noto Sans KR', sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; font-size: 16px; box-sizing: border-box; color: #3c4043;\">\n<div style=\"background-color: #e8f4fd; padding: 15px; border-radius: 8px; font-style: italic; margin-bottom: 25px; font-size: 15px;\">2026년, 유튜브는 저품질 AI 콘텐츠 '슬롭'과의 전쟁을 선포하며 플랫폼의 질을 높이는 데 주력하고 있습니다. 크리에이터들은 어떻게 변화하는 유튜브 생태계에서 살아남아 고품질 AI 콘텐츠로 인정받고 성공할 수 있을까요? 단순히 AI가 생성한 '슬롭'을 넘어, 인간적인 통찰과 창의성이 담긴 콘텐츠로 시청자를 사로잡을 수 있는 핵심 전략을 이 글에서 자세히 소개합니다.</div>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"download.jpg\" data-origin-width=\"1200\" data-origin-height=\"1200\"><span data-url=\"https://blog.kakaocdn.net/dn/02eKT/dJMcacWjCe7/URKqI5bOhkIULc7kLkhJDk/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/02eKT/dJMcacWjCe7/URKqI5bOhkIULc7kLkhJDk/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/02eKT/dJMcacWjCe7/URKqI5bOhkIULc7kLkhJDk/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F02eKT%2FdJMcacWjCe7%2FURKqI5bOhkIULc7kLkhJDk%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"유튜브가 저품질 AI 콘텐츠 '슬롭'에 맞서 싸우고 고품질 콘텐츠를 지향하는 모습을 표현한 추상적인 디지털 일러스트.\" loading=\"lazy\" width=\"500\" height=\"500\" data-filename=\"download.jpg\" data-origin-width=\"1200\" data-origin-height=\"1200\"/></span></figure>\n\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">요즘 유튜브 피드, 혹시 좀 달라진 것 같지 않으세요? 특히 로그인하지 않은 상태로 쇼츠를 보다 보면, 기이하고 반복적인 AI 콘텐츠들이 넘쳐나는 것을 느끼셨을 거예요. 아, 이런 걸 'AI 슬롭(AI Slop)'이라고 부르는데, 솔직히 좀 피곤하죠. 의미도 없고 재미도 없이 그저 조회수만 노리는 영상들이 너무 많아서, 저조차도 가끔은 피로감을 느끼곤 했습니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">그런데 2026년인 지금, 유튜브가 이 'AI 슬롭'과의 전면전을 선포했습니다. 제가 볼 때는, 이건 단순히 플랫폼의 정화 작업을 넘어 크리에이터들에게는 정말 중요한 변화의 시그널이에요. 과거에는 양으로 승부하는 시대였다면, 이제는 질과 진정성이 훨씬 더 중요해진 거죠. 대체 유튜브가 왜 이런 결정을 내렸고, 우리는 어떻게 이 변화에 발맞춰 나가야 할까요?</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>⚔️ 유튜브, 'AI 슬롭'과의 전쟁을 선포하다</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">최근 <a style=\"color: #1a73e8; text-decoration: none;\" href=\"https://www.androidpolice.com/youtube-ai-slop-removal/\">Android Police</a>가 보도한 내용에 따르면, 유튜브는 저품질 AI 콘텐츠, 즉 'AI 슬롭'에 대한 대대적인 정화 작업을 벌이고 있다고 해요. 이들의 분석은 온라인 영상 편집 회사인 Kapwing의 2025년 11월 보고서에 기반한 것인데, 정말 놀라운 결과가 나왔습니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">보고서 발표 이후 불과 두 달 만에, Kapwing이 선정한 <b>상위 100개 AI 콘텐츠 채널 중 16개가 플랫폼에서 사라졌다고 합니다.</b> 이 16개 채널은 총 3천5백만 명의 구독자를 보유하고 있었으며, 무려 <b>47억 뷰가 넘는 조회수</b>를 기록했었어요. 단순히 영상만 삭제된 경우도 있지만, 아예 채널 자체가 사라진 사례도 상당수라고 하니, 유튜브의 의지가 얼마나 강력한지 짐작할 수 있습니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">특히 눈에 띄는 것은 'CuentosFacianantes'라는 채널입니다. 595만 명의 구독자를 거느렸던 이 채널은 드래곤볼 관련 AI 생성 쇼츠로 2025년 말까지 12.8억 뷰를 쓸어 담았지만, 지금은 사라진 상태입니다. 이 외에도 587만 구독자의 'Imperio de Jesus', 421만 구독자의 'Super Cat League' 같은 대형 채널들도 함께 문을 닫았죠. 이쯤 되면 유튜브가 정말 작정하고 나섰다는 생각이 듭니다.</p>\n<div style=\"background-color: #e8f4fd; border-left: 4px solid #1a73e8; padding: 15px; margin: 20px 0; border-radius: 0 8px 8px 0;\">  <b>'AI 슬롭(AI Slop)'이란?</b>\n<p style=\"margin-bottom: 0; font-size: 15px; color: #3c4043;\" data-ke-size=\"size16\">의미 있는 내용이나 독창성 없이 단순히 AI 도구를 이용해 대량으로 생성된, 저품질의 반복적이고 자극적인 콘텐츠를 지칭합니다. 주로 조회수나 광고 수익만을 목적으로 하며, 시청자에게 피로감만 안겨주는 경우가 많아요.</p>\n</div>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"download.png\" data-origin-width=\"1344\" data-origin-height=\"768\"><span data-url=\"https://blog.kakaocdn.net/dn/dsXcVK/dJMcai92zG6/Kx4K5KbZRvKlhdOVTZLHV0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/dsXcVK/dJMcai92zG6/Kx4K5KbZRvKlhdOVTZLHV0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/dsXcVK/dJMcai92zG6/Kx4K5KbZRvKlhdOVTZLHV0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdsXcVK%2FdJMcai92zG6%2FKx4K5KbZRvKlhdOVTZLHV0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"인간의 손이 AI가 생성한 파편화된 디지털 요소를 섬세하게 다듬어 고품질 예술 작품을 만드는 모습.\" loading=\"lazy\" width=\"1344\" height=\"768\" data-filename=\"download.png\" data-origin-width=\"1344\" data-origin-height=\"768\"/></span></figure>\n\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>  모한 CEO의 2026년 비전: AI, 도구인가 대체재인가?</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">유튜브의 이러한 움직임은 닐 모한(Neal Mohan) CEO의 비전과 정확히 일치합니다. 그는 2026년 1월 21일 발표한 <b>'2026년 유튜브의 비전'</b>이라는 글에서 AI 콘텐츠에 대한 회사의 입장을 명확히 밝혔어요.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">모한 CEO는 AI가 창의적인 사람들에게 엄청난 기회가 될 것이라고 말하며, 포토샵이나 CGI 같은 도구에 비유했습니다. <b>\"AI는 표현을 위한 도구이지, 인간을 대체하는 것이 아니다\"</b>라고 강조했죠. 저도 이 말에 정말 공감하는데요, 결국 기술은 사람이 어떻게 쓰느냐에 따라 달라지는 것 아니겠어요?</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">하지만 그는 AI 콘텐츠의 위험성도 동시에 지적했습니다. 진짜 영상과 AI 생성 영상을 구별하기 어려워지는 현실을 우려하며, <b>\"커뮤니티 가이드라인을 위반하는 유해한 합성 미디어를 제거하고, 딥페이크를 식별하고 차단하는 도구를 크리에이터에게 제공할 것\"</b>이라고 밝혔습니다. 이는 창의성을 존중하되, 윤리적 책임도 다하겠다는 유튜브의 강한 의지를 보여주는 대목이죠.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">특히 흥미로웠던 점은 그가 직접 <b>'AI 슬롭 관리(Managing AI slop)'</b>라는 섹션을 만들었다는 거예요. 모한 CEO는 유튜브의 목표가 '자유로운 표현의 장'인 동시에 '사람들이 시간을 보내면서 기분 좋게 느낄 수 있는 곳'이라고 말했습니다. 이를 위해 <b>\"저품질 AI 콘텐츠의 확산을 줄이기 위해 스팸 및 클릭베이트 퇴치에 성공했던 기존 시스템을 적극적으로 활용하고 있다\"</b>고 덧붙였죠.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">결론적으로 유튜브는 AI 생성 콘텐츠 자체를 반대하는 것이 아니라, <b>의미 없고 반복적이며 질 낮은 '슬롭'을 근절하겠다는 분명한 선을 그은 것입니다.</b> 이는 유튜브 사용자 모두에게 좋은 소식일 뿐만 아니라, 앞으로 AI 콘텐츠를 제작하려는 크리에이터들에게도 명확한 가이드라인이 될 것이라고 생각합니다.</p>\n<div style=\"background-color: #fce8e6; border-left: 4px solid #d93025; padding: 15px; margin: 20px 0; border-radius: 0 8px 8px 0;\">⚠️ <b>경고: AI 콘텐츠 제작 시 유의할 점!</b>\n<p style=\"margin-bottom: 0; font-size: 15px; color: #3c4043;\" data-ke-size=\"size16\">유튜브는 AI 생성 콘텐츠에 대한 <b>투명성 의무</b>를 강화하고 있습니다. 콘텐츠가 AI에 의해 변경되었거나 생성된 경우, 이를 명확히 밝히지 않으면 불이익을 받을 수 있습니다. 항상 유튜브의 <a style=\"color: #d93025; text-decoration: none;\" href=\"https://support.google.com/youtube/answer/13926947\">커뮤니티 가이드라인</a>을 준수해주세요.</p>\n</div>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>  고품질 AI 콘텐츠를 위한 크리에이터 생존 전략</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">그렇다면 우리는 이 변화의 물결 속에서 어떻게 살아남고, 심지어 더 큰 기회를 만들 수 있을까요? 제 생각에는, 결국 <b>'인간적인 터치'</b>와 <b>'가치 전달'</b>에 달려 있다고 봅니다. 단순히 AI가 뽑아낸 결과물을 그대로 올리는 것을 넘어, 크리에이터의 역할이 더욱 중요해지는 시점이죠. 다음은 제가 제안하는 고품질 AI 콘텐츠 생존 전략입니다.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin-top: 25px; margin-bottom: 10px;\" data-ke-size=\"size23\">1. 인간적인 독창성과 통찰력을 더하라</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">AI는 데이터를 학습하고 패턴을 찾아내는 데는 탁월하지만, 고유한 경험, 감정, 그리고 깊이 있는 통찰력을 만들어내지는 못합니다. 여러분만의 시각, 분석, 혹은 개인적인 스토리를 AI 콘텐츠에 녹여내세요. 예를 들어, AI로 생성된 정보를 바탕으로 하더라도, 그 정보를 해석하고 독자에게 공감을 불러일으킬 만한 자신만의 서사를 추가하는 식이죠. 결국 사람의 마음을 움직이는 건 사람의 이야기 아니겠어요?</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin-top: 25px; margin-bottom: 10px;\" data-ke-size=\"size23\">2. AI 사용의 투명성을 확보하라</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">유튜브는 2026년부터 AI 생성 콘텐츠에 대한 <b>표시 의무</b>를 강화하고 있습니다. 이제 단순히 'AI가 만들었나?' 하고 의심받는 것을 넘어, 크리에이터 스스로 AI 사용 여부를 명확히 밝히는 것이 신뢰를 구축하는 핵심이 될 것입니다. 시청자들은 정직함을 더 높이 평가할 거예요. 저라면, 영상 시작이나 설명란에 '이 영상은 AI 기술의 도움을 받아 제작되었습니다'라고 명확히 밝힐 것 같아요.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin-top: 25px; margin-bottom: 10px;\" data-ke-size=\"size23\">3. '양'보다 '질'에 집중하라</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">저품질 AI 슬롭이 넘쳐나는 시대에, 대량 생산은 더 이상 답이 아닙니다. 오히려 하나의 고품질 콘텐츠에 시간과 노력을 투자하는 것이 훨씬 효과적입니다. 좋은 프롬프트 엔지니어링, AI 결과물의 세심한 후반 작업, 그리고 인간의 검토와 편집은 필수적입니다. AI를 단순한 생산 도구로 생각하기보다, 여러분의 창의적인 비전을 실현하는 '협력자'로 활용해야 합니다.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin-top: 25px; margin-bottom: 10px;\" data-ke-size=\"size23\">4. 커뮤니티와 적극적으로 소통하라</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">진정성 있는 커뮤니케이션은 AI가 대체할 수 없는 인간 고유의 영역입니다. 시청자의 댓글에 응답하고, 라이브 스트리밍을 통해 직접 소통하며, 설문조사 등을 통해 콘텐츠 기획에 참여시키는 것은 강력한 팬덤을 구축하는 데 도움이 됩니다. 결국 팬들은 AI가 아닌, 여러분이라는 <b>인간 크리에이터</b>와 소통하고 싶어 할 거예요.</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"download.png\" data-origin-width=\"1344\" data-origin-height=\"768\"><span data-url=\"https://blog.kakaocdn.net/dn/dEEPEN/dJMcaiCdQcZ/rTgVjq11NwjW6YgDD8LcJ1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/dEEPEN/dJMcaiCdQcZ/rTgVjq11NwjW6YgDD8LcJ1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/dEEPEN/dJMcaiCdQcZ/rTgVjq11NwjW6YgDD8LcJ1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdEEPEN%2FdJMcaiCdQcZ%2FrTgVjq11NwjW6YgDD8LcJ1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"지루하고 반복적인 AI 슬롭과 독창적이고 생동감 넘치는 고품질 AI 콘텐츠 흐름이 명확히 구분되는 추상적인 디지털 시각화.\" loading=\"lazy\" width=\"1344\" height=\"768\" data-filename=\"download.png\" data-origin-width=\"1344\" data-origin-height=\"768\"/></span></figure>\n\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">정리하자면, 2026년 유튜브는 AI 콘텐츠의 홍수 속에서 <b>'질'</b>과 <b>'인간적인 가치'</b>를 더욱 중요하게 여기는 방향으로 진화하고 있습니다. 아래 표를 통해 'AI 슬롭'과 '고품질 AI 콘텐츠'의 차이를 한눈에 비교해보세요.</p>\n<div style=\"margin-bottom: 20px; overflow-x: auto;\">\n<table style=\"width: 100%; border-collapse: collapse; text-align: left; border: 1px solid #dadce0;\" data-ke-align=\"alignLeft\">\n<thead style=\"background-color: #e8eaed;\">\n<tr>\n<th style=\"padding: 12px 15px; border: 1px solid #dadce0; color: #3c4043;\">항목</th>\n<th style=\"padding: 12px 15px; border: 1px solid #dadce0; color: #3c4043;\">AI 슬롭</th>\n<th style=\"padding: 12px 15px; border: 1px solid #dadce0; color: #3c4043;\">고품질 AI 콘텐츠</th>\n</tr>\n</thead>\n<tbody>\n<tr style=\"background-color: #f8f9fa;\">\n<td style=\"padding: 12px 15px; border: 1px solid #dadce0; color: #3c4043;\"><b>주요 목표</b></td>\n<td style=\"padding: 12px 15px; border: 1px solid #dadce0; color: #3c4043;\">조회수, 광고 수익 극대화</td>\n<td style=\"padding: 12px 15px; border: 1px solid #dadce0; color: #3c4043;\">가치 제공, 시청자 만족, 브랜드 구축</td>\n</tr>\n<tr style=\"background-color: #f1f3f4;\">\n<td style=\"padding: 12px 15px; border: 1px solid #dadce0; color: #3c4043;\"><b>콘텐츠 특징</b></td>\n<td style=\"padding: 12px 15px; border: 1px solid #dadce0; color: #3c4043;\">반복적, 자극적, 의미 없음, 독창성 결여</td>\n<td style=\"padding: 12px 15px; border: 1px solid #dadce0; color: #3c4043;\">독창적, 통찰력, 고품질 편집, 인간적 감성</td>\n</tr>\n<tr style=\"background-color: #f8f9fa;\">\n<td style=\"padding: 12px 15px; border: 1px solid #dadce0; color: #3c4043;\"><b>AI 활용 방식</b></td>\n<td style=\"padding: 12px 15px; border: 1px solid #dadce0; color: #3c4043;\">생성 도구에 전적으로 의존, 무단 사용</td>\n<td style=\"padding: 12px 15px; border: 1px solid #dadce0; color: #3c4043;\">크리에이터의 비전을 돕는 보조 도구</td>\n</tr>\n<tr style=\"background-color: #f1f3f4;\">\n<td style=\"padding: 12px 15px; border: 1px solid #dadce0; color: #3c4043;\"><b>플랫폼 반응</b></td>\n<td style=\"padding: 12px 15px; border: 1px solid #dadce0; color: #3c4043;\">삭제 및 채널 제재</td>\n<td style=\"padding: 12px 15px; border: 1px solid #dadce0; color: #3c4043;\">긍정적 평가, 노출 기회 증가</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div style=\"background-color: #f8f9fa; border: 1px solid #dadce0; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); padding: 25px; margin: 40px 0;\">\n<div style=\"font-size: 26px; color: #1a73e8; font-weight: bold; padding-bottom: 15px; border-bottom: 2px solid #1a73e8; margin-bottom: 20px;\">  핵심 요약</div>\n<p style=\"margin-bottom: 15px; font-size: 17px; color: #3c4043;\" data-ke-size=\"size16\">1. <b>유튜브는 저품질 'AI 슬롭'과의 전쟁을 선포했습니다.</b> 2026년 현재 수십억 뷰가 삭제되고 수많은 채널이 제재를 받았습니다.</p>\n<p style=\"margin-bottom: 15px; font-size: 17px; color: #3c4043;\" data-ke-size=\"size16\">2. 닐 모한 CEO는 AI를 <b>'표현의 도구'</b>로 보며, 유해하거나 질 낮은 AI 콘텐츠에 대한 강력한 조치를 예고했습니다.</p>\n<p style=\"margin-bottom: 15px; font-size: 17px; color: #3c4043;\" data-ke-size=\"size16\">3. 크리에이터는 <b>인간적인 독창성, 투명성, 품질 중심 전략</b>으로 변화에 대응해야 합니다.</p>\n<p style=\"margin-bottom: 0; font-size: 17px; color: #3c4043;\" data-ke-size=\"size16\">4. AI는 단순한 생산 도구가 아닌, <b>'인간 크리에이터의 가치를 높이는 협력자'</b>로서 활용되어야 합니다.</p>\n<div style=\"margin-top: 25px; padding-top: 15px; border-top: 1px solid #dadce0; font-size: 14px; color: #5f6368;\">*위 내용은 2026년 2월 12일 기준 유튜브 정책 및 시장 동향을 바탕으로 작성되었습니다.</div>\n</div>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>❓ 자주 묻는 질문 (FAQ)</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\"><b>Q1: 2026년에도 AI 콘텐츠를 유튜브에 올려도 괜찮을까요?</b></p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">A1: 네, 물론입니다. 유튜브는 AI 콘텐츠 자체를 금지하는 것이 아니라, 저품질의 'AI 슬롭'을 제재하는 것입니다. 중요한 것은 콘텐츠에 여러분의 독창적인 아이디어와 인간적인 가치를 담고, AI 사용 여부를 투명하게 공개하는 것입니다. AI는 강력한 창작 도구가 될 수 있습니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\"><b>Q2: 유튜브가 'AI 슬롭'을 판단하는 구체적인 기준은 무엇인가요?</b></p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">A2: 유튜브는 공식적으로 구체적인 기준을 명시하지는 않지만, 닐 모한 CEO의 발언을 통해 유추할 수 있습니다. 반복적이고 의미 없는 내용, 조회수나 광고 수익만을 위한 자극적인 콘텐츠, 커뮤니티 가이드라인을 위반하는 유해한 합성 미디어 등이 주요 대상입니다. 또한, AI 생성 여부를 숨기거나 오해를 불러일으키는 콘텐츠도 제재 대상이 될 수 있습니다.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\"><b>Q3: AI 콘텐츠 제작 시 크리에이터가 가장 중요하게 생각해야 할 것은 무엇인가요?</b></p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">A3: 단연 '품질'과 '진정성'입니다. AI는 보조 도구일 뿐, 콘텐츠의 기획, 스토리텔링, 편집 등 모든 과정에 크리에이터의 인간적인 노력과 통찰력이 들어가야 합니다. 시청자에게 실질적인 가치를 제공하고, 신뢰를 쌓는 것이 장기적인 성공의 열쇠입니다.</p>\n<script type=\"application/ld+json\">\n  {\n    \"@context\": \"https://schema.org\",\n    \"@type\": \"FAQPage\",\n    \"mainEntity\": [\n      {\n        \"@type\": \"Question\",\n        \"name\": \"2026년에도 AI 콘텐츠를 유튜브에 올려도 괜찮을까요?\",\n        \"acceptedAnswer\": {\n          \"@type\": \"Answer\",\n          \"text\": \"네, 물론입니다. 유튜브는 AI 콘텐츠 자체를 금지하는 것이 아니라, 저품질의 'AI 슬롭'을 제재하는 것입니다. 중요한 것은 콘텐츠에 여러분의 독창적인 아이디어와 인간적인 가치를 담고, AI 사용 여부를 투명하게 공개하는 것입니다. AI는 강력한 창작 도구가 될 수 있습니다.\"\n        }\n      },\n      {\n        \"@type\": \"Question\",\n        \"name\": \"유튜브가 'AI 슬롭'을 판단하는 구체적인 기준은 무엇인가요?\",\n        \"acceptedAnswer\": {\n          \"@type\": \"Answer\",\n          \"text\": \"유튜브는 공식적으로 구체적인 기준을 명시하지는 않지만, 닐 모한 CEO의 발언을 통해 유추할 수 있습니다. 반복적이고 의미 없는 내용, 조회수나 광고 수익만을 위한 자극적인 콘텐츠, 커뮤니티 가이드라인을 위반하는 유해한 합성 미디어 등이 주요 대상입니다. 또한, AI 생성 여부를 숨기거나 오해를 불러일으키는 콘텐츠도 제재 대상이 될 수 있습니다.\"\n        }\n      },\n      {\n        \"@type\": \"Question\",\n        \"name\": \"AI 콘텐츠 제작 시 크리에이터가 가장 중요하게 생각해야 할 것은 무엇인가요?\",\n        \"acceptedAnswer\": {\n          \"@type\": \"Answer\",\n          \"text\": \"단연 '품질'과 '진정성'입니다. AI는 보조 도구일 뿐, 콘텐츠의 기획, 스토리텔링, 편집 등 모든 과정에 크리에이터의 인간적인 노력과 통찰력이 들어가야 합니다. 시청자에게 실질적인 가치를 제공하고, 신뢰를 쌓는 것이 장기적인 성공의 열쇠입니다.\"\n        }\n      }\n    ]\n  }\n  </script>\n<p style=\"margin-bottom: 20px; text-align: center; margin-top: 40px;\" data-ke-size=\"size16\">2026년, 유튜브의 AI 콘텐츠 생태계는 새로운 전환점을 맞이하고 있습니다. '슬롭'을 피하고 고품질 콘텐츠로 인정받는 것은 더 이상 선택이 아닌 필수입니다. 여러분의 창의적인 열정과 인간적인 통찰력을 담아, 변화하는 유튜브에서 새로운 기회를 잡으시길 바랍니다! 감사합니다.</p>\n</div>",
        "contentSnippet": "2026년, 유튜브는 저품질 AI 콘텐츠 '슬롭'과의 전쟁을 선포하며 플랫폼의 질을 높이는 데 주력하고 있습니다. 크리에이터들은 어떻게 변화하는 유튜브 생태계에서 살아남아 고품질 AI 콘텐츠로 인정받고 성공할 수 있을까요? 단순히 AI가 생성한 '슬롭'을 넘어, 인간적인 통찰과 창의성이 담긴 콘텐츠로 시청자를 사로잡을 수 있는 핵심 전략을 이 글에서 자세히 소개합니다.\n\n\n요즘 유튜브 피드, 혹시 좀 달라진 것 같지 않으세요? 특히 로그인하지 않은 상태로 쇼츠를 보다 보면, 기이하고 반복적인 AI 콘텐츠들이 넘쳐나는 것을 느끼셨을 거예요. 아, 이런 걸 'AI 슬롭(AI Slop)'이라고 부르는데, 솔직히 좀 피곤하죠. 의미도 없고 재미도 없이 그저 조회수만 노리는 영상들이 너무 많아서, 저조차도 가끔은 피로감을 느끼곤 했습니다.\n그런데 2026년인 지금, 유튜브가 이 'AI 슬롭'과의 전면전을 선포했습니다. 제가 볼 때는, 이건 단순히 플랫폼의 정화 작업을 넘어 크리에이터들에게는 정말 중요한 변화의 시그널이에요. 과거에는 양으로 승부하는 시대였다면, 이제는 질과 진정성이 훨씬 더 중요해진 거죠. 대체 유튜브가 왜 이런 결정을 내렸고, 우리는 어떻게 이 변화에 발맞춰 나가야 할까요?\n⚔️ 유튜브, 'AI 슬롭'과의 전쟁을 선포하다\n최근 Android Police가 보도한 내용에 따르면, 유튜브는 저품질 AI 콘텐츠, 즉 'AI 슬롭'에 대한 대대적인 정화 작업을 벌이고 있다고 해요. 이들의 분석은 온라인 영상 편집 회사인 Kapwing의 2025년 11월 보고서에 기반한 것인데, 정말 놀라운 결과가 나왔습니다.\n보고서 발표 이후 불과 두 달 만에, Kapwing이 선정한 상위 100개 AI 콘텐츠 채널 중 16개가 플랫폼에서 사라졌다고 합니다. 이 16개 채널은 총 3천5백만 명의 구독자를 보유하고 있었으며, 무려 47억 뷰가 넘는 조회수를 기록했었어요. 단순히 영상만 삭제된 경우도 있지만, 아예 채널 자체가 사라진 사례도 상당수라고 하니, 유튜브의 의지가 얼마나 강력한지 짐작할 수 있습니다.\n특히 눈에 띄는 것은 'CuentosFacianantes'라는 채널입니다. 595만 명의 구독자를 거느렸던 이 채널은 드래곤볼 관련 AI 생성 쇼츠로 2025년 말까지 12.8억 뷰를 쓸어 담았지만, 지금은 사라진 상태입니다. 이 외에도 587만 구독자의 'Imperio de Jesus', 421만 구독자의 'Super Cat League' 같은 대형 채널들도 함께 문을 닫았죠. 이쯤 되면 유튜브가 정말 작정하고 나섰다는 생각이 듭니다.\n  'AI 슬롭(AI Slop)'이란?\n의미 있는 내용이나 독창성 없이 단순히 AI 도구를 이용해 대량으로 생성된, 저품질의 반복적이고 자극적인 콘텐츠를 지칭합니다. 주로 조회수나 광고 수익만을 목적으로 하며, 시청자에게 피로감만 안겨주는 경우가 많아요.\n\n\n  모한 CEO의 2026년 비전: AI, 도구인가 대체재인가?\n유튜브의 이러한 움직임은 닐 모한(Neal Mohan) CEO의 비전과 정확히 일치합니다. 그는 2026년 1월 21일 발표한 '2026년 유튜브의 비전'이라는 글에서 AI 콘텐츠에 대한 회사의 입장을 명확히 밝혔어요.\n모한 CEO는 AI가 창의적인 사람들에게 엄청난 기회가 될 것이라고 말하며, 포토샵이나 CGI 같은 도구에 비유했습니다. \"AI는 표현을 위한 도구이지, 인간을 대체하는 것이 아니다\"라고 강조했죠. 저도 이 말에 정말 공감하는데요, 결국 기술은 사람이 어떻게 쓰느냐에 따라 달라지는 것 아니겠어요?\n하지만 그는 AI 콘텐츠의 위험성도 동시에 지적했습니다. 진짜 영상과 AI 생성 영상을 구별하기 어려워지는 현실을 우려하며, \"커뮤니티 가이드라인을 위반하는 유해한 합성 미디어를 제거하고, 딥페이크를 식별하고 차단하는 도구를 크리에이터에게 제공할 것\"이라고 밝혔습니다. 이는 창의성을 존중하되, 윤리적 책임도 다하겠다는 유튜브의 강한 의지를 보여주는 대목이죠.\n특히 흥미로웠던 점은 그가 직접 'AI 슬롭 관리(Managing AI slop)'라는 섹션을 만들었다는 거예요. 모한 CEO는 유튜브의 목표가 '자유로운 표현의 장'인 동시에 '사람들이 시간을 보내면서 기분 좋게 느낄 수 있는 곳'이라고 말했습니다. 이를 위해 \"저품질 AI 콘텐츠의 확산을 줄이기 위해 스팸 및 클릭베이트 퇴치에 성공했던 기존 시스템을 적극적으로 활용하고 있다\"고 덧붙였죠.\n결론적으로 유튜브는 AI 생성 콘텐츠 자체를 반대하는 것이 아니라, 의미 없고 반복적이며 질 낮은 '슬롭'을 근절하겠다는 분명한 선을 그은 것입니다. 이는 유튜브 사용자 모두에게 좋은 소식일 뿐만 아니라, 앞으로 AI 콘텐츠를 제작하려는 크리에이터들에게도 명확한 가이드라인이 될 것이라고 생각합니다.\n⚠️ 경고: AI 콘텐츠 제작 시 유의할 점!\n유튜브는 AI 생성 콘텐츠에 대한 투명성 의무를 강화하고 있습니다. 콘텐츠가 AI에 의해 변경되었거나 생성된 경우, 이를 명확히 밝히지 않으면 불이익을 받을 수 있습니다. 항상 유튜브의 커뮤니티 가이드라인을 준수해주세요.\n  고품질 AI 콘텐츠를 위한 크리에이터 생존 전략\n그렇다면 우리는 이 변화의 물결 속에서 어떻게 살아남고, 심지어 더 큰 기회를 만들 수 있을까요? 제 생각에는, 결국 '인간적인 터치'와 '가치 전달'에 달려 있다고 봅니다. 단순히 AI가 뽑아낸 결과물을 그대로 올리는 것을 넘어, 크리에이터의 역할이 더욱 중요해지는 시점이죠. 다음은 제가 제안하는 고품질 AI 콘텐츠 생존 전략입니다.\n1. 인간적인 독창성과 통찰력을 더하라\nAI는 데이터를 학습하고 패턴을 찾아내는 데는 탁월하지만, 고유한 경험, 감정, 그리고 깊이 있는 통찰력을 만들어내지는 못합니다. 여러분만의 시각, 분석, 혹은 개인적인 스토리를 AI 콘텐츠에 녹여내세요. 예를 들어, AI로 생성된 정보를 바탕으로 하더라도, 그 정보를 해석하고 독자에게 공감을 불러일으킬 만한 자신만의 서사를 추가하는 식이죠. 결국 사람의 마음을 움직이는 건 사람의 이야기 아니겠어요?\n2. AI 사용의 투명성을 확보하라\n유튜브는 2026년부터 AI 생성 콘텐츠에 대한 표시 의무를 강화하고 있습니다. 이제 단순히 'AI가 만들었나?' 하고 의심받는 것을 넘어, 크리에이터 스스로 AI 사용 여부를 명확히 밝히는 것이 신뢰를 구축하는 핵심이 될 것입니다. 시청자들은 정직함을 더 높이 평가할 거예요. 저라면, 영상 시작이나 설명란에 '이 영상은 AI 기술의 도움을 받아 제작되었습니다'라고 명확히 밝힐 것 같아요.\n3. '양'보다 '질'에 집중하라\n저품질 AI 슬롭이 넘쳐나는 시대에, 대량 생산은 더 이상 답이 아닙니다. 오히려 하나의 고품질 콘텐츠에 시간과 노력을 투자하는 것이 훨씬 효과적입니다. 좋은 프롬프트 엔지니어링, AI 결과물의 세심한 후반 작업, 그리고 인간의 검토와 편집은 필수적입니다. AI를 단순한 생산 도구로 생각하기보다, 여러분의 창의적인 비전을 실현하는 '협력자'로 활용해야 합니다.\n4. 커뮤니티와 적극적으로 소통하라\n진정성 있는 커뮤니케이션은 AI가 대체할 수 없는 인간 고유의 영역입니다. 시청자의 댓글에 응답하고, 라이브 스트리밍을 통해 직접 소통하며, 설문조사 등을 통해 콘텐츠 기획에 참여시키는 것은 강력한 팬덤을 구축하는 데 도움이 됩니다. 결국 팬들은 AI가 아닌, 여러분이라는 인간 크리에이터와 소통하고 싶어 할 거예요.\n\n\n정리하자면, 2026년 유튜브는 AI 콘텐츠의 홍수 속에서 '질'과 '인간적인 가치'를 더욱 중요하게 여기는 방향으로 진화하고 있습니다. 아래 표를 통해 'AI 슬롭'과 '고품질 AI 콘텐츠'의 차이를 한눈에 비교해보세요.\n항목\nAI 슬롭\n고품질 AI 콘텐츠\n\n\n\n\n주요 목표\n조회수, 광고 수익 극대화\n가치 제공, 시청자 만족, 브랜드 구축\n\n\n콘텐츠 특징\n반복적, 자극적, 의미 없음, 독창성 결여\n독창적, 통찰력, 고품질 편집, 인간적 감성\n\n\nAI 활용 방식\n생성 도구에 전적으로 의존, 무단 사용\n크리에이터의 비전을 돕는 보조 도구\n\n\n플랫폼 반응\n삭제 및 채널 제재\n긍정적 평가, 노출 기회 증가\n\n\n\n\n\n  핵심 요약\n1. 유튜브는 저품질 'AI 슬롭'과의 전쟁을 선포했습니다. 2026년 현재 수십억 뷰가 삭제되고 수많은 채널이 제재를 받았습니다.\n2. 닐 모한 CEO는 AI를 '표현의 도구'로 보며, 유해하거나 질 낮은 AI 콘텐츠에 대한 강력한 조치를 예고했습니다.\n3. 크리에이터는 인간적인 독창성, 투명성, 품질 중심 전략으로 변화에 대응해야 합니다.\n4. AI는 단순한 생산 도구가 아닌, '인간 크리에이터의 가치를 높이는 협력자'로서 활용되어야 합니다.\n*위 내용은 2026년 2월 12일 기준 유튜브 정책 및 시장 동향을 바탕으로 작성되었습니다.\n❓ 자주 묻는 질문 (FAQ)\nQ1: 2026년에도 AI 콘텐츠를 유튜브에 올려도 괜찮을까요?\nA1: 네, 물론입니다. 유튜브는 AI 콘텐츠 자체를 금지하는 것이 아니라, 저품질의 'AI 슬롭'을 제재하는 것입니다. 중요한 것은 콘텐츠에 여러분의 독창적인 아이디어와 인간적인 가치를 담고, AI 사용 여부를 투명하게 공개하는 것입니다. AI는 강력한 창작 도구가 될 수 있습니다.\nQ2: 유튜브가 'AI 슬롭'을 판단하는 구체적인 기준은 무엇인가요?\nA2: 유튜브는 공식적으로 구체적인 기준을 명시하지는 않지만, 닐 모한 CEO의 발언을 통해 유추할 수 있습니다. 반복적이고 의미 없는 내용, 조회수나 광고 수익만을 위한 자극적인 콘텐츠, 커뮤니티 가이드라인을 위반하는 유해한 합성 미디어 등이 주요 대상입니다. 또한, AI 생성 여부를 숨기거나 오해를 불러일으키는 콘텐츠도 제재 대상이 될 수 있습니다.\nQ3: AI 콘텐츠 제작 시 크리에이터가 가장 중요하게 생각해야 할 것은 무엇인가요?\nA3: 단연 '품질'과 '진정성'입니다. AI는 보조 도구일 뿐, 콘텐츠의 기획, 스토리텔링, 편집 등 모든 과정에 크리에이터의 인간적인 노력과 통찰력이 들어가야 합니다. 시청자에게 실질적인 가치를 제공하고, 신뢰를 쌓는 것이 장기적인 성공의 열쇠입니다.\n2026년, 유튜브의 AI 콘텐츠 생태계는 새로운 전환점을 맞이하고 있습니다. '슬롭'을 피하고 고품질 콘텐츠로 인정받는 것은 더 이상 선택이 아닌 필수입니다. 여러분의 창의적인 열정과 인간적인 통찰력을 담아, 변화하는 유튜브에서 새로운 기회를 잡으시길 바랍니다! 감사합니다.",
        "guid": "https://muzbox.tistory.com/483708",
        "categories": [
          "AI, 미래기술/AI 인사이트",
          "ai 생성 콘텐츠",
          "ai 슬롭",
          "고품질 AI 영상",
          "닐 모한 CEO",
          "유튜브 AI 콘텐츠",
          "유튜브 정책 2026",
          "유튜브 채널 성장",
          "유튜브 커뮤니티 가이드라인",
          "인간적인 AI 콘텐츠",
          "크리에이터 생존 전략"
        ],
        "isoDate": "2026-02-11T23:12:24.000Z"
      }
    ]
  },
  {
    "name": "동우리의 블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "하테나",
    "category": "개인",
    "posts": []
  },
  {
    "name": "늑돌이네 라지온",
    "category": "개인",
    "posts": []
  },
  {
    "name": "루리웹 - 루리웹 리뷰 게시판",
    "category": "게임",
    "posts": [
      {
        "creator": "｜RULIWEB｜",
        "title": "힘차게 달려라 설국열차999, 프로스트레인 2",
        "link": "https://bbs.ruliweb.com/news/board/11/read/2421",
        "pubDate": "Sun, 15 Feb 2026 18:39:47 +0900",
        "author": "｜RULIWEB｜",
        "content": "<img width=\"236\" height=\"177\" src=\"https://i3.ruliweb.com/thumb/26/02/15/19c60a8cd8b51ad6b.webp\">",
        "contentSnippet": "",
        "categories": [
          "게임툰"
        ],
        "isoDate": "2026-02-15T09:39:47.000Z"
      },
      {
        "creator": "｜RULIWEB｜",
        "title": "악역영애 4컷 만화 - 39화, 계획대로데스와",
        "link": "https://bbs.ruliweb.com/news/board/11/read/2420",
        "pubDate": "Wed, 11 Feb 2026 22:40:27 +0900",
        "author": "｜RULIWEB｜",
        "content": "<img width=\"236\" height=\"177\" src=\"https://i3.ruliweb.com/thumb/26/02/11/19c4cedee3c51ad6b.webp\">",
        "contentSnippet": "",
        "categories": [
          "웹툰"
        ],
        "isoDate": "2026-02-11T13:40:27.000Z"
      }
    ]
  },
  {
    "name": "Reasontobe",
    "category": "개인",
    "posts": []
  },
  {
    "name": "에스티마의 인터넷이야기 EstimaStory.com",
    "category": "개인",
    "posts": []
  },
  {
    "name": "나긋한 개발 - 데비안 리눅스와 프로그램 언어",
    "category": "개인",
    "posts": [
      {
        "creator": "summerandwinter",
        "title": "LVM 파티션 사용하는 디스크에 용량 추가 후 작업",
        "link": "https://sacstory.tistory.com/entry/LVM-%ED%8C%8C%ED%8B%B0%EC%85%98-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%94%94%EC%8A%A4%ED%81%AC%EC%97%90-%EC%9A%A9%EB%9F%89-%EC%B6%94%EA%B0%80-%ED%9B%84-%EC%9E%91%EC%97%85",
        "pubDate": "Mon, 16 Feb 2026 15:46:03 +0900",
        "author": "summerandwinter",
        "comments": "https://sacstory.tistory.com/entry/LVM-%ED%8C%8C%ED%8B%B0%EC%85%98-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%94%94%EC%8A%A4%ED%81%AC%EC%97%90-%EC%9A%A9%EB%9F%89-%EC%B6%94%EA%B0%80-%ED%9B%84-%EC%9E%91%EC%97%85#entry398comment",
        "content": "추가된 디스크 용량 확인\nlsblk\n&nbsp;\n추가된 용량 할당\ncfdisk\n&nbsp;\n&nbsp;\n물리 사이즈 재인식토록 갱신\npvresize /dev/sda3\n&nbsp;\n논리 파티션에 용량 추가\nlvextend -r -l +100%FREE /dev/mapper/DBMS--vg-var",
        "contentSnippet": "추가된 디스크 용량 확인\nlsblk\n \n추가된 용량 할당\ncfdisk\n \n \n물리 사이즈 재인식토록 갱신\npvresize /dev/sda3\n \n논리 파티션에 용량 추가\nlvextend -r -l +100%FREE /dev/mapper/DBMS--vg-var",
        "guid": "https://sacstory.tistory.com/398",
        "categories": [
          "리눅스/리눅스 - 데비안 계열"
        ],
        "isoDate": "2026-02-16T06:46:03.000Z"
      }
    ]
  },
  {
    "name": "일상을 여행처럼...",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Just hack'em",
    "category": "개인",
    "posts": []
  },
  {
    "name": "C++ Truths",
    "category": "개인",
    "posts": []
  },
  {
    "name": "jacking75",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Joel on Software",
    "category": "개인",
    "posts": []
  },
  {
    "name": "벤자민로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "악보쓰는 프로그래머",
    "category": "개인",
    "posts": []
  },
  {
    "name": "쭌안아빠",
    "category": "개인",
    "posts": []
  },
  {
    "name": "A Gangster World",
    "category": "개인",
    "posts": []
  },
  {
    "name": "요우의 내맘대로 블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "개발자스럽다",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "Against All Odds.",
    "category": "개인",
    "posts": []
  },
  {
    "name": "움직이는 게임서버",
    "category": "개인",
    "posts": []
  },
  {
    "name": "이상욱",
    "category": "개인",
    "posts": []
  },
  {
    "name": "임철재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "어쩐지 오늘은",
    "category": "개인",
    "posts": []
  },
  {
    "name": "oddpoet’s étude",
    "category": "개인",
    "posts": []
  },
  {
    "name": "0x00 - NULL",
    "category": "개인",
    "posts": []
  },
  {
    "name": "퇴근 후 서버다운",
    "category": "개인",
    "posts": [
      {
        "creator": "SIDNFT",
        "title": "유니티 강좌 - 픽셀아트를 위한 2D 라이팅 같이 배우기 / 골드메탈",
        "link": "https://serverdown.tistory.com/1573",
        "pubDate": "Sun, 15 Feb 2026 23:08:53 +0900",
        "author": "SIDNFT",
        "comments": "https://serverdown.tistory.com/1573#entry1573comment",
        "content": "<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"184\" data-origin-height=\"156\"><span data-url=\"https://blog.kakaocdn.net/dn/kRPay/dJMcacWk5KF/vP5EMsruuu7j6RT4Alu8iK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/kRPay/dJMcacWk5KF/vP5EMsruuu7j6RT4Alu8iK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/kRPay/dJMcacWk5KF/vP5EMsruuu7j6RT4Alu8iK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FkRPay%2FdJMcacWk5KF%2FvP5EMsruuu7j6RT4Alu8iK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"184\" height=\"156\" data-origin-width=\"184\" data-origin-height=\"156\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">영상: <a href=\"https://www.youtube.com/watch?v=rR5SrfRX4qU\" target=\"_blank\" rel=\"noopener&nbsp;noreferrer\">https://www.youtube.com/watch?v=rR5SrfRX4qU</a></p>\n<figure data-ke-type=\"video\" data-ke-style=\"alignCenter\" data-video-host=\"youtube\" data-video-url=\"https://www.youtube.com/watch?v=rR5SrfRX4qU\" data-video-thumbnail=\"https://scrap.kakaocdn.net/dn/mxeFJ/dJMb8TB5R5q/PBz1tO70QsjIViHlMdAYd0/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720,https://scrap.kakaocdn.net/dn/b6GUZ3/dJMb8SXt5Qv/y2w5yj6e9K3Em1iugK8Kik/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720\" data-video-width=\"860\" data-video-height=\"484\" data-video-origin-width=\"860\" data-video-origin-height=\"484\" data-ke-mobilestyle=\"widthContent\" data-video-title=\"픽셀아트를 위한 2D 라이팅 같이 배우기\" data-original-url=\"\"><iframe src=\"https://www.youtube.com/embed/rR5SrfRX4qU\" width=\"860\" height=\"484\" frameborder=\"\" allowfullscreen=\"true\"></iframe>\n<figcaption style=\"display: none;\"></figcaption>\n</figure>\n<p data-ke-size=\"size16\">여러가지 몰랐던 UI 들을 사용하네요</p>\n<p data-ke-size=\"size16\">나중에 진지하게 봐야겠당</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">비주얼스크립팅</p>\n<p data-ke-size=\"size16\">글로벌 광원</p>\n<p data-ke-size=\"size16\">픽셀퍼팩트</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"236\" data-origin-height=\"217\"><span data-url=\"https://blog.kakaocdn.net/dn/cDLaMc/dJMcafMk5NS/ZroP3msJWidUQQapiKPXYk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/cDLaMc/dJMcafMk5NS/ZroP3msJWidUQQapiKPXYk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/cDLaMc/dJMcafMk5NS/ZroP3msJWidUQQapiKPXYk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcDLaMc%2FdJMcafMk5NS%2FZroP3msJWidUQQapiKPXYk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"236\" height=\"217\" data-origin-width=\"236\" data-origin-height=\"217\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">라이트</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"262\" data-origin-height=\"279\"><span data-url=\"https://blog.kakaocdn.net/dn/tpSen/dJMcad1YSDs/QBtrmymu9zKKMkyB0EChkK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/tpSen/dJMcad1YSDs/QBtrmymu9zKKMkyB0EChkK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/tpSen/dJMcad1YSDs/QBtrmymu9zKKMkyB0EChkK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FtpSen%2FdJMcad1YSDs%2FQBtrmymu9zKKMkyB0EChkK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"262\" height=\"279\" data-origin-width=\"262\" data-origin-height=\"279\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">크림자 영역</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"586\" data-origin-height=\"405\"><span data-url=\"https://blog.kakaocdn.net/dn/b22tMo/dJMcabiRUKC/xdxL6ezhwZyGWNJTH4otK0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/b22tMo/dJMcabiRUKC/xdxL6ezhwZyGWNJTH4otK0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/b22tMo/dJMcabiRUKC/xdxL6ezhwZyGWNJTH4otK0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb22tMo%2FdJMcabiRUKC%2FxdxL6ezhwZyGWNJTH4otK0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"586\" height=\"405\" data-origin-width=\"586\" data-origin-height=\"405\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">2D 셰이더</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"152\" data-origin-height=\"136\"><span data-url=\"https://blog.kakaocdn.net/dn/rqxTG/dJMcagLeQPi/L3CXkXnsnY82pK2qRf0Nz1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/rqxTG/dJMcagLeQPi/L3CXkXnsnY82pK2qRf0Nz1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/rqxTG/dJMcagLeQPi/L3CXkXnsnY82pK2qRf0Nz1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FrqxTG%2FdJMcagLeQPi%2FL3CXkXnsnY82pK2qRf0Nz1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"152\" height=\"136\" data-origin-width=\"152\" data-origin-height=\"136\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">빛나는 외곽선</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"286\" data-origin-height=\"225\"><span data-url=\"https://blog.kakaocdn.net/dn/7pOS8/dJMcahwCchr/77m8s6DzngSWTKU9KEgL10/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/7pOS8/dJMcahwCchr/77m8s6DzngSWTKU9KEgL10/img.png\"><img src=\"https://blog.kakaocdn.net/dn/7pOS8/dJMcahwCchr/77m8s6DzngSWTKU9KEgL10/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F7pOS8%2FdJMcahwCchr%2F77m8s6DzngSWTKU9KEgL10%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"286\" height=\"225\" data-origin-width=\"286\" data-origin-height=\"225\"/></span></figure>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"252\" data-origin-height=\"201\"><span data-url=\"https://blog.kakaocdn.net/dn/5c2ZK/dJMcadVdLV9/WkDBry2Tk7c8JBK6FQgjxK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/5c2ZK/dJMcadVdLV9/WkDBry2Tk7c8JBK6FQgjxK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/5c2ZK/dJMcadVdLV9/WkDBry2Tk7c8JBK6FQgjxK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F5c2ZK%2FdJMcadVdLV9%2FWkDBry2Tk7c8JBK6FQgjxK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"252\" height=\"201\" data-origin-width=\"252\" data-origin-height=\"201\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">하이트맵 뚝딱 (2D 노멀맵)</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"282\" data-origin-height=\"300\"><span data-url=\"https://blog.kakaocdn.net/dn/mpIIN/dJMcahi5IOr/A6ERPqwdtuG4wW1reuBClK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/mpIIN/dJMcahi5IOr/A6ERPqwdtuG4wW1reuBClK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/mpIIN/dJMcahi5IOr/A6ERPqwdtuG4wW1reuBClK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FmpIIN%2FdJMcahi5IOr%2FA6ERPqwdtuG4wW1reuBClK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"282\" height=\"300\" data-origin-width=\"282\" data-origin-height=\"300\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">2D 스프라이트 애니메이션 전부에 적용</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"215\" data-origin-height=\"211\"><span data-url=\"https://blog.kakaocdn.net/dn/ODbXF/dJMcac9P43N/RlpOdMH7NHgOjenK7h05h1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/ODbXF/dJMcac9P43N/RlpOdMH7NHgOjenK7h05h1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/ODbXF/dJMcac9P43N/RlpOdMH7NHgOjenK7h05h1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FODbXF%2FdJMcac9P43N%2FRlpOdMH7NHgOjenK7h05h1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"215\" height=\"211\" data-origin-width=\"215\" data-origin-height=\"211\"/></span></figure>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"210\" data-origin-height=\"186\"><span data-url=\"https://blog.kakaocdn.net/dn/dkJ2ut/dJMcabwmMOX/IkxLcPTJ7CbkamtSsrW5H0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/dkJ2ut/dJMcabwmMOX/IkxLcPTJ7CbkamtSsrW5H0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/dkJ2ut/dJMcabwmMOX/IkxLcPTJ7CbkamtSsrW5H0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdkJ2ut%2FdJMcabwmMOX%2FIkxLcPTJ7CbkamtSsrW5H0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"210\" height=\"186\" data-origin-width=\"210\" data-origin-height=\"186\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">환경광 먹은거</p>\n<p data-ke-size=\"size16\">하이트맵 먹은 부분의 밝기가 달라지나봅니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "영상: https://www.youtube.com/watch?v=rR5SrfRX4qU\n\n\n\n여러가지 몰랐던 UI 들을 사용하네요\n나중에 진지하게 봐야겠당\n \n비주얼스크립팅\n글로벌 광원\n픽셀퍼팩트\n\n\n라이트\n\n\n크림자 영역\n\n\n2D 셰이더\n\n\n빛나는 외곽선\n\n\n\n하이트맵 뚝딱 (2D 노멀맵)\n\n\n2D 스프라이트 애니메이션 전부에 적용\n\n\n\n \n환경광 먹은거\n하이트맵 먹은 부분의 밝기가 달라지나봅니다.",
        "guid": "https://serverdown.tistory.com/1573",
        "categories": [
          "프로그래밍/개발메모"
        ],
        "isoDate": "2026-02-15T14:08:53.000Z"
      },
      {
        "creator": "SIDNFT",
        "title": "423일차 간단한밸런스맞추기 1일차 / AddForce1Laser",
        "link": "https://serverdown.tistory.com/1572",
        "pubDate": "Fri, 13 Feb 2026 03:46:49 +0900",
        "author": "SIDNFT",
        "comments": "https://serverdown.tistory.com/1572#entry1572comment",
        "content": "<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"1920\" data-origin-height=\"1080\"><span data-url=\"https://blog.kakaocdn.net/dn/cHN7cr/dJMcaibcxH1/mOrgx5BqxMWgklQcBsXgdK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/cHN7cr/dJMcaibcxH1/mOrgx5BqxMWgklQcBsXgdK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/cHN7cr/dJMcaibcxH1/mOrgx5BqxMWgklQcBsXgdK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcHN7cr%2FdJMcaibcxH1%2FmOrgx5BqxMWgklQcBsXgdK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1920\" height=\"1080\" data-origin-width=\"1920\" data-origin-height=\"1080\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">영상: <a href=\"https://www.youtube.com/watch?v=-Im8F-JHlmc\" target=\"_blank\" rel=\"noopener&nbsp;noreferrer\">https://www.youtube.com/watch?v=-Im8F-JHlmc</a></p>\n<figure data-ke-type=\"video\" data-ke-style=\"alignCenter\" data-video-host=\"youtube\" data-video-url=\"https://www.youtube.com/watch?v=-Im8F-JHlmc\" data-video-thumbnail=\"https://scrap.kakaocdn.net/dn/bsHxmh/dJMb9c9tZ7Z/iiOmNtTzb1CvBGsm4hnLNk/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720,https://scrap.kakaocdn.net/dn/MAfdd/dJMb9bvYoMa/Vqke4bGqTtxrKkVUHZsnV0/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720,https://scrap.kakaocdn.net/dn/QM5uC/dJMb9kl8T1j/4yk0nmEsL5W24FrDXbAFG0/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720\" data-video-width=\"860\" data-video-height=\"484\" data-video-origin-width=\"860\" data-video-origin-height=\"484\" data-ke-mobilestyle=\"widthContent\" data-video-title=\"423일차 간단한밸런스맞추기 1일차\" data-original-url=\"\"><iframe src=\"https://www.youtube.com/embed/-Im8F-JHlmc\" width=\"860\" height=\"484\" frameborder=\"\" allowfullscreen=\"true\"></iframe>\n<figcaption style=\"display: none;\"></figcaption>\n</figure>\n<p data-ke-size=\"size16\">영상 퀄리티가 열악합니다.</p>\n<p data-ke-size=\"size16\">말도 잘 못하고</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">영상에서 제대로 설명을 못해서 여기에 제대로 적겠습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">위험도</h2>\n<p data-ke-size=\"size16\">수치를 바꾸다 보면 더 어려워졌는지 쉬워졌는지 판단이 어렵습니다.</p>\n<p data-ke-size=\"size16\">그래서 위험도라는 숫자 하나로 계산할 수 있도록 했습니다.</p>\n<p data-ke-size=\"size16\">계산 공식은&nbsp;</p>\n<p data-ke-size=\"size16\">체력 * 속도 * 1/생성시간 = 위험도&nbsp;</p>\n<p data-ke-size=\"size16\">입니다.</p>\n<p data-ke-size=\"size16\">체력: 1<br />속도: 2<br />생성시간: 1초<br />면 위헙도 2 가 됩니다.</p>\n<p data-ke-size=\"size16\">체력: 2<br />속도: 1<br />생성시간: 1초<br />이래도 위험도는 2 가 됩니다.</p>\n<p data-ke-size=\"size16\">즉 두개의 적은 동인한 난이도 라는 것읍니다.</p>\n<p data-ke-size=\"size16\">위험도른 기준으로 정렬해가며 정을 등장시키면 적덜한 난이도가 될 것입니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">데이터를 적용하는 단계</h2>\n<p data-ke-size=\"size16\">데이터는 구글 시트에 있고</p>\n<p data-ke-size=\"size16\">게임은 유니티로 만들어졌습니다.</p>\n<p data-ke-size=\"size16\">유니티에서 구글 시트를 직접 읽지는 못하고 <br />JSON 파일로 뽑아서 전달하게 됩니다.</p>\n<p data-ke-size=\"size16\">작업순서는</p>\n<p data-ke-size=\"size16\">1. 구글시트 수정<br />2. NODE.JS 로 구글시트를 읽음<br />3. JSON 파일로 만들어 유니티 폴더에 넣어둠<br />4. 유니티의 c# 에서 JsonUtility.FromJson 을 이용해 읽습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">설명하는 능력이 부족해서 방송 연습<br />+ 디펜스 게임을 만드는 과정을 남기는 것 만으로도 의미는 있을것 같습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">게임 설명 페이지 - <a href=\"https://apps.sidnft.com/af\" target=\"_blank\" rel=\"noopener&nbsp;noreferrer\">https://apps.sidnft.com/af</a></p>\n<p data-ke-size=\"size16\">게임은 구글 플레이 스토어에 올라가 있습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "영상: https://www.youtube.com/watch?v=-Im8F-JHlmc\n\n\n\n영상 퀄리티가 열악합니다.\n말도 잘 못하고\n \n영상에서 제대로 설명을 못해서 여기에 제대로 적겠습니다.\n \n위험도\n수치를 바꾸다 보면 더 어려워졌는지 쉬워졌는지 판단이 어렵습니다.\n그래서 위험도라는 숫자 하나로 계산할 수 있도록 했습니다.\n계산 공식은 \n체력 * 속도 * 1/생성시간 = 위험도 \n입니다.\n체력: 1\n속도: 2\n생성시간: 1초\n면 위헙도 2 가 됩니다.\n체력: 2\n속도: 1\n생성시간: 1초\n이래도 위험도는 2 가 됩니다.\n즉 두개의 적은 동인한 난이도 라는 것읍니다.\n위험도른 기준으로 정렬해가며 정을 등장시키면 적덜한 난이도가 될 것입니다.\n \n데이터를 적용하는 단계\n데이터는 구글 시트에 있고\n게임은 유니티로 만들어졌습니다.\n유니티에서 구글 시트를 직접 읽지는 못하고 \nJSON 파일로 뽑아서 전달하게 됩니다.\n작업순서는\n1. 구글시트 수정\n2. NODE.JS 로 구글시트를 읽음\n3. JSON 파일로 만들어 유니티 폴더에 넣어둠\n4. 유니티의 c# 에서 JsonUtility.FromJson 을 이용해 읽습니다.\n \n설명하는 능력이 부족해서 방송 연습\n+ 디펜스 게임을 만드는 과정을 남기는 것 만으로도 의미는 있을것 같습니다.\n \n게임 설명 페이지 - https://apps.sidnft.com/af\n게임은 구글 플레이 스토어에 올라가 있습니다.",
        "guid": "https://serverdown.tistory.com/1572",
        "categories": [
          "프로그래밍/자작",
          "AddForce1Laser"
        ],
        "isoDate": "2026-02-12T18:46:49.000Z"
      }
    ]
  },
  {
    "name": "coolspeed",
    "category": "개인",
    "posts": []
  },
  {
    "name": "오늘도 끄적끄적",
    "category": "개인",
    "posts": []
  },
  {
    "name": "dx11 Vanica's Lifelog - 夢が夢で終わらないように",
    "category": "개인",
    "posts": []
  },
  {
    "name": "초코사랑",
    "category": "개인",
    "posts": []
  },
  {
    "name": "ZeroCho Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "imays게임엔진개발자",
    "category": "개인",
    "posts": []
  },
  {
    "name": "RSS feed for hurinmon Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "기억보단 기록을",
    "category": "개인",
    "posts": [
      {
        "creator": "향로 (기억보단 기록을)",
        "title": "AI 시대에는 어떤 글을 써야할까?",
        "link": "https://jojoldu.tistory.com/865",
        "pubDate": "Sun, 15 Feb 2026 23:01:24 +0900",
        "author": "향로 (기억보단 기록을)",
        "comments": "https://jojoldu.tistory.com/865#entry865comment",
        "content": "<p data-ke-size=\"size16\">최근에 BC 신용카드를 해지하려고 했다.<br />BC 카드는 페이북이 공식 앱이라 여기서 해지를 하려고 했다.<br />그런데 아무리 찾아봐도 카드 해지가 없었다.<br />분명 여러 블로그 글에는 페이북 앱에서 <code>해지</code> 검색 -&gt; <code>카드 해지</code> 를 하면 된다고 나오는데도 말이다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"1.jpg\" data-origin-width=\"1080\" data-origin-height=\"2400\"><span data-url=\"https://blog.kakaocdn.net/dn/dnqJPJ/dJMcagxFwh1/sX4JTKL8EWypHl1Ke8iDU0/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/dnqJPJ/dJMcagxFwh1/sX4JTKL8EWypHl1Ke8iDU0/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/dnqJPJ/dJMcagxFwh1/sX4JTKL8EWypHl1Ke8iDU0/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdnqJPJ%2FdJMcagxFwh1%2FsX4JTKL8EWypHl1Ke8iDU0%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"406\" height=\"902\" data-filename=\"1.jpg\" data-origin-width=\"1080\" data-origin-height=\"2400\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">주말이라 고객센터 연락이 안 될 것 같아 앱 내에서 지원하는 AI 챗봇에게 물어봤다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"2.jpg\" data-origin-width=\"1080\" data-origin-height=\"2400\"><span data-url=\"https://blog.kakaocdn.net/dn/RotIf/dJMb99L4MPZ/QIhyzxXoCFXOQCdC2enkA0/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/RotIf/dJMb99L4MPZ/QIhyzxXoCFXOQCdC2enkA0/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/RotIf/dJMb99L4MPZ/QIhyzxXoCFXOQCdC2enkA0/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FRotIf%2FdJMb99L4MPZ%2FQIhyzxXoCFXOQCdC2enkA0%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"405\" height=\"900\" data-filename=\"2.jpg\" data-origin-width=\"1080\" data-origin-height=\"2400\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">구체적인 해지 방법은 안내해주지 못하고 카드 해지 주의 사항에 대해서만 계속 안내했다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"3.jpg\" data-origin-width=\"1080\" data-origin-height=\"2400\"><span data-url=\"https://blog.kakaocdn.net/dn/cm1uyw/dJMcac25Yhq/yIwJeCkhrKfd9HYfpJC2F1/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/cm1uyw/dJMcac25Yhq/yIwJeCkhrKfd9HYfpJC2F1/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/cm1uyw/dJMcac25Yhq/yIwJeCkhrKfd9HYfpJC2F1/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcm1uyw%2FdJMcac25Yhq%2FyIwJeCkhrKfd9HYfpJC2F1%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"382\" height=\"849\" data-filename=\"3.jpg\" data-origin-width=\"1080\" data-origin-height=\"2400\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">답답한 마음에 습관처럼 여러 AI 도구들에게 해지 방법을 물어봤다.</p>\n<p data-ke-size=\"size16\">Claude에게도</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"claude.png\" data-origin-width=\"1668\" data-origin-height=\"1080\"><span data-url=\"https://blog.kakaocdn.net/dn/ca1pJJ/dJMcafev34t/ONQfmcaKKK1AEJG3cAikIk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/ca1pJJ/dJMcafev34t/ONQfmcaKKK1AEJG3cAikIk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/ca1pJJ/dJMcafev34t/ONQfmcaKKK1AEJG3cAikIk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fca1pJJ%2FdJMcafev34t%2FONQfmcaKKK1AEJG3cAikIk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1668\" height=\"1080\" data-filename=\"claude.png\" data-origin-width=\"1668\" data-origin-height=\"1080\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">GPT에게도</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"gpt.png\" data-origin-width=\"1930\" data-origin-height=\"1004\"><span data-url=\"https://blog.kakaocdn.net/dn/bB0FwB/dJMcac9P4SD/VDGKS3OtsT6lE9OsNlUuUk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bB0FwB/dJMcac9P4SD/VDGKS3OtsT6lE9OsNlUuUk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bB0FwB/dJMcac9P4SD/VDGKS3OtsT6lE9OsNlUuUk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbB0FwB%2FdJMcac9P4SD%2FVDGKS3OtsT6lE9OsNlUuUk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1930\" height=\"1004\" data-filename=\"gpt.png\" data-origin-width=\"1930\" data-origin-height=\"1004\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">Gemini에게도</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"gemini.png\" data-origin-width=\"1630\" data-origin-height=\"1172\"><span data-url=\"https://blog.kakaocdn.net/dn/cZLDJM/dJMcaihXG4U/0BnwQX1QupeY0IBb98rJRK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/cZLDJM/dJMcaihXG4U/0BnwQX1QupeY0IBb98rJRK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/cZLDJM/dJMcaihXG4U/0BnwQX1QupeY0IBb98rJRK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcZLDJM%2FdJMcaihXG4U%2F0BnwQX1QupeY0IBb98rJRK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1630\" height=\"1172\" data-filename=\"gemini.png\" data-origin-width=\"1630\" data-origin-height=\"1172\"/></span></figure>\n</p>\n<blockquote data-ke-style=\"style1\">\n<p data-ke-size=\"size16\">참고로 3개 AI 서비스 모두 가장 높은 모드로 질문했다.</p>\n</blockquote>\n<p data-ke-size=\"size16\"><b>모두가 페이북 앱 내에 해지가 있다고 안내했다</b>.</p>\n<p data-ke-size=\"size16\">좀 더 찾아보니 BC 카드는 예전에 BC카드 앱이 별도로 있다가 페이북 앱으로 전환이 되었다.<br />과거의 페이북 앱에는 직접 해지가 있었지만, 최근의 페이북 앱에는 해지 기능이 없어졌다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"4.png\" data-origin-width=\"1196\" data-origin-height=\"596\"><span data-url=\"https://blog.kakaocdn.net/dn/rfnFK/dJMcaihXG5y/EvLILR0zhnaJZJedwSKZhk/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/rfnFK/dJMcaihXG5y/EvLILR0zhnaJZJedwSKZhk/img.png\"><img src=\"https://blog.kakaocdn.net/dn/rfnFK/dJMcaihXG5y/EvLILR0zhnaJZJedwSKZhk/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FrfnFK%2FdJMcaihXG5y%2FEvLILR0zhnaJZJedwSKZhk%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1196\" height=\"596\" data-filename=\"4.png\" data-origin-width=\"1196\" data-origin-height=\"596\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">그래서 대부분의 블로그 글이 과거의 BC카드 앱이나 과거의 페이북 앱 기준으로 작성된 글이 많았다.<br />아무래도 AI는 과거의 데이터를 기반으로 답변을 하니 어쩔 수 없단 생각에 직접 찾아보기로 했다.</p>\n<p data-ke-size=\"size16\">가장 최신의 방법을 찾으면 되니, 최신 기준으로 네이버 검색을 다시 했다.</p>\n<p data-ke-size=\"size16\">그런데 <b>가장 최신의 글들도 대부분이 AI가 작성한 블로그 글이었고, 그 AI는 예전에 작성된 글을 참고해서 여전히 과거의 해지 방법을 소개</b>하고 있었다.</p>\n<p data-ke-size=\"size16\">최신순으로 검색해도 내가 원하는 정보를 못 찾는 상황이 된 것이다.</p>\n<p data-ke-size=\"size16\">Claude에게 내가 원하는 조건의 해지 방법을 갖고 있는 네이버 블로그 글을 탐색하도록 시키고, 혹시 몰라 나도 하나씩 읽어보면서 찾아봤다.</p>\n<p data-ke-size=\"size16\">결국엔 <a href=\"https://blog.naver.com/yuretomo9/224108014416\">디지털 ARS로는 해지가 가능하다</a>는 것을 알게 되어서 디지털 ARS로 해지를 신청했다.</p>\n<p data-ke-size=\"size16\">신용카드 하나 해지하는 데 2시간을 보냈다.</p>\n<hr data-ke-style=\"style1\" />\n<p data-ke-size=\"size16\">AI가 답변을 다 해주는데 블로그 글을 쓰는 게 의미가 있는 것인가, 무엇을 써야 하는 것인가에 대한 질문을 자주 받는다.</p>\n<p data-ke-size=\"size16\">이번 일을 겪으면서 오히려 그 반대라는 확신이 생겼다.<br /><b>직접 해본 경험기를 남기는 것</b>이 훨씬 더 중요해졌다.</p>\n<p data-ke-size=\"size16\">AI 시대가 되면서, 어떤 방법이든 해결만 하면 되는 문제는 확실히 예전보다 쉬워졌다.<br />하지만 내가 원하는 특정 방법을 찾는 것은 오히려 더 어려워졌다.<br />실제로 그 일을 하지 않고도 그 일을 한 것처럼 과정을 공유하는 것이 너무나 쉬워졌기 때문이다.</p>\n<p data-ke-size=\"size16\">예전에도 원하는 정보를 찾는 것은 어려웠다.<br />그런데 이젠 가장 최신의 글조차도 과거 글의 복제본이어서 무효한 경우가 다반사가 되었다.</p>\n<p data-ke-size=\"size16\">그래서 점점 이런 것이 중요해진다고 생각한다.</p>\n<p data-ke-size=\"size16\">\"<b>저 사람은 항상 자신이 직접 하는 사람이야, 실제로 본인이 해본 것을 기록하는 사람이야</b>.\"</p>\n<p data-ke-size=\"size16\">작성자보다는 콘텐츠가 중요하다고 하는 시기도 있었지만, 이제는 작성자가 훨씬 더 중요하다.<br /><b>작성자가 신뢰할 만한 사람이냐</b>가 콘텐츠의 가치를 결정하는 시대가 된 것이다.</p>\n<p data-ke-size=\"size16\">그러니 AI의 도움을 받아 더 많이 직접 실행하고 경험해봐야 한다.<br />그리고 그걸 기록으로 남겨야 한다.</p>\n<p data-ke-size=\"size16\">AI로 얻게 된 가장 큰 장점은 더 많은 경험을 더 빠르게 해볼 수 있다는 것이다.<br /><b>AI를 간접 경험의 도구가 아니라, 직접 경험을 더 쉽고 빠르게, 더 깊게 해볼 수 있는 도구로 사용하는 것.</b><br />그것이 앞으로의 시대를 준비하는 가장 좋은 방법이 아닐까 싶다.</p>",
        "contentSnippet": "최근에 BC 신용카드를 해지하려고 했다.\nBC 카드는 페이북이 공식 앱이라 여기서 해지를 하려고 했다.\n그런데 아무리 찾아봐도 카드 해지가 없었다.\n분명 여러 블로그 글에는 페이북 앱에서 해지 검색 -> 카드 해지 를 하면 된다고 나오는데도 말이다.\n\n\n주말이라 고객센터 연락이 안 될 것 같아 앱 내에서 지원하는 AI 챗봇에게 물어봤다.\n\n\n구체적인 해지 방법은 안내해주지 못하고 카드 해지 주의 사항에 대해서만 계속 안내했다.\n\n\n답답한 마음에 습관처럼 여러 AI 도구들에게 해지 방법을 물어봤다.\nClaude에게도\n\n\nGPT에게도\n\n\nGemini에게도\n\n\n\n참고로 3개 AI 서비스 모두 가장 높은 모드로 질문했다.\n모두가 페이북 앱 내에 해지가 있다고 안내했다.\n좀 더 찾아보니 BC 카드는 예전에 BC카드 앱이 별도로 있다가 페이북 앱으로 전환이 되었다.\n과거의 페이북 앱에는 직접 해지가 있었지만, 최근의 페이북 앱에는 해지 기능이 없어졌다.\n\n\n그래서 대부분의 블로그 글이 과거의 BC카드 앱이나 과거의 페이북 앱 기준으로 작성된 글이 많았다.\n아무래도 AI는 과거의 데이터를 기반으로 답변을 하니 어쩔 수 없단 생각에 직접 찾아보기로 했다.\n가장 최신의 방법을 찾으면 되니, 최신 기준으로 네이버 검색을 다시 했다.\n그런데 가장 최신의 글들도 대부분이 AI가 작성한 블로그 글이었고, 그 AI는 예전에 작성된 글을 참고해서 여전히 과거의 해지 방법을 소개하고 있었다.\n최신순으로 검색해도 내가 원하는 정보를 못 찾는 상황이 된 것이다.\nClaude에게 내가 원하는 조건의 해지 방법을 갖고 있는 네이버 블로그 글을 탐색하도록 시키고, 혹시 몰라 나도 하나씩 읽어보면서 찾아봤다.\n결국엔 디지털 ARS로는 해지가 가능하다는 것을 알게 되어서 디지털 ARS로 해지를 신청했다.\n신용카드 하나 해지하는 데 2시간을 보냈다.\nAI가 답변을 다 해주는데 블로그 글을 쓰는 게 의미가 있는 것인가, 무엇을 써야 하는 것인가에 대한 질문을 자주 받는다.\n이번 일을 겪으면서 오히려 그 반대라는 확신이 생겼다.\n직접 해본 경험기를 남기는 것이 훨씬 더 중요해졌다.\nAI 시대가 되면서, 어떤 방법이든 해결만 하면 되는 문제는 확실히 예전보다 쉬워졌다.\n하지만 내가 원하는 특정 방법을 찾는 것은 오히려 더 어려워졌다.\n실제로 그 일을 하지 않고도 그 일을 한 것처럼 과정을 공유하는 것이 너무나 쉬워졌기 때문이다.\n예전에도 원하는 정보를 찾는 것은 어려웠다.\n그런데 이젠 가장 최신의 글조차도 과거 글의 복제본이어서 무효한 경우가 다반사가 되었다.\n그래서 점점 이런 것이 중요해진다고 생각한다.\n\"저 사람은 항상 자신이 직접 하는 사람이야, 실제로 본인이 해본 것을 기록하는 사람이야.\"\n작성자보다는 콘텐츠가 중요하다고 하는 시기도 있었지만, 이제는 작성자가 훨씬 더 중요하다.\n작성자가 신뢰할 만한 사람이냐가 콘텐츠의 가치를 결정하는 시대가 된 것이다.\n그러니 AI의 도움을 받아 더 많이 직접 실행하고 경험해봐야 한다.\n그리고 그걸 기록으로 남겨야 한다.\nAI로 얻게 된 가장 큰 장점은 더 많은 경험을 더 빠르게 해볼 수 있다는 것이다.\nAI를 간접 경험의 도구가 아니라, 직접 경험을 더 쉽고 빠르게, 더 깊게 해볼 수 있는 도구로 사용하는 것.\n그것이 앞으로의 시대를 준비하는 가장 좋은 방법이 아닐까 싶다.",
        "guid": "https://jojoldu.tistory.com/865",
        "categories": [
          "생각정리",
          "AI",
          "AI 생성 콘텐츠",
          "BC카드 해지",
          "바로 클리어 카드 해지",
          "블로그",
          "페이북 카드 해지"
        ],
        "isoDate": "2026-02-15T14:01:24.000Z"
      },
      {
        "creator": "향로 (기억보단 기록을)",
        "title": "회사에서 AI 개발 도구를 지원하지 않는다면",
        "link": "https://jojoldu.tistory.com/864",
        "pubDate": "Sun, 15 Feb 2026 00:52:38 +0900",
        "author": "향로 (기억보단 기록을)",
        "comments": "https://jojoldu.tistory.com/864#entry864comment",
        "content": "<p>요즘 다들 AI 도구에 대한 FOMO를 이야기하지만,<br>이메일로 고민 상담을 들어보면 여전히 AI 도구에 대한 사내의 공식적인 지원이 없는 경우가 많다.<br>(예전에 JetBrains 도구의 지원이 없어서 STS, Eclipse를 쓰던 것처럼..)</p>\n<p>물론, 금융권과 같이 특수하게 높은 보안 레벨을 유지해야 하는 서비스의 특성상 AI 도구 자체가 금지되는 경우도 있다.<br>이런 경우는 당연히 어쩔 수 없다.  </p>\n<p>하지만 그런 도메인의 특성이 없는 경우에도 AI 도구를 지원하지 않는 경우가 많다.<br>그럴 땐 어떻게 해야 할까?  </p>\n<p>10년 전이나 지금이나 개인의 생산성과 성장을 위해 필요하면 개인 비용으로 당연히 사용할 수 있다.</p>\n<p>그렇지만, 개인에 따라 그 비용도 부담스러울 수 있다.<br>요즘의 환율로 계산하면 거의 월 30만원은 돼야 하나의 도구를 제대로 써볼 수 있는 시대에는 특히나 그렇다.  </p>\n<p>지금의 AI 시대를 그냥 회사 탓하면서 보낼 순 없다.<br>그럴 경우엔 개인적으로 Amazon Web Services (AWS) Kiro와 JetBrains Junie, 이 두 가지를 검토해보는 것을 추천한다.</p>\n<p>이유는 단순하다.<br>이 두 도구는 이미 회사에서 쓰고 있을 가능성이 높은 플랫폼 위에 있기 때문이다.  </p>\n<p>JetBrains의 Junie는 All Product 라이선스가 있을 경우 AI Pro 라이선스로 사용할 수 있다.<br>(물론, JetBrains All Product 라이선스를 지원하면 AI 도구도 충분히 지원하는 회사이지 않겠냐고 할 수 있다.<br>이미 JetBrains IDE에 대한 지원을 하고 거기에 추가로 AI 도구까지 지원하는 그 +α의 비용을 부담스러워해서 미지원하는 경우가 생각보다 많다.)  </p>\n<p>이 경우 JetBrains의 Junie는 충분히 AI 개발 경험을 쌓을 수 있는 도구다.</p>\n<p>다른 AI 도구들의 Max Plan처럼 미친 듯이 토큰을 사용할 수 있을 정도는 아니지만, 그래도 어느 정도는 에이전틱 프로그래밍에 대한 경험을 쌓아볼 수는 있다.  </p>\n<p>AWS는 국내에서는 거의 대부분의 서비스가 사용하고 있는 클라우드이다.</p>\n<p>AWS Kiro의 장점은 바로 그 AWS Billing에서 비용이 관리되어 별도의 계약이나 재무적 협의가 필요하지 않다는 점, 그리고 요즘 AWS에서 크레딧 지원을 많이 해준다는 점이다.  </p>\n<p>Claude Code, Codex, Cursor와 같은 도구들은 아무리 좋아도 회사에 도입하려면 새로운 벤더와의 계약, 신규 예산 품의, 보안 검토 등을 처음부터 시작해야 한다.  </p>\n<p>그런데 Kiro와 Junie는 이미 회사가 쓰고 있는 AWS, 젯브레인 생태계 안에 있다.<br>즉, 새로운 벤더를 찾아 계약하는 것이 아니라 큰 협의 과정 없이 시작할 수 있다.</p>\n<p>앞서 이야기한 예산 품의, 보안 심사 같은 산들이 한꺼번에 낮아진다.</p>\n<p>솔직히 AI 코딩 도구의 성능 차이는 몇 달 단위로 뒤집힌다.<br>오늘 A가 좋아도 내일 B가 더 좋아질 수 있고, 그 반대도 마찬가지다.</p>\n<p>그래서 나는 &quot;어떤 도구가 가장 뛰어난가&quot;보다 &quot;어떤 도구를 우리 팀이 가장 빨리 쓸 수 있는가&quot;가 더 중요하다고 생각한다.</p>\n<p>아무리 좋은 도구도 도입까지 6개월이 걸리면 의미가 없다.<br>지금 당장 쓸 수 있는 도구를 먼저 도입하고, 팀이 AI 코딩에 익숙해지는 것이 훨씬 더 가치 있다.</p>\n<p>그런 관점에서 AI 도구를 사용하기 위해 사내에서 협의해야 할 것들이 너무 많다면, AWS Kiro와 JetBrains Junie를 고민해보는 것을 추천한다.</p>\n<hr>\n<p>&quot;좋은 건 알겠는데, 리더에게 어떻게 말씀드리지?&quot; 라는 고민이 있을 수 있다.    </p>\n<p>특히 Kiro의 경우 AWS 위에 있다 보니, 기존 예산 체계나 계정 관리 측면에서 리더를 설득할 수 있는 구체적인 포인트들이 있다.<br>Yan So 님이 이런 부분을 잘 정리해주신 글이 있어서 공유한다.</p>\n<p><a href=\"https://yanlog.yanbert.com/ko/blog/dear-team-lead-lets-adopt-kiro-20260211/\">Kiro를 기업에서 쓰기 좋은 이유 (IAM Identity Center)</a></p>\n<p>&quot;왜 우리 리더들은 AI 도구를 지원하지 않는 것인가&quot;에 대해 리더의 입장에서의 시각과 그들을 설득하기 위한 충분한 근거를 자세히 설명해 주셨다.<br>예산과 비용 처리 관점에서 왜 도입이 지연되는지, 그리고 리더 입장에서 Kiro를 어떤 포인트로 검토하면 좋을지가 실무적으로 정리되어 있다.</p>\n<p>AI 코딩 도구 도입이 아직인 조직이 있다면, 한번 읽어보고 팀 리더에게 제안해보자.</p>",
        "contentSnippet": "요즘 다들 AI 도구에 대한 FOMO를 이야기하지만,\n이메일로 고민 상담을 들어보면 여전히 AI 도구에 대한 사내의 공식적인 지원이 없는 경우가 많다.\n(예전에 JetBrains 도구의 지원이 없어서 STS, Eclipse를 쓰던 것처럼..)\n물론, 금융권과 같이 특수하게 높은 보안 레벨을 유지해야 하는 서비스의 특성상 AI 도구 자체가 금지되는 경우도 있다.\n이런 경우는 당연히 어쩔 수 없다.  \n하지만 그런 도메인의 특성이 없는 경우에도 AI 도구를 지원하지 않는 경우가 많다.\n그럴 땐 어떻게 해야 할까?  \n10년 전이나 지금이나 개인의 생산성과 성장을 위해 필요하면 개인 비용으로 당연히 사용할 수 있다.\n그렇지만, 개인에 따라 그 비용도 부담스러울 수 있다.\n요즘의 환율로 계산하면 거의 월 30만원은 돼야 하나의 도구를 제대로 써볼 수 있는 시대에는 특히나 그렇다.  \n지금의 AI 시대를 그냥 회사 탓하면서 보낼 순 없다.\n그럴 경우엔 개인적으로 Amazon Web Services (AWS) Kiro와 JetBrains Junie, 이 두 가지를 검토해보는 것을 추천한다.\n이유는 단순하다.\n이 두 도구는 이미 회사에서 쓰고 있을 가능성이 높은 플랫폼 위에 있기 때문이다.  \nJetBrains의 Junie는 All Product 라이선스가 있을 경우 AI Pro 라이선스로 사용할 수 있다.\n(물론, JetBrains All Product 라이선스를 지원하면 AI 도구도 충분히 지원하는 회사이지 않겠냐고 할 수 있다.\n이미 JetBrains IDE에 대한 지원을 하고 거기에 추가로 AI 도구까지 지원하는 그 +α의 비용을 부담스러워해서 미지원하는 경우가 생각보다 많다.)  \n이 경우 JetBrains의 Junie는 충분히 AI 개발 경험을 쌓을 수 있는 도구다.\n다른 AI 도구들의 Max Plan처럼 미친 듯이 토큰을 사용할 수 있을 정도는 아니지만, 그래도 어느 정도는 에이전틱 프로그래밍에 대한 경험을 쌓아볼 수는 있다.  \nAWS는 국내에서는 거의 대부분의 서비스가 사용하고 있는 클라우드이다.\nAWS Kiro의 장점은 바로 그 AWS Billing에서 비용이 관리되어 별도의 계약이나 재무적 협의가 필요하지 않다는 점, 그리고 요즘 AWS에서 크레딧 지원을 많이 해준다는 점이다.  \nClaude Code, Codex, Cursor와 같은 도구들은 아무리 좋아도 회사에 도입하려면 새로운 벤더와의 계약, 신규 예산 품의, 보안 검토 등을 처음부터 시작해야 한다.  \n그런데 Kiro와 Junie는 이미 회사가 쓰고 있는 AWS, 젯브레인 생태계 안에 있다.\n즉, 새로운 벤더를 찾아 계약하는 것이 아니라 큰 협의 과정 없이 시작할 수 있다.\n앞서 이야기한 예산 품의, 보안 심사 같은 산들이 한꺼번에 낮아진다.\n솔직히 AI 코딩 도구의 성능 차이는 몇 달 단위로 뒤집힌다.\n오늘 A가 좋아도 내일 B가 더 좋아질 수 있고, 그 반대도 마찬가지다.\n그래서 나는 \"어떤 도구가 가장 뛰어난가\"보다 \"어떤 도구를 우리 팀이 가장 빨리 쓸 수 있는가\"가 더 중요하다고 생각한다.\n아무리 좋은 도구도 도입까지 6개월이 걸리면 의미가 없다.\n지금 당장 쓸 수 있는 도구를 먼저 도입하고, 팀이 AI 코딩에 익숙해지는 것이 훨씬 더 가치 있다.\n그런 관점에서 AI 도구를 사용하기 위해 사내에서 협의해야 할 것들이 너무 많다면, AWS Kiro와 JetBrains Junie를 고민해보는 것을 추천한다.\n\"좋은 건 알겠는데, 리더에게 어떻게 말씀드리지?\" 라는 고민이 있을 수 있다.    \n특히 Kiro의 경우 AWS 위에 있다 보니, 기존 예산 체계나 계정 관리 측면에서 리더를 설득할 수 있는 구체적인 포인트들이 있다.\nYan So 님이 이런 부분을 잘 정리해주신 글이 있어서 공유한다.\nKiro를 기업에서 쓰기 좋은 이유 (IAM Identity Center)\n\"왜 우리 리더들은 AI 도구를 지원하지 않는 것인가\"에 대해 리더의 입장에서의 시각과 그들을 설득하기 위한 충분한 근거를 자세히 설명해 주셨다.\n예산과 비용 처리 관점에서 왜 도입이 지연되는지, 그리고 리더 입장에서 Kiro를 어떤 포인트로 검토하면 좋을지가 실무적으로 정리되어 있다.\nAI 코딩 도구 도입이 아직인 조직이 있다면, 한번 읽어보고 팀 리더에게 제안해보자.",
        "guid": "https://jojoldu.tistory.com/864",
        "categories": [
          "생각정리",
          "AI 코딩",
          "aws kiro",
          "jetbrains junie",
          "kiro",
          "vibe coding",
          "바이브 코딩",
          "에이전틱 코딩",
          "젯브레인"
        ],
        "isoDate": "2026-02-14T15:52:38.000Z"
      }
    ]
  },
  {
    "name": "WestwoodForever's Dev Log",
    "category": "개인",
    "posts": []
  },
  {
    "name": "허니몬(Honeymon)의 자바guru",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Wolf Loves Fox :: 일상",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Game Programmer Life",
    "category": "개인",
    "posts": []
  },
  {
    "name": "yuchi's dev",
    "category": "개인",
    "posts": []
  },
  {
    "name": "만화로 나누는 자유/오픈소스 소프트웨어 이야기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "신현석(Hyeonseok Shin)",
    "category": "개인",
    "posts": []
  },
  {
    "name": "즐거운 개발자 :: 네이버  블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "황제펭귄의 게임개발이야기 [여기는 한국]",
    "category": "개인",
    "posts": []
  },
  {
    "name": "LINE ENGINEERING",
    "category": "기업",
    "posts": [
      {
        "title": "Claude Code Action: 조직 전반의 코드 품질을 지키는 AI 코드 리뷰 플랫폼화",
        "link": "https://techblog.lycorp.co.jp/ko/building-ai-code-review-platform-with-claude-code-action",
        "pubDate": "Fri, 13 Feb 2026 02:00:00 GMT",
        "content": "들어가며안녕하세요. LINE NEXT DevOps 팀에서 일하고 있는 이동원입니다. 저는 쿠버네티스 기반 인프라 운영과 CI/CD 구축, 모니터링 및 장애 대응 등 인프라 운영 관...",
        "contentSnippet": "들어가며안녕하세요. LINE NEXT DevOps 팀에서 일하고 있는 이동원입니다. 저는 쿠버네티스 기반 인프라 운영과 CI/CD 구축, 모니터링 및 장애 대응 등 인프라 운영 관...",
        "guid": "https://techblog.lycorp.co.jp/ko/building-ai-code-review-platform-with-claude-code-action",
        "isoDate": "2026-02-13T02:00:00.000Z"
      },
      {
        "title": "슬로우 쿼리 해결기: 함수형 인덱스로 비트 연산 쿼리 최적화하기",
        "link": "https://techblog.lycorp.co.jp/ko/solving-slow-queries-optimizing-bitwise-operation-queries-with-functional-indexes",
        "pubDate": "Fri, 13 Feb 2026 02:00:00 GMT",
        "content": "들어가며안녕하세요. LINE VOOM 서비스의 포스트 서버를 개발하고 있는 서용준입니다. 이번 글에서는 저희 팀이 약 7개월에 걸쳐 슬로우 쿼리 문제를 해결한 과정과 그 과정에서 ...",
        "contentSnippet": "들어가며안녕하세요. LINE VOOM 서비스의 포스트 서버를 개발하고 있는 서용준입니다. 이번 글에서는 저희 팀이 약 7개월에 걸쳐 슬로우 쿼리 문제를 해결한 과정과 그 과정에서 ...",
        "guid": "https://techblog.lycorp.co.jp/ko/solving-slow-queries-optimizing-bitwise-operation-queries-with-functional-indexes",
        "isoDate": "2026-02-13T02:00:00.000Z"
      }
    ]
  },
  {
    "name": "뱅크샐러드 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "우아한형제들 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "TOAST Meetup",
    "category": "기업",
    "posts": []
  },
  {
    "name": "ZUM 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "SK Planet",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Spoqa tech blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "팀 왈도 번역팀",
    "category": "게임",
    "posts": []
  },
  {
    "name": "근원님",
    "category": "개인",
    "posts": []
  },
  {
    "name": "황의윤",
    "category": "개인",
    "posts": []
  },
  {
    "name": "호돌맨",
    "category": "개인",
    "posts": [
      {
        "creator": "호돌맨",
        "title": "클로드 세션 토큰 만료에 대한 생각",
        "link": "https://hodolman.tistory.com/84",
        "pubDate": "Thu, 12 Feb 2026 17:04:59 +0900",
        "author": "호돌맨",
        "comments": "https://hodolman.tistory.com/84#entry84comment",
        "content": "<p data-ke-size=\"size16\">클로드 세션토큰이 만료 되기 직전에야 마음껏 돌린다.<br />노래방에서 종료 1분 남겨두고 급하게&nbsp;&nbsp;she's gone , better than yesterday 예약하는 기분이다.</p>",
        "contentSnippet": "클로드 세션토큰이 만료 되기 직전에야 마음껏 돌린다.\n노래방에서 종료 1분 남겨두고 급하게  she's gone , better than yesterday 예약하는 기분이다.",
        "guid": "https://hodolman.tistory.com/84",
        "categories": [
          "우당탕탕 대모험"
        ],
        "isoDate": "2026-02-12T08:04:59.000Z"
      }
    ]
  },
  {
    "name": "박우빈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "문다영",
    "category": "개인",
    "posts": []
  },
  {
    "name": "유수민",
    "category": "개인",
    "posts": []
  },
  {
    "name": "안건주",
    "category": "개인",
    "posts": []
  },
  {
    "name": "손현호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "STARTUP BIBLE",
    "category": "개인",
    "posts": [
      {
        "creator": "Kihong Bae",
        "title": "정치적 사업",
        "link": "https://www.thestartupbible.com/2026/02/political-businesses.html",
        "pubDate": "Wed, 11 Feb 2026 21:34:00 +0000",
        "content:encodedSnippet": "미국 출장 중에 어떤 미국 VC가 혹시 스트롱이 이런 회사에 투자 관심이 있는지 물어봤는데, 이 분은 우리가 어떤 전략으로, 어떤 시장에, 어떤 조건으로 투자하는지 전혀 모르기 때문에 이런 딜을 공유해 준 것 같다. 희토류 관련 사업을 하는 미국 회사인데 현재 약 3조 원 밸류로 여러 VC와 이야기를 하고 있었다. 우리가 투자하기엔 너무나 비싼 회사였고, 분야도 좀 그렇고, 시장도 한국이 아니라서 듣자마자 바로 패스이긴 했는데, 그냥 희토류 시장 자체가 좀 궁금해서 여러 가지 질문을 했다.\n15분 정도의 대화를 통해서 알게 된 사실은 전 세계 희토류 시장의 90%가 중국에 있어서 우리가 이야기하던 이 3조 원 밸류의 회사는 남은 10%의 시장에서 사업을 하고 있다는 점이다. 그리고 더 충격적인 사실은 현재 실제 매출은 전혀 없고, 예약된 매출만 있다는 것이었다. 예약된 매출이란 중국과 미국의 마찰 때문에 미국 정부가 희토류 확보를 위해서 이 회사를 엄청나게 밀어주고 있어서 미국 정부로부터 부킹?된 매출이다. 나는 이 사업이 과연 잘 될지, 그리고 매출 0원인 회사의 기업가치가 3조 원이라는 게 말이 되는지, 도대체 이 회사에 이미 커밋한 투자자들은 누구이고 이들은 무슨 생각을 하는지 진짜 궁금하긴 했다.\n미래에 예약된(=부킹된) 매출이 발생하는 영역에서 사업하는 우리 투자사들도 있는데, 이 부킹이 된 매출 중 실제 매출로 전환되는 금액은 대부분 0이거나 매우 작다. 아마도 위에서 이야기한 희토류 회사는 미국 정부로부터 부킹된 매출이라서 전환율은 조금 더 높을 것 같지만, 현재 미국 정부가 하는 걸 보면 이 또한 불확실성 투성이다.\n이런 사업을 우리는 정치적인 사업(political business)이라고 한다. 상업적인 사업(commercial business)과는 완전히 다른 성격의 사업이고, 조금 더 쉽게 풀어 설명하면 제대로 된 사업이 아니라는 말이다. 중국과의 마찰 때문에 미국 정부가 밀어주는 분야라서 갑자기 떴고, 주목받는 사업이라서 이런 회사에 3조 원 밸류에이션에 돈을 쏘는 투자자도 있는 것 같은데, 갑자기 미중 관계가 좋아지거나, 또는 미국 정부의 희토류에 대한 기조가 바뀐다면 하루 만에 망할 수도 있는 그런 사업이다.\n한국에서도 가끔 이런 사업을 하는 창업가들을 만난다. 자생할 수 있는 기술, 제품, 서비스는 없고 정부의 기조 때문에 운 좋게 큰 계약을 하거나 큰 매출을 만드는 정치적인 사업이 실은 한국에도 꽤 많다. 워낙 한국 정부가 과감한 드라이브를 많이 걸고, 특정 분야가 뜬다고 하면 이 분야에 막대한 투자를 하면서 온갖 정부 과제와 보조금이 갑자기 생기기 때문이다.\n창업자나 투자자도 본인들이 하는 사업과 투자 검토하는 사업이 정치적인 사업인지 시장의 논리로 돌아가는 상업적인 사업인지 잘 구분해야 한다. 관건은 이 사업에서 정치적인 부분이 없어져도 자생할 수 있는 제품, 매출, 그리고 고객을 보유하고 있는 사업인지 여부다.",
        "dc:creator": "Kihong Bae",
        "comments": "https://www.thestartupbible.com/2026/02/political-businesses.html#respond",
        "content": "미국 출장 중에 어떤 미국 VC가 혹시 스트롱이 이런 회사에 투자 관심이 있는지 물어봤는데, 이 분은 우리가 어떤 전략으로, 어떤 시장에, 어떤 조건으로 투자하는지 전혀 모르기 때문에 이런 딜을 공유해 준 것 같다. 희토류 관련 사업을 하는 미국 회사인데 현재 약 3조 원 밸류로 여러 VC와 이야기를 하고 있었다. 우리가 투자하기엔 너무나 비싼 회사였고, 분야도(...)",
        "contentSnippet": "미국 출장 중에 어떤 미국 VC가 혹시 스트롱이 이런 회사에 투자 관심이 있는지 물어봤는데, 이 분은 우리가 어떤 전략으로, 어떤 시장에, 어떤 조건으로 투자하는지 전혀 모르기 때문에 이런 딜을 공유해 준 것 같다. 희토류 관련 사업을 하는 미국 회사인데 현재 약 3조 원 밸류로 여러 VC와 이야기를 하고 있었다. 우리가 투자하기엔 너무나 비싼 회사였고, 분야도(...)",
        "guid": "https://www.thestartupbible.com/?p=9688",
        "categories": [
          "Uncategorized",
          "failure",
          "general",
          "vc"
        ],
        "isoDate": "2026-02-11T21:34:00.000Z"
      }
    ]
  },
  {
    "name": "매거진 입맛",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "요즘 IT",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "쿠팡 엔지니어링",
    "category": "기업",
    "posts": []
  },
  {
    "name": "지마켓 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "리멤버 (드라마 앤 컴퍼니)",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Kimchi hill",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "Hudi.blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "토스",
    "category": "기업",
    "posts": [
      {
        "title": "Can life be as simple as sending money?",
        "link": "https://toss.im/tossfeed/article/makefinanceaccessible",
        "pubDate": "Thu, 12 Feb 2026 07:00:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-of5acw{margin:24px 0 8px;text-align:center;}.css-1pgssrp{max-width:100%;border-radius:16px;}.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}.css-1kxrhf3{white-space:pre-wrap;}Toss discovered people’s deep desire to use emerging technology to solve inconveniences they never thought of solving. From there, Toss began transforming finance itself. As the flow of money changed, so did our daily lives. Meet the Toss Community: ten affiliates and subsidiaries working together to make finance simple and accessible for everyone, and to build a world where people can live with greater peace and security.\n.css-1feg9au{font-size:24px;letter-spacing:0em;line-height:1.6;font-weight:bold;color:var(--adaptiveGrey900);margin:24px 0 4px;}\nFROM UNCLEAR TO CLEAR\nFROM COMPLEX TO SIMPLE\nMAKING FINANCE ACCESSIBLE\n.css-q3ktjb{white-space:pre-wrap;font-weight:bold;}Toss Bank\nA bank of innovation and inclusion, opening the benefits of the banking sector to more people. Since its launch in October 2021, Toss Bank has introduced user-centric services that transformed banking into something entirely new that customers now find the old system unimaginable. Its offerings include the Toss Bank Account, which allows customers to receive interest whenever they choose; a foreign exchange account with zero exchange fees; and services enabling foreigner users to conveniently open accounts remotely.\nToss Securities\nA securities firm that makes trading easy and intuitive, seeking to put the power of investment into everyone’s hands. In March 2021, Toss Securities officially launched a user-friendly Mobile Trading System (MTS). Offering services such as real-time fractional trading of overseas stocks and stock savings, it set a new standard in stock investing. Toss Securities continues to lower the barriers to entry with AI technology, pursuing its vision of delivering power to all investors.\nToss Insurance\nBased on the core values of trust and transparency, Toss Insurance is a licensed insurance agency (GA) that delivers experiences customers can understand and feel confident in. Rather than simply selling insurance, it recommends coverage tailored to each customer’s needs, while providing planners with training to strengthen their expertise and fair compensation. By building a healthy virtuous cycle where customer trust and planner growth go hand in hand, Toss Insurance is setting a more reasonable standard for the industry.\nToss Income\nToss Income helps customers manage taxes that are often difficult to handle on their own. From regular filing of comprehensive income tax to rectification claims and late filings, everything can be resolved in one simple solution. It is also creating a new culture around tax refunds with services such as advance refunds and a compensation program. Looking ahead, Toss Income plans to expand into a comprehensive service that covers all aspects of income, which is the starting point of everyone’s financial life.\nA RELIABLE PARTNER \nFOR COMPANIES BIG AND SMALL\nToss Payments\nA payment partner that makes online business seamless. Toss Payments simplifies the once complex and tedious PG integration process, quickly providing payments for everyone. It offers every electronic payment method an online business might need, from cards and account transfers to billing and simple payments. By supporting the growth of countless businesses, both big and small, Toss Payments is shaping a new future for the payments industry.\nToss Place\nA store management solution that began with a “pretty device.” Toss Place has grown into an all-in-one service that works like a kiosk and offers features such as order pick-up, inventory management, gift cards, coupons, loyalty points, and customer management. Its goal is to ensure that businesses aren’t left behind due to limited access to capital or technology in a rapidly changing world. By making store operations more efficient, Toss Place is building an innovative ecosystem for business owners.\nBOUNDLESS EXPANSION \nERASING EVERY INCONVENIENCE\nToss Mobile\nToss Mobile, a mobile virtual network operator (MVNO) service, introduces a new way to connect with flexible plans and a seamless experience. Users enjoy data cashback, personalized pricing, quick and easy signup, and fast USIM delivery. With everything from bill checks to data management available in the Toss app, simplicity and convenience is at everyone’s fingertips. Toss Mobile combines affordability with quality, opening a new era of mobile communication designed to be easy and intuitive.\nVCNC\nDriven by the belief that better mobility creates a better life, Tada delivers a comfortable and seamless ride experience from pickup to drop-off. By redefining the standard of mobility, VCNC is building a world where every journey is free from stress.\nToss CX\nToss CX is our customer representation center that completes the financial experience at the closest point to customers. At the very forefront of customer touch points, Toss CX is responsible for both product operations and customer experience. Issues uncovered during consultations are directly connected to product and service improvements, and even with waves of new services, CX is one step ahead and ready to serve. To deliver on the value of a perfectly satisfying service experience, Toss CX provides non-face-to-face consultation 24 hours a day, 7 days a week.\nToss Insight\nA Financial Business Research Institute that sets the direction for future finance and fintech innovation. Through in-depth analysis of domestic and global financial and economic trends, it delivers meaningful insights to the industry, while also shaping growth strategies and policy recommendations for Toss. By offering perspectives that span across finance and technology, industry and regulation, Toss Insight ensures that the financial innovation achieved by Toss translates into broader benefits and progress for society.\n.css-1ifza5r{border:0;margin-top:0;margin-bottom:0;height:1px;opacity:1;background:var(--tHairlineBackground);margin:0;}\n.css-18442ym{font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}.css-1ebvaan{white-space:pre-wrap;font-weight:bold;font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}Writer\n.css-jfs1hr{white-space:pre-wrap;font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}Donghae Yoon, Toss Brand Manager\nSoeun Joo, Toss Brand Manager\n\n✽Toss Securities\n- Investors have the right to receive sufficient explanations from Toss Securities regarding financial investment products. Please read the product prospectus and terms and conditions carefully before investing.\n- This financial product is not protected under the Depositor Protection Act.\n- Financial investment products may result in losses of principal (0–100%) due to fluctuations in asset prices, exchange rates, and credit ratings, and such losses are borne by the investor.\n- The overseas stock trading fee is 0.1%. Please refer to our website for further details.\n- For overseas stock investments, the currency exchange fee is based on a spread of 1% over the base exchange rate. During market hours (KST business days 09:10–15:20), a 95% discount applies, resulting in a fee of 0.05%. During non-market hours (KST 15:20–09:10), weekends, and holidays, a 50% discount applies, resulting in a fee of 0.5%.\n- The base exchange rate refers to (buying rate + selling rate)/2, and Toss Securities applies the real-time base exchange rate provided by the settlement bank.\n- Since there is a difference between the buying and selling exchange rates, costs may be incurred during currency exchange.\n- Transfers to other securities firms are only available for whole share balances, and fractional shares cannot be transferred to other firms.\n- Fractional share trading services are not available for all stocks, so investors must confirm which stocks are eligible with the securities firm.\n- As securities firms aggregate multiple investors’ fractional trading orders for execution, there may be differences between the investor’s order time and the actual execution time. As a result, the trading price or the actual number of shares allocated may vary.\n- Toss Securities Compliance Officer Review No. 2026-41 (2026-02-11 ~ 2027-02-10)",
        "guid": "https://toss.im/tossfeed/article/makefinanceaccessible",
        "isoDate": "2026-02-12T07:00:00.000Z"
      },
      {
        "title": "2026 상반기 토스 얼라인먼트 위크 들여다보기",
        "link": "https://toss.im/tossfeed/article/alignmentweek2026-1",
        "pubDate": "Wed, 11 Feb 2026 06:28:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-1feg9au{font-size:24px;letter-spacing:0em;line-height:1.6;font-weight:bold;color:var(--adaptiveGrey900);margin:24px 0 4px;}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}.css-q3ktjb{white-space:pre-wrap;font-weight:bold;}Way to Win\n: 토스팀의 성공 방정식\n.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n.css-1kxrhf3{white-space:pre-wrap;}지난 1월, 토스에서는 얼라인먼트 위크(Alignment Week)가 열렸어요. 얼라인먼트 위크에서는 그 시기 팀원들에게 가장 필요한 슬로건을 내세웁니다. 이 슬로건은 단순한 행사의 테마가 아니라, 지난 시간을 회고하고 앞으로의 선택을 가늠하는 공통 언어가 되죠.\n이번 얼라인먼트 위크의 슬로건은 “Way to Win”이었어요. 이 슬로건을 통해 토스 팀원들에게 어떤 메시지를 전하고 싶었는지 행사를 준비한 Culture Event Manager 김희은 님과 손현빈 님에게 기획 의도를 물었어요.\n.css-of5acw{margin:24px 0 8px;text-align:center;}.css-1pgssrp{max-width:100%;border-radius:16px;}\nQ. ‘Way to Win’이라는 슬로건은 어떤 배경에서 나오게 됐나요?\n희은: 승건 님이 먼저 “토스의 성공 방정식을 한번 이야기해보면 어떨까”라는 제안을 주셨어요. 2025년에도 토스팀의 규모가 빠르게 커졌잖아요. 인원이 늘어나는 만큼, 앞으로 더 큰 성장을 만들기 위해 각자가 생각하는 성공의 기준과 방식을 나눌 필요가 있다고 느꼈어요.\n다만 ‘성공 방정식’이라는 말만 쓰면 발표의 무게 중심이 회고에 머물 수 있다고 생각했어요. 얼라인먼트 위크에서는 앞으로의 비전도 함께 다뤄야 하잖아요. 그래서 과거와 미래를 자연스럽게 이을 수 있는 표현을 고민하다가 Way to Win이라는 슬로건이 나왔습니다.\nQ. 행사장 벽면에 써있던 문구도 같은 맥락이었겠네요.\n.css-2sk6rv{font-size:19px;letter-spacing:0em;line-height:1.6;font-weight:bold;color:var(--adaptiveGrey900);white-space:pre-wrap;margin:24px 0;padding-left:20px;position:relative;}.css-2sk6rv::before{content:'';display:block;position:absolute;top:4px;left:0;width:2px;height:calc(100% - 4px * 2);padding:4px 0;background-color:var(--adaptiveGrey800);}\n이제껏, 우리가 해냈던 방식\n지금, 우리가 이루어 내고 있는 여정\n앞으로, 우리가 성공할 방법.css-7mseny>*{margin-left:0;margin-right:0;}.css-7mseny>:last-child{margin-bottom:0;}blockquote>.css-7mseny:first-child>:first-child{margin-top:0;}\n현빈: 맞아요. 그 문구는 토스의 과거, 현재, 미래를 보다 직접적으로 담으려는 메시지였어요. 팀원들이 더 깊이 생각해볼 수 있도록 각자의 Way to Win을 작성해보실 수 있는 공간도 행사장에 마련했죠. 나와 우리 팀이 어디를 바라보고 어떻게 가고 있는지 ‘Align’하고 긍정적인 자극을 받을 수 있도록 다양한 방법들과 비전이 오가길 기대했어요.\n토스 리더들의\nWay to Win\n얼라인먼트 위크 기간 동안 8개 계열사에서는 총 139개의 세션이 열렸습니다. 각 팀이 각자의 문제와 성과, 고민을 공유하는 자리가 이어졌죠. 이중에서도 모든 커뮤니티 구성원이 한자리에 모이는 순간이 있습니다. 법인 리더들이 무대에 올라, 토스의 방향과 선택을 이야기하는 ‘비전 세션’입니다.\n\n이번 비전 세션에는 9명의 리더가 지난 학기의 성과를 돌아보고, 앞으로의 비전을 공유했습니다. 리더들은 Way to Win이라는 주제를 각자 어떻게 풀어냈을까요? 토스팀이 어떤 가치를 중심에 두고 나아가는지 발표 내용을 요약해서 전달할게요. \n\n“유니크한 가치, 그리고 임팩트”\n— 토스팀 리더 이승건\n승건 님은 \"성공이란 무엇인가?\"라는 근본적인 질문을 던지며 발표를 시작했습니다. 승건 님은 질문에 대한 답으로 《제로 투 원》의 저자 피터 틸의 성공 방정식을 공유했어요. 작더라도 완전히 새로운 유니크한 가치를 만들어낼 때 진짜 경쟁력이 시작된다는 문장이었죠.\n\n이 질문은 이윤에 대한 관점으로도 이어졌습니다. 승건 님은 \"기업에게 이윤이란 자동차에게 연료와 같다.\"라는 사이먼 시넥의 말을 인용했습니다. 어딘가로 가기 위해 차를 사는 것이지, 연료를 채우기 위해 차를 사는 것은 아니라는 의미죠.\n\n그동안 토스는 선한 영향력을 퍼뜨리기 위해 여정을 이어왔고, 항상 기준을 높여왔습니다. 성공과 이윤에 대한 승건 님의 발표는 토스팀이 끊임없이 유난한 도전을 거듭하면서 대담한 아이디어를 향해 나아가는 이유를 설명해주고 있었습니다.\n\n“속도는 기준에서 나온다”\n— 토스인사이트 리더 손병두\n병두 님은 토스팀이 가진 경쟁력의 핵심은 '속도'라고 답했습니다. 풀어야 하는 문제가 생길 때마다 급히 판단하려면 빠를 수 없지만, 무엇을 문제로 볼지 이미 정리되어 있어야 신속하게 움직일 수 있다는 설명도 덧붙였습니다.\n\n이 과정에서 여러 번 되짚은 단어는 '기준'이었습니다. 병두님은 토스인사이트가 토스의 빠른 실행력에 '판단의 기준'을 얹어주는 팀이라고 설명했습니다. 지금 해결해야 할 핵심이 무엇인지, 어떤 관점에서 바라봐야 하는지에 대한 기준이 커뮤니티 안에 공유되어 있을 때, 각 팀은 망설이지 않고 움직일 수 있습니다. 판단에 쓰는 시간을 따로 두지 않아도 되기 때문이죠. \n\n속도를 주제로 시작한 발표였지만, 그 안에는 토스팀이 어떻게 함께 생각하고, 어떻게 함께 움직이는 팀인지를 차분하게 보여주는 메시지가 담겨 있었습니다.\n\n“투자의 힘을 모두에게”\n— 토스증권 리더 김규빈\n2025년에도 토스증권은 눈에 띄는 성장을 만들었습니다. 하지만 규빈 님은 그 결과를 시장 상황으로 설명하지 않았습니다. 대신 사용자 수의 증가를 넘어 매출의 질을 높이는 선택, 단기 성과보다 먼 미래를 준비하는 결정들을 강조했죠. 국내 리테일 증권사 1위라는 결과 역시 그런 선택들이 쌓인 결과라고 말했습니다.\n\n규빈 님이 제시한 토스증권의 비전은 명확했습니다. '투자의 힘을 모두에게.' 국내에서 증명한 방식을 글로벌로 확장하고, 소수의 전유물이었던 자산관리를 누구나 접근할 수 있는 영역으로 만들겠다는 비전이었죠. 매출 규모를 키우는 것보다 매출의 질을 높이는 것, 투자라는 영역 전체를 토스증권의 무대로 만들겠다는 포부가 담겨 있었습니다.\n\n”뱅킹의 본질로 돌아가다”\n— 토스뱅크 리더 이은미 \n은미 님은 토스뱅크가 이룬 성과를 나열하기에 앞서 \"기술이 이렇게 발전했는데도, 왜 뱅킹은 여전히 어렵고 불편할까?\"라는 질문을 던졌습니다. 이 질문을 통해 토스뱅크가 추구해온 뱅킹의 본질이 무엇인지 다시 돌아보게 된다고 했습니다.\n\n이에 대해 은미 님이 제안하는 방향은 분명했습니다. 뱅킹의 본질은 사람들에게 금융 자원을 온전히 다룰 힘을 주는 것. 개인이든 사업자든, 현금이든 디지털 화폐든, 형태와 관계없이 고객이 더 나은 선택을 할 수 있도록 돕는 일이라는 것이죠. 그래서 앞으로 토스뱅크는 이 본질을 더 정확하고 완전하게 구현하는 데 집중한다고 했습니다. 금융을 향한 모든 질문이 사라질 때까지요.\n\n”Tax를 넘어 Income으로”\n— 토스인컴 리더 최성희 \n2025년 토스인컴은 '숨은 환급액 찾기'를 중심으로 빠르게 성장했습니다. 성희 님은 유입 고객 수, 신고 전환율, 유저당 결제액이라는 세 가지 핵심 지표를 개선해온 과정을 짚었어요. 서비스 성장의 레버가 되는 지표에 집중했던 것이, 더 많은 유저에게 더 많은 환급액을 돌려주는 결과로 이어졌다는 설명이었습니다.\n이어서 토스인컴은 앞으로 ‘세금’을 넘어 ‘소득 전체’를 다루는 서비스로 확장하겠다는 비전을 제시했어요. 개인의 세금은 물론, 사업자의 매출과 비용까지 아우르며 소득이 발생하는 순간부터 세금으로 이어지는 전 과정을 더 쉽고 단순하게 만들겠다는 목표입니다. 토스인컴이 소득이 있는 모두에게 꼭 필요한 서비스가 될 때까지요.\n\n“We make the way”\n— 토스플레이스 리더 최재호\n토스플레이스는 빠르게 성장하고 있습니다. 어느덧 전국 가맹점은 28만 개를 넘어섰고* 오프라인 인프라 중에서도 손꼽히는 속도를 만들었죠. 재호 님은 이 성장은 가맹점 운영 전체의 과정을 이해하려고 한 덕분이라고 했습니다. 단순히 단말기를 빠르게 보급하는 데서 멈추지 않고, 가맹점의 하루를 처음부터 끝까지 세부적으로 설계한 선택이 쌓인 결과라는 이야기였습니다.\n.css-18442ym{font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}.css-jfs1hr{white-space:pre-wrap;font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}* 2026년 2월 기준\n재호 님은 앞으로 토스플레이스가 POS*를 하나의 기능이 아니라, 가맹점 운영을 떠받치는 플랫폼으로 만들 것이라고 강조했어요. 결제부터 재고, 직원, 매출 데이터까지 한 흐름으로 연결하고, 현장의 기준을 바꾸겠다는 거죠. 재호 님은 이를 두고 \"길이 없으면, 우리가 만든다\"고 표현했어요. 앞으로도 토스플레이스만의 유니크한 길을 만들어 갈 것이라는 자신감 넘치는 메시지였습니다.\n* 매장 주문·결제·매출 관리 서비스\n\n“One & Only”\n— 토스페이먼츠 리더 임한욱\n2025년 토스페이먼츠는 분명한 변화를 만들었습니다. 오랜 시간 이어졌던 구조를 정리하고, 사업의 체질을 하나씩 바꿔나갔죠. 한욱 님은 이 변화가 ‘Focus on Impact’ 라는 토스의 핵심 가치에 집중하고 반복해온 덕분이라고 강조했습니다.\n토스페이먼츠는 많은 것을 동시에 진행하려 하지 않았습니다. 가맹점 경험을 중심에 두고, 결제의 흐름을 단순하게 만들며, 시스템을 하나의 방식으로 묶어가는 선택을 이어왔죠. 한욱 님은 이를 'One & Only 전략'이라고 설명했습니다. 토스페이먼츠의 Way to Win은 남들과 비슷해지기보다 하나의 방향을 끝까지 밀고 나간 것에 있었습니다.\n\n“성공의 시간축을 '될 때까지'로 세팅한다”\n— 토스인슈어런스 리더 조병익\n병익 님이 정의하는 보험의 본질은 명확합니다. 위험이 닥치기 전에 최소한의 선택지는 남겨두는 것. 그래서 토스인슈어런스는 고객이 선택한 선택지가 최선의 선택이 될 수 있도록 설계사를 빠르게 늘리면서도, 퀄리티를 잃지 않는 데 집중하고 있습니다.\n병익 님은 보험을 '시간의 게임'이라고 표현했습니다. 국내 보험 시장은 연간 200조 원이 넘는 규모의 거대한 시장*이지만, 새로운 시도가 쉽지 않고 변화의 속도도 느린 곳입니다. 그렇기 때문에 토스인슈어런스는 단기 성과보다, 얼마나 오래 버티며 제대로 해낼 수 있는지를 먼저 고민한다고 말했습니다. 병익 님은 이를 두고 '성공의 시간축을 될 때까지로 설정하면 되는 일'이라고 표현했는데요. 보험이 꼭 필요한 순간에 외면 받는 고객이 없도록 토스인슈어런스가 계속 나아가겠다는 약속이었습니다.\n* 자료: 보험연구원\n\n”Back to Basics, 만족을 향한 집요함”\n— 토스CX 리더 강진석 \n2025년 토스CX는 지금의 방식으로도 상담을 계속 이어갈 수 있을지, 이 구조가 앞으로도 지속 가능할지에 대한 과제를 안고 있었습니다. 진석 님은 이를 '생존의 문제'라고 표현했죠. 이 과제는 토스CX가 해온 모든 가정을 다시 점검하게 만들었습니다. 그 결과, 더 많은 것을 시도하기보다 무엇이 정말 중요한지부터 다시 정의하는 선택을 했다고 말했습니다.\n\"We're not going to do anything fancy.\" 진석 님은 NBA를 대표하는 전설적인 감독 그렉 포포비치 감독의 말을 인용했습니다. 화려한 전략 대신 탄탄한 기본기를 강조한 철학이죠. 이 말과 함께 토스CX가 커뮤니티 전체의 성장을 함께 떠받치는 파트너가 되겠다고 했습니다. 상담의 품질과 속도, 비용의 균형을 지키며 고객 만족을 향해 나아가는 토스CX의 기본을 끝까지 지켜내겠다는 선언이었습니다.\n\n2026년 상반기 얼라인먼트 위크에서는 Way to Win이라는 슬로건 아래 지난 학기의 성과와 실패를 공유하고, 앞으로의 방향을 함께 확인하며 원팀으로 뭉칠 수 있었습니다. \n토스팀의 실험과 도전은 지금 이 순간에도 이어지고 있습니다. 그리고 이 과정에서 쌓이는 이야기들은 다음 얼라인먼트 위크에서 다시 펼쳐질 겁니다. 언제나 그랬듯이요.\n.css-nv7vyi{margin:24px 0 8px;padding:16px 40px 32px;border-radius:16px;background-color:var(--adaptiveGrey100);}.css-123co55{font-size:19px;letter-spacing:0em;line-height:1.6;margin:24px 0 0;font-weight:400;color:var(--adaptiveGrey900);background-color:transparent;}\n얼라인먼트 위크는 반년마다 열리는 토스 커뮤니티의 가장 큰 이벤트입니다. 일주일간 이어지는 이 행사에서는 전 계열사 팀원들이 함께 지난 6개월을 돌아보고, 다가올 여정의 방향을 함께 나눕니다.\n\n얼라인먼트 위크에서는 성과뿐만 아니라 실패와 시행착오, 그 과정에서 얻은 인사이트까지 솔직하게 공유하는 자리를 가져요. 이 과정을 통해 각자의 영역에서 서로 다른 문제를 풀고 있지만, 결국에는 모두가 하나의 목표를 향하고 있다는 사실을 다시금 확인하죠.\n\n토스팀의 얼라인먼트 위크에 대해 더 잘 알고 싶다면 .css-iynyr0{white-space:pre-wrap;cursor:pointer;color:var(--adaptiveGrey600);-webkit-text-decoration:underline!important;text-decoration:underline!important;}이 아티클을 통해 자세히 확인할 수 있어요!\n.css-1ifza5r{border:0;margin-top:0;margin-bottom:0;height:1px;opacity:1;background:var(--tHairlineBackground);margin:0;}\nEdit 윤동해 Photo 이욱영",
        "content": "Way to Win: 토스의 성공 방정식",
        "contentSnippet": "Way to Win: 토스의 성공 방정식",
        "guid": "https://toss.im/tossfeed/article/alignmentweek2026-1",
        "isoDate": "2026-02-11T06:28:00.000Z"
      }
    ]
  },
  {
    "name": "모나미",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김진홍",
    "category": "개인",
    "posts": []
  }
]