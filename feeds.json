[
  {
    "name": "C++ Team Blog",
    "category": "기업",
    "posts": [
      {
        "creator": "Sinem Akinci",
        "title": "C++ symbol context and CMake build configuration awareness for GitHub Copilot in VS Code",
        "link": "https://devblogs.microsoft.com/cppblog/c-symbol-context-and-cmake-build-configuration-awareness-for-github-copilot-in-vs-code/",
        "pubDate": "Thu, 19 Feb 2026 16:13:03 +0000",
        "content:encodedSnippet": "C++ code navigation and build system tooling play an important role in the developer inner-loop. Code navigation tooling provides a precise, semantic understanding of your codebase, while build system tooling helps you express build configurations and variants for reproducible builds. In the VS Code ecosystem, these powerful capabilities are available through our C/C++ and CMake Tools extensions.\nWith the latest updates to GitHub Copilot in VS Code, we’re bringing the same C++-specific intelligence directly into agent mode by surfacing key language and build system capabilities as tools the agent can invoke. The goal is simple: make AI-assisted C++ workflows more consistent and performant by grounding them in the same symbol and build context developers already use and trust.\nThese tools are available through the new C/C++ DevTools extension, which ships as a part of the C/C++ extension pack for VS Code. To view the full documentation, please visit our C++ DevTools docs.\nC++ code understanding tools\nWith the new C++ code understanding tools, agent mode now has access to rich C++ symbol context. Instead of relying solely on text search or file search, the agent can now reason about C++ code at the symbol level across your workspace, which it can leverage to intelligently perform code editing operations across a codebase.\nThe current tools available for Copilot Chat include:\nGet symbol definition – Retrieve detailed information about a C++ symbol, including where it is defined and its associated metadata\nGet symbol references – Find all references to a given symbol across the codebase\nGet symbol call hierarchy – Surface incoming and outgoing calls for a function to understand call patterns and dependencies\nTo enable these tools, select the Enable Cpp Code Editing Tools setting in your VS Code user settings.\n\nExample use cases\nMemory safety\nMemory safety issues in C++ are rarely isolated to a single line of code. Safely modernizing or hardening code requires understanding where memory is allocated, who owns it, and how it flows through the system. With C++ code understanding tools, agent mode can reason about these questions using symbol-level context rather than text search alone. For example, in the following refactor, Copilot was able to quickly locate relevant symbol information and all references to the symbol to rapidly collect relevant context rather than manually searching through .cpp and .h files.\n\nDependency analysis\nBefore moving a component to a new library or changing an API surface, developers need to understand what components depend on it. Call hierarchies let Copilot analyze dependency chains and highlight potential ripple effects before changes are made. For example, Copilot is able to determine call hierarchies related to btDdvtBroadphase to determine the relevant calls to and from a given function.\n\nCMake build and test configuration tools\nIn C++ development, every change must compile, link, and pass unit tests across the project’s active build configuration, not just look correct in the editor.\nCMake build and test configuration tools leverage the build configurations identified and provided by the CMake tools extension, so Copilot Chat seamlessly builds and tests your project using the exact configuration you already have selected in VS Code. By working with the same CMake Tools integration you use in the editor, Copilot avoids relying on ad-hoc command-line invocations and stays aligned with your chosen targets, presets, and build state. This enables the agent to perform end-to-end C++ workflows greater accuracy and reliability.\nThe current tools available to Copilot Chat for build configuration include:\nBuild with CMake: Build a CMake project using active configuration\nRun CTests: Run CTest tests using active test suite\nList Build Targets: List the available set of build targets for a CMake project\nList CTest tests: List the available set of tests for a CMake project\nExample use cases\nFixing build errors\nIf a change to a codebase introduces a compiler or build error, Copilot can invoke the new CMake build tool using the active configuration, inspect the failure, and iterate on this fix until the project builds successfully with CMake.\n\nModify code to pass test suite\nWhen a change is introduced to code, Copilot can run relevant tests and adjust the code until they pass, using the same test infrastructure developers rely on manually, ensuring that the code not only builds successfully but passes the test suite.\n\nTips for best results\nBe specific: Identify the exact symbol, file, or component you’re asking about (for example, “refactor the getConfig() function” rather than “make this faster”)\nReference context: Ask Copilot Chat to consider specific files, functions, or modules when analyzing changes.\nDirectly reference tools: Directly reference relevant tools using # in chat to ensure invocation.\n\nUse custom instructions: Set up custom instructions to guide Copilot Chat.\nLeverage latest models: Use the latest AI models that support tool-calling for the most accurate code understanding and tool usage.\nOptimize tool performance: Only enable relevant tools to your development workflow to avoid context bloat.\nLet us know your feedback!\nWe’re excited to continue improving these tools and other C++ integration points based on feedback, and we encourage you to try them out and let us know how they fit into your C++ workflows. Download the C/C++ DevTools extension and give it a try. Please file any issues or feedback in the appropriate repository. For CMake-related functionality: Issues · microsoft/vscode-cmake-tools and for C++-related functionality: Issues · microsoft/vscode-cpptools\nThe post C++ symbol context and CMake build configuration awareness for GitHub Copilot in VS Code appeared first on C++ Team Blog.",
        "dc:creator": "Sinem Akinci",
        "comments": "https://devblogs.microsoft.com/cppblog/c-symbol-context-and-cmake-build-configuration-awareness-for-github-copilot-in-vs-code/#comments",
        "content": "<p>C++ code navigation and build system tooling play an important role in the developer inner-loop. Code navigation tooling provides a precise, semantic understanding of your codebase, while build system tooling helps you express build configurations and variants for reproducible builds. In the VS Code ecosystem, these powerful capabilities are available through our C/C++ and CMake [&#8230;]</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/cppblog/c-symbol-context-and-cmake-build-configuration-awareness-for-github-copilot-in-vs-code/\">C++ symbol context and CMake build configuration awareness for GitHub Copilot in VS Code</a> appeared first on <a href=\"https://devblogs.microsoft.com/cppblog\">C++ Team Blog</a>.</p>\n",
        "contentSnippet": "C++ code navigation and build system tooling play an important role in the developer inner-loop. Code navigation tooling provides a precise, semantic understanding of your codebase, while build system tooling helps you express build configurations and variants for reproducible builds. In the VS Code ecosystem, these powerful capabilities are available through our C/C++ and CMake […]\nThe post C++ symbol context and CMake build configuration awareness for GitHub Copilot in VS Code appeared first on C++ Team Blog.",
        "guid": "https://devblogs.microsoft.com/cppblog/?p=36293",
        "categories": [
          "C++",
          "CMake",
          "Copilot",
          "Visual Studio Code"
        ],
        "isoDate": "2026-02-19T16:13:03.000Z"
      },
      {
        "creator": "Augustin Popa",
        "title": "Microsoft C++ (MSVC) Build Tools v14.51 Preview Released: How to Opt In",
        "link": "https://devblogs.microsoft.com/cppblog/microsoft-c-msvc-build-tools-v14-51-preview-released-how-to-opt-in/",
        "pubDate": "Thu, 19 Feb 2026 02:37:48 +0000",
        "content:encodedSnippet": "Today we are releasing the first preview of the Microsoft C++ (MSVC) Build Tools version 14.51. This update, shipping in the latest Visual Studio 2026 version 18.4 Insiders release, introduces many C++23 conformance changes, bug fixes, and runtime performance improvements. Check out the release notes for an in-progress list of what’s new. Conformance improvements and bug fixes will be detailed in an upcoming blog post and Insiders release notes in the near future.\nWe plan to ship more frequent, incremental MSVC Build Tools previews, just as we are shipping more frequent IDE updates. As a result, we have adjusted the process for enabling and using MSVC previews, and this post describes the new process.\nWe encourage you to explore MSVC previews to adapt to breaking changes and report issues early. MSVC previews do not receive servicing patches and thus should not be used in production environments.\nHow to opt in\nVisual Studio 2026 has changed the process for opting in to MSVC Build Tools previews. Most Visual Studio updates will include fresh MSVC previews, bringing compiler changes to you far faster than ever before. These updates will occur more frequently in the Insiders channel. Soon, you will also be able to install MSVC previews from the Stable channel, though these will be less recent than the builds available in Insiders.\nInstalling MSVC previews\nTo install MSVC v14.51 Preview, you must select one or both of these components in the Visual Studio installer depending on what architectures you are targeting for your builds:\nMSVC Build Tools for x64/x86 (Preview)\nMSVC Build Tools for ARM64/ARM64EC (Preview)\nYou can install these from the Workloads tab under Desktop development with C++ or from the Individual components tab.\nHow to install MSVC v14.51 Preview from the C++ desktop workload\n\nMSVC v14.51 components under “Individual components”\n\nAn easy way to find the relevant components under Individual components is to search for “preview”. Here you will also find support libraries and frameworks like MFC, ATL, C++/CLI, and Spectre-mitigated libraries compatible with this MSVC preview.\nThe components are the same as stable MSVC releases, except they are marked with “(MSVC Preview)” rather than “(Latest)” or a specific version number. Whenever you update Visual Studio, your MSVC preview will also be updated to the latest available build in that installer channel. Preview MSVC builds are not designed with version pinning in mind and do not receive servicing updates, though you can always download fresh builds as you update the IDE.\nIf you only want to build in the command line, you can also install MSVC v14.51 Preview using the Build Tools for Visual Studio 2026, by selecting the same checkboxes.\nConfiguring Command Prompts\nYou can configure MSVC Preview command-line builds by navigating to this path and running the appropriate vcvars for your desired environment:\ncmd.exe example for x64 builds:\ncd \"C:\\Program Files\\Microsoft Visual Studio\\18\\Insiders\\VC\\Auxiliary\\Build\"\r\n.\\vcvars64.bat -vcvars_ver=Preview\n\nConfiguring MSBuild Projects\nFor MSBuild projects, you must enable MSVC preview builds in the project system by setting the new Use MSVC Build Tools Preview property to “Yes” and making sure the MSVC Build Tools Version property is set to “Latest supported”. If MSVC Build Tools Version is set to something other than “Latest supported”, that MSVC version will be used for builds instead. If you wish to switch back to a stable MSVC build, you should set Use MSVC Build Tools Preview to “No”.\nInstructions – Enabling MSVC previews in MSBuild projects\nFirst, right-click the project you want to modify in Solution Explorer, select Properties.\nNext, make sure your Configuration and Platform at the top are set to what you want to modify.\nUnder the General tab (open by default), set Use MSVC Build Tools Preview to “Yes”.\n\nMake sure the MSVC Build Tools Version property is set to “Latest supported”, or else your project will build with the version specified there instead.\nLastly, run a build to make sure it works. Your project will now build using the latest preview tools.\nNote: For command-line builds, you can also set the new property by running:\nmsbuild <project_or_solution_file> /p:MSVCPreviewEnabled=true\nConfiguring CMake Projects\nFor CMake projects, you should specify the MSVC version in a CMakePresets.json file under the toolset property. The same process applies regardless of what version of MSVC you want to use (and whether it’s a Preview or not).\nInstructions – Enabling MSVC previews in CMake projects\nFirst, open your CMake project in Visual Studio. Ensure your workspace has a CMakePresets.json file in the root directory. See Configure and build with CMake Presets | Microsoft Learn if you need help configuring a CMakePresets file.\nThen, add a base preset under configurePresets that specifies MSVC v14.51:\n{\r\n    \"name\": \"windows-msvc-v1451-base\",\r\n    \"description\": \"Base preset for MSVC v14.51\",\r\n    \"hidden\": true,\r\n    \"inherits\": \"windows-base\",\r\n    \"toolset\": {\r\n        \"value\": \"v145,host=x64,version=14.51\"\r\n    }\r\n}\nNext, add more specific presets for each individual architecture, e.g.:\n{\r\n    \"name\": \"x64-debug-msvc-v1451-preview\",\r\n    \"displayName\": \"x64 Debug (MSVC v14.51 Preview)\",\r\n    \"inherits\": \"windows-msvc-v1451-base\",\r\n    \"architecture\": {\r\n        \"value\": \"x64\",\r\n        \"strategy\": \"external\"\r\n    },\r\n    \"cacheVariables\": {\r\n        \"CMAKE_BUILD_TYPE\": \"Debug\"\r\n    }\r\n}\nNext, slect the new build configuration from the list of targets beside the Play button at the top of the IDE.\nLastly, run a build to make sure it works. You can create additional presets the same way for other MSVC versions to easily swap between them.\nKnown issues\nThere are several known issues that will be fixed in a future MSVC Build Tools Preview and/or Visual Studio Insiders release.\nCMake targets using Visual Studio generator\nThere is a bug configuring CMake targets using the Visual Studio (MSBuild) generator. A workaround is described below.\nFirst, open Developer Command Prompt for VS Insiders (or the prompt for the version of Visual Studio you are using) as an administrator.\nThen, run the following commands, which create a new folder and copy a file from another location to it:\npushd %VCINSTALLDIR%\\Auxiliary\\Build\r\nmkdir 14.51\r\ncopy .\\v145\\Microsoft.VCToolsVersion.VC.14.51.props .\\14.51\\Microsoft.VCToolsVersion.14.51.props\r\ncopy .\\v145\\Microsoft.VCToolsVersion.VC.14.51.txt .\\14.51\\Microsoft.VCToolsVersion.14.51.txt\nLastly, run a build to make sure it works.\nCommand-line builds using PowerShell\nCommand line builds in PowerShell (including via Launch-VsDevShell.ps1) are not yet configured for the preview.\nC++ CMake tools for Windows dependency on latest stable MSVC\nIf you are using the CMake tools in Visual Studio, their installer component still has a dependency on the latest stable version of MSVC. Therefore you will need to install both latest stable and latest preview MSVC Build Tools until we correct this dependency relationship.\nTry out MSVC v14.51 Preview in Visual Studio 2026!\nWe encourage you to try out Visual Studio 2026 version 18.4 on the Insiders Channel, along with MSVC version 14.51 Preview. For MSVC, your feedback can help us address any bugs and improve build and runtime performance. Submit feedback using the Help > Send Feedback menu from the IDE, or by navigating directly to Visual Studio Developer Community.\nThe post Microsoft C++ (MSVC) Build Tools v14.51 Preview Released: How to Opt In appeared first on C++ Team Blog.",
        "dc:creator": "Augustin Popa",
        "comments": "https://devblogs.microsoft.com/cppblog/microsoft-c-msvc-build-tools-v14-51-preview-released-how-to-opt-in/#comments",
        "content": "<p>Today we are releasing the first preview of the Microsoft C++ (MSVC) Build Tools version 14.51. This update, shipping in the latest Visual Studio 2026 version 18.4 Insiders release, introduces many C++23 conformance changes, bug fixes, and runtime performance improvements. Check out the release notes for an in-progress list of what’s new. Conformance improvements and [&#8230;]</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/cppblog/microsoft-c-msvc-build-tools-v14-51-preview-released-how-to-opt-in/\">Microsoft C++ (MSVC) Build Tools v14.51 Preview Released: How to Opt In</a> appeared first on <a href=\"https://devblogs.microsoft.com/cppblog\">C++ Team Blog</a>.</p>\n",
        "contentSnippet": "Today we are releasing the first preview of the Microsoft C++ (MSVC) Build Tools version 14.51. This update, shipping in the latest Visual Studio 2026 version 18.4 Insiders release, introduces many C++23 conformance changes, bug fixes, and runtime performance improvements. Check out the release notes for an in-progress list of what’s new. Conformance improvements and […]\nThe post Microsoft C++ (MSVC) Build Tools v14.51 Preview Released: How to Opt In appeared first on C++ Team Blog.",
        "guid": "https://devblogs.microsoft.com/cppblog/?p=36310",
        "categories": [
          "C++",
          "Visual Studio",
          "MSVC",
          "visual studio"
        ],
        "isoDate": "2026-02-19T02:37:48.000Z"
      }
    ]
  },
  {
    "name": "Facebook Engineering",
    "category": "기업",
    "posts": [
      {
        "creator": "",
        "title": "RCCLX: Innovating GPU communications on AMD platforms",
        "link": "https://engineering.fb.com/2026/02/24/data-center-engineering/rrcclx-innovating-gpu-communications-amd-platforms-meta/",
        "pubDate": "Tue, 24 Feb 2026 21:30:54 +0000",
        "content:encodedSnippet": "We are open-sourcing the initial version of RCCLX – an enhanced version of RCCL that we developed and tested on Meta’s internal workloads. RCCLX is fully integrated with Torchcomms and aims to empower researchers and developers to accelerate innovation, regardless of their chosen backend.\nCommunication patterns for AI models are constantly evolving, as are hardware capabilities. We want to iterate on collectives, transports, and novel features quickly on AMD platforms. Earlier, we developed and open-sourced CTran, a custom transport library on the NVIDIA platform. With RCCLX, we have integrated CTran to AMD platforms, enabling the AllToAllvDynamic – a GPU-resident collective. While not all the CTran features are currently integrated into the open source RCCLX library, we’re aiming to have them available in the coming months. \nIn this post, we highlight two new features – Direct Data Access (DDA) and Low Precision Collectives. These features provide significant performance improvements on AMD platforms and we are excited to share this with the community. \nDirect Data Access (DDA) – Lightweight Intra-node Collectives\nLarge language model inference operates through two distinct computational stages, each with fundamentally different performance characteristics: \nThe prefill stage processes the input prompt, which can span thousands of tokens, to generate a key-value (KV) cache for each transformer layer of the model. This stage is compute-bound because the attention mechanism scales quadratically with sequence length, making it highly demanding on GPU computational resources.\nThe decoding stage then utilizes and incrementally updates the KV cache to generate tokens one by one. Unlike prefill, decoding is memory-bound, as the I/O time of reading memory dominates attention time, with model weights and the KV cache occupying the majority of memory.\nTensor parallelism enables models to be distributed across multiple GPUs by sharding individual layers into smaller, independent blocks that execute on different devices. However, one important challenge is the AllReduce communication operation can contribute up to 30% of end-to-end (E2E) latency. To address this bottleneck, Meta developed two DDA algorithms. \nThe DDA flat algorithm improves small message-size allreduce latency by allowing each rank to directly load memory from other ranks and perform local reduce operations, reducing latency from O(N) to O(1) by increasing the data exchange from O(n) to O(n²).\nThe DDA tree algorithm breaks the allreduce into two phases (reduce-scatter and all-gather) and uses direct data access in each step, moving the same amount of data as the ring algorithm but reducing latency to a constant factor for slightly larger message sizes.\n \nThe performance improvements of DDA over baseline communication libraries are substantial, particularly on AMD hardware. With AMD MI300X GPUs, DDA outperforms the RCCL baseline by 10-50% for decode (small message sizes) and yields 10-30% speedup for prefill. These improvements resulted in approximately 10% reduction in time-to-incremental-token (TTIT), directly enhancing the user experience during the critical decoding phase.\nLow-precision Collectives\nLow-precision (LP) collectives are a set of distributed communication algorithms — AllReduce, AllGather, AlltoAll, and ReduceScatter — optimized for AMD Instinct MI300/MI350 GPUs to accelerate AI training and inference workloads. These collectives support both FP32 and BF16 data types, leveraging FP8 quantization for up to 4:1 compression, which significantly reduces communication overhead and improves scalability and resource utilization for large message sizes (≥16MB). \nThe algorithms use parallel peer-to-peer (P2P) mesh communication, fully exploiting AMD’s Infinity Fabric for high bandwidth and low latency, while compute steps are performed in high precision (FP32) to maintain numerical stability. Precision loss is primarily dictated by the number of quantization operations — typically one or two per data type in each collective — and whether the data can be adequately represented within the FP8 range. \nBy dynamically enabling LP collectives, users can selectively activate these optimizations in E2E scenarios that benefit most from performance gains. Based on internal experiments, we have observed significant speed up for FP32 and notable improvements for BF16; it’s important to note that these collectives have been tuned for single-node deployments at this time. \nReducing the precision of types can potentially have an impact on numeric accuracy so we tested for this and we found that it provided acceptable numerical accuracy for our workloads. This flexible approach allows teams to maximize throughput while maintaining acceptable numerical accuracy, and is now fully integrated and available in RCCLX for AMD platforms — simply set the environment variable RCCL_LOW_PRECISION_ENABLE=1 to get started.\nMI300 – Float LP AllReduce speedup.\nMI300 – Float LP AllGather speedup.\nMI300 – Float LP AllToAll speedup.\nMI300 – Float LP ReduceScatter speedup.\nWe are observing the following results from E2E inference workload evaluations when selectively enabling LP collectives:\nApproximately ~0.3% delta on GSM8K evaluation runs.\n~9–10% decrease in latency.\n~7% increase in throughput.\nThe throughput measurements shown in the graphs were obtained using param-bench rccl-tests. For the MI300, the tests were run on RCCLX built with ROCm 6.4, and for the MI350, on RCCLX built with ROCm 7.0. Each test included 10 warmup iterations followed by 100 measurement iterations. The reported results represent the average throughput across the measurement iterations.\nEasy adaptation of AI models\nRCCLX is integrated with the Torchcomms API as a custom backend. We aim for this backend to have feature parity with our NCCLX backend (for NVIDIA platforms). Torchcomms allows users to have a single API for communication for different platforms. A user would not need to change the APIs they’re familiar with to port their applications across AMD, or other platforms even when using the novel features provided by CTran. \n\n\nRCCLX Quick Start Guide\nInstall Torchcomms with RCCLX backend by following the installation instructions in the Torchcomms repo.\nimport torchcomms\r\n\r\n# Eagerly initialize a communicator using MASTER_PORT/MASTER_ADDR/RANK/WORLD_SIZE environment variables \r\nprovided by torchrun.\r\n# This communicator is bound to a single device.\r\ncomm = torchcomms.new_comm(\"rcclx\", torch.device(\"hip\"), name=\"my_comm\")\r\nprint(f\"I am rank {comm.get_rank()} of {comm.get_size()}!\")\r\n\r\nt = torch.full((10, 20), value=comm.rank, dtype=torch.float)\r\n\r\n# run an all_reduce on the current stream\r\ncomm.allreduce(t, torchcomms.ReduceOp.SUM, async_op=False)\r\n\nAcknowledgements\nWe extend our gratitude to the AMD RCCL team for their ongoing collaboration. We also want to recognize the many current and former Meta employees whose contributions were vital in developing torchcomms and torchcomms-backends for production-scale training and inference. In particular, we would like to give special thanks to Dingming Wu, Qiye Tan, Cen Zhao, Yan Cui, Zhe Qu, Ahmed Khan, Ajit Mathews, CQ Tang, Srinivas Vaidyanathan, Harish Kumar Chandrappa, Peng Chen, Shashi Gandham, and Omar Baldonado\nThe post RCCLX: Innovating GPU communications on AMD platforms appeared first on Engineering at Meta.",
        "dc:creator": "",
        "content": "<p>We are open-sourcing the initial version of RCCLX – an enhanced version of RCCL that we developed and tested on Meta’s internal workloads. RCCLX is fully integrated with Torchcomms and aims to empower researchers and developers to accelerate innovation, regardless of their chosen backend. Communication patterns for AI models are constantly evolving, as are hardware [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2026/02/24/data-center-engineering/rrcclx-innovating-gpu-communications-amd-platforms-meta/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2026/02/24/data-center-engineering/rrcclx-innovating-gpu-communications-amd-platforms-meta/\">RCCLX: Innovating GPU communications on AMD platforms</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
        "contentSnippet": "We are open-sourcing the initial version of RCCLX – an enhanced version of RCCL that we developed and tested on Meta’s internal workloads. RCCLX is fully integrated with Torchcomms and aims to empower researchers and developers to accelerate innovation, regardless of their chosen backend. Communication patterns for AI models are constantly evolving, as are hardware [...]\nRead More...\nThe post RCCLX: Innovating GPU communications on AMD platforms appeared first on Engineering at Meta.",
        "guid": "https://engineering.fb.com/?p=23617",
        "categories": [
          "AI Research",
          "Data Center Engineering",
          "ML Applications",
          "Networking & Traffic"
        ],
        "isoDate": "2026-02-24T21:30:54.000Z"
      }
    ]
  },
  {
    "name": "eBay Tech Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Twitter Blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Netflix TechBlog",
    "category": "기업",
    "posts": [
      {
        "creator": "Netflix Technology Blog",
        "title": "MediaFM: The Multimodal AI Foundation for Media Understanding at Netflix",
        "link": "https://netflixtechblog.com/mediafm-the-multimodal-ai-foundation-for-media-understanding-at-netflix-e8c28df82e2d?source=rss----2615bd06b42e---4",
        "pubDate": "Mon, 23 Feb 2026 18:24:32 GMT",
        "content:encodedSnippet": "Avneesh Saluja, Santiago Castro, Bowei Yan, Ashish Rastogi\nIntroduction\nNetflix’s core mission is to connect millions of members around the world with stories they’ll love. This requires not just an incredible catalog, but also a deep, machine-level understanding of every piece of content in that catalog, from the biggest blockbusters to the most niche documentaries. As we onboard new types of content such as live events and podcasts, the need to scalably understand these nuances becomes even more critical to our productions and member-facing experiences.\nMany of these media-related tasks require sophisticated long-form video understanding e.g., identifying subtle narrative dependencies and emotional arcs that span entire episodes or films. Previous work has found that to truly grasp the content’s essence, our models must leverage the full multimodal signal. For example, the audio soundtrack is a crucial, non-visual modality that can help more precisely identify clip-level tones or when a new scene starts. Can we use our collection of shows and movies to learn how to a) fuse modalities like audio, video, and subtitle text together and b) develop robust representations that leverage the narrative structure that is present in long form entertainment? Consisting of tens of millions of individual shots across multiple titles, our diverse yet entertainment-specific dataset provides the perfect foundation to train multimodal media understanding models that enable many capabilities across the company such as ads relevancy, clip popularity prediction, and clip tagging.\nFor these reasons, we developed the Netflix Media Foundational Model (MediaFM), our new, in-house, multimodal content embedding model. MediaFM is the first tri-modal (audio, video, text) model pretrained on portions of the Netflix catalog. Its core is a multimodal, Transformer-based encoder designed to generate rich, contextual embeddings¹ for shots from our catalog by learning the temporal relationships between them through integrating visual, audio, and textual information. The resulting shot-level embeddings are powerful representations designed to create a deeper, more nuanced, and machine-readable understanding of our content, providing the critical backbone for effective cold start of newly launching titles in recommendations, optimized promotional assets (like art and trailers), and internal content analysis tools.\nFigure 1: MediaFM Architecture\nInput Representation & Preprocessing\nThe model’s fundamental unit of input is a shot, derived by segmenting a movie or episode (collectively referred to as “title”) using a shot boundary detection algorithm. For each shot, we generate three distinct embeddings from its core modalities:\n\nVideo: an internal model called SeqCLIP (a CLIP-style model fine-tuned on video retrieval datasets) is used to embed frames sampled at uniform intervals from segmented shots\nAudio: the audio samples from the same shots are embedded using Meta FAIR’s wav2vec2\nTimed Text: OpenAI’s text-embedding-3-large model is used to encode the corresponding timed text (e.g., closed captions, audio descriptions, or subtitles) for each shot\n\nFor each shot, the three embeddings² are concatenated and unit-normed to form a single 2304-dimensional fused embedding vector. The transformer encoder is trained on sequences of shots, so each example in our dataset is a temporally-ordered sequence of these fused embeddings from the same movie or episode (up to 512 shots per sequence). We also have access to title-level metadata which is used to provide global context for each sequence (via the [GLOBAL]token). The title-level embedding is computed by passing title-level metadata (such as synopses and tags) through the text-embedding-3-large model.\nModel Architecture and Training Objective\nThe core of our model is a transformer encoder, architecturally similar to BERT. A sequence of preprocessed shot embeddings is passed through the following stages:\n\nInput Projection: The fused shot embeddings are first projected down to the model’s hidden dimension via a linear layer.\nSequence Construction & Special Tokens: Before entering the Transformer, two special embeddings are prepended to the sequence:\n• a learnable [CLS] embedding is added at the very beginning.\n• the title-level embedding is projected to the model’s hidden dimension and inserted after the [CLS] token as the [GLOBAL] token, providing title-level context to every shot in the sequence and participating in the self-attention process.\nContextualization: The sequence is enhanced with positional embeddings and fed through the Transformer stack to provide shot representations based on their surrounding context.\nOutput Projection: The contextualized hidden states from the Transformer are passed through a final linear layer, projecting them from the hidden layers back up to the 2304-dimensional fused embedding space for prediction.\n\nWe train the model using a Masked Shot Modeling (MSM) objective. In this self-supervised task, we randomly mask 20% of the input shot embeddings in each sequence by replacing them with a learnable [MASK] embedding. The model’s objective is to predict the original, unmasked fused embedding for these masked positions. The model is optimized by minimizing the cosine distance between its predicted embedding and the ground-truth embedding for each masked shot.\nWe optimized the hidden parameters with Muon and the remaining parameters with AdamW. It’s worth noting that the switch to Muon resulted in noticeable improvements.\nEvaluation\nTo evaluate the learned embeddings, we learn task-specific linear layers on top of frozen representations (i.e., linear probes). Most of the tasks are clip-level, i.e., each example is a short clip ranging from a few seconds to a minute which are often presented to our members while recommending a title to them. When embedding these clips, we find that “embedding in context”, namely extracting the embeddings from within a larger sequence (e.g., the episode containing the clip), naturally does much better than embedding only the shots from a clip.\nTasks\nOur embeddings are foundational and we find that they bring value to applications across Netflix. Here are a few:\n\nAd Relevancy: A multilabel classification task to categorize Netflix clips for relevant ad placement, measured by Average Precision. In this task, these representations operate at the retrieval stage, where they help in identifying the candidate set and in turn are fed into the ad serving system for relevance optimization.\nClip Popularity Ranking: A ranking task to predict the relative performance (in click-through rate, CTR) of a media clip relative to other clips from that show or movie, measured by a ten-fold with Kendall’s tau correlation coefficient.\nClip Tone: A multi-label classification of hook clips into 100 tone categories (e.g., creepy, scary, humorous) from our internal Metadata & Ratings team, measured by micro Average Precision (averaged across tone categories).\nClip Genre: A multi-label classification of clips into eleven core genres (Action, Anime, Comedy, Documentary, Drama, Fantasy, Horror, Kids, Romance, Sci-fi, Thriller) derived from the genre of the parent title, measured by macro Average Precision (averaged across genres).\nClip Retrieval: a binary classification of clips from movies or episodes into “clip-worthy” (i.e., a good clip to showcase the title) or not, as determined by human annotators, and as measured by Average Precision. The positive to negative clip ratio is 1:3, and for each title we select 6–10 positive clips and the corresponding number of negatives.\n\nIt’s worth noting that for the tasks above (as well as other tasks that use our model), the model outputs are utilized as information that the relevant teams use when driving to a decision rather than being used in a completely end-to-end fashion. Many of the improvements are also in various stages of deployment.\nResults\nFigure 2³ compares MediaFM to several strong baselines:\n\nThe previously mentioned SeqCLIP, which also provides the video embedding input for MediaFM\nGoogle’s VertexAI multimodal embeddings\nTwelveLabs’ Marengo 2.7 embeddings\nFigure 2: Performance of MediaFM vs. external and internal models.\nOn all tasks, MediaFM is better than the baselines. Improvements seem to be larger for tasks that require more detailed narrative understanding e.g., predicting the most relevant ads for an ad break given the surrounding context. We look further into this next.\nAblations\nMediaFM’s primary improvements over previous Netflix work stem from two key areas: combining multiple modalities and learning to contextualize shot representations. To determine the contribution of each factor across different tasks, we compared MediaFM to a baseline. This baseline concatenates the three input embeddings, essentially providing the same complete, shot-level input as MediaFM but without the contextualization step. This comparison allows us to isolate which tasks benefit most from the contextualization aspect.\n\nAdditional modalities help somewhat for tone but the main improvement comes from contextualization.\n\nOddly, multiple uncontextualized modalities hurts the clip popularity ranking model, but adding contextualization significantly improves performance.\n\nFor clip retrieval we see a natural progression of around 15% for each improvement.\nNext Steps\nMediaFM presents a way to learn how to fuse and/or contextualize shot-level information by leveraging Netflix’s catalog in a self-supervised manner. With this perspective, we are actively investigating how pretrained multimodal (audio, video/image, text) LLMs like Qwen3-Omni, where the modality fusion has already been learned, can provide an even stronger starting point for subsequent model generations.\nNext in this series of blog posts, we will present our method to embed title-level metadata and adapt it to our needs. Stay tuned!\nFootnotes\n\nWe chose embeddings over generative text outputs to prioritize modular design. This provides a tighter, cleaner abstraction layer: we generate the representation once, and it is consumed across our entire suite of services. This avoids the architectural fragility of fine-tuning, allowing us to enhance our existing embedding-based workflows with new modalities more flexibly.\nAll of our data has audio and video; we zero-pad for missing timed text data, which is relatively likely to occur (e.g., in shots without dialogue).\nThe title-level tasks couldn’t be evaluated with the VertexAI MM and Marengo embedding models as the videos exceed the length limit set by the APIs.\n\nAcknowledgements\nWe would like to thank Matt Thanabalan and Chaitanya Ekanadham for their contributions to this work.\n\nMediaFM: The Multimodal AI Foundation for Media Understanding at Netflix was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Netflix Technology Blog",
        "guid": "https://medium.com/p/e8c28df82e2d",
        "categories": [
          "foundation-models",
          "machine-learning",
          "multimodal",
          "artificial-intelligence",
          "media"
        ],
        "isoDate": "2026-02-23T18:24:32.000Z"
      }
    ]
  },
  {
    "name": "JetBrains: Developer Tools for Professionals and Teams – Company Blog | JetBrains",
    "category": "기업",
    "posts": [
      {
        "creator": "Olga Bedrina",
        "title": "Beyond the Build Log: How TeamCity Provides Actionable Build Insights",
        "link": "https://blog.jetbrains.com/teamcity/2026/02/beyond-the-build-log/",
        "pubDate": "Tue, 24 Feb 2026 11:35:16 +0000",
        "content:encodedSnippet": "This article was brought to you by Kumar Harsh, draft.dev.\nWhere there is a CI/CD pipeline, there will be build logs. And while they’re important, anyone who’s stared at one knows the pain: thousands of lines of plain text, buried errors, and endless scrolling just to find out why something failed. What should be a quick diagnosis turns into a needle-in-a-haystack hunt.\nRaw logs are useful, but they’re not enough. Developers don’t just need to know that a build failed; they need to know where, why, and how often it’s happening. That’s the difference between the text you read and the insights you act on.\nIn this article, we’ll look at how TeamCity goes beyond the build log. You’ll see how its structured log view, visual pipeline insights, and trend analysis help you navigate failures faster, spot performance regressions, and even anticipate recurring issues. \nBy the end, you’ll understand how TeamCity turns “just another log” into a tool for building better software faster.\nThe wall of text\nEvery developer has faced the dreaded wall of text. A build fails, and suddenly you’re staring at thousands of lines of console output. Somewhere inside is the clue you need: an error code, a failed test, or a timeout. But it’s buried under a flood of status messages and stack traces.\nSearching helps, but you need to know exact keywords, and oftentimes you don’t know the exact error messages to search for. The longer the build, the longer its output, and the harder it gets to pinpoint issues.\nTraditional CI systems don’t make this any easier. Take Jenkins. Its build logs are essentially flat text files. You can scroll, you can search, but there’s no real structure. A build step is just a line in the log, indistinguishable from all the noise around it. \nIf you want to know which step failed or how long each stage took, you’re left to manually scan through endless lines.\nThis lack of structure creates three major pain points:\nFlat logs with no hierarchy: There’s no easy way to jump between stages and steps or to test the results\nZero visual cues: Errors don’t stand out. You have to read line by line.\nHard-to-trace correlations: Connecting a failing test back to its stage or seeing how long a step ran becomes a lengthy, manual exercise.\nThe result is that debugging builds becomes a time sink. Instead of focusing on fixing issues, you waste cycles just trying to interpret the logs yourself.\nThat’s the problem TeamCity set out to solve.\nWhat does a TeamCity build look like?\nTeamCity rethinks how build information is presented. Instead of forcing you to scroll endlessly through output text, it structures results in a way that’s easy to navigate, interpret, and act on.\nThink of it as moving from a raw server console to a dedicated dashboard built for surfing through logs. The build results page is designed to be a living view of your pipeline, complete with context, hierarchy, and visual cues. \nYou can see the entire flow of a build at a glance, drill into specific steps with a click, and watch logs update in real time as the build progresses.\n\n\n\n\nThis shift to a logs “browser” instead of a logs “dumper” is what makes TeamCity different. It focuses on highlighting problems so you can spend more time fixing issues than searching for them.\nHow does TeamCity structure its logs differently?\nThe first, most obvious thing you’ll notice in TeamCity is that logs aren’t just dumped into a giant text file. They’re organized hierarchically, following the natural flow of your pipeline.\n\n\n\n\nEach layer is collapsible, which means you don’t have to scroll past hundreds of lines just to find the one step you care about. \nWant to focus on a failing test step? Collapse everything else and zoom in on the problem area.\n\n\n\n\nThis structure also updates in real time. As your build runs, you can watch each step expand with fresh output while the rest of the log stays neatly tucked away. No more hunting for the latest lines in a never-ending scroll.\n\n\n\n\nThis means that, more often than not, you’ll pick up the error message and reason right in front of you as it happens instead of having to wade through lines after a build has failed and has dumped its logs.\nHow do TeamCity’s visual tools improve developer productivity?\nStructured logs are a big step forward, but TeamCity doesn’t stop there. It layers visual context on top of the raw output, so developers can spot problems and patterns without needing to parse every line.\n\n\n\n\nAt a glance, you get a visual overview of the entire pipeline: each step, its status, and how long it took. This makes it easy to see whether a build failed fast or slowed down during a specific stage. Instead of you having to run a stopwatch in your head, TeamCity does the timing analysis for you.\nAlso, errors are highlighted in context, so they stand out immediately. No scrolling through lines of green “success” messages just to find the single red flag buried at the bottom. \nThe failed step jumps out, both on the build timeline and the results page. Click on the timeline to quickly scroll to the error output line:\n\n\n\n\nThe Tests tab gives you detailed insight into how your tests performed across build steps:\n\n\n\n\nYou can click on a failed test to see its output in the current run and the run where it failed for the first time:\n\n\n\n\nAnd because builds rarely fail just once, TeamCity also gives you historical statistics for your builds. You can see trends across multiple runs, like how often your builds have failed across days or how often your tests have failed with each build.\n\n\n\n\nWhat types of insights are considered “actionable”?\nNot every log line deserves your attention – what you really need are insights that point directly to the next steps.\nTeamCity’s build statistics help you surface exactly those kinds of actionable signals.\nFor example, if a build suddenly takes twice as long, TeamCity can help you pinpoint the slow step. You can set up charts for each step that show trends over time so you know whether it’s a new dependency, a misconfigured cache, or an overloaded test suite. Instead of guessing, you see the bottleneck right away.\nThe following pipeline has had about seven builds so far, some of which failed as well. The build duration history looks like this:\n\n\n\n\nAs you can see, after build #4 failed, build #5 succeeded, but it took way longer than usual to complete. The issue was somehow resolved in the next two build runs, #6 and #7.\nThis seems unusual at first. However, when you look at the stepwise build duration history for the build configuration, you find this:\n\n\n\n\nIt’s clear that on build #5, the Fetch Secrets step took thirty seconds, which is way off from its usual two-to-five-second runtime. Since fetching secrets usually involves making network requests to a remote secrets manager, this could indicate an issue with your third-party secrets manager or with the network setup.\nAnd you were able to narrow down the cause to the scope of a single step by looking at only two graphs.TeamCity also helps with other build-wide trends, like artifact size, block-level/class-level/line-level/method-level code coverage, time spent in the queue, and more. \nYou can see the full list of available statistics here. If you want to add a custom statistic for your pipeline, you can use service messages and easily create charts and graphs out of them.\nWhat are the benefits?\nSo why does this matter for developers and teams working under pressure?\nFaster root cause analysis: The most obvious benefit is speed. TeamCity’s structured approach means you spend less time hunting and more time fixing. In high-velocity environments where every minute of downtime delays releases, this faster feedback loop makes a tangible difference.\nUnderstanding build performance: A failed test is one problem, but a slow build can be just as damaging. TeamCity’s step-by-step duration breakdown lets you spot build performance bottlenecks at a glance. Maybe a secrets fetch is dragging, or a test suite’s runtime has doubled. By surfacing this information, TeamCity gives you a starting point for optimization. You don’t just know that a build is slow; you know why it’s slow, and where to focus your efforts.\nDetecting patterns and preventing repeats: Another benefit is pattern recognition. Builds rarely fail for the first time out of nowhere. Often, you’ll see the same flaky test appear intermittently across runs or the same misconfigured environment variable pop up in different branches. Traditional logs leave you to connect those dots manually, but TeamCity makes those patterns visible through historical comparisons.\nSupporting proactive improvement: All these insights shift the developer mindset from reactive to proactive. Maybe you notice a steady increase in build duration, or memory consumption keeps varying without reason. TeamCity gives you the data to intervene before those issues become blockers.\nLaying the groundwork for AI-powered insights: TeamCity’s structured data and historical awareness lay the foundation for what’s coming next. For instance, the upcoming AI Build Analyzer will analyze builds from multiple angles to suggest likely root causes and possible fixes. You won’t just read logs anymore but collaborate with an intelligent system to solve problems even faster.\n\n\n\n\nConclusion\nBuild logs will always be an important part of CI/CD debugging workflows, but they aren’t enough on their own. Raw text logs leave developers to do the heavy lifting of interpretation, slowing down feedback loops and burying critical issues in noise.\nWhat developers need are insights: structured, visual, and actionable signals that point directly to the next step.\nThat’s what TeamCity delivers out of the box. From hierarchical logs and visual pipeline overviews to historical trends and pattern detection, TeamCity turns builds into a source of continuous learning rather than just reactive debugging. \nThe result is faster root-cause analysis, improved build performance, and a smoother path from code commit to deployment. And with innovations like the new AI Build Analyzer, the future of build intelligence looks even brighter.",
        "dc:creator": "Olga Bedrina",
        "content": "This article was brought to you by Kumar Harsh, draft.dev. Where there is a CI/CD pipeline, there will be build logs. And while they&#8217;re important, anyone who&#8217;s stared at one knows the pain: thousands of lines of plain text, buried errors, and endless scrolling just to find out why something failed. What should be a [&#8230;]",
        "contentSnippet": "This article was brought to you by Kumar Harsh, draft.dev. Where there is a CI/CD pipeline, there will be build logs. And while they’re important, anyone who’s stared at one knows the pain: thousands of lines of plain text, buried errors, and endless scrolling just to find out why something failed. What should be a […]",
        "guid": "https://blog.jetbrains.com/?post_type=teamcity&p=683232",
        "categories": [
          "devopspains",
          "how-to",
          "teamcity"
        ],
        "isoDate": "2026-02-24T11:35:16.000Z"
      },
      {
        "creator": "Dmitrii Mikhailovskii",
        "title": "#1 on Spider 2.0–DBT Benchmark – How Databao Agent Did It",
        "link": "https://blog.jetbrains.com/databao/2026/02/how-databao-agent-ranked-1-spider-2-0-dbt/",
        "pubDate": "Tue, 24 Feb 2026 07:54:58 +0000",
        "content:encodedSnippet": "As of February 2026, Databao Agent ranks #1 in the Spider 2.0–DBT benchmark. This ranking measures how well agents can operate in a real dbt project, including reading the repository, understanding what’s broken, implementing the missing models, and validating everything by actually running code.\nOur team ended up achieving the highest score in the benchmark, but we didn’t do it just because “we used a better model.” We got the biggest gains by treating the agent the same way you would mentor a junior colleague – providing better context, restricting chaos, and enforcing a reliable workflow.\nThis post is a practical account of what we changed and why it mattered. Read on to learn about the engineering decisions that made the difference, including how we reduced uncertainty, upgraded context, tightened up tool discipline, and rewrote a messy pile of prompts into a clear policy the agent could follow. The lessons we learned the hard way are that reliability beats cleverness, and prompts alone don’t buy you reliability – you have to design for it.\nWhat is a dbt project?\ndbt (data build tool) treats analytics like software. Instead of ad-hoc SQL embedded in dashboards and notebooks, data transformations live in a version-controlled repository, are reviewed like code, and can reliably rebuild the same analytics layer.\nThe main unit of work in dbt is a model: an .sql file that defines a dataset (usually a table or a view) built from other datasets. Models depend on other models, and dbt builds them in dependency order, turning the project into a directed graph rather than a pile of disconnected queries.\nA typical dbt repository contains the following parts:\nThe models/ directory with SQL models (often organized into layers, such as staging → intermediate → marts).\nYAML files that document the project and add tests and constraints (sources, descriptions, uniqueness tests, freshness, etc.).\nA workflow built around commands like dbt run or dbt build. These commands materialize models, run tests, and tell you what failed, where, and why.\nWorking with dbt means navigating a codebase, respecting conventions and dependencies, iterating, and not declaring victory until the build is green. The Spider 2.0–DBT benchmark asks agents to do exactly that.\nWhat Spider 2.0–DBT evaluates\nThe Spider 2.0–DBT benchmark turns a day-to-day dbt workflow into an evaluation. In the version we ran, the benchmark had 68 tasks. Each of them was a folder containing:\nAn incomplete dbt project (models were missing or incorrect).\nA DuckDB database file with the available data.\nThe agent’s job was to behave like a careful data engineer:\nRead the repository to understand what the repo is trying to produce. \nIdentify what’s missing or wrong.\nImplement the missing SQL models or fixes.\nRun dbt.\nKeep iterating until the project builds.\nThe evaluation compares the produced database with a “golden database” and checks whether the agent produced the right tables and columns.\nEven though it may sound like simple SQL generations that many LLMs can do well, the hard part is operating in a repository environment. Some tasks are large – like, “data warehouse” sized – tables with 2,500+ columns, dozens of models in a single task, and thousands of lines of SQL across the project. \nThis scale forces the agent to behave like a real contributor. You can’t paste the entire repository and schema into a single prompt and expect consistent reasoning. The agent has to navigate the project, read selectively, build a mental map of the project, and stay oriented after each run.\nWhere we started: Baselines and the real enemy\nWe didn’t start from scratch – our first agent was based on a popular LLM and could inspect a data project, run commands, and make edits using standard data tools. Surprisingly enough, its performance right out of the gate wasn’t too shabby – it could solve about a quarter of the tasks in our benchmark.\nEncouraged, we built a more flexible version of the agent by giving it some more tools not available in default setups of other agents. This gave us more control and room to experiment. On paper, these were all improvements. But in practice, consistency was sorely lacking. The agent behaved a little differently each time we ran it. It would nail one task, then completely whiff on the next.\nThis inconsistency turned out to be the real enemy. When we looked closer, the issue wasn’t that the agent couldn’t write SQL or “do data stuff.” The problem was that it struggled to behave consistently and to understand what the actual task was – something a careful data or analytics engineer wouldn’t have any issues with.\nImportant kinds of uncertainty\nAs we dug deeper, we realized there were two main culprits behind the agent’s randomness.\nThe first was missing or unclear context. The agent often didn’t have enough visibility into how the project was structured, what tables existed, or what conventions were being followed. This uncertainty is fixable. If you provide better, targeted context, the agent stops guessing.\nThe second was natural ambiguity. Human language is fuzzy by nature. Even with good instructions, there can be multiple reasonable ways to solve a task, but only one of them matches the benchmark’s expected answer. You can’t fully eliminate this kind of uncertainty.\nUnderstanding this distinction changed what we worked on. Once we did, we were able to re-allocate our efforts, focusing less on fixing the model and more on fixing the environment around it.\nOur strategy shift: From model tuning to workflow engineering\nEarly on, we gave the agent lots of freedom and lots of tools. That felt powerful, but failed in predictable ways: the agent wandered around, tried random actions, undid its own work, and generally got lost.\nSo, we changed our mindset. Instead of asking, “What can this agent do?” we asked, “What would a human engineer actually do here?”\nWe focused on two things:\nBetter context: Make the right information easy to access and hard to miss.\nA clear, disciplined workflow: Reduce chaos by forcing a specific order of operations.\nBetter context\nWe made sure the agent didn’t have to hunt for information.\nWe showed the important project files upfront, so the agent wouldn’t waste time opening the wrong things, and added a quick database overview at the beginning, so the agent knew which tables already existed. These fixed a surprising number of failures, especially on tasks where the correct action was to do nothing at all.\nWe also helped the agent connect the dots between requirements and data sources instead of guessing names. When it ran data builds, we summarized the results instead of dumping long, noisy logs. This kept the agent focused on what mattered next.\nThe result? Fewer blind mistakes and fewer “I didn’t find the right thing” failures.\nA clear, disciplined workflow\nContext helped, but it didn’t solve the failures entirely, so we tightened up the rules.\nIn the first version, we gave the agent access to many tools. It could read, write, edit, and add any file in the dbt project, and it had unrestricted access to the terminal. In theory, this was supposed to make the agent powerful, but unfortunately, the agent used its power to break things.\nWe removed the general scope tools and limited access to a narrow set of specific commands, such as dbt run or dbt build. File edits were restricted so that the agent could mostly edit .sql files in specific directories. We also gave the agent a clear checklist: inspect first, make minimal changes, verify, and only then declare success.\nIn several tasks, the agent didn’t inspect the database state carefully and could unintentionally overwrite existing tables with incorrect results. To prevent this, we added a few hard rules like never touching tables that already exist but aren’t part of the project, and never submitting an answer unless the final validation step succeeds.\nThis dramatically reduced chaotic behavior, loops, and premature “I’m done!” moments.\nWhat we learned: Stability over cleverness\nIt goes without saying that not every idea paid off. Adding more clever mechanisms (e.g., re-running the agent several times and choosing the “best” output, simulating human reviewers, or layering on extra tools) often gave us even less reliable results.\nAnd then there was the “prompt onion” problem. Initially, whenever we wanted to improve performance or change the logic, we added another rule or clarification. But soon enough, rules started overlapping and conflicting, and the execution flow became murky.In the end, stability beat cleverness. We took a step back and rewrote everything into a clean, human-readable policy. Redundancies and contradictions were removed, and the workflow became linear and predictable, leaving less room for interpretation – for both humans and the agent.\nHow this translates to real agents (Databao)\nThe biggest takeaway was about behavior, not SQL or models. Agents work best when:\nThey can clearly see their environment.\n\n\n\n\nThey follow a human-like workflow.\n\n\n\n\nTheir freedom is intentionally limited.\n\n\n\n\n\nIn real systems, prompts alone aren’t enough. Safety and reliability need to be enforced at the tool and system levels, not just in the instructions.\nWhat’s next: Reducing variance and catching errors automatically\nRanking #1 on the benchmark wasn’t the finish line for us. We’re already working on reducing variance, implementing smarter error detection, and splitting responsibilities across multiple specialized agents.\nIf data agents interest you, you can get involved. The open-source data agent code is already available on GitHub, and support for dbt will be added soon.\nIf you’d rather use agents than develop them, you can build Databao into your workflow or join us in building a proof of concept together. We’ll work with you to understand your use case, define a context-building process, and give the agent access to a selected group of business users. Together, we’ll evaluate the quality of the responses and overall satisfaction with the results.\n      \n      TALK TO THE TEAM",
        "dc:creator": "Dmitrii Mikhailovskii",
        "content": "As of February 2026, Databao Agent ranks #1 in the Spider 2.0–DBT benchmark. This ranking measures how well agents can operate in a real dbt project, including reading the repository, understanding what’s broken, implementing the missing models, and validating everything by actually running code. Our team ended up achieving the highest score in the benchmark, [&#8230;]",
        "contentSnippet": "As of February 2026, Databao Agent ranks #1 in the Spider 2.0–DBT benchmark. This ranking measures how well agents can operate in a real dbt project, including reading the repository, understanding what’s broken, implementing the missing models, and validating everything by actually running code. Our team ended up achieving the highest score in the benchmark, […]",
        "guid": "https://blog.jetbrains.com/?post_type=databao&p=683094",
        "categories": [
          "data-agent"
        ],
        "isoDate": "2026-02-24T07:54:58.000Z"
      },
      {
        "creator": "Rachel Appel",
        "title": "C# Extension Members",
        "link": "https://blog.jetbrains.com/dotnet/2026/02/23/csharp-extension-members/",
        "pubDate": "Mon, 23 Feb 2026 14:44:31 +0000",
        "content:encodedSnippet": "Overview: What are extension members?\nExtension members allow you to define additional members for existing types without modifying their definitions. With them, you can add functionality to existing types you don’t have access to or don’t control, for example, built-in types or types from an API or commercial library. \nExtension methods have been a feature of C# since version 3.0 in 2007, so the concept has been around for some time in .NET. However, traditional extension methods were just that – methods only. Extensions could not be created for properties, fields, or operators. You couldn’t create static extensions and they couldn’t easily participate in interfaces. However, new syntax in C# 14 allows both instance and static properties and methods, as well as operators.\nClassic extension methods\nLet’s quickly review what a classic extension method looks like. We’ll extend the DateTime structure to check the first Monday of any quarter. You might see code like this in manufacturing scenarios where production runs need to start on a specific day, such as the first Monday of a quarter. The code looks something like this:\n    public static DateTime FirstMondayOfQuarter(this DateTime dateTime, int quarter)\n    {\n        if (quarter is < 1 or > 4)\n            throw new ArgumentOutOfRangeException(nameof(quarter), \n                \"Quarter must be between 1 and 4.\");\n\n        var year = dateTime.Year;\n        var firstMonth = (quarter - 1) * 3 + 1;\n\n        var date = new DateTime(year, firstMonth, 1);\n\n        var offset = ((int)DayOfWeek.Monday - (int)date.DayOfWeek + 7) % 7;\n        return date.AddDays(offset);\n    }\nNotice that to make the extension method you must make the class and method static, and use the this keyword to indicate which type to extend. While the definition uses the static keyword, it’s not a static member. \nCode to use this extension method looks like the following:\nDateTime myDate = DateTime.Now;\n\nfor (var i = 1; i <= 4; i++)\n{\n    Console.WriteLine(myDate.FirstMondayOfQuarter(i).ToShortDateString());\n}\nBecause it’s not a static method, you can’t just call DateTime.FirstMondayOfQuarter(2). Calling DateTime.Now (or any DateTime member) creates a new instance of a DateTime.\nExtension members in C# 14\nUse the new extension block inside a static class to define extensions. The extension block accepts the receiver type (the type you want to make an extension for), and optionally, a receiver parameter name for instance members. Adding the parameter name is recommended for clarity. Here’s the syntax:\nextension(Type) { … }    // plain extension block\n\nextension(Type parameterName) { … }   // extension block with a parameter name\nIf we want to convert a classic extension method to a new extension member, we can use Rider. Rider has a handy intention action for this, just press Alt + Enter and choose Move to extension block:\n\n\n\n\nThe code to use it doesn’t change. However, you can now call the code without having to create an instance first, like this:\nConsole.WriteLine(DateTime.Now.FirstMondayOfQuarter(i).ToShortDateString());\nSo you won’t need to change any calling code unless you want to. \nTo create an extension property, use an extension block like you would for any extension member. The rest of the code looks very natural like regular C# code.\npublic static class DateTimeExtensions\n{\n    extension(DateTime date)\n    {\n        public bool IsWeekend => date.DayOfWeek is DayOfWeek.Saturday or DayOfWeek.Sunday;\n    }\n}\n\n// To use it:\n\nif (DateTime.Today.IsWeekend)\n{\n    // No work today, yay!\n}\nNotice that in the extension block you define methods, properties, and other members without using the this parameter syntax for each member.\nA goal of the C# team was to ensure that existing code doesn’t break, so then the syntax you use becomes a matter of style. There’s no need to change any of your existing extension methods, but Rider’s handy intention action makes it fast and easy to do so. \nIn Summary\nExtension members are beneficial for several scenarios, including transforming helper methods into properties, organizing related extensions, incorporating static constants or factories into existing types, defining operators on external types, and making third-party APIs feel more integrated or native.",
        "dc:creator": "Rachel Appel",
        "content": "Overview: What are extension members? Extension members allow you to define additional members for existing types without modifying their definitions. With them, you can add functionality to existing types you don’t have access to or don’t control, for example, built-in types or types from an API or commercial library.&#160; Extension methods have been a feature [&#8230;]",
        "contentSnippet": "Overview: What are extension members? Extension members allow you to define additional members for existing types without modifying their definitions. With them, you can add functionality to existing types you don’t have access to or don’t control, for example, built-in types or types from an API or commercial library.  Extension methods have been a feature […]",
        "guid": "https://blog.jetbrains.com/?post_type=dotnet&p=679230",
        "categories": [
          "net-tools",
          "c",
          "c-14"
        ],
        "isoDate": "2026-02-23T14:44:31.000Z"
      },
      {
        "creator": "Colette Des Georges",
        "title": "AI Tool Switching Is Stealth Friction – Beat It at the Access Layer",
        "link": "https://blog.jetbrains.com/ai/2026/02/ai-tool-switching-is-stealth-friction-beat-it-at-the-access-layer/",
        "pubDate": "Mon, 23 Feb 2026 13:19:21 +0000",
        "content:encodedSnippet": "Has your team’s sprint velocity actually improved since you approved all those AI coding tools?\nIf not, recent research by JetBrains and UC Irvine shows your developers may be facing a new dimension of context switching that resists the usual fixes.  \nThe key findings were that most AI-assisted developers switched in and out of their IDEs more but 74% of those surveyed didn’t notice it. When context switching doesn’t feel like context switching, behavioral policies won’t catch it.\nConsolidating AI tools would catch it but at the cost of flexibility. Model capabilities evolve constantly. Locking into one vendor limits your team’s ability to learn, experiment, and stay competitive.     \nThe good news is that there’s a solution that sidesteps both challenges – consolidating the access layer. \nHere’s the research behind it, why it works, and how to apply it. \nDevelopers complain about switching, just not this kind\nIn general, developers are outspoken about context switching killing productivity. Atlassian’s State of Developer Experience Report 2025 found developers citing switching context between tools as one of their biggest drags on productivity.\nAt the same time, developers report record productivity thanks to an ever-increasing array of AI tools. In the 2025 DORA State of AI-Assisted Software Development Report, respondents said that AI had a positive impact on delivery throughput, code quality, and almost every other key performance outcome. \nParadoxically DORA also found no relationship between AI adoption and reduced friction or burnout. The organizational wins weren’t translating to a lighter day-to-day experience.\nThis disconnect between experience and performance points to something deeper. When researchers combine self-reported perceptions with objective behavioral data, the gap becomes clear.\nIn the JetBrains/UC Irvine study mentioned above, 74% of surveyed AI-assisted developers didn’t notice an increase in their switching. Telemetry on 151 million IDE window activations across 800 developers told a different story. Over the two-year study period, AI users’ monthly window switching trended upward while non-AI users’ did not. This divergence was mostly invisible to those experiencing it. Conducted from October 2022 to October 2024, the research spanned ChatGPT’s launch and the initial scramble to adopt AI coding tools.\n74% said switching hadn’t gone up. \nTelemetry disagreed.\n\n\n\n\n\n\n\nExperienced open-source developers in a 2025 METR study believed AI tools made them 20% faster. Screen recordings showed the opposite.\n\n\n\n\n\nAll this research suggests that AI’s productivity benefits come with a hidden cost when distributed across different tools and interfaces. The switching feels productive and voluntary, so it is nearly impossible to manage behaviorally. When developers don’t perceive the friction, they can’t self-correct. When they don’t report it, you can’t coach around it.\nThe solution isn’t measuring or managing – it’s architectural. And there’s a proven pattern for architectural solutions to developer friction.\nThe platform-engineering lesson: Consolidation reduces cognitive load\nPlatform engineering is all about building internal tooling and infrastructure that lets developers self-service what they need without hitting speed bumps like tickets or approvals. The goal is to create “golden paths” that make the right ways the easy ways.\nTraditionally, platform engineering has focused on the “outer loop” of everything after git push. This includes CI/CD pipelines, deployment automation, infrastructure provisioning, and security scanning.\nAI tools, on the other hand, fragment the “inner loop” of everything before git push. GitLab’s 2025 Global DevSecOps Report found that 49% of development teams use more than five AI tools across use cases like code generation, testing, and documentation. \nStandardization was the top motivation for platform initiatives according to Weave Intelligence’s State of AI in Platform Engineering 2025 report, but standardizing around a single AI tool doesn’t work when different models are better at different tasks. \nReducing developers’ cognitive load was the second-highest motivation. Apply that principle to AI tools: consolidate the access layer, not the options.\nOne environment, multiple AI tools\nSince our study data was finalized in 2024, we’ve shipped two features that make JetBrains IDEs the consolidated access layer for your team’s AI tools of choice: \nBring Your Own Key (BYOK) lets your team use OpenAI, Anthropic, or any OpenAI-compatible provider with existing API keys. You maintain cost visibility through provider dashboards while developers access models directly in the IDE.\n\n\n\n\n\nNo browser tabs required. LLMs work inside the IDE.\n\n\n\n\n\nAgent Client Protocol (ACP) support means any ACP-compatible coding agent can work within JetBrains IDEs. ACP is an open standard we’re partnering with Zen on to ensure agents function across editors without vendor lock-in. The recently launched ACP Registry makes finding and configuring agents quick and easy.\n\n\n\n\n\n\n\n\nAll ACP-compatible agents are available in the IDE.\nTakeaway\nAI-related switching doesn’t surface the same way as shifts between meetings, projects, or traditional tools. Developers notice it less, so they report it less. Behavioral policies can’t apply to what isn’t visible.\nThe fix is architectural, not managerial. In platform engineering, this principle applies to post-commit workflows. Apply it to pre-commit AI workflows by standardizing where developers access the tools: in the environment where they already write, test, and debug code.",
        "dc:creator": "Colette Des Georges",
        "content": "Has your team&#8217;s sprint velocity actually improved since you approved all those AI coding tools? If not, recent research by JetBrains and UC Irvine shows your developers may be facing a new dimension of context switching that resists the usual fixes.&#160;&#160; The key findings were that most AI-assisted developers switched in and out of their [&#8230;]",
        "contentSnippet": "Has your team’s sprint velocity actually improved since you approved all those AI coding tools? If not, recent research by JetBrains and UC Irvine shows your developers may be facing a new dimension of context switching that resists the usual fixes.   The key findings were that most AI-assisted developers switched in and out of their […]",
        "guid": "https://blog.jetbrains.com/?post_type=ai&p=683012",
        "categories": [
          "insights",
          "jetbrains-ai",
          "ai-in-ides",
          "ai-in-software-development"
        ],
        "isoDate": "2026-02-23T13:19:21.000Z"
      },
      {
        "creator": "Julia Shashkova",
        "title": "IntelliJ IDEA 2025.3.3 Is Out!",
        "link": "https://blog.jetbrains.com/idea/2026/02/intellij-idea-2025-3-3/",
        "pubDate": "Fri, 20 Feb 2026 20:06:43 +0000",
        "content:encodedSnippet": "We’ve just released the next minor update for IntelliJ IDEA 2025.3 – v2025.3.3.\nYou can update to this version from inside the IDE, using the Toolbox App, or using snaps if you are a Ubuntu user. You can also download it from our website.\nHere are the most notable updates:\nMCP server output schema now correctly handles default properties in the schema, preventing invalid schema errors during structured output. [IJPL-230494]\nDownloading the IDE backend now works as intended with a configured proxy. [IJPL-164318]\nResolved an issue that prevented successful proxy authentication. [IJPL-231829]\nPackage annotations referenced through JAR dependencies are now correctly reflected in the PSI model. [IDEA-383732]\nTo find out more details about the issues resolved, please refer to the release notes.\nIf you encounter any bugs, please report them to our issue tracker.\nHappy developing!",
        "dc:creator": "Julia Shashkova",
        "content": "We’ve just released the next minor update for IntelliJ IDEA 2025.3 – v2025.3.3. You can update to this version from inside the IDE, using the Toolbox App, or using snaps if you are a Ubuntu user. You can also download it from our website. Here are the most notable updates: To find out more details [&#8230;]",
        "contentSnippet": "We’ve just released the next minor update for IntelliJ IDEA 2025.3 – v2025.3.3. You can update to this version from inside the IDE, using the Toolbox App, or using snaps if you are a Ubuntu user. You can also download it from our website. Here are the most notable updates: To find out more details […]",
        "guid": "https://blog.jetbrains.com/?post_type=idea&p=682846",
        "categories": [
          "releases",
          "bug-fix-update",
          "intellij-idea",
          "intellij-idea-2025-3"
        ],
        "isoDate": "2026-02-20T20:06:43.000Z"
      },
      {
        "creator": "Arseniy Terekhov",
        "title": "Write Modern Go Code With Junie and Claude Code",
        "link": "https://blog.jetbrains.com/go/2026/02/20/write-modern-go-code-with-junie-and-claude-code/",
        "pubDate": "Fri, 20 Feb 2026 13:29:52 +0000",
        "content:encodedSnippet": "Go developers can now confidently write modern Go code using the AI agents Junie and Claude Code. We at JetBrains have just released a new plugin with guidelines for AI agents that ensure they use the latest features and follow best Go practices compatible with your current version of Go, as specified in go.mod. You can find the relevant GitHub repository at go-modern-guidelines.\nWhy do you need it?\nWith two major releases every year, Go is one of the more frequently updated languages. Yet we’ve found that all AI agents – Junie and Claude Code included – tend to generate obsolete Go code.\nThe Go team also pointed to that problem in one of their articles: “During the frenzied adoption of LLM coding assistants, we became aware that such tools tended – unsurprisingly – to produce Go code in a style similar to the mass of Go code used during training, even when there were newer, better ways to express the same idea.”\nAs an example, here’s a sample code snippet of how an agent used a manual loop to find an element in the slice:\n// HasAccess checks if the user's role has access to protected resources.\nfunc HasAccess(role string) bool {\n    for _, allowed := range allowedRoles {\n       if role == allowed {\n          return true\n       }\n    }\n    return false\n}\nThere are two main reasons why agents would favor obsolete architecture where cleaner and more idiomatic solutions are available:\nData cutoff: Even the most current AI models are trained on time-bound datasets that have a “cutoff” date (for example, for Claude Opus 4.6, that’s May 2025). They will not recognize or suggest features that were introduced after that point, such as those found in the Go 1.26 release.\nFrequency bias: AI models are primarily trained on open-source codebases that may not be readily updated. It’s natural that such datasets will feature more “old” code than “new”, and since AI models favor alternatives that are more frequent, they will suggest obsolete code as a result.\nJust like the Go team, we at GoLand, strive to keep the Go ecosystem modern and idiomatic. So to mitigate the effect of AI agents contributing to this issue, we’ve created a plugin with a set of guidelines for Junie and Claude Code, that helps them generate modern Go code. The plugin automatically recognizes the current version of your Go code (specified in go.mod) and instructs the agent to use corresponding features, such as new(val) instead of x := val; &x or errors.AsType[T](err) instead of errors.As(err, &target) (both introduced in Go 1.26), as well as stdlib additions available up to and including that version.\nHere’s the same code sample from before with our guidelines applied, where the agent actually uses the modern slices.Contains() method introduced in Go 1.21.\nvar allowedRoles = []string{\"admin\", \"moderator\", \"editor\"}\n\n// HasAccess checks if the user's role has access to protected resources.\nfunc HasAccess(role string) bool {\n\treturn slices.Contains(allowedRoles, role)\n}\nHow to enable\nIn Junie\nFor Junie version 2xx.620.xx or higher, you don’t have to do anything – these guidelines will be applied right out of the box.\nIf you’re running an older version, go to Settings → Plugins → Installed, find Junie, and then click Update.\nIf, for some reason, you want to disable these settings, you can do so in Settings → Tools → Junie → Project Settings → Go. The Provide modern Go guidelines option is enabled by default, so untick the box.\nDownload GoLand\n                                                    \nIn Claude Code\nTo use these guidelines in Claude Code, run the following commands inside a Claude Code session to install it.\nAdd this repository as a marketplace:\n/plugin marketplace add JetBrains/go-modern-guidelines\nInstall the plugin:\n/plugin install modern-go-guidelines\nTo activate it, run the following command at the start of a session:\n/use-modern-go\nClaude Code will then detect the Go version from go.mod and instruct the agent to use modern features compatible with that version.\n> /use-modern-go\n\nThis project is using Go 1.24, so I'll stick to modern Go best practices\n\nand freely use language features up to and including this version.\n\nIf you'd prefer a different target version, just let me know.\nAfter this, any Go code that the agent writes will follow the guidelines.\nHappy coding!\nThe GoLand team",
        "dc:creator": "Arseniy Terekhov",
        "content": "Go developers can now confidently write modern Go code using the AI agents Junie and Claude Code. We at JetBrains have just released a new plugin with guidelines for AI agents that ensure they use the latest features and follow best Go practices compatible with your current version of Go, as specified in go.mod. You [&#8230;]",
        "contentSnippet": "Go developers can now confidently write modern Go code using the AI agents Junie and Claude Code. We at JetBrains have just released a new plugin with guidelines for AI agents that ensure they use the latest features and follow best Go practices compatible with your current version of Go, as specified in go.mod. You […]",
        "guid": "https://blog.jetbrains.com/?post_type=go&p=682681",
        "categories": [
          "goland",
          "news"
        ],
        "isoDate": "2026-02-20T13:29:52.000Z"
      },
      {
        "creator": "Kris Kang",
        "title": "The Missing Link Between AI and Business Value",
        "link": "https://blog.jetbrains.com/ai/2026/02/the-missing-link-between-ai-and-business-value/",
        "pubDate": "Fri, 20 Feb 2026 06:27:54 +0000",
        "content:encodedSnippet": "Let’s call a spade a spade. Some enterprises are already using AI agents, but very few can explain their impact on business performance.\nMetrics such as DORA, SPACE, and developer experience indicators captured through third-party platforms offer insight into delivery velocity and developer quality of life, but it is still difficult to cleanly map this all the way to business impact. \nUnless you work directly on model development, model metrics themselves rarely determine whether AI is creating enterprise value. \nThe gap between technical performance signals and sustained business outcomes is an obstacle to scaling AI responsibly.\nFrom technical metrics to business value\nAbstract benchmarks such as SWE-Bench Pro and Tau2-bench are directionally useful in selecting AI tools, but can be orthogonal to how these tools perform in enterprise systems. An agent that performs well in a controlled environment can fail once integrated into production workflows. What matters is not benchmark scores, but the impactfulness, traceability, and resilience of AI systems under real-world conditions. \nRecent data underscores the urgent need to find an accurate way of measuring these variables. Though 88% of employees use AI at work today, just 5% use it “in a transformative way”, according to the EY 2025 Work Reimagined Survey.\nBlindly adopting AI is unlikely to be fruitful. Enterprises should instead experiment with and evaluate AI through operational metrics on the systems they are accountable to build and operate. The focus should be on the lifetime cost of maintaining systems, the average time humans spend over baseline, and throughput as a function of Total Cost of Ownership (TCO).\nAuditability matters for tracing decisions and meeting governance needs, while human readability ensures teams can understand and manage system behavior now, and later. These are table stakes for technical teams to have in place as they adopt AI at scale.\nThe ROI problem\nEvery enterprise wants to link AI to ROI, but the data rarely aligns. The problem is not limited to model telemetry. AI is embedded into enterprise systems and assigned responsibility for specific parts of the SDLC and operational workflows.\nEvidence of its impact must therefore span system behaviour, human intervention, and downstream business KPIs. These signals live in different systems and move on different timescales, which creates a gap between AI activity and measurable business outcomes. This is why most organizations rely on proxies or assumptions rather than proof.\nClosing the gap\nThe next generation of AI orchestration platforms will need to close this gap by correlating technical performance with operational and financial signals. When those systems mature, ROI will shift from being an abstract target to a measurable outcome grounded in data.\nThe impact of this gap is already visible in enterprise outcomes. The WRITER 2025 Enterprise AI Adoption Report found that organizations without a formal AI strategy report only 37% success when adopting AI, compared with 80% for those that tie performance to clear operational outcomes. \nThe data is unambiguous. Only when an organization measures technical and operational signals together does it finally gain a true picture of AI’s value.\nTowards continuous benchmarking\nWhat underlies enterprise AI is not static. Data drifts, workflows evolve, and compliance obligations expand. Measurement must therefore become a continuous feedback loop rather than an annual report. \nThe same principle should apply across the enterprise: Performance metrics should remain stable, but they must either stay independent of changing conditions or explicitly measure those changes over time.\nMeasuring what matters\nMeaningful AI performance measurement is not about bigger numbers or more dashboards. It is about connecting operational signals with business truth. \nEnterprise leaders must grapple with model performance alongside how intelligently it scales, how transparently it operates, and how clearly its impact can be proven.\nTaking benchmark numbers at face value resembles trusting a car manufacturer’s fuel efficiency without ever driving the car to see if it holds up in real conditions. \nOnly when these can be addressed with real data will AI become a truly accountable part of the enterprise stack.\nThe real question for leaders is simple: Are you measuring the numbers that prove AI is working in practice, or just parroting back the numbers on public benchmarks?",
        "dc:creator": "Kris Kang",
        "content": "Let’s call a spade a spade. Some enterprises are already using AI agents, but very few can explain their impact on business performance. Metrics such as DORA, SPACE, and developer experience indicators captured through third-party platforms offer insight into delivery velocity and developer quality of life, but it is still difficult to cleanly map this [&#8230;]",
        "contentSnippet": "Let’s call a spade a spade. Some enterprises are already using AI agents, but very few can explain their impact on business performance. Metrics such as DORA, SPACE, and developer experience indicators captured through third-party platforms offer insight into delivery velocity and developer quality of life, but it is still difficult to cleanly map this […]",
        "guid": "https://blog.jetbrains.com/?post_type=ai&p=678856",
        "categories": [
          "ai",
          "opinion",
          "agentic-ai",
          "enterprise-ai",
          "roi"
        ],
        "isoDate": "2026-02-20T06:27:54.000Z"
      },
      {
        "creator": "Dmitrii Korovin",
        "title": "TeamCity 2025.11.3 Is Here",
        "link": "https://blog.jetbrains.com/teamcity/2026/02/teamcity-2025-11-3-bug-fix/",
        "pubDate": "Thu, 19 Feb 2026 16:48:20 +0000",
        "content:encodedSnippet": "Today we’re rolling out another bug-fix update for your TeamCity 2025.11 On-Premises servers. This update addresses a number of issues, inlcuding:\nQoL updates for the bundled IntelliJ Platform Plugin;\nVariable GID value for Docker inside TeamCity Docker images;\nNPE login fails;\nConnection settings are reset after changing the parent project.\nAll TeamCity bug-fix updates include performance and security improvements, so we recommend that you never skip these minor updates. See TeamCity 2025.11.3 Release Notes for the complete list of resolved issues.\nWhy update?\nStaying up to date with minor releases ensures your TeamCity instance benefits from the following:\nPerformance improvements.\nBetter compatibility with integrations.\nFaster, more stable builds.\nEnhanced security for your workflows.\nCompatibility\nTeamCity 2025.11.3 shares the same data format as all 2025.11.x releases. You can upgrade or downgrade within this series without the need for backup and restoration.\nHow to upgrade\nUse the automatic update feature in your current TeamCity version.\nDownload the latest version directly from the JetBrains website.\nPull the updated TeamCity Docker image.\nNeed help?\nThank you for reporting issues and providing feedback! If you have questions or run into any problems, please let us know via the TeamCity Forum or Issue Tracker.\nHappy building!",
        "dc:creator": "Dmitrii Korovin",
        "content": "Today we&#8217;re rolling out another bug-fix update for your TeamCity 2025.11 On-Premises servers. This update addresses a number of issues, inlcuding: All TeamCity bug-fix updates include performance and security improvements, so we recommend that you never skip these minor updates. See TeamCity 2025.11.3 Release Notes for the complete list of resolved issues. Why update? Staying [&#8230;]",
        "contentSnippet": "Today we’re rolling out another bug-fix update for your TeamCity 2025.11 On-Premises servers. This update addresses a number of issues, inlcuding: All TeamCity bug-fix updates include performance and security improvements, so we recommend that you never skip these minor updates. See TeamCity 2025.11.3 Release Notes for the complete list of resolved issues. Why update? Staying […]",
        "guid": "https://blog.jetbrains.com/?post_type=teamcity&p=682588",
        "categories": [
          "bug-fix"
        ],
        "isoDate": "2026-02-19T16:48:20.000Z"
      },
      {
        "creator": "Alina Dolgikh",
        "title": "Java to Kotlin Conversion Comes to Visual Studio Code",
        "link": "https://blog.jetbrains.com/kotlin/2026/02/java-to-kotlin-conversion-comes-to-visual-studio-code/",
        "pubDate": "Thu, 19 Feb 2026 16:25:00 +0000",
        "content:encodedSnippet": "At JetBrains, we aim to make Kotlin development as accessible and efficient as possible across the entire ecosystem. While IntelliJ IDEA remains the premier IDE for Kotlin, we recognize that many developers use Visual Studio Code for a variety of tasks and projects.\nTo help streamline the transition from Java to Kotlin for VS Code users, we are pleased to introduce the Java to Kotlin (J2K) converter extension.\nDownload the Extension\n         \nSeamless Conversion in VS Code\nThis new extension allows you to convert individual Java files into Kotlin code with a simple context menu action, significantly reducing the manual effort required when migrating legacy codebases or switching languages mid-project.\nBecause the extension leverages the same underlying engine used in our primary IDEs, you can expect a reliable conversion that respects Kotlin idioms and syntax requirements.\nSee it in action\nThe converter is designed to be unobtrusive and easy to use. Watch the short demo below to see how it handles the conversion process.\n\n\n\n\n\n\nHow to get started\nTo begin using the converter, simply:\nInstall the Java to Kotlin Converter extension from the Visual Studio Marketplace.\nOpen any .java file in your workspace.\nRight-click anywhere in the editor or on the file in the Explorer and select Convert to Kotlin.\nOur Commitment to the Ecosystem\nThis extension is part of our ongoing effort to support Kotlin users wherever they choose to write code. It joins other initiatives aimed at improving the developer experience outside of IntelliJ IDEA, such as the Kotlin LSP, which provides IDE-independent language support via the Language Server Protocol.\nAs this is a new release, we highly value your feedback. If you encounter any issues or have suggestions for improvements, please report them on YouTrack or through the extension’s marketplace page.",
        "dc:creator": "Alina Dolgikh",
        "content": "At JetBrains, we aim to make Kotlin development as accessible and efficient as possible across the entire ecosystem. While IntelliJ IDEA remains the premier IDE for Kotlin, we recognize that many developers use Visual Studio Code for a variety of tasks and projects. To help streamline the transition from Java to Kotlin for VS Code [&#8230;]",
        "contentSnippet": "At JetBrains, we aim to make Kotlin development as accessible and efficient as possible across the entire ecosystem. While IntelliJ IDEA remains the premier IDE for Kotlin, we recognize that many developers use Visual Studio Code for a variety of tasks and projects. To help streamline the transition from Java to Kotlin for VS Code […]",
        "guid": "https://blog.jetbrains.com/?post_type=kotlin&p=682675",
        "categories": [
          "kotlin",
          "news",
          "release"
        ],
        "isoDate": "2026-02-19T16:25:00.000Z"
      },
      {
        "creator": "Katie Fraser",
        "title": "How Students Are Using AI-Powered Hints in Programming Courses",
        "link": "https://blog.jetbrains.com/research/2026/02/how-ai-powered-hints-used/",
        "pubDate": "Thu, 19 Feb 2026 12:02:23 +0000",
        "content:encodedSnippet": "Learning to code in an online course can feel like hitting a wall again and again, without much help moving forward. Our research team wanted to change that. That’s why our Education Research team built an AI-powered hints tool that doesn’t just point out errors but helps students understand them. As described in our blog post last summer, students can use the tool in JetBrains Academy courses to get tailored guidance nudging them towards the solution instead of staring blankly at a stubborn piece of code or waiting for help on a forum. \nTry our plugin\n                                                    \nSince then, our researchers have investigated how students actually use the tool and published the results in a paper that they will present at the Technical Symposium on Computer Science Education (SIGCSE TS) February 19 in St. Louis, Missouri. In this blog post, we will explain how the study was set up, its results, and our analysis.\nLearning to code with the help of smart feedback\nIn the last post, we covered the history of online learning and the importance of feedback in any type of learning environment. Online programming courses can differ a lot in format, especially in how they encourage students to practice writing and running code within the course. \nMany online learning platforms, such as Udemy, Codecademy, and edX, provide online editors for students to complete the exercises. Learning programming inside a code editor is easier than installing and learning new software, but in the long run it means that the student is unfamiliar with the relevant professional environment. Other platforms, such as Hyperskill, offer a hybrid option, where the student can also learn in a professional integrated developer environment (IDE) if they choose to do so. The downside of a hybrid course is that some students might not choose to use the IDE, for example if they consider it a hurdle to install and learn to navigate new software.\nFor our JetBrains Academy courses, we released the JetBrains Academy plugin a few years ago, which lets educators build complete courses directly inside the IDE. With this setup, students can read theory and immediately apply it by solving practical tasks in the very same environment they use to write code. As a result, they can become comfortable working in IDEs early on and are better prepared for real-world software development roles. \nA recent addition to our plugin is the AI-powered hints tool, developed by our   Education Research team. The tool leverages powerful IDE features by combining static analysis and code quality checks with LLMs to provide personalized hints to students. In the previous paper, we conducted evaluation studies to gauge how useful students found the tool while learning, as described in the previous blog post.\nTo help students even further, we wanted to know more about how they are really interacting with the tool and discover possible new behavior patterns. In the rest of this blog post, we will talk about exactly this: how students are actually interacting with the AI-powered hints tool.\nSmart feedback in online learning\nRecent years have seen many studies exploring how students interact with intelligent tutoring systems, offering useful directions for future system design. In this short subsection, we will provide a summary of recent studies, plus describe what our study adds to the field.\nResearchers in this study examined an intelligent next-step hint system to understand why students ask for help, looking at factors such as time elapsed and code completeness after each hint, and identifying reasons like difficulty starting an exercise. This work provides general insights into students’ hint-seeking patterns, but without the details (e.g. keystroke data). This study analyzed when and how to provide feedback and hints using keystroke datasets annotated with points where experts would intervene, but did not look at students’ actual interactions with hint systems. \nIn this paper, researchers investigated how different hint levels support task completion and why students request hints, using a think‑aloud study with pre‑ and post‑tests for 12 students. However, we would have liked to see broader usage patterns or how learners cope with unhelpful hints.\nMore recently, researchers studied how students use on-demand automated hints in an SQL course and how these hints affect learning, using A/B testing to identify behavioral differences and simple patterns. For example, they found that students often request a second hint within 10 seconds of the first one. While this research sheds light on basic behavioral trends, it mainly asks whether adding a hint system supports learning, rather than closely examining the interaction patterns themselves. \nOverall, prior work has focused on why and when hints are given, but far less on how students actually work with them in practice. In our study, we applied process mining techniques to uncover detailed behavioral patterns in how students interact with hints while they solve programming tasks. In addition, we conducted interviews with a subset of the students to learn more about how they interact with the hints tool in specific scenarios.\nOur investigation into students’ behavioral patterns\nWith our study, we wanted to better understand how the students are really interacting with the tool, as well as how students navigate situations with less helpful hints. Specifically, our two research questions were:\nWhat behavioral patterns can be identified based on students’ interaction with the next-step hint system?\nWhat strategies do students use to overcome unhelpful hints?\nOur study collected data in two steps. First, we collected information about all of the students’ interactions with the hints system, analyzing these interactions quantitatively. Then, we selected a subset of six students to interview for about 30 minutes, so that we had a qualitative and in-depth complement to the quantitative analysis. In the next subsection, we will tell you about the methodology for each step. The subsection after that will present the results and analysis.\nQuantitative behavioral analysis\nFor the first part of the study, we analyzed detailed problem-solving data from first- and second-year Bachelor’s students working on programming tasks in our introductory Kotlin course. The selected projects represent different levels of complexity and cover basic topics such as variables, loops, conditional operators, and functions. The 34 participants had previously completed at least one programming course in another language, and they had no experience with Kotlin before. We asked these students to complete three projects from the course, using the hint system whenever they wanted. \nWe used KOALA to capture keystrokes, IDE actions, the windows they accessed, and rich information about their hint usage. Hint-usage information included details like when hints were requested and for which specific tasks. The resulting dataset contains:\n6,658,936 lines of code\n1,364,943 IDE activity events\n960 textual hint requests\n453 code hint requests \nNote: This dataset is open source, and we encourage you to take a look. It can be used for further analysis in a research project, or even just for educators to see how beginner learners are programming.\nProcess mining and our dataset: Preliminaries\nAfter collecting this data, we conducted a quantitative analysis of the students’ behavior. To do this, we used a process mining technique. Process mining turns log or event data into behavioral models using action analysis, taking into account both event frequency and sequence. In business settings, the technique can help companies see where improvements are needed, as is argued for in this process mining manifesto, and save a lot of money, for example, by finding instances of duplicate payments or minimizing complex order delays.\nAlthough it is primarily used in industry settings, process mining has already been applied in educational domains (see this paper and this paper for examples). As far as we know, we are the first to use process mining to analyze programming hint systems.\nTo apply this technique to our hint-systems dataset, we first extracted all hint-request sessions from the dataset (1,050 in total). The data from these sessions are the input for the analysis, which uses specialized algorithms to map the activities and transitions between them.\nThe sessions began when a student requested a hint. By clicking Get Hint or Retry, they ended when it was clear that the hint was processed, either by accepting the code hint or closing the text hint banner. The session was also considered “over” if the student ran into an error or there was no activity for five minutes.\nWe also defined different activities within sessions. Here are the activities:\nHint Button Clicked: The student clicks Get Hint to generate a hint.\nHint Retry Clicked: The student clicks Retry to regenerate a hint.\nHint Code Generated: The code hint is generated. By tool design, this action is triggered every time Get Hint is clicked.\nHint Banner Shown: The banner with a textual hint is shown to the student. Although this action is carried out by the system, not by the student, it marks the moment when the student has viewed the hint. This action will be used for a more detailed analysis.\nShow Code Hint Clicked: The student clicks Show Code to show a code hint.\nHint Accepted: The student clicks Accept to accept a code hint.\nHint Banner Closed: The student closes the textual hint banner.\nHint Canceled: The student clicks Cancel to decline a code hint.\nError Occurred: This action includes cases where internal errors occurred during the hint generation process, e.g. problems with the internet connection, or when the LLM did not provide a response.\nAll sessions began with items 1 or 2, i.e. clicking Get Hint or Retry. A session could end with different activities, with the most common being items 6, 8, and 9: Hint Accepted, Hint Canceled, or Error Occurred. \nIn the analysis, we coded the sequence of activities within each session. We also grouped the sessions into the following three main types:\nPositive Student accepted the code hint\nNeutral Student saw a hint but didn’t clearly accept or reject it\nNegative Student canceled a hint or got an error\nThe first and last types are straightforward: For the positive type, the session ended when the student accepted the code hint. For the negative, the session ended because the student did not use the hint, having received an error or actively cancelled the hint. \nThe neutral type will be looked at in more detail in the next section, as it required additional analysis to determine how helpful the student found the hint. We know by the actions carried out that the student saw a text or code hint, but they proceeded without explicitly accepting the code hint. Examples of neutral sessions include the following (Hint Code Generated has been omitted, as it automatically occurs as the second step in each):\nHint Retry Clicked → Hint Banner Shown \nHint Button Clicked → Hint Banner Shown → Show Code Hint Clicked\nHint Retry Clicked → Hint Banner Shown → Hint Banner Closed\nThe numbers below shows the distribution of session types among the 1,050 total sessions.\nPositive: 299 (28%)\nNeutral: 420 (40%)\nNegative: 284 (27%)\n\n\n\n\nThe above shows that strictly positive sessions make up about a third of all sessions. As will be discussed below, we found out that the neutral cases often actually were successful in helping the students with their tasks, even if not positive as defined above. Namely, in these cases students clearly analyzed and used the content of the hints but did not automatically apply them.\nNote that 266 of the 284 (94%) negative sessions ended with errors, rather than the student cancelling the hint. These errors were either internet connection issues on the student side or a system issue on our side; we have already fixed any such system errors. In addition to these three types listed above, we collected 47 data points, accounting for 4,5% of the sessions, that were not classifiable based on the available data. \nWith these preliminary data about the “events”, we were able to move on to applying the process mining technique for our analysis. The next subsection describes these results. \nProcess mining and our dataset: Analysis\nWe turned the data described in the previous subsection into a map of how students moved through the system. This sort of map is also called a process behavior graph. The graph for our dataset is displayed below; as it is quite complex, we will explain the most important parts individually.\n\n\n\n\nIn the above graph, some boxes are colored, while others are not. The blue boxes at the top represent activities that begin a session, either by generating a new hint or retrying a hint; the red boxes on the left represent activities that end a negative session; the green box at the bottom is the activity that can end a positive session; and the other activities represent activities that can otherwise happen during a session. \nThe arrows indicate the direction of transitions, and the numbers next to the arrows indicate the average time in seconds for each transition. Longer times, particularly those around 900 seconds, indicate that the student was solving the task and then returned for another hint. In those cases, a new event would have been triggered.\nEach arrow’s transparency and thickness represent information about the frequency and average prevalence. As can be seen in the fragment below, the most frequent transitions occurred from the blue box Hint Button Clicked to white boxes Hint Code Generated and then to Hint Banner Shown. \n\nThe arrows that circle back to themselves, such as in the snippet below, represent those instances in which students repeatedly clicked the same button. In the case of the white box Show Code Hint Clicked, we think this suggests that the student might not have understood how the code hint worked, and that a future iteration of the tool would need to provide more explanation to help the student.\n\nLooking more closely at the negative sessions, we can see in the following image two arrows coming from the red box labelled ERR. The right arrow with an 18.7-second average leads to HRC, or Hint Retry Clicked. The left arrow with > 900 s leads to HBC, or Hint Button Clicked.\n\nThese patterns indicate that even when the student encountered an error, such as a bad internet connection, they persisted in generating a new hint, either within the same task or in another. This is an important insight because it shows that students are motivated to use the hints tool even when encountering errors or negative scenarios.\nIn the image below, we can also see that between the blue Hint Button Clicked and various white boxes, there is an arrow going back to the blue HBC button. We interpret these transitions as indicating that the students modified their code to generate additional variations of the same hint before integrating the information into a solution. That is, the pattern suggests that they did not simply click Accept Hint, but tried to solve the task themselves.\n\nIn the classification part of the analysis described in the previous subsection, these types of activities usually comprised the sessions labelled as neutral, as they often did not end with the student accepting the hint. These and other unexpected patterns were why we wanted to hear more from these students about why they behaved the way they did.\nQualitative behavioral analysis\nAs described in the previous section, we observed three types of sessions when the students interacted with the AI-powered hint system. We categorized a session as positive when the student ended the trajectory by clicking Accept Hint, as the tool is working as intended. We categorized a session as negative if the student received an error message or they themselves canceled the hint. Interestingly, the students still persevered in trying for new hints despite errors. \nFinally, we categorized sessions as neutral when the student clearly saw the hint but did not end the session like in positive or negative sessions. This means they saw the hint, but did not apply it to their code; for this reason, we do not consider these sessions negative or unsuccessful. Our analysis shows that neutral sessions make up 40% of all sessions, and we were curious about what exactly the students’ strategies were. \nIn order to learn more about what the students were thinking, we selected a subset of the participants so that they represented different experience levels, as well as locations. We conducted six semi-structured interviews, each lasting approximately 30 minutes. The interviews had two main sections: asking the participants about their experience with AI-powered hints and about any other forms of assistance they might have used while completing the tasks.\nLet’s look at what is going on in the neutral scenarios. We will focus on reporting two kinds of behaviors in these neutral scenarios: selective use of hints and combining partial solutions.\nSelective use of hints\nFrom these interviews, we learned that some participants chose to work with the code hints manually instead of clicking Accept Hint. The reasons for this depended on the situation: sometimes they chose to manually alter their code for improved learning, i.e. so that they could better remember how to fix the problem next time; sometimes they only wanted to use part of the code hint. We call this behavior selective use of hints.\nIn the log data, we could see that the students opened the visual diff window in the code editor. This window allows the student to view both their own code and the code hint. After opening this window, the students then manually typed in the recommended code from the hint – if the hint was clear. \nWe can show you two examples where the hints were unclear. In the first, the text hint suggested that an image should be trimmed before applying filters, and that this trimming could be done by storing a temporary result in a variable. This can be seen in the image below, inside the box bordered by a dotted line.\n\nAs can be seen above, the student did not completely follow the hint; instead they only trimmed the image and did not create a new variable. An advantage of the in-IDE format over a course in an online editor is that the IDE will show the student an optimized way to proceed with the removed variable, even though the hint had suggested otherwise. The student’s strategy in this example suggests that students are not unquestioningly accepting hints, but are actually analyzing and adapting them for their benefit. \nIn a second case, the text hint suggested that the student should add a Boolean variable to store the result of a game. The image below displays the hint, as well as the various student attempts, which included a few compilation errors. \n\nThe text hint was not wrong, but it did not provide the full story: in Kotlin, Boolean variables can be initialized later when used in a loop, and so it provided the code without any initial value. Based on the student’s attempts to compile, they did not understand this language-specific feature. \nFrom looking more closely at the students’ behavior concerning selective use of hints, we learned both that students are actively trying to adapt the hints so that they can learn better and that, in creating the hints, we should take into account language-specific features that a beginner student might not yet know about. Both takeaways form a solid basis for future investigations. \nCombining partial solutions\nIn the qualitative analysis of the AI-powered hints tool, we additionally learned that the students sometimes combined information from multiple hint attempts, instead of using only one hint. In the log data, this appeared as clusters of closely spaced hint requests on the same portion of code, followed by students editing the solution themselves rather than just copy-and-pasting code. This behavior is depicted schematically in the image below.\n\nThis behavior, which we call combining partial solutions, can be seen as a strategy to develop a working solution by troubleshooting and making small changes to their solutions, each time generating a new hint. As with the previous behavior, students did not accept the code hint, which is why we categorized it as a neutral and not a positive scenario. \nWithin this analysis, we found that students often toggled between their original code version and multiple hints: they kept the previous code hint windows open. We did not expect this behavior from students using the hint tool and will investigate further in future work.\nWe also observed that students sometimes copied their original code outside the task environment, reusing it later alongside new attempts. In the end, they were able to obtain a working solution with this strategy. This is another behavior that we did not expect from the students, and we are interested in investigating it further in the future. More specifically, we would like to provide the participants with multiple hints simultaneously (instead of sequentially), so that they can construct a working solution efficiently and effectively. \nFrom this analysis, we have learned that the hint tool could benefit from better support for partial adoption of hints and designs that embrace the way students naturally mix system suggestions with their own understanding and ideas, so that the student does not have to manually open multiple windows or copy-and-paste code from outside the IDE.\nFurthermore, what we can understand from this behavior of combining partial solutions is that the students are active problem solvers. That is, they are not just clicking through the hints so that they can advance to the next step or lesson. Instead, students analyze the hints and adapt them to get even a better solution – critical thinking skills like these are invaluable in the LLM era.\nExplore it yourself \nInterested in learning programming with the help of our AI-powered hints tool? Check out our JetBrains Academy course catalog. \n\nYou can also use our data to do your own research to discover something new about how students use smart feedback in online programming courses. \nExplore our dataset\n                                                    \nAnd finally, if you’re interested in collaborating with our team, let us know!",
        "dc:creator": "Katie Fraser",
        "content": "Learning to code in an online course can feel like hitting a wall again and again, without much help moving forward. Our research team wanted to change that. That’s why our Education Research team built an AI-powered hints tool that doesn’t just point out errors but helps students understand them. As described in our blog [&#8230;]",
        "contentSnippet": "Learning to code in an online course can feel like hitting a wall again and again, without much help moving forward. Our research team wanted to change that. That’s why our Education Research team built an AI-powered hints tool that doesn’t just point out errors but helps students understand them. As described in our blog […]",
        "guid": "https://blog.jetbrains.com/?post_type=research&p=682019",
        "categories": [
          "research",
          "ai-based-hints",
          "education-research",
          "jetbrains-academy",
          "jetbrains-research"
        ],
        "isoDate": "2026-02-19T12:02:23.000Z"
      },
      {
        "creator": "Cheuk Ting Ho",
        "title": "LangChain Python Tutorial: 2026’s Complete Guide",
        "link": "https://blog.jetbrains.com/pycharm/2026/02/langchain-tutorial-2026/",
        "pubDate": "Thu, 19 Feb 2026 10:40:15 +0000",
        "content:encodedSnippet": "If you’ve read the blog post How to Build Chatbots With LangChain, you may want to know more about LangChain. This blog post will dive deeper into what LangChain offers and guide you through a few more real-world use cases. And even if you haven’t read the first post, you might still find the info in this one helpful for building your next AI agent.\nLangChain fundamentals\nLet’s have a look at what LangChain is. LangChain provides a standard framework for building AI agents powered by LLMs, like the ones offered by OpenAI, Anthropic, Google, etc., and is therefore the easiest way to get started. LangChain supports most of the commonly used LLMs on the market today.\nLangChain is a high-level tool built on LangGraph, which provides a low-level framework for orchestrating the agent and runtime and is suitable for more advanced users. Beginners and those who only need a simple agent build are definitely better off with LangChain.\nWe’ll start by taking a look at several important components in a LangChain agent build.\nAgents\nAgents are what we are building. They combine LLMs with tools to create systems that can reason about tasks, decide which tools to use for which steps, analyze intermittent results, and work towards solutions iteratively.\n\n\n\n\nCreating an agent is as simple as using the `create_agent` function with a few parameters:\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n\n   \"gpt-5\",\n\n   tools=tools\n\n)\nIn this example, the LLM used is GPT-5 by OpenAI. In most cases, the provider of the LLM can be inferred. To see a list of all supported providers, head over here.\nLangChain Models: Static and Dynamic\nThere are two types of agent models that you can build: static and dynamic. Static models, as the name suggests, are straightforward and more common. The agent is configured in advance during creation and remains unchanged during execution.\nimport os\n\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nmodel = init_chat_model(\"gpt-5\")\n\nprint(model.invoke(\"What is PyCharm?\"))\n\nDynamic models allow you to build an agent that can switch models during runtime based on customized logic. Different models can then be picked based on the current state and context. For example, we can use ModelFallbackMiddleware (described in the Middleware section below) to have a backup model in case the default one fails.\nfrom langchain.agents import create_agent\n\nfrom langchain.agents.middleware import ModelFallbackMiddleware\n\nagent = create_agent(\n\n   model=\"gpt-4o\",\n\n   tools=[],\n\n   middleware=[\n\n       ModelFallbackMiddleware(\n\n           \"gpt-4o-mini\",\n\n           \"claude-3-5-sonnet-20241022\",\n\n       ),\n\n   ],\n\n)\nTools\nTools are important parts of AI agents. They make AI agents effective at carrying out tasks that involve more than just text as output, which is a fundamental difference between an agent and an LLM. Tools allow agents to interact with external systems – such as APIs, databases, or file systems. Without tools, agents would only be able to provide text output, with no way of performing actions or iteratively working their way toward a result.\nLangChain provides decorators for systematically creating tools for your agent, making the whole process more organized and easier to maintain. Here are a couple of examples:\nBasic tool\n@tool\n\ndef search_db(query: str, limit: int = 10) -> str:\n\n   \"\"\"Search the customer database for records matching the query.\n\n   \"\"\"\n\n...\n\n   return f\"Found {limit} results for '{query}'\"\nTool with a custom name\n@tool(\"pycharm_docs_search\", return_direct=False)\n\ndef pycharm_docs_search(q: str) -> str:\n\n   \"\"\"Search the local FAISS index of JetBrains PyCharm documentation and return relevant passages.\"\"\"\n\n...\n\n   docs = retriever.get_relevant_documents(q)\n\n   return format_docs(docs)\nMiddleware\nMiddleware provides ways to define the logic of your agent and customize its behavior. For example, there is middleware that can monitor the agent during runtime, assist with prompting and selecting tools, or even help with advanced use cases like guardrails, etc.\nHere are a few examples of built-in middleware. For the full list, please refer to the LangChain middleware documentation.\n\nMiddlewareDescription\nSummarizationAutomatically summarize the conversation history when approaching token limits.\nHuman-in-the-loopPause execution for human approval of tool calls.\nContext editingManage conversation context by trimming or clearing tool uses.\nPII detectionDetect and handle personally identifiable information (PII).\n\n\n\n\n\nReal-world LangChain use cases\nLangChain use cases cover a varied range of fields, with common instances including: \nAI-powered chatbots\nDocument question answering systems\nContent generation tools\nAI-powered chatbots\nWhen we think of AI agents, we often think of chatbots first. If you’ve read the How to Build Chatbots With LangChain blog post, then you’re already up to speed about this use case. If not, I highly recommend checking it out.\nDocument question answering systems\nAnother real-world use case for LangChain is a document question answering system. For example, companies often have internal documents and manuals that are rather long and unwieldy. A document question answering system provides a quick way for employees to find the info they need within the documents, without having to manually read through each one.\nTo demonstrate, we’ll create a script to index the PyCharm documentation. Then we’ll create an AI agent that can answer questions based on the documents we indexed. First let’s take a look at our tool:\n@tool(\"pycharm_docs_search\")\n\ndef pycharm_docs_search(q: str) -> str:\n\n   \"\"\"Search the local FAISS index of JetBrains PyCharm documentation and return relevant passages.\"\"\"\n\n   # Load vector store and create retriever\n\n   embeddings = OpenAIEmbeddings(\n\n       model=settings.openai_embedding_model, api_key=settings.openai_api_key\n\n   )\n\n   vector_store = FAISS.load_local(\n\n       settings.index_dir, embeddings, allow_dangerous_deserialization=True\n\n   )\n\n   k = 4\n\n   retriever = vector_store.as_retriever(\n\n       search_type=\"mmr\", search_kwargs={\"k\": k, \"fetch_k\": max(k * 3, 12)}\n\n   )\n\n   docs = retriever.invoke(q)\nWe are using a vector store to perform a similarity search with embeddings provided by OpenAI. Documents are embedded so the doc search tool can perform similarity searches to fetch the relevant documents when called. \ndef main():\n\n   parser = argparse.ArgumentParser(\n\n       description=\"Ask PyCharm docs via an Agent (FAISS + GPT-5)\"\n\n   )\n\n   parser.add_argument(\"question\", type=str, nargs=\"+\", help=\"Your question\")\n\n   parser.add_argument(\n\n       \"--k\", type=int, default=6, help=\"Number of documents to retrieve\"\n\n   )\n\n   args = parser.parse_args()\n\n   question = \" \".join(args.question)\n\n   system_prompt = \"\"\"You are a helpful assistant that answers questions about JetBrains PyCharm using the provided tools.\n\n   Always consult the 'pycharm_docs_search' tool to find relevant documentation before answering.\n\n   Cite sources by including the 'Source:' lines from the tool output when useful. If information isn't found, say you don't know.\"\"\"\n\n   agent = create_agent(\n\n       model=settings.openai_chat_model,\n\n       tools=[pycharm_docs_search],\n\n       system_prompt=system_prompt,\n\n       response_format=ToolStrategy(ResponseFormat),\n\n   )\n\n   result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": question}]})\n\n   print(result[\"structured_response\"].content)\n \nSystem prompts are provided to the LLM together with the user’s input prompt. We are using OpenAI as the LLM provider in this example, and we’ll need an API key from them. Head to this page to check out OpenAI’s integration documentation. When creating an agent, we’ll have to configure the settings for `llm`, `tools`, and `prompt`.\nFor the full scripts and project, see here.\nContent generation tools\nAnother example is an agent that generates text based on content fetched from other sources. For instance, we might use this when we want to generate marketing content with info taken from documentation. In this example, we’ll pretend we’re doing marketing for Python and creating a newsletter for the latest Python release.\nIn tools.py, a tool is set up to fetch the relevant information, parse it into a structured format, and extract the necessary information.\n@tool(\"fetch_python_whatsnew\", return_direct=False)\n\ndef fetch_python_whatsnew() -> str:\n\n   \"\"\"\n\n   Fetch the latest \"What's New in Python\" article and return a concise, cleaned\n\n   text payload including the URL and extracted section highlights.\n\n   The tool ignores the input argument.\n\n   \"\"\"\n\n   index_html = _fetch(BASE_URL)\n\n   latest = _find_latest_entry(index_html)\n\n   if not latest:\n\n       return \"Could not determine latest What's New entry from the index page.\"\n\n   article_html = _fetch(latest.url)\n\n   highlights = _extract_highlights(article_html)\n\n   return f\"URL: {latest.url}\\nVERSION: {latest.version}\\n\\n{highlights}\"\nAs for the agent in agent.py. \nSYSTEM_PROMPT = (\n\n   \"You are a senior Product Marketing Manager at the Python Software Foundation. \"\n\n   \"Task: Draft a clear, engaging release marketing newsletter for end users and developers, \"\n\n   \"highlighting the most compelling new features, performance improvements, and quality-of-life \"\n\n   \"changes in the latest Python release.\\n\\n\"\n\n   \"Process: Use the tool to fetch the latest 'What's New in Python' page. Read the highlights and craft \"\n\n   \"a concise newsletter with: (1) an attention-grabbing subject line, (2) a short intro paragraph, \"\n\n   \"(3) 4–8 bullet points of key features with user benefits, (4) short code snippets only if they add clarity, \"\n\n   \"(5) a 'How to upgrade' section, and (6) links to official docs/changelog. Keep it accurate and avoid speculation.\"\n\n)\n\n...\n\ndef run_newsletter() -> str:\n\n   load_dotenv()\n\n   agent = create_agent(\n\n       model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o\"),\n\n       tools=[fetch_python_whatsnew],\n\n       system_prompt=SYSTEM_PROMPT,\n\n       # response_format=ToolStrategy(ResponseFormat),\n\n   )\n\n...\nAs before, we provide a system prompt and the API key for OpenAI to the agent.\nFor the full scripts and project, see here.\nAdvanced LangChain concepts\nLangChain’s more advanced features can be extremely useful when you’re building a more sophisticated AI agent. Not all AI agents require these extra elements, but they are commonly used in production. Let’s look at some of them.\nMCP adapter\nThe MCP (Model Context Protocol) allows you to add extra tools or functionalities to an AI agent, making it increasingly popular among active AI agent users and AI enthusiasts alike. \nLangChain’s Client module provides a MultiServerMCPClient class that allows the AI agent to accept MCP server connections. For example:\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\n\nclient = MultiServerMCPClient(\n\n   {\n\n       \"postman-server\": {\n\n          \"type\": \"http\",\n\n          \"url\": \"https://mcp.eu.postman.com\",\n\n           \"headers\": {\n\n               \"Authorization\": \"Bearer ${input:postman-api-key}\"\n\n           }\n\n       }\n\n   }\n\n)\n\nall_tools = await client.get_tools()\nThe above connects to the Postman MCP server in the EU with an API key.\nGuardrails\nAs with many AI technologies, since the logic is not pre-determined, the behavior of an AI agent is non-deterministic. Guardrails are necessary for managing AI behavior and ensuring that it is policy-compliant.\nLangChain middleware can be used to set up specific guardrails. For example, you can use PII detection middleware to protect personal information or human-in-the-loop middleware for human verification. You can even create custom middleware for more specific guardrail policies. \nFor instance, you can use the `@before_agent` or `@after_agent` decorators to declare guardrails for the agent’s input or output. Below is an example of a code snippet that checks for banned keywords:\nfrom typing import Any\n\nfrom langchain.agents.middleware import before_agent\n\nbanned_keywords = [\"kill\", \"shoot\", \"genocide\", \"bomb\"]\n\n@before_agent(can_jump_to=[\"end\"])\n\ndef content_filter() -> dict[str, Any] | None:\n\n  \"\"\"Block requests containing banned keywords.\"\"\"\n\n  content = first_message.content.lower()\n\n# Check for banned keywords\n\n  for keyword in banned_keywords:\n\n      if keyword in content:\n\n          return {\n\n              \"messages\": [{\n\n                  \"role\": \"assistant\",\n\n                  \"content\": \"I cannot process your requests due to inappropriate content.\"\n\n              }],\n\n              \"jump_to\": \"end\"\n\n          }\n\n  return None\n\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n\n  model=\"gpt-4o\",\n\n  tools=[search_tool],\n\n  middleware=[content_filter],\n\n)\n\n# This request will be blocked\n\nresult = agent.invoke({\n\n  \"messages\": [{\"role\": \"user\", \"content\": \"How to make a bomb?\"}]\n\n})\nFor more details, check out the documentation here.\nTesting\nJust like in other software development cycles, testing needs to be performed before we can start rolling out AI agent products. LangChain provides testing tools for both unit tests and integration tests. \nUnit tests\nJust like in other applications, unit tests are used to test out each part of the AI agent and make sure it works individually. The most helpful tools used in unit tests are mock objects and mock responses, which help isolate the specific part of the application you’re testing. \nLangChain provides GenericFakeChatModel, which mimics response texts. A response iterator is set in the mock object, and when invoked, it returns the set of responses one by one. For example:\nfrom langchain_core.language_models.fake_chat_models import GenericFakeChatModel\n\ndef respond(msgs, **kwargs):\n\n   text = msgs[-1].content if msgs else \"\"\n\n   examples = {\"Hello\": \"Hi there!\", \"Ping\": \"Pong.\", \"Bye\": \"Goodbye!\"}\n\n   return examples.get(text, \"OK.\")\n\nmodel = GenericFakeChatModel(respond=respond)\n\nprint(model.invoke(\"Hello\").content)\nIntegration tests\nOnce we’re sure that all parts of the agent work individually, we have to test whether they work together. For an AI agent, this means testing the trajectory of its actions. To do so, LangChain provides another package: AgentEvals.\nAgentEvals provides two main evaluators to choose from:\nTrajectory match – A reference trajectory is required and will be compared to the trajectory of the result. For this comparison, you have 4 different models to choose from.\nLLM judge – An LLM judge can be used with or without a reference trajectory. An LLM judge evaluates whether the resulting trajectory is on the right path.\nLangChain support in PyCharm\nWith LangChain, you can develop an AI agent that suits your needs in no time. However, to be able to effectively use LangChain in your application, you need an effective debugger. In PyCharm, we have the AI Agents Debugger plugin, which allows you to power up your experience with LangChain.\nIf you don’t yet have PyCharm, you can download it here.\nUsing the AI Agents Debugger is very straightforward. Once you install the plug-in, it will appear as an icon on the right-hand side of the IDE.\n\n\n\n\nWhen you click on this icon, a side window will open with text saying that no extra code is needed – just run your agent and traces will be shown automatically.\nAs an example, we will run the content generation agent that we built above. If you need a custom run configuration, you will have to set it up now by following this guide on custom run configurations in PyCharm.\n\n\n\n\nOnce it is done, you can review all the input prompts and output responses at a glance. To inspect the LangGraph, click on the Graph button in the top-right corner.\n\n\n\n\nThe LangGraph view is especially useful if you have an agent that has complicated steps or a customized workflow.\nSumming up\nLangChain is a powerful tool for building AI agents that work for many use cases and scenarios. It’s built on LangGraph, which provides low-level orchestration and runtime customization, as well as compatibility with a vast variety of LLMs on the market. Together, LangChain and LangGraph set a new industry standard for developing AI agents.",
        "dc:creator": "Cheuk Ting Ho",
        "content": "If you’ve read the blog post How to Build Chatbots With LangChain, you may want to know more about LangChain. This blog post will dive deeper into what LangChain offers and guide you through a few more real-world use cases. And even if you haven’t read the first post, you might still find the info [&#8230;]",
        "contentSnippet": "If you’ve read the blog post How to Build Chatbots With LangChain, you may want to know more about LangChain. This blog post will dive deeper into what LangChain offers and guide you through a few more real-world use cases. And even if you haven’t read the first post, you might still find the info […]",
        "guid": "https://blog.jetbrains.com/?post_type=pycharm&p=681664",
        "categories": [
          "data-science",
          "tutorials",
          "ai",
          "ai-agents",
          "chatbots",
          "langchain"
        ],
        "isoDate": "2026-02-19T10:40:15.000Z"
      }
    ]
  },
  {
    "name": "Airbnb Engineering & Data Science",
    "category": "기업",
    "posts": [
      {
        "creator": "Malay Haldar",
        "title": "Academic Publications & Airbnb Tech: 2025 Year in Review",
        "link": "https://medium.com/airbnb-engineering/academic-publications-airbnb-tech-2025-year-in-review-7d79f57d3b52?source=rss----53c7c27702d5---4",
        "pubDate": "Tue, 24 Feb 2026 18:36:39 GMT",
        "content:encodedSnippet": "2025 was a big year for research at Airbnb, as we made significant progress toward our mission to use AI, data science, and machine learning to become the best travel and living platform.\nSpecifically, we doubled down on our presence at long-standing venues like KDD and CIKM — two of the most selective conferences in machine learning. At the same time, we expanded our research footprint by sharing our work in NLP, optimization, and measurement science at conferences such as COLING, LION, and VLDB.\nAcross these conferences, Airbnb researchers engaged directly with academic and industry peers by publishing and presenting papers, learning about the latest innovations, launching new collaborations, and mentoring emerging researchers. In this blog post, we’ll recap the conferences and key papers we presented in 2025, organized by research themes.\nApplied machine learning for search, ranking, and personalization\nKDD (Knowledge and Data Mining)\nKDD is a flagship conference in data science research. Hosted annually by a special interest group of the Association for Computing Machinery (ACM), it’s where researchers learn about some of the most groundbreaking developments in data mining, knowledge discovery, and large-scale data analytics, which are critical to Airbnb’s efforts to improve core products like search and recommendations.\nOur participation\nWe’ve been presenting at KDD since 2018, and 2025 was another strong year for us. We received multiple contributions across the applied data science track and workshops, which were well-received by the broader community and even inspired us to consider open-sourcing some of our technology. We were also inspired by the related research in this area and are eager to explore these methods through new collaborations.\nResearch highlights\n\nHarnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking: While A/B tests are crucial for developing ranking algorithms and recommender systems, they’re difficult to set up and can take extensive time to reach statistical significance (especially for products with long conversion cycles, like accommodation booking). In this paper, we shared techniques for rapid pre-A/B online assessments that help teams identify the most promising experiments, streamlining the overall process without sacrificing accuracy.\nHigh Precision Audience Expansion via Extreme Classification in a Two-Sided Marketplace: Airbnb search balances diverse global inventory with varied guest preferences for location, amenities, style, and price. This process requires efficient location retrieval to find the listings guests might realistically book by determining which geographic areas to query. We introduce a new approach to location retrieval by using a set of relevant, high-precision categorical location cells.\n\nLink to all papers\n\nHarnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking (Qing Zhang, Alex Deng, Michelle Du, Huiji Gao, Liwei He, Sanjeev Katariya)\nHigh Precision Audience Expansion via Extreme Classification in a Two-Sided Marketplace (Dillon Davis, Huiji Gao, Thomas Legrand, Juan Manuel Caicedo Carvajal, Malay Haldar, Kedar Bellare, Moutupsi Paul, Soumyadip Banerjee, Liwei He, Stephanie Moyerman, and Sanjeev Katariya)\nTSMO: Two-sided Marketplace Optimization\n\nCIKM (Conference on Information and Knowledge Management)\nCIKM is a premier forum for discussing and presenting research at the intersection of information and knowledge management, including topics like AI, data mining, database systems, and information retrieval. Many of these topics directly intersect with our core product challenges, such as search, ranking, and recommendations.\nOur participation\nAt CIKM 2025, Airbnb’s Relevance and Personalization team had five peer-reviewed papers accepted for publication, building on our participation in 2023 and 2024. These papers focused on advanced AI/ML techniques for search and recommendations, and sharing real-world insights from using these technologies at Airbnb’s scale. Industry and academic researchers, especially those working on two-sided marketplaces, engaged with our work and provided valuable feedback.\nResearch highlights\n\nAugmenting Guest Search Results with Recommendations at Airbnb: When guests use overly narrow criteria to search for accommodations, they often receive insufficient results, leading to a frustrating experience. This paper introduces a recommendation system that dynamically suggests alternatives — different dates, relaxed amenities, or adjusted price ranges — to help guests find suitable accommodations and improve the platform’s booking rate. Authors: Haowei Zhang, Philbert Lin, Dishant Ailawadi, Soumyadip Banerjee, Shashank Dabriwal, Hao Li, Kedar Bellare, Liwei He, Sanjeev Katariya\nMaps Ranking Optimization in Airbnb: Maps play a crucial role in Airbnb search and bookings, accounting for roughly 80% of search interactions. Yet map ranking has traditionally reused feed-ranking assumptions, which break down when we examine the NDCG (Normalized Discounted Cumulative Gain) metric. This paper explains why list-based NDCG fails to model user attention on maps, introduces a map-specific NDCG, and reports experiments showing that optimizing it yields booking gains. Authors: Hongwei Zhang, Malay Haldar, Kedar Bellare, Sherry Chen, Soumyadip Banerjee, Xiaotang Wang, Mustafa Abdool, Huiji Gao, Pavan Tapadia, Liwei He, Sanjeev Katariya, Stephanie Moyerman\nBListing: Modality Alignment for Listings: To improve search ranking, we introduce BiListing (Bimodal Listing) embeddings to use unstructured text and photo listing data as ranking signals. BiListing leverages large-language models and pretrained language-image models to create unified representations of diverse unstructured data into a single embedding vector per list and modality. Our experiment results show a 0.425% increase in NDCB (Normalized Discounted Cumulative Booking) gain and drove tens of millions in incremental revenue. Authors: Guillaume Guy, Mihajlo Grbovic, Chun How Tan, Han Zhao\nBeyond Pairwise Learning-To-Rank At Airbnb: In this paper, we introduce a method to improve the accuracy of pairwise learning-to-rank algorithms, the bedrock of modern search stacks. This approach captures interactions between items during pairwise comparisons, thereby giving us a better sense of what searchers truly want. We also share ways to implement this algorithm performantly, and results from online and offline experiments. Authors: Malay Haldar, Daochen Zha, Huiji Gao, Liwei He, Sanjeev Katariya\nLearning to Comparison-Shop: Traditional ranking models often evaluate items in isolation, disregarding the context in which users compare multiple items on a search results page. In this paper, we propose a novel ranking architecture, the Learning-to-Comparison-Shop (LTCS) System, that explicitly models and learns users’ comparison-shopping behaviors. Our experiments show statistically significant improvements of 1.7% in Normalized Discounted Cumulative Gain (NDCG) and 0.6% in booking conversion rate. Authors: Jie Tang, Daochen Zha, Xin Liu, Huiji Gao, Liwei He, Stephanie Moyerman, Sanjeev Katariya\n\nNLP & building LLM systems in production\nEMNLP (Empirical Methods in Natural Language Processing)\nEMNLP is a top-tier NLP conference that brings together practitioners and researchers to discuss new architectures and training strategies for language models, safety and evaluation strategies for LLMs, and real-world NLP applications. These research areas directly intersect with many of Airbnb’s product surfaces, such as customer support, search & discovery, and trust & safety. Additionally, each EMNLP cycle includes the release of new datasets, evaluation suites, and open-source libraries to help teams benchmark their progress against community standards.\nOur participation\nIn 2025, we sponsored EMNLP and presented two papers on humans-in-the-loop in AI systems and advanced summarization techniques. We also used EMNLP’s community datasets to benchmark our system, which showcased where we excel and where we can build upon our success with additional best practices. The conference deepened academic collaborations through discussions on LLM evaluation, safety, and agentic AI design, including mentoring students and early-career researchers.\nResearch highlights\n\nAgent-in-the-Loop, A Data Flywheel for Continuous Improvement in LLM-based Customer Support: To improve our LLM-based customer support system, this paper introduces an Agent-in-the-Loop (AITL) framework that leverages new interaction data to continuously enhance model performance. This flywheel can help the system stay up to date with new product features, shifting user preferences, and updated support policies and procedures. We launched a pilot in the US, and the results demonstrate significant improvement in accuracy and helpfulness. Authors: Cen Mia Zhao, Tiantian Zhang, Hanchen Su, Yufeng Wayne Zhang, Shaowei Su, Mingzhi Xu, Yu Elaine Liu, Wei Han, Jeremy Werner, Claire Na Cheng, Yashar Mehdad\nIncremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback: Customer service agents multitask during support interactions, identifying core issues, tracking prior actions, and producing accurate notes. To streamline this workflow, we introduced an incremental summarization system that intelligently determines when to generate concise bullet notes during conversations, reducing agents’ context-switching effort without sacrificing quality. To improve the system over time, we also introduced a learning framework that enables agents to make real-time edits, immediately refining online note generation. Authors: Yisha Wu, Cen Mia Zhao, Yuanpei Cao, Xiaoqing Su, Yashar Mehdad, Mindy Ji, Claire Na Cheng\n\nCOLING (International Conference on Computational Linguistics)\nCOLING is a top-tier NLP conference that covers both foundational research and industry applications of language models, including reasoning, evaluation, multilingual NLP, and real-world LLM systems. The work presented at this conference helps validate Airbnb’s technical direction and directly informs future investments.\nOur participation\nIn 2025, Airbnb presented at COLING for the first time, sharing a paper titled “LLM-Friendly Knowledge Representation for Customer Support” by Hanchen Su, Wei Luo, Wei Han, Yu Elaine Liu, Yufeng Wayne Zhang, Cen Mia Zhao, Ying Joy Zhang, and Yashar Mehdad. The paper presents a new format, Intent, Context, and Action (ICA), for structuring business knowledge in LLM-based QA and customer support workflows. Initial experiments in production show promising results. We also discovered relevant research in knowledge retrieval, LLM evaluation, and hallucination detection that will inspire future projects.\nOptimization, causal inference, and measurement science\nMIT CODE (Conference on Digital Experimentation)\nMIT CODE is one of the premier venues for researchers and practitioners to discuss topics in online digital experimentation, causal inference, and data-driven product innovation. The conference supports our commitment to data-driven decision-making and using experimentation to understand the long-term impacts on guests, hosts, and marketplace health.\nOur participation\nIn 2025, we had another strong showing at CODE, with a cohort of 6 data scientists and 3 academic collaborators. We gave talks in two sessions and presented a poster, which led to meaningful discussions with peer companies and interest in collaborating with academic research groups.\nResearch highlights\n\nBeyond the Experiment Window: Prospective Impacts Under Long-Term Ranking Dynamics: Product teams frequently leverage A/B tests to assess different rankers. While these experiments are typically conducted over shorter periods, we also recognize the value of understanding longer-term dynamics (such as seasonality and user evolution) to further support sustained business objectives, like marketplace health. To solve this problem, we developed a causal framework that allows us to estimate the long-term impacts of ranking changes with strategic goals (like marketplace health) using A/B testing data.\nTrustworthy Bayesian Inference in Batch-Adaptive Experimentation: Adaptive experimentation, like multi-arm bandit methods, can improve experiment efficiency by reallocating traffic toward promising treatments. Continued advancements in these approaches are expanding our ability to maintain high standards of statistical validity. This paper introduces a practical Bayesian framework for inference in batch-adaptive experiments, specifically tailored to the operational realities of online platforms.\n\nLink to all papers\n\nBeyond the Experiment Window: Prospective Impacts Under Long-Term Ranking Dynamics (Lo-Hua Yuan)\nTrustworthy Bayesian Inference in Batch-Adaptive Experimentation (Yicheng Li)\nExperimental Design for Product Launches with Collaborative User Networks (Monu Kala)\n\nINFORMS (Institute for Operations Research and the Management Sciences)\nINFORMS brings together academics and industry professionals to discuss and share research across data science, machine learning, economics, behavioral science, and analytics.\nOur participation\nIn 2025, our data science team was invited to INFORMS to present two talks in a session about bridging the gap between statistical methods and industry applications.\nResearch highlights\n\nBeyond Multi-Arm Bandits: Tackling Challenges in Adaptive Experiments at Airbnb. In this talk, we walked through the metrics and infrastructure challenges when using classic bandit algorithms, which make it difficult to operationalize adaptive experiments. We propose a hybrid approach that incorporates bandit algorithms into A/B experiments to enable adaptive testing. We also discussed how we onboard and validate adaptive experiments across individual product domains at Airbnb.\n\nLink to all papers\n\nBeyond the Experiment Window: Prospective Impacts Under Long-Term Ranking Dynamics (Lo-Hua Yuan)\nBeyond Multi-Arm Bandits: Tackling Challenges in Adaptive Experiments at Airbnb (Yicheng Li)\n\nLION (Learning and Intelligent Optimization)\nThe LION conference is a premier gathering of researchers exploring the intersection of machine learning, artificial intelligence, and mathematical optimization.\nOur participation\nWhile Airbnb has attended LION in the past, 2025 was the first time we presented at the conference. Nathan Brixius presented “Optimal Matched Block Design For Multi-Arm\nExperiments,” which introduces a new optimization formula using mixed-integer programming (MIP) to group subjects in multi-armed experiments, leading to more balanced groups and, in turn, more accurate experimental results. We also connected with leading experts in metaheuristics and AI fairness to help shape our future roadmap and sponsored the awards for the best papers presented at the conference.\nData systems\nVLDB (Very Large Data Bases)\nThe VLDB Conference is one of the top 2 flagship conferences in data management and large-scale data systems, with over 1,500 researchers and practitioners attending.\nOur participation\n“In 2025, we published our first paper at VLDB: ‘SQL:Trek Automated Index Design at Airbnb’ by Sam Lightstone and Ping Wang. The paper presents a novel approach for automated index design (code-named SQL:Trek). It uses query compiler cost models to identify effective indexes across many relational databases, including most MySQL and PostgreSQL derivatives. Additionally, the Airbnb team attended sessions on system efficiency, graph computing, and AI databases, and had the opportunity to meet other researchers.\nConclusion\nConferences remain a big part of our research program at Airbnb, helping us validate and refine our ideas through community feedback and providing a forum to share real-world insights that advance the field. In 2025, we doubled down on this vision by publishing papers for the first time at conferences in domains such as NLP, optimization, causal inference, and data systems, reflecting our ongoing commitment to using these technologies to create the best possible travel experiences.\nAs we look to 2026, we’re eager to expand our presence at these conferences and discover new ways to use AI, machine learning, and data science to build a best-in-class travel and living platform. If you’re interested in doing this type of work with us, consider joining us. Apply for one of our open positions.\n\nAcademic Publications & Airbnb Tech: 2025 Year in Review was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Malay Haldar",
        "guid": "https://medium.com/p/7d79f57d3b52",
        "categories": [
          "machine-learning",
          "engineering",
          "ai",
          "data-science",
          "technology"
        ],
        "isoDate": "2026-02-24T18:36:39.000Z"
      },
      {
        "creator": "Cosmo W. Q",
        "title": "Safeguarding Dynamic Configuration Changes at Scale",
        "link": "https://medium.com/airbnb-engineering/safeguarding-dynamic-configuration-changes-at-scale-5aca5222ed68?source=rss----53c7c27702d5---4",
        "pubDate": "Wed, 18 Feb 2026 17:01:01 GMT",
        "content:encodedSnippet": "How Airbnb ships dynamic config changes safely and reliably\n\nBy Cosmo Qiu, Bo Teng, Siyuan Zhou, Ankur Soni, Willis Harvey\nDynamic configuration is a core infrastructure capability in modern systems. It allows developers to change runtime behavior without restarting or redeploying services, even as the number of services and requests grows. In practice, that might mean rolling out a new address form for a region launch, tightening an authorization rule, or adjusting timeouts when a dependency is slow.\nLike any powerful tool, dynamic configuration is a double-edged sword. While it enables fast iteration and rapid incident response, a bad change can cause regressions or even outages. This is a common challenge across the industry: balancing developer flexibility with system reliability.\nIn this post, we will outline the expectations of a modern dynamic configuration platform, then walk through the high-level architecture of Airbnb’s dynamic config platform and how its core components work together to enable safe, flexible config changes.\nModern config platform essentials\nAs Airbnb’s business grows, our expectations for the dynamic config platform have evolved over time through our own learnings as well as industry best practices. These shape our view of what a good dynamic config platform should provide, including:\n\nA coherent experience for config management: The platform provides a streamlined, end-to-end experience for defining, reviewing, testing, and rolling out config changes. It covers the most common needs out of the box with rich built-in features, while still offering escape hatches for edge cases.\nStrong reliability, availability and safety guarantees: All config changes are validated, reviewed, and rolled out progressively, with clear ownership and well-defined access control. Treating config as code is a key focus: config changes are versioned, reviewed, and auditable like service code, but remain dynamic at runtime. The platform itself must be highly available so that services can reliably fetch and apply configs. Changes should be observable, with support for fast rollbacks when needed.\nSafe testing in isolated environments: Developers can validate config changes in isolated local or canary environments before they reach production.\nFlexible multi-tenant support: In a multi-tenant platform, different tenants have different risk profiles. The platform should allow config owners to customize how their configs behave per tenant, including deployment triggers, guardrails, and rollout strategy (for example, AWS zone or Kubernetes pod percentage-based rollouts).\nFast and controlled incident response: During an incident, responders can ship emergency configs as needed with clear auditability. The platform also provides observability for config changes, so incident responders can tell what changed, who was affected, when the change was made, and who made the change. This enables them to effectively identify the culprit and take action.\n\nHigh-level architecture\nAt Airbnb, Sitar is the internal name for our dynamic config platform. It provides a common way for teams to manage runtime behavior safely. At a high level, Sitar has four main parts: a developer-facing layer, a control plane, a data plane, and the clients and agents that run alongside application code.\n\nThe developer-facing layer is where config changes are created and reviewed. By default, configs are managed through a Git-based workflow, while a few exceptions are managed in the web interface (sitar-portal), which is also used for admin operations such as emergency deployments.\nThe control plane is responsible for orchestrating config changes. It enforces schema validation, ownership, and access control, and decides how each change should be rolled out: for example, which environments or AWS zone to target, what percentage of Kubernetes pods to start with, and how to progress the rollout over time. The control plane also specifies how to roll back the changes when needed, and supports routing in-flight configs to specific environments or slices of subscribers for fast testing.\nThe data plane provides scalable storage and efficient distribution of configs. It acts as the source of truth for config values and versions, and propagates updates to services reliably, consistently, and quickly.\nOn the product services side, an agent sidecar running alongside each service fetches the subscribed configs from the data plane and maintains a local cache. Client libraries inside the service then read from this cache and expose configs to application logic with fast, in-process access and optional fallbacks.\nPutting these together, a typical change starts from a Git flow, proceeds through control-plane validation and rollout decisions, into the data plane for distribution, and finally to agents and client libraries that apply the config updates to application logic.\nKey design choices\nIn this section, we highlight a few key design choices that shape how the platform looks and is operated.\nConfigs as code with a Git-based workflow\nConfig changes are by default managed by a Git-centric workflow. We use GitHub as the primary interface for managing configs, because we have an established and responsive internal team to manage GitHub Enterprise. GitHub integrates naturally with our existing CI/CD tooling, so we can reuse rich validation and deployment pipelines without re-inventing the wheel. This approach gives developers a consistent experience to make code changes: open a pull request, get reviews, merge, and deploy. GitHub also brings additional benefits such as mandatory reviewers, review and approval flows, and a change history. Configs under the same theme are grouped into tenants, with clear owners, customizable tests, and a dedicated CD pipeline.\nWhile the Git-based flow is the default, we keep a UI portal for teams that prefer a portal-based experience and as a shortcut for specific operational needs, such as fast emergency config updates that can bypass the normal CI/CD pipeline.\nStaged rollouts and fast rollbacks\nWhen a change is proposed, schema validation (checking that the config matches the expected structure and types) and other automated checks run in CI. The change is always reviewed and approved before rollout.\nOnce merged in the main branch, the control plane performs a staged rollout where the change is first deployed to a limited scope, then gradually expanded to a larger scope if things look good. At each stage of this rollout, the change is evaluated, the author and the stakeholders are notified if regressions are detected, and a fast rollback can be triggered if needed. Staged rollouts can greatly reduce the blast radius of bad changes and improve the overall reliability of the platform.\nSeparated control and data planes\nWe separate the “decide” and “deliver” responsibilities. The control plane focuses on validation, authorization, and rollout decisions, while the data plane focuses on storing configs and distributing them reliably at scale. This separation allows us to evolve rollout strategies and policies without disrupting the underlying storage and delivery mechanisms, and vice versa.\nLocal caching and resilient clients\nOn the product services side, we introduced a local caching layer between the agent sidecar and the client library to improve resilience and availability. The agent sidecar runs alongside the main service container, regardless of which language the service is written in, and periodically fetches subscribed configs from the backend and persists them locally. The client libraries then read from this local cache. Even if the backend is temporarily unavailable or degraded, services can continue operating on the last known good configs from the local cache.\nImpact on product teams\nIt is essential for the Sitar system to make life easier for product teams. In practice, its architecture changes how teams ship and operate in a few ways:\n\nRollouts become safer and more predictable. New behaviors, such as refined authorization rules, can be introduced gradually, verified on a small slice of traffic or in a specific environment, and rolled back quickly if needed. Teams spend less time worrying about “big bang” releases and more time iterating on behavior.\nTeams get more flexibility in how their configs are managed and rolled out. Each team can tailor a config flow to its own risk profile and release schedules. For example, teams can choose between automatic, manual, or cron rollouts, select the rollout strategy, and add extra checks. This lets teams keep their existing ways of working while still benefiting from a common platform and shared guardrails.\nIncident mitigation becomes faster and more controlled. When something goes wrong in production, incident responders can use observability tools that integrate config events to quickly locate the culprit change, then take quick action using the portal’s emergency flow. These emergency updates are fully auditable for future review.\n\nBesides these examples, the platform includes other improvements in usability, safety, and observability that we will not cover in detail here. Together, they contribute to a smoother day-to-day experience for teams that rely on dynamic configuration.\nConclusions and next steps\nDynamic configuration is a foundational capability of modern infrastructure. It enables fast iteration and rapid incident response, but only when it is equipped with strong safety features and provides a good developer experience. In this post, we shared how we think about a modern dynamic config platform at Airbnb, and how we developed Sitar’s architecture to meet those expectations.\nThe work is ongoing. As Airbnb’s business grows, we are continuing to refine rollout strategies, improve config testing, invest in observability and smart incident response tooling, and evolve other platform components.\nIn future posts, we plan to dive deeper into specific areas of the platform, such as how we optimize the Kubernetes sidecar that delivers config updates and how we design the developer experience around config management.\nIf this type of work interests you check out our open roles.\nAcknowledgments\nOur progress with Sitar would not have been possible without the support and contributions of many people. We’d like to thank Craig Sosin, Nikolaj Nielsen, Daniel Fagnan, Alex Edwards, Xian Gao, Nick Morgan, Carolina Calderon, Hanfei Lin, Joyce Li, Yunong Liu, Alex Berghage, Brian Wolfe, Yann Ramin, Denis Sheahan, Richa Khandelwal, Swetha Vaidy, Abhishek Parmar, Adam Kocoloski, Adam Miskiewicz, and all the other engineers and teams at Airbnb who joined design reviews and offered valuable feedback, as this work would not have been possible without them.\nAll product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.\n\nSafeguarding Dynamic Configuration Changes at Scale was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "dc:creator": "Cosmo W. Q",
        "guid": "https://medium.com/p/5aca5222ed68",
        "categories": [
          "engineering",
          "software-development",
          "infrastructure",
          "distributed-systems",
          "software-architecture"
        ],
        "isoDate": "2026-02-18T17:01:01.000Z"
      }
    ]
  },
  {
    "name": "PayPal Engineering",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Visual Studio Blog",
    "category": "기업",
    "posts": [
      {
        "creator": "Mark Downie",
        "title": "Visual Studio February Update",
        "link": "https://devblogs.microsoft.com/visualstudio/visual-studio-february-update/",
        "pubDate": "Tue, 24 Feb 2026 22:16:30 +0000",
        "content:encodedSnippet": "This month’s Visual Studio update continues our focus on helping you move faster and stay in flow, with practical improvements across AI assistance, debugging, testing, and modernization. Building on the momentum from January’s editor updates, the February release brings smarter diagnostics and targeted support for real world development scenarios, from WinForms maintenance to C++ modernization.\nAll of the features highlighted are available in the Visual Studio 2026 Stable Channel as part of the February 2026 feature update (18.3). Please update to the latest version to try out these new features!\nWinForms Expert Agent\nThe WinForms Expert agent provides a focused guide for handling key challenges in WinForms development. It covers several important areas:\nDesigner vs. regular code: Understand which C# features apply to designer-generated code and business logic.\nModern .NET patterns: Updated for .NET 8-10, including MVVM with Community Toolkit, async/await with proper InvokeAsync overloads, Dark mode with high-DPI support, and nullable reference types.\nLayout: Advice on using TableLayoutPanel and FlowLayoutPanel for responsive, cross-device design.\nCodeDOM serialization: Rules for property serialization and avoiding common issues with [DefaultValue] and ShouldSerialize*() methods.\nException handling: Patterns for async event handlers and robust application-level error handling.\nThe agent serves as an expert reviewer for your WinForms code, providing comprehensive guidance on everything from naming controls to ensuring accessibility. The WinForms Agent is automatically implemented and included in the system prompt when necessary.\nSmarter Test Generation with GitHub Copilot\nVisual Studio now includes intelligent test generation with GitHub Copilot, making it faster to create and refine unit tests for your C# code. This purpose-built workflow works seamlessly with xUnit, NUnit, and MSTest.\n\nSimply type @Test in GitHub Copilot Chat, describe what you want to test, and Copilot generates the test code for you. Whether you’re starting fresh or improving coverage on existing projects, this feature helps you write tests faster without leaving your workflow.\nSlash Commands for Custom Prompts\nInvoke your favorite custom prompts faster using slash commands in Copilot Chat. Type / and your custom prompts appear at the top of the list, marked with a bookmark icon for easy identification.\n\nWe’ve also added two additional commands:\n– /generateInstructions: Automatically generate a copilot-instructions.md file for your repository using project context like coding style and preferences\n– /savePrompt: Extract a reusable prompt from your current chat thread and save it for later use via / commands\nThese shortcuts make it easier to build and reuse your workflow patterns.\nC++ App Modernization\nGitHub Copilot app modernization for C++ is now available in Public Preview. GitHub Copilot app modernization for C++ helps you update your C++ projects to use the latest versions of MSVC and to resolve upgrade-related issues. You can find our user documentation on Microsoft Learn.\n\nDataTips in IEnumerable Visualizer\nYou can now use DataTips in the IEnumerable Visualizer while debugging. Just hover over any cell in the grid to see the full object behind that value, the same DataTip experience you’re used to in the editor or Watch window.\nWhen you hover over a cell, a DataTip shows all the object’s properties in one place. This makes it much easier to debug collections with complex or nested data. Whether it’s a List<T> of objects or a dictionary with structured values, one hover lets you quickly inspect everything inside.\n\nAnalyze Call Stack with Copilot\nYou can now Analyze Call Stack with Copilot to help you quickly understand what your app is doing when debugging stops. When you pause execution, you can select Analyze with Copilot in the Call Stack window. Copilot reviews the current stack and explains why the app isn’t progressing whether the thread is waiting on work, looping, or blocked by something.\nThis makes the call stack more than just a list of frames. It becomes a helpful guide that shows what’s happening in your app so you can move faster toward the real fix.\nhttps://devblogs.microsoft.com/visualstudio/wp-content/uploads/sites/4/2026/02/callstackanalysis.mp4\n\nProfiler agent with Unit Test support\nThe Profiler Agent (@profiler) now works with unit tests. You can use your existing tests to check performance improvements, making it easier to measure and optimize your code in more situations. The agent can discovers relevant unit tests/BenchmarkDotNet benchmarks that exercise performance-critical code paths.\nIf no good tests or benchmarks are available, it automatically creates a small measurement setup so you can capture a baseline and compare results after changes. This unit-test-focused approach also makes the Profiler Agent useful for C++ projects, where benchmarks aren’t always practical, but unit tests often already exist.\n\nFaster and More Reliable Razor Hot Reload\nHot Reload for Razor files are now faster and more reliable. By hosting the Razor compiler inside the Roslyn process, edits to .razor files apply more quickly and avoid delays that previously slowed Blazor workflows. We also reduced the number of blocked edits, with more changes now applying without requiring a rebuild, including file renames and several previously unsupported code edits. When a rebuild is still required, Hot Reload can now automatically restart the app instead of ending the debug session, helping you stay in flow.\nWe are continuing to invest in features that help you understand, test, and improve existing code, not just write new code. Try these updates in the Visual Studio 2026 Stable Channel and let us know what is working well and where we can improve. Your feedback directly shapes what we build next.\nThe post Visual Studio February Update appeared first on Visual Studio Blog.",
        "enclosure": {
          "url": "https://devblogs.microsoft.com/visualstudio/wp-content/uploads/sites/4/2026/02/callstackanalysis.mp4",
          "length": "1732737",
          "type": "video/mp4"
        },
        "dc:creator": "Mark Downie",
        "comments": "https://devblogs.microsoft.com/visualstudio/visual-studio-february-update/#respond",
        "content": "<p>This month’s Visual Studio update continues our focus on helping you move faster and stay in flow, with practical improvements across AI assistance, debugging, testing, and modernization. Building on the momentum from January’s editor updates, the February release brings smarter diagnostics and targeted support for real world development scenarios, from WinForms maintenance to C++ modernization. [&#8230;]</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/visualstudio/visual-studio-february-update/\">Visual Studio February Update</a> appeared first on <a href=\"https://devblogs.microsoft.com/visualstudio\">Visual Studio Blog</a>.</p>\n",
        "contentSnippet": "This month’s Visual Studio update continues our focus on helping you move faster and stay in flow, with practical improvements across AI assistance, debugging, testing, and modernization. Building on the momentum from January’s editor updates, the February release brings smarter diagnostics and targeted support for real world development scenarios, from WinForms maintenance to C++ modernization. […]\nThe post Visual Studio February Update appeared first on Visual Studio Blog.",
        "guid": "https://devblogs.microsoft.com/visualstudio/?p=255690",
        "categories": [
          "GitHub Copilot",
          "Productivity",
          ".NET",
          "Debugging and Diagnostics"
        ],
        "isoDate": "2026-02-24T22:16:30.000Z"
      },
      {
        "creator": "Rhea Patel, Kelly Fam",
        "title": "Custom Agents in Visual Studio: Built in and Build-Your-Own agents",
        "link": "https://devblogs.microsoft.com/visualstudio/custom-agents-in-visual-studio-built-in-and-build-your-own-agents/",
        "pubDate": "Thu, 19 Feb 2026 21:02:13 +0000",
        "content:encodedSnippet": "Agents in Visual Studio now go beyond a single general-purpose assistant. We’re shipping a set of curated preset agents that tap into deep IDE capabilities; debugging, profiling, testing alongside a framework for building your own custom agents tailored to how your team works.\nBuilt in agents\nEach preset agent is designed around a specific developer workflow and integrates with Visual Studio’s native tooling in ways that a generic assistant can’t.\nDebugger – Goes beyond “read the error message.” Uses your call stacks, variable state, and diagnostic tools to walk through error diagnosis systematically across your solution.\nProfiler – Connects to Visual Studio’s profiling infrastructure to identify bottlenecks and suggest targeted optimizations grounded in your codebase, not generic advice.\nTest – (when solution is loaded) Generates unit tests tuned to your project’s framework and patterns, not boilerplate that your CI will reject.\nModernize (.NET and C++ only) -Framework and dependency upgrades with awareness of your actual project graph. Flags breaking changes, generates migration code, and follows your existing patterns.\nAccess them through the agent picker in the chat panel or using ‘@’ in chat.\nBring your own: custom agents (preview)\nThe presets cover workflows we think matter most, but your team knows your workflow better than we do. Custom agents let you build your own using the same foundation—workspace awareness, code understanding, tools accessed by your prompts, your preferred model, and your tools.\nWhere it gets powerful is MCP. You can connect custom agents to external knowledge sources internal documentation, design systems, APIs, and databases so the agent isn’t limited to what’s in your repo.\nA few patterns we’re seeing from teams:\nCode review that checks PRs against your actual conventions, connected via MCP to your style guide or ADR repository\nDesign system enforcement connected to your Figma files or component libraries to catch UI drift before it ships\nPlanning helps you think through a feature or task before any code is written. Gathers requirements, asks clarifying questions, and builds out a plan that you can hand off\nThe awesome-copilot repo has community-contributed agent configurations you can use as starting points.\nGet started\nCustom agents are defined as .agent.md files in your repository’s .github/agents/ folder:\nyour-repo/\r\n└── .github/\r\n    └── agents/\r\n        └── code-reviewer.agent.md\nA few things to note:\nThis is a preview feature; the format of these files may change over to support different capabilities\nIf you don’t specify a model, the agent uses whatever is selected in the model picker\nTool names vary across GitHub Copilot platforms- check the tools available in Visual Studio specifically to make sure your agent works as expected\nConfigurations from the awesome-copilot repo are a great starting point, but verify tool names before using them in VS\nTell us what you’re building\nShare your configurations in the awesome-copilot repo or file feedback here.\nThe post Custom Agents in Visual Studio: Built in and Build-Your-Own agents appeared first on Visual Studio Blog.",
        "dc:creator": "Rhea Patel, Kelly Fam",
        "comments": "https://devblogs.microsoft.com/visualstudio/custom-agents-in-visual-studio-built-in-and-build-your-own-agents/#comments",
        "content": "<p>Agents in Visual Studio now go beyond a single general-purpose assistant. We&#8217;re shipping a set of curated preset agents that tap into deep IDE capabilities; debugging, profiling, testing alongside a framework for building your own custom agents tailored to how your team works. Built in agents Each preset agent is designed around a specific developer [&#8230;]</p>\n<p>The post <a href=\"https://devblogs.microsoft.com/visualstudio/custom-agents-in-visual-studio-built-in-and-build-your-own-agents/\">Custom Agents in Visual Studio: Built in and Build-Your-Own agents</a> appeared first on <a href=\"https://devblogs.microsoft.com/visualstudio\">Visual Studio Blog</a>.</p>\n",
        "contentSnippet": "Agents in Visual Studio now go beyond a single general-purpose assistant. We’re shipping a set of curated preset agents that tap into deep IDE capabilities; debugging, profiling, testing alongside a framework for building your own custom agents tailored to how your team works. Built in agents Each preset agent is designed around a specific developer […]\nThe post Custom Agents in Visual Studio: Built in and Build-Your-Own agents appeared first on Visual Studio Blog.",
        "guid": "https://devblogs.microsoft.com/visualstudio/?p=255658",
        "categories": [
          "Copilot",
          "GitHub Copilot",
          "agents",
          "custom agents",
          "Visual Studio"
        ],
        "isoDate": "2026-02-19T21:02:13.000Z"
      }
    ]
  },
  {
    "name": "Joshua",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권재명",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김석기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김선영",
    "category": "개인",
    "posts": [
      {
        "creator": "sunyzero",
        "title": "mobaXterm에서 한글 타이핑시 &quot;응답 없음&quot;으로 죽는 문제",
        "link": "https://sunyzero.tistory.com/325",
        "pubDate": "Mon, 23 Feb 2026 21:07:50 +0900",
        "author": "sunyzero",
        "comments": "https://sunyzero.tistory.com/325#entry325comment",
        "content": "<p data-ke-size=\"size16\">ssh접속을 위해 mobaXterm을 쓰다보면 한영 전환을 하거나 한글 타이핑을 하면 \"응답 없음\" 상태로 변하면서 프로세스가 죽는 문제가 생기곤 했다. 특히 한글을 타이핑하다가 오타가 발생해서 백스페이스로 자음이나 모음을 지울때 종종 발생했다. 그래서 mobaXterm 한글이 죽는 이유를 찾다보니 마이크로소프트가 IME(Input Method Editor)를 2가지를 가지고 있다는 것을 알았습니다. 그리고 최신의 IME(TSF방식)이 구형 프레임워크나 개발툴로 개발된 애플리케이션과 종종 충돌을 일으킨다는 것도 알게되었다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">1. 해결 방법</h2>\n<p data-ke-size=\"size16\">우선 해결 방법을 먼저 말하자면 <span style=\"background-color: #f6e199;\">설정</span>에서 \"시간 및 언어\", \"<span style=\"background-color: #99cefa;\">언어 및 지역</span>\", \"옵션\", \"<span style=\"background-color: #c1bef9;\">Microsoft 입력기</span>\" 메뉴에서 <span style=\"background-color: #f6e199;\">\"이전 버전의 Microsoft IME</span>\"를 <span style=\"color: #ee2323;\">켬(on)</span>으로 해두면 된다. 문제는 이 옵션이 꽤 깊숙한 곳에 있어서 찾아가는게 좀 어렵다. 그래서 찾아가는 방법을 한땀한땀 보여주도록 하겠다.</p>\n<p data-ke-size=\"size16\">먼저 \"설정\"을 실행하고, \"시간 및 언어\" 메뉴를 선택한다. 그리고 아래 그림처럼 \"<span style=\"color: #ee2323;\">한국어</span>\"의 점3개를 눌러서 \"<span style=\"background-color: #f6e199;\">언어 옵션</span>\"을 선택한다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"1382\" data-origin-height=\"834\"><span data-url=\"https://blog.kakaocdn.net/dn/nMM6l/dJMcaaj2HBG/hpAqHc0N8u4CgKgBngxsd1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/nMM6l/dJMcaaj2HBG/hpAqHc0N8u4CgKgBngxsd1/img.png\" data-alt=\"윈11설정, 시간 및 언어, 언어 및 지역\"><img src=\"https://blog.kakaocdn.net/dn/nMM6l/dJMcaaj2HBG/hpAqHc0N8u4CgKgBngxsd1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FnMM6l%2FdJMcaaj2HBG%2FhpAqHc0N8u4CgKgBngxsd1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1382\" height=\"834\" data-origin-width=\"1382\" data-origin-height=\"834\"/></span><figcaption>윈11설정, 시간 및 언어, 언어 및 지역</figcaption>\n</figure>\n</p>\n<p data-ke-size=\"size16\">언어 옵션을 선택해서 들어가면 아랫쪽에 키보드 메뉴가 있다. 여기서 아래 그림처럼 \"<span style=\"background-color: #f6e199;\">Microsoft 입력기</span>\"라는 것을 찾고, 점3개를 눌러서 \"<span style=\"color: #ee2323;\">키보드 옵션</span>\"을 선택한다.</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"1516\" data-origin-height=\"604\"><span data-url=\"https://blog.kakaocdn.net/dn/b31lei/dJMcafZVygH/GRtg274xq5JNkqXfc4Y0d1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/b31lei/dJMcafZVygH/GRtg274xq5JNkqXfc4Y0d1/img.png\" data-alt=\"한국어, 키보드, Microsoft 입력기\"><img src=\"https://blog.kakaocdn.net/dn/b31lei/dJMcafZVygH/GRtg274xq5JNkqXfc4Y0d1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb31lei%2FdJMcafZVygH%2FGRtg274xq5JNkqXfc4Y0d1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1516\" height=\"604\" data-origin-width=\"1516\" data-origin-height=\"604\"/></span><figcaption>한국어, 키보드, Microsoft 입력기</figcaption>\n</figure>\n</p>\n<p data-ke-size=\"size16\">키보드 옵션을 선택해서 들어가면 하단에 호환성 부분에 \"<span style=\"background-color: #ffc1c8;\">이전 버전의 Microsoft IME</span>\"를 <span style=\"color: #ee2323;\">켬(on)</span>으로 설정하면 끝난다.&nbsp;</p>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"1319\" data-origin-height=\"971\"><span data-url=\"https://blog.kakaocdn.net/dn/nH5lc/dJMcaiPQYN2/AcIhJA59l6izLgAcnkXLqK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/nH5lc/dJMcaiPQYN2/AcIhJA59l6izLgAcnkXLqK/img.png\" data-alt=\"키보드 옵션, 호환성, 이전 버전의 Microsoft IME\"><img src=\"https://blog.kakaocdn.net/dn/nH5lc/dJMcaiPQYN2/AcIhJA59l6izLgAcnkXLqK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FnH5lc%2FdJMcaiPQYN2%2FAcIhJA59l6izLgAcnkXLqK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"1319\" height=\"971\" data-origin-width=\"1319\" data-origin-height=\"971\"/></span><figcaption>키보드 옵션, 호환성, 이전 버전의 Microsoft IME</figcaption>\n</figure>\n</p>\n<p data-ke-size=\"size16\">이렇게 한 뒤에 되도록이면 재부팅을 하는 것을 권장한다. 애초에 이런 설정이 필요없도록 하려면 tabby 같은 ssh client를 사용하는 것도 괜찮다. 다만 개인적으로는 mobaXterm이 더 편해서 이걸 주로 사용하는 편이다. 그리고 깨알 지식으로 mobaXterm을 사용할 때는 리가처 폰트(ligature font)를 사용하면 이상하게 화면 스크롤 속도가 느려진다. <span style=\"color: #f89009;\">따라서 되도록이면 ligature 글꼴 대신 일반 글꼴을 사용하는 것을 권장한다</span>.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">2. 기술적 배경 지식</h2>\n<p data-ke-size=\"size16\">왜 이런 문제가 생겼는지 알기 위해서는 IME의 변화에 대해서 알아야만 한다. 마이크로소프트는 XP에서 <span style=\"background-color: #9feec3;\">TSF(Text Services Framework)</span>라는 새로운 IME를 도입했다. 그리고 윈도10이나 윈도11에는 구형 IME인 <span style=\"background-color: #f6e199;\">IMM32</span>와 신형 IME인 TSF가 둘다 설치되고, 기본값으로는 TSF로 작동한다는 점이다.&nbsp;</p>\n<p data-ke-size=\"size16\">TSF의 특징은 비동기로 작동한다는 특징이 있다. 따라서 타이핑을 하는 것을 버퍼에 채워두었다가 완성시키거나 혹은 음성 변환이나 여러가지 기능이 있는데, 이게 기능을 추가하다보니 어느 시점부터 옛날 시스템과 뭔가 안맞는게 생겼던 것 같다. 게다가&nbsp; mobaXterm은 <span style=\"background-color: #f6e199;\">델파이(Delphi)</span>라고 굉장히 옛날 개발툴로 만들어지다보니 TSF와는 잘 맞지 않는 부분이 있는 것 같다. 그럼에도 불구하고 영어권은 큰 문제가 없어 보인다. 한글이나 중국 번체, 일어 한문등이 가장 큰 문제가 되는것 같은데, 이들 문자들의 특징은 여러 타이핑을 조합해서 하나의 문자가 생기는 케이스가 있다는 점이다. 영어는 hello를 칠때 h를 치면 즉각 h를 표시하면 그만이지만, 한글로 \"<span style=\"color: #ee2323;\">헬로</span>\"를 타이핑하려면 \"<span style=\"color: #ee2323;\">ㅎ</span> + <span style=\"color: #ee2323;\">ㅔ</span> + <span style=\"color: #ee2323;\">ㄹ</span>\" 까지 쳐야 \"<span style=\"color: #ee2323;\">헬</span>\" 글자 1개가 완성된다. 여기서 ㅎ 다음에 \"<span style=\"color: #ee2323;\">ㅔ</span>\" 대신에 \"<span style=\"color: #ee2323;\">ㅐ</span>\"를 잘못 타이핑했다면 backspace로 지울때 현재까지 완성된 \"<span style=\"color: #ee2323;\">해</span>\"를 지우는게 아니라 딱 \"<span style=\"color: #ee2323;\">ㅐ</span>\"만 지우고, \"<span style=\"color: #ee2323;\">ㅎ</span>\" 자음은 남겨야 하는데, 이런 부분이 오류를 만드는 원인이 되는 것 같다. 게다가 TSF가 비동기식이라 어떤 경우에는 잘 작동하지만, 어떤 경우에는 비동기 이벤트를 놓쳐서 \"응답 없음\"오류를 만드는 것 같다.</p>\n<p data-ke-size=\"size16\">물론 2026년을 기준으로 보면 대부분의 개발툴들은 TSF를 제대로 지원하는 경우가 많아서 큰 문제는 안되지만, 앞서 언급한 델파이나 아니면 cygwin, Qt을 이용하는 시스템에서는 문제가 종종 생기는 것 같다. 결국 문제가 생기는 애플리케이션을 주로 쓴다면 결국 TSF를 버리고 \"이전 버전의 Microsoft IME\"인 IMM32를 쓰는 수 밖에 없어 보인다. (<span style=\"color: #8a3db6;\">마소가 근본적인 문제를 고쳐서 TSF를 쓸때도 문제없게 해주면 좋겠지만, 솔직히 기대도 안한다. 왜냐하면 지금 윈11은 툭하면 업데이트 버그가 쏟아지기 때문이다. 사소한 버그도 해결 못하는 것을 보면 윈도 개발팀의 프로그래밍 능력이 수준 이하라고 생각된다.</span>)</p>\n<p data-ke-size=\"size16\">게다가 TSF를 쓰면 성능도 좋아진다고는 하지만 솔직히 요새 시스템에서 TSF를 써서 얻는 성능적 이점은 쥐꼬리의 털조각 1개만큼도 안되는 수준이라, 성능적 이점은 사실상 없다. 다만 TSF는 한자를 더 많이 지원하는 장점이 있다고 하는데, 요새 한자를 병용하거나 한자를 타이핑하는 경우는 드물기 때문에 큰 메리트가 없어 보인다.</p>\n<p data-ke-size=\"size16\">결론적으로 mobaXterm을 쓸때는 TSF를 쓰지 않고 IMM32를 쓰도록 \"이전 버전의 Microsoft IME\"를 설정하도록 하자.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">히스토리</h2>\n<p data-ke-size=\"size16\">2026.02.23 릴리즈</p>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "ssh접속을 위해 mobaXterm을 쓰다보면 한영 전환을 하거나 한글 타이핑을 하면 \"응답 없음\" 상태로 변하면서 프로세스가 죽는 문제가 생기곤 했다. 특히 한글을 타이핑하다가 오타가 발생해서 백스페이스로 자음이나 모음을 지울때 종종 발생했다. 그래서 mobaXterm 한글이 죽는 이유를 찾다보니 마이크로소프트가 IME(Input Method Editor)를 2가지를 가지고 있다는 것을 알았습니다. 그리고 최신의 IME(TSF방식)이 구형 프레임워크나 개발툴로 개발된 애플리케이션과 종종 충돌을 일으킨다는 것도 알게되었다.\n \n1. 해결 방법\n우선 해결 방법을 먼저 말하자면 설정에서 \"시간 및 언어\", \"언어 및 지역\", \"옵션\", \"Microsoft 입력기\" 메뉴에서 \"이전 버전의 Microsoft IME\"를 켬(on)으로 해두면 된다. 문제는 이 옵션이 꽤 깊숙한 곳에 있어서 찾아가는게 좀 어렵다. 그래서 찾아가는 방법을 한땀한땀 보여주도록 하겠다.\n먼저 \"설정\"을 실행하고, \"시간 및 언어\" 메뉴를 선택한다. 그리고 아래 그림처럼 \"한국어\"의 점3개를 눌러서 \"언어 옵션\"을 선택한다.\n윈11설정, 시간 및 언어, 언어 및 지역\n\n\n언어 옵션을 선택해서 들어가면 아랫쪽에 키보드 메뉴가 있다. 여기서 아래 그림처럼 \"Microsoft 입력기\"라는 것을 찾고, 점3개를 눌러서 \"키보드 옵션\"을 선택한다.\n한국어, 키보드, Microsoft 입력기\n\n\n키보드 옵션을 선택해서 들어가면 하단에 호환성 부분에 \"이전 버전의 Microsoft IME\"를 켬(on)으로 설정하면 끝난다. \n키보드 옵션, 호환성, 이전 버전의 Microsoft IME\n\n\n이렇게 한 뒤에 되도록이면 재부팅을 하는 것을 권장한다. 애초에 이런 설정이 필요없도록 하려면 tabby 같은 ssh client를 사용하는 것도 괜찮다. 다만 개인적으로는 mobaXterm이 더 편해서 이걸 주로 사용하는 편이다. 그리고 깨알 지식으로 mobaXterm을 사용할 때는 리가처 폰트(ligature font)를 사용하면 이상하게 화면 스크롤 속도가 느려진다. 따라서 되도록이면 ligature 글꼴 대신 일반 글꼴을 사용하는 것을 권장한다.\n \n \n2. 기술적 배경 지식\n왜 이런 문제가 생겼는지 알기 위해서는 IME의 변화에 대해서 알아야만 한다. 마이크로소프트는 XP에서 TSF(Text Services Framework)라는 새로운 IME를 도입했다. 그리고 윈도10이나 윈도11에는 구형 IME인 IMM32와 신형 IME인 TSF가 둘다 설치되고, 기본값으로는 TSF로 작동한다는 점이다. \nTSF의 특징은 비동기로 작동한다는 특징이 있다. 따라서 타이핑을 하는 것을 버퍼에 채워두었다가 완성시키거나 혹은 음성 변환이나 여러가지 기능이 있는데, 이게 기능을 추가하다보니 어느 시점부터 옛날 시스템과 뭔가 안맞는게 생겼던 것 같다. 게다가  mobaXterm은 델파이(Delphi)라고 굉장히 옛날 개발툴로 만들어지다보니 TSF와는 잘 맞지 않는 부분이 있는 것 같다. 그럼에도 불구하고 영어권은 큰 문제가 없어 보인다. 한글이나 중국 번체, 일어 한문등이 가장 큰 문제가 되는것 같은데, 이들 문자들의 특징은 여러 타이핑을 조합해서 하나의 문자가 생기는 케이스가 있다는 점이다. 영어는 hello를 칠때 h를 치면 즉각 h를 표시하면 그만이지만, 한글로 \"헬로\"를 타이핑하려면 \"ㅎ + ㅔ + ㄹ\" 까지 쳐야 \"헬\" 글자 1개가 완성된다. 여기서 ㅎ 다음에 \"ㅔ\" 대신에 \"ㅐ\"를 잘못 타이핑했다면 backspace로 지울때 현재까지 완성된 \"해\"를 지우는게 아니라 딱 \"ㅐ\"만 지우고, \"ㅎ\" 자음은 남겨야 하는데, 이런 부분이 오류를 만드는 원인이 되는 것 같다. 게다가 TSF가 비동기식이라 어떤 경우에는 잘 작동하지만, 어떤 경우에는 비동기 이벤트를 놓쳐서 \"응답 없음\"오류를 만드는 것 같다.\n물론 2026년을 기준으로 보면 대부분의 개발툴들은 TSF를 제대로 지원하는 경우가 많아서 큰 문제는 안되지만, 앞서 언급한 델파이나 아니면 cygwin, Qt을 이용하는 시스템에서는 문제가 종종 생기는 것 같다. 결국 문제가 생기는 애플리케이션을 주로 쓴다면 결국 TSF를 버리고 \"이전 버전의 Microsoft IME\"인 IMM32를 쓰는 수 밖에 없어 보인다. (마소가 근본적인 문제를 고쳐서 TSF를 쓸때도 문제없게 해주면 좋겠지만, 솔직히 기대도 안한다. 왜냐하면 지금 윈11은 툭하면 업데이트 버그가 쏟아지기 때문이다. 사소한 버그도 해결 못하는 것을 보면 윈도 개발팀의 프로그래밍 능력이 수준 이하라고 생각된다.)\n게다가 TSF를 쓰면 성능도 좋아진다고는 하지만 솔직히 요새 시스템에서 TSF를 써서 얻는 성능적 이점은 쥐꼬리의 털조각 1개만큼도 안되는 수준이라, 성능적 이점은 사실상 없다. 다만 TSF는 한자를 더 많이 지원하는 장점이 있다고 하는데, 요새 한자를 병용하거나 한자를 타이핑하는 경우는 드물기 때문에 큰 메리트가 없어 보인다.\n결론적으로 mobaXterm을 쓸때는 TSF를 쓰지 않고 IMM32를 쓰도록 \"이전 버전의 Microsoft IME\"를 설정하도록 하자.\n \n \n히스토리\n2026.02.23 릴리즈",
        "guid": "https://sunyzero.tistory.com/325",
        "categories": [
          "컴퓨터 관련/윈도 패밀리",
          "IME",
          "mobaXterm crash",
          "SSH Client",
          "이전 버전의 Microsoft IME",
          "한글 입력기"
        ],
        "isoDate": "2026-02-23T12:07:50.000Z"
      }
    ]
  },
  {
    "name": "강대명",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권정혁",
    "category": "개인",
    "posts": []
  },
  {
    "name": "줌구",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수보",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김시은",
    "category": "개인",
    "posts": []
  },
  {
    "name": "곽민수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김범진",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민서",
    "category": "개인",
    "posts": []
  },
  {
    "name": "I am not Okay",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권창현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권기호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수민",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강태욱",
    "category": "개인",
    "posts": [
      {
        "title": "ViT 및 VLM 메커니즘 이해 및 코드 스크래치하기",
        "link": "http://daddynkidsmakers.blogspot.com/2026/02/vlm.html",
        "pubDate": "2026-02-24T11:15:00.000Z",
        "author": "Daddy Maker",
        "content": "<div style=\"text-align: left;\">이 글은&nbsp;VLM 스크래치하는 방법을 나눔한다.<br /></div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEhCcVVuSQvauRL8UwRWUmS88gpIymnXfWYUVYrEwyraFszdBz6g3J5K72KrQc4509lbQrtyqz_OxLa8YtDwZFZMq-bolQV3f_iJfq2HL2fb3R7S5OEpioZ7ZmGF_fWBkYqR4f6VlHUFZGdNEkkFa9hDaBRw1YN2UcDjMP9dTIBDQvWEoHw7eKt4WW1P4jj2\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"865\" data-original-width=\"1100\" height=\"252\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEhCcVVuSQvauRL8UwRWUmS88gpIymnXfWYUVYrEwyraFszdBz6g3J5K72KrQc4509lbQrtyqz_OxLa8YtDwZFZMq-bolQV3f_iJfq2HL2fb3R7S5OEpioZ7ZmGF_fWBkYqR4f6VlHUFZGdNEkkFa9hDaBRw1YN2UcDjMP9dTIBDQvWEoHw7eKt4WW1P4jj2=w320-h252\" width=\"320\" /></a></div><br /></div></div><div style=\"text-align: left;\"><b>VLM 레퍼런스</b></div><div style=\"text-align: left;\"><ul style=\"text-align: left;\"><li><a href=\"https://qiita.com/asaoka/items/fb8780fadeb24aaa552f\">Vision Language Model from scratch in Pytorch #vlm - Qiita</a></li><li><a href=\"https://github.com/AviSoori1x/seemore\">AviSoori1x/seemore: From scratch implementation of a vision language model in pure PyTorch</a></li><li><a href=\"https://huggingface.co/blog/nanovlm\">nanoVLM: The simplest repository to train your VLM in pure PyTorch</a></li><li><a href=\"https://github.com/huggingface/nanoVLM\">huggingface/nanoVLM: The simplest, fastest repository for training/finetuning small-sized VLMs.</a></li><li><a href=\"https://medium.com/@saptarshimt/training-a-vision-language-model-from-scratch-vlm-multi-modal-a0b43c5966b1\">Training a Vision Language Model from scratch (VLM multi-modal) | by Saptarshi MT | Medium</a></li><li><a href=\"https://medium.com/@achrafabbaoui/implementation-of-vision-language-models-vlm-from-scratch-a-comprehensive-technical-deep-dive-d348322f9b3c\">Implementation of Vision language models (VLM) from scratch: A Technical Deep Dive. | by Achraf Abbaoui | Medium</a></li><li><a href=\"https://medium.com/@govindarajpriyanthan/wiring-the-multimodal-mind-building-a-vision-language-model-vlm-from-scratch-part-1-1aee93f1d474\">Wiring the Multimodal Mind: Building a Vision Language Model (VLM) from Scratch - Part 1 | by Priyanthan Govindaraj | Medium</a></li><li><a href=\"https://huggingface.co/blog/AviSoori1x/seemore-vision-language-model\">seemore: Implement a Vision Language Model from Scratch</a></li><li><a href=\"https://github.com/Vidit-Ostwal/VLM-from-scratch\">Vidit-Ostwal/VLM-from-scratch: This is majorly for my own learning purpose.</a></li><li><a href=\"https://www.vizuaranewsletter.com/p/building-a-nano-vision-language-model\">Building a Nano Vision-Language Model from Scratch</a></li><li><a href=\"https://github.com/nipunbatra/vlm-from-scratch\">nipunbatra/vlm-from-scratch</a></li><li><a href=\"https://medium.com/@shanmuka.sadhu/building-paligemma-vlm-from-scratch-using-pytorch-7bc6bb58efd2\">Building PaliGemma VLM From Scratch using Pytorch | by Shanmuka Sadhu | Jan, 2026 | Medium</a></li><li><a href=\"https://huggingface.co/blog/smolvlm\">SmolVLM - small yet mighty Vision Language Model</a></li></ul><div><b>ViT 레퍼런스</b></div><div><ul style=\"text-align: left;\"><li><a href=\"https://www.kaggle.com/code/raufmomin/vision-transformer-vit-from-scratch\">Vision Transformer (ViT) from Scratch</a></li><li><a href=\"https://medium.com/@manindersingh120996/building-vision-transformers-vit-from-scratch-1f46a36ed44b\">Building Vision Transformers (ViT) from Scratch | by Maninder Singh | Medium</a></li></ul></div></div>",
        "contentSnippet": "이 글은 VLM 스크래치하는 방법을 나눔한다.\n\n\n\n\n\nVLM 레퍼런스\n\nVision Language Model from scratch in Pytorch #vlm - Qiita\nAviSoori1x/seemore: From scratch implementation of a vision language model in pure PyTorch\nnanoVLM: The simplest repository to train your VLM in pure PyTorch\nhuggingface/nanoVLM: The simplest, fastest repository for training/finetuning small-sized VLMs.\nTraining a Vision Language Model from scratch (VLM multi-modal) | by Saptarshi MT | Medium\nImplementation of Vision language models (VLM) from scratch: A Technical Deep Dive. | by Achraf Abbaoui | Medium\nWiring the Multimodal Mind: Building a Vision Language Model (VLM) from Scratch - Part 1 | by Priyanthan Govindaraj | Medium\nseemore: Implement a Vision Language Model from Scratch\nVidit-Ostwal/VLM-from-scratch: This is majorly for my own learning purpose.\nBuilding a Nano Vision-Language Model from Scratch\nnipunbatra/vlm-from-scratch\nBuilding PaliGemma VLM From Scratch using Pytorch | by Shanmuka Sadhu | Jan, 2026 | Medium\nSmolVLM - small yet mighty Vision Language Model\n\nViT 레퍼런스\n\nVision Transformer (ViT) from Scratch\nBuilding Vision Transformers (ViT) from Scratch | by Maninder Singh | Medium",
        "id": "tag:blogger.com,1999:blog-5201956450461596914.post-4261089744151773268",
        "isoDate": "2026-02-24T11:15:00.000Z"
      },
      {
        "title": "가우시안 스플리터의 한계와 공간모델 개발",
        "link": "http://daddynkidsmakers.blogspot.com/2026/02/blog-post_24.html",
        "pubDate": "2026-02-24T10:32:25.942Z",
        "author": "Daddy Maker",
        "content": "<div style=\"background-color: #ffffff; color: #000000;\">\n  <p>오토데스크나 제조업에서 요구하는 진정한 '공간 지능'과 '파라메트릭 CAD'를 구현하려면, AI가 단순한 점과 면(Mesh)의 집합이 아닌 B-rep(경계 표현)이나 CSG(Constructive Solid Geometry) 같은 수학적 스케치와 돌출(Extrude) 명령어 시퀀스를 생성할 수 있어야 한다.</p>\n  <p>이러한 치수 제어 및 파라메트릭 모델링, 그리고 공간 지능(LWM)을 향해 연구되고 있는 오픈소스 및 프로젝트들을 엄선해 조사했다.</p>\n\n  <p><b>1. 파라메트릭 CAD 생성 및 절차적 3D 모델 (AI to CAD)</b></p>\n  <p>단순한 메쉬(.obj)가 아니라, 치수를 조절할 수 있는 STEP 파일이나 CAD 명령어 스크립트를 생성하는 프로젝트들이다.</p>\n\n  <p><b>DeepCAD (A Deep Generative Network for CAD Models)</b></p>\n  <p>설명: 3D CAD 모델을 단순한 3D 도형이 아니라, '스케치(Profile) → 돌출(Extrude) → 필렛(Fillet)' 같은 CAD 명령어의 시퀀스로 인식하고 생성하는 선구적인 프로젝트이다. AI가 설계자의 작업 순서를 학습하여 파라메트릭 수정이 가능한 데이터를 추출한다.<br>\n  특징: 출력물이 명령어 시퀀스이므로 Fusion 360이나 SolidWorks 같은 툴에서 치수를 즉각적으로 수정할 수 있다.<br>\n  GitHub: <a href=\"https://github.com/ChrisWu1997/DeepCAD\">ChrisWu1997/DeepCAD</a></p>\n\n  <p><img src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjX0fAtriavQwVltbjX0t6kZfOMH82BnXtJRMRCK2spG6IeM4deE-xOkrmpbLYgVoCo4KGxE_sEYEYkjDFiUXzHXIbsoism5BsqBCsQe3N1FExs_FpBsABZc6Yq1dG2r0em0Q70MrJ-5fI4HYe5mBF2xUMdp-t5ZkXkGsxmghUjHtv83H4lwWx028WZ745s=w500-h150\" alt=\"DeepCAD Architecture\" /><br>\n  [3D 스캔/점군] → PointNet++ → z → Decoder → [CAD 시퀀스. L | A | E(θ,φ,e1,e2)]</p>\n\n  <p><b>Zoo Text-to-CAD API</b></p>\n  <p>설명: 텍스트를 입력하면 (예: \"20개의 톱니가 있고 중심축 구멍 지름이 5mm인 기어\") 즉석에서 파라메트릭 CAD 코드(KCL - KittyCAD Language)를 생성하여 STEP, IGES 등의 포맷으로 변환해 주는 프로젝트이다.<br>\n  특징: 기하학적 제약 조건(Constraints)을 AI가 이해하고 코드로 작성하기 때문에 완벽한 치수 제어가 가능하다. 핵심 엔진 부분을 오픈소스로 공개하며 발전하고 있다.<br>\n  GitHub: <a href=\"https://github.com/KittyCAD\">Zoo-dev / kittyCAD 인프라</a></p>\n\n  <p><b>Infinigen</b></p>\n  <p>설명: 자연계와 사물을 100% 절차적(Procedural)인 수학 공식과 노드(Node) 트리로 생성해 내는 거대한 3D 프레임워크이다.<br>\n  특징: \"나뭇잎의 길이\", \"의자 다리의 두께\" 등을 파라미터(수치)로 조절할 수 있다. 가우시안 덩어리가 아니라 처음부터 수학적 규칙으로 짜인 세계를 만들기 때문에 완벽한 편집이 가능하다.<br>\n  GitHub: <a href=\"https://github.com/princeton-vl/infinigen\">princeton-vl/infinigen</a></p>\n\n  <p><b>2. 공간 지능 (Spatial Intelligence) 및 LWM(Large World Model)</b></p>\n  <p>단순한 2D의 연속이 아니라 물리적 3D 공간의 깊이, 기하학, 영속성을 이해하는 기초 모델(Foundation Model) 연구이다.</p>\n\n  <p><b>LargeWorldModel (LWM) - UC Berkeley</b></p>\n  <p>설명: 프로젝트 이름 자체가 LWM이다. 100만(1M) 토큰의 컨텍스트 창을 가진 비디오/언어 모델이다.<br>\n  특징: 긴 영상이나 여러 장의 이미지를 보고 그 안의 3D 공간 구조를 기억하고 이해한다. 당장 CAD 모델을 뱉어내는 용도는 아니지만, AI가 다중 시점을 통해 공간의 3차원적 기하학(Geometry)을 스스로 깨우치게 만드는 '공간 지능'의 가장 대표적인 베이스라인 모델이다.<br>\n  GitHub: <a href=\"https://github.com/LargeWorldModel/LWM\">LargeWorldModel/LWM</a></p>\n  <p><img src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjjBjjThP6y0dy1VHdsZdirL5U26wOcN-8Mi6Wm3aGWOtdfc0ybcWvXtxaMmBX0ilFpr8TS5WWbP_zEgCyvTveJS2Txz6pbP3d5WDfQQbOVnmNNtJQVXkfUO6PPODZHsKhVv3xmSq_7yi7agFS1D9AymEnEWfIJEokbgZozvnnouL_g44CrERJPO3eMEYYd=w287-h320\" alt=\"LWM Diagram\" /></p>\n\n  <p><b>Zero123</b></p>\n  <p>설명: 단일 이미지를 보고 물체의 보이지 않는 뒷면과 다른 각도의 시점을 기하학적으로 일관되게 추론해 내는 모델이다.<br>\n  특징: 이 기술 자체는 파라메트릭 CAD가 아니지만, 2D 이미지를 3D 파라메트릭 데이터로 역설계(Reverse Engineering)하기 위해 필수적으로 거쳐야 하는 \"공간의 시점 변화 이해\"를 담당한다.<br>\n  GitHub: <a href=\"https://github.com/SUDO-AI-3D/zero123plus\">SUDO-AI-3D/zero123plus</a></p>\n  <p><img src=\"https://blogger.googleusercontent.com/img/a/AVvXsEiqTffhGf7o0o3Y_V-oHfopChvBzLMichQAZkk57UgcboWGw0K4iGSyk0uQhsU5RPOEARh3mwgelUKSjGI5jh0LgXsuuP-uOptNb0NxHgTE4e5cFvCZSi_zykpAObhGGDRMhkVcv30uTR7EPGHt4e1zKg1U3BkOHkM7_5ogiLURlQsOFqE0a_P-jnYYCu-f=w400-h300\" alt=\"Zero123 Examples\" /></p>\n\n  <p><b>현재 기술의 한계와 돌파구</b></p>\n  <p>현재의 한계 (Image to 3D): 이미지를 보고 가우시안 스플래팅이나 메쉬(OBJ)를 만드는 것은 빠르지만, 산업용 설계나 정밀한 편집에는 한계가 명확하다.<br>\n  미래의 방향 (AI to CAD): LWM과 공간 지능이 발전함에 따라, AI가 이미지를 분석한 뒤 \"이것은 반지름 5cm의 원통과 10x10의 직육면체가 결합된 형태\"라고 수학적으로 분해(CSG)하여 코드를 짜주는 방식으로 발전하고 있다. 그 선두에 DeepCAD와 Zoo(Text-to-CAD) 같은 프로젝트가 위치해 있다. 가우시안 스플래팅(3DGS)은 시각적 복원에 초점을 맞추기 때문에 스케일이 없는(Non-scale) 폴리곤 메쉬만을 생성할 뿐, 산업용으로 조작 가능한 CSG나 B-rep 데이터를 만들지 못한다.</p>\n\n  <p><b>부록: 두 방식 발전 방향</b></p>\n  <p>두 방식 중 어느 것이 '더 좋은가'는 목적에 따라 완전히 갈리며, 페이페이 리(Fei-Fei Li) 교수의 월드랩스(World Labs)가 추구하는 거대 세계 모델(LWM)의 방향성도 이 두 기술의 교차점에 있다. 이를 심층적으로 분석하고 최신 SOTA 프로젝트를 조사한다.</p>\n\n  <p><b>1. 시퀀스 생성(DeepCAD 계열) vs 시각적 렌더링(3DGS 계열) 비교</b></p>\n  <p>결론부터 말하자면, 제조/설계(AEC/CAD) 분야에서는 DeepCAD 방식이 압도적으로 우월하고, 엔터테인먼트/가상현실/로보틱스 비전 분야에서는 3DGS 방식이 절대적으로 유리하다.</p>\n  <p>먼저 DeepCAD 계열(AI to CAD Sequence)은 산업용 설계 도면을 만들어내는 데 특화되어 있다. 이 기술의 핵심 원리는 3D 형상의 겉모습만 묘사하는 것이 아니라, 대상을 모델링하기 위한 수학적 명령어의 순서를 인공지능이 직접 추론해 내는 것이다. 그 결과물은 단순한 점토 덩어리가 아니라, 실제 설계 프로그램에서 즉시 다룰 수 있는 파라메트릭 CAD 데이터(STEP, IGES, CSG 스크립트 등) 형태로 출력된다. 이 방식의 가장 큰 무기는 완벽한 절대 치수 제어와 세밀한 곡률 반경 수정이 가능하다는 점이다. 하지만 수학적인 공식으로 딱 떨어지지 않는 자연물(사람, 나무 등)이나 비정형적이고 복잡한 형상을 표현하는 데는 뚜렷한 한계를 보인다.</p>\n  <p>반면 가우시안 스플래팅(Image to 3DGS/Mesh)은 현실 세계의 시각적인 복원에 모든 초점을 맞추고 있다. 빛의 반사와 색상 정보를 지닌 무수히 많은 타원체 입자를 3D 공간에 흩뿌려 세상을 사실적으로 표현하는 것이 핵심 원리이다. 그렇기 때문에 결과물 역시 속이 꽉 찬 설계 데이터가 아니라, 텅 빈 공간에 떠 있는 포인트 클라우드나 비정형 메쉬(PLY, OBJ) 형태로 도출된다. 이 방식은 사진처럼 정밀하고 압도적인 시각 효과를 주지만, 물리적인 절대 치수(Scale) 개념이 없고 임의의 상대 비율만 존재하여 토폴로지(구조) 편집이 원천적으로 불가능하다. 따라서 0.1mm의 오차도 허용되지 않는 산업용 금형 제작이나 정밀 조립을 위한 공차 설계 등에는 사용할 수 없다.</p>\n  <p>최근의 산업 트렌드는 이 둘을 결합하여, \"3DGS로 현실 세계를 빠르게 스캔한 뒤, AI가 그 포인트 클라우드에서 기하학적 특징(원통, 평면 등)을 역산하여 CAD 시퀀스로 변환하는 방식(Scan-to-BIM / Scan-to-CAD)\"으로 진화하고 있다.</p>\n\n  <p><b>2. 각 계열의 최신 SOTA 깃허브 프로젝트</b></p>\n  \n  <p><b>A. CAD 시퀀스 및 B-rep 생성 (DeepCAD의 진화형)</b></p>\n  <p>단순히 모양을 맞추는 것을 넘어, 위상(Topology)과 스케치 제약 조건(Constraints)을 완벽하게 학습하는 모델들이다.</p>\n  <p>SkexGen (Sketch-and-Extrude Generation)<br>\n  설명: DeepCAD를 발전시켜, 트랜스포머(Transformer) 구조를 이용해 2D 스케치 프로파일과 돌출(Extrude) 파라미터를 자동 회귀(Autoregressive) 방식으로 생성하는 최신 모델이다. 토폴로지 일관성이 훨씬 뛰어나다.<br>\n  GitHub: <a href=\"https://github.com/yccyenchiao/SkexGen\">yccyenchiao/SkexGen</a></p>\n  \n  <p>Hextree / SECAD-Net<br>\n  설명: CAD 모델의 모서리(Edge)와 면(Face)의 상호작용을 그래프(Graph) 신경망으로 학습하여, 훨씬 복잡한 솔리드(Solid) 모델을 B-rep 형태로 생성해 낸다.<br>\n  GitHub: <a href=\"https://github.com/Puhao11/SECAD-Net\">Puhao11/SECAD-Net</a></p>\n\n  <p><b>B. 기하학적 정밀도를 높인 가우시안 스플래팅 (3DGS의 진화형)</b></p>\n  <p>3DGS의 단점인 '수학적 표면(Surface)이 없다'는 문제를 해결하여, 고품질의 메쉬를 뽑아내기 위한 모델들이다.</p>\n  <p>SuGaR (Surface-Aligned Gaussian Splatting)<br>\n  설명: 가우시안 타원체들이 물체의 실제 표면에 납작하게 달라붙도록 강제(Alignment)하여, 3DGS에서 아주 깔끔하고 정확한 메쉬(Mesh)를 추출해 내는 SOTA 기술이다.<br>\n  GitHub: <a href=\"https://github.com/Anttwo/SuGaR\">Anttwo/SuGaR</a></p>\n  \n  <p>2D Gaussian Splatting (2DGS)<br>\n  설명: 3D 부피를 가진 타원체 대신 2D 디스크 형태의 가우시안을 사용하여 형상의 경계와 표면을 극도로 정밀하게 재구성한다. 자율주행이나 로보틱스 매핑에 많이 쓰인다.<br>\n  GitHub: <a href=\"https://github.com/hbb1/2d-gaussian-splatting\">hbb1/2d-gaussian-splatting</a></p>\n\n  <p><b>3. 페이페이 리 교수(World Labs)의 LWM 설계 방식 추론</b></p>\n  <p>그녀는 수학적 기반의 B-rep이나 파라메트릭 CAD 전문가는 아니지만, 컴퓨터 비전(ImageNet 창시자)과 로보틱스(Embodied AI)의 권위자로서 '카메라 렌즈를 통해 3D 물리 공간의 구조와 깊이를 추론하는 방식'에는 세계 최고 수준의 이해도를 가지고 있다.</p>\n  <p>따라서 월드랩스의 LWM(마블)은 제조용 CAD 생성이 아니라, 물리 법칙이 작용하는 시뮬레이션 환경 구축에 초점을 맞추어 다음과 같이 설계될 것으로 추론된다.</p>\n  \n  <p>입력 및 추론 (2D/비디오 파운데이션 기반): 디퓨전 모델이나 트랜스포머가 단일 이미지/텍스트를 입력받아 보이지 않는 뒷면과 공간의 깊이(Depth)를 추론한다. (Zero123과 유사한 공간 상상력).<br>\n  공간의 표현 (하이브리드 3DGS/NeRF): 생성된 공간을 B-rep이나 명령어 시퀀스가 아니라, 렌더링 속도가 빠른 3DGS나 Neural Fields로 빠르게 메모리에 올린다.<br>\n  물리적 지능 부여 (Semantic &amp; Physical Grounding): 여기가 마블(Marble)의 핵심이 될 것이다. 단순한 픽셀 덩어리(3DGS)에 분할(Segmentation) 라벨을 씌워 \"이 가우시안 덩어리는 '유리'이고 깨질 수 있다\", \"저 덩어리는 '의자'이며 중력의 영향을 받는다\"라는 물리적 속성을 부여한다.<br>\n  출력 (Interactive 3D World): 치수 측정이 가능한 CAD가 아니라, 언리얼 엔진이나 오토데스크 Maya에서 즉시 카메라를 돌려보고 객체를 물리적으로 움직여볼 수 있는 '인터랙티브 3D 씬(Scene)' 자체를 내뱉는다.</p>\n  \n  <p>CAD 진영(DeepCAD)은 설계 도면을 역공학하는 방향으로 발전하고 있고, 비전 진영(World Labs, 3DGS)은 카메라에 찍힌 세상에 물리 엔진을 덧씌워 가상 현실을 창조하는 방향으로 평행선을 달리고 있다.</p>\n  <p>최근의 역설계 SOTA 모델들은 이 두 가지(신경망의 패턴 인식 + 수학적 피팅)를 하나의 파이프라인으로 합친 미분 가능한 피팅(Differentiable Fitting) 방식을 사용한다.</p>\n  <p>신경망이 점들을 분류하고 치수를 대략 추정하면, 수학적 오차(Loss)가 발생한다. 이 오차 값을 역전파(Backpropagation) 시켜서 다시 신경망을 훈련하는 구조다. 즉, AI가 단순히 '비슷하게 생겼네'하고 끝내는 것이 아니라, \"내가 예측한 원통의 반지름이 실제 스캔 점들의 분포와 수학적으로 0.2mm 오차가 있으니 가중치를 수정해야겠다\"라고 스스로 학습하는 경지에 이르렀다. (관련 대표 오픈소스: ParseNet, HPNet)</p>\n\n  <p><b>레퍼런스</b></p>\n  <p><a href=\"https://github.com/fz-20/BGPSeg\">fz-20/BGPSeg: BGPSeg: Boundary-Guided Primitive Instance Segmentation of Point Clouds</a></p>\n</div>",
        "contentSnippet": "오토데스크나 제조업에서 요구하는 진정한 '공간 지능'과 '파라메트릭 CAD'를 구현하려면, AI가 단순한 점과 면(Mesh)의 집합이 아닌 B-rep(경계 표현)이나 CSG(Constructive Solid Geometry) 같은 수학적 스케치와 돌출(Extrude) 명령어 시퀀스를 생성할 수 있어야 한다.\n이러한 치수 제어 및 파라메트릭 모델링, 그리고 공간 지능(LWM)을 향해 연구되고 있는 오픈소스 및 프로젝트들을 엄선해 조사했다.\n1. 파라메트릭 CAD 생성 및 절차적 3D 모델 (AI to CAD)\n단순한 메쉬(.obj)가 아니라, 치수를 조절할 수 있는 STEP 파일이나 CAD 명령어 스크립트를 생성하는 프로젝트들이다.\nDeepCAD (A Deep Generative Network for CAD Models)\n설명: 3D CAD 모델을 단순한 3D 도형이 아니라, '스케치(Profile) → 돌출(Extrude) → 필렛(Fillet)' 같은 CAD 명령어의 시퀀스로 인식하고 생성하는 선구적인 프로젝트이다. AI가 설계자의 작업 순서를 학습하여 파라메트릭 수정이 가능한 데이터를 추출한다.\nChrisWu1997/DeepCAD\n\nZoo Text-to-CAD API\n설명: 텍스트를 입력하면 (예: \"20개의 톱니가 있고 중심축 구멍 지름이 5mm인 기어\") 즉석에서 파라메트릭 CAD 코드(KCL - KittyCAD Language)를 생성하여 STEP, IGES 등의 포맷으로 변환해 주는 프로젝트이다.\nZoo-dev / kittyCAD 인프라\nInfinigen\n설명: 자연계와 사물을 100% 절차적(Procedural)인 수학 공식과 노드(Node) 트리로 생성해 내는 거대한 3D 프레임워크이다.\nprinceton-vl/infinigen\n2. 공간 지능 (Spatial Intelligence) 및 LWM(Large World Model)\n단순한 2D의 연속이 아니라 물리적 3D 공간의 깊이, 기하학, 영속성을 이해하는 기초 모델(Foundation Model) 연구이다.\nLargeWorldModel (LWM) - UC Berkeley\n설명: 프로젝트 이름 자체가 LWM이다. 100만(1M) 토큰의 컨텍스트 창을 가진 비디오/언어 모델이다.\nLargeWorldModel/LWM\n\nZero123\n설명: 단일 이미지를 보고 물체의 보이지 않는 뒷면과 다른 각도의 시점을 기하학적으로 일관되게 추론해 내는 모델이다.\nSUDO-AI-3D/zero123plus\n\n현재 기술의 한계와 돌파구\n현재의 한계 (Image to 3D): 이미지를 보고 가우시안 스플래팅이나 메쉬(OBJ)를 만드는 것은 빠르지만, 산업용 설계나 정밀한 편집에는 한계가 명확하다.\n부록: 두 방식 발전 방향\n두 방식 중 어느 것이 '더 좋은가'는 목적에 따라 완전히 갈리며, 페이페이 리(Fei-Fei Li) 교수의 월드랩스(World Labs)가 추구하는 거대 세계 모델(LWM)의 방향성도 이 두 기술의 교차점에 있다. 이를 심층적으로 분석하고 최신 SOTA 프로젝트를 조사한다.\n1. 시퀀스 생성(DeepCAD 계열) vs 시각적 렌더링(3DGS 계열) 비교\n결론부터 말하자면, 제조/설계(AEC/CAD) 분야에서는 DeepCAD 방식이 압도적으로 우월하고, 엔터테인먼트/가상현실/로보틱스 비전 분야에서는 3DGS 방식이 절대적으로 유리하다.\n먼저 DeepCAD 계열(AI to CAD Sequence)은 산업용 설계 도면을 만들어내는 데 특화되어 있다. 이 기술의 핵심 원리는 3D 형상의 겉모습만 묘사하는 것이 아니라, 대상을 모델링하기 위한 수학적 명령어의 순서를 인공지능이 직접 추론해 내는 것이다. 그 결과물은 단순한 점토 덩어리가 아니라, 실제 설계 프로그램에서 즉시 다룰 수 있는 파라메트릭 CAD 데이터(STEP, IGES, CSG 스크립트 등) 형태로 출력된다. 이 방식의 가장 큰 무기는 완벽한 절대 치수 제어와 세밀한 곡률 반경 수정이 가능하다는 점이다. 하지만 수학적인 공식으로 딱 떨어지지 않는 자연물(사람, 나무 등)이나 비정형적이고 복잡한 형상을 표현하는 데는 뚜렷한 한계를 보인다.\n반면 가우시안 스플래팅(Image to 3DGS/Mesh)은 현실 세계의 시각적인 복원에 모든 초점을 맞추고 있다. 빛의 반사와 색상 정보를 지닌 무수히 많은 타원체 입자를 3D 공간에 흩뿌려 세상을 사실적으로 표현하는 것이 핵심 원리이다. 그렇기 때문에 결과물 역시 속이 꽉 찬 설계 데이터가 아니라, 텅 빈 공간에 떠 있는 포인트 클라우드나 비정형 메쉬(PLY, OBJ) 형태로 도출된다. 이 방식은 사진처럼 정밀하고 압도적인 시각 효과를 주지만, 물리적인 절대 치수(Scale) 개념이 없고 임의의 상대 비율만 존재하여 토폴로지(구조) 편집이 원천적으로 불가능하다. 따라서 0.1mm의 오차도 허용되지 않는 산업용 금형 제작이나 정밀 조립을 위한 공차 설계 등에는 사용할 수 없다.\n최근의 산업 트렌드는 이 둘을 결합하여, \"3DGS로 현실 세계를 빠르게 스캔한 뒤, AI가 그 포인트 클라우드에서 기하학적 특징(원통, 평면 등)을 역산하여 CAD 시퀀스로 변환하는 방식(Scan-to-BIM / Scan-to-CAD)\"으로 진화하고 있다.\n2. 각 계열의 최신 SOTA 깃허브 프로젝트\nA. CAD 시퀀스 및 B-rep 생성 (DeepCAD의 진화형)\n단순히 모양을 맞추는 것을 넘어, 위상(Topology)과 스케치 제약 조건(Constraints)을 완벽하게 학습하는 모델들이다.\nSkexGen (Sketch-and-Extrude Generation)\nyccyenchiao/SkexGen\nHextree / SECAD-Net\nPuhao11/SECAD-Net\nB. 기하학적 정밀도를 높인 가우시안 스플래팅 (3DGS의 진화형)\n3DGS의 단점인 '수학적 표면(Surface)이 없다'는 문제를 해결하여, 고품질의 메쉬를 뽑아내기 위한 모델들이다.\nSuGaR (Surface-Aligned Gaussian Splatting)\nAnttwo/SuGaR\n2D Gaussian Splatting (2DGS)\nhbb1/2d-gaussian-splatting\n3. 페이페이 리 교수(World Labs)의 LWM 설계 방식 추론\n그녀는 수학적 기반의 B-rep이나 파라메트릭 CAD 전문가는 아니지만, 컴퓨터 비전(ImageNet 창시자)과 로보틱스(Embodied AI)의 권위자로서 '카메라 렌즈를 통해 3D 물리 공간의 구조와 깊이를 추론하는 방식'에는 세계 최고 수준의 이해도를 가지고 있다.\n따라서 월드랩스의 LWM(마블)은 제조용 CAD 생성이 아니라, 물리 법칙이 작용하는 시뮬레이션 환경 구축에 초점을 맞추어 다음과 같이 설계될 것으로 추론된다.\n입력 및 추론 (2D/비디오 파운데이션 기반): 디퓨전 모델이나 트랜스포머가 단일 이미지/텍스트를 입력받아 보이지 않는 뒷면과 공간의 깊이(Depth)를 추론한다. (Zero123과 유사한 공간 상상력).\nCAD 진영(DeepCAD)은 설계 도면을 역공학하는 방향으로 발전하고 있고, 비전 진영(World Labs, 3DGS)은 카메라에 찍힌 세상에 물리 엔진을 덧씌워 가상 현실을 창조하는 방향으로 평행선을 달리고 있다.\n최근의 역설계 SOTA 모델들은 이 두 가지(신경망의 패턴 인식 + 수학적 피팅)를 하나의 파이프라인으로 합친 미분 가능한 피팅(Differentiable Fitting) 방식을 사용한다.\n신경망이 점들을 분류하고 치수를 대략 추정하면, 수학적 오차(Loss)가 발생한다. 이 오차 값을 역전파(Backpropagation) 시켜서 다시 신경망을 훈련하는 구조다. 즉, AI가 단순히 '비슷하게 생겼네'하고 끝내는 것이 아니라, \"내가 예측한 원통의 반지름이 실제 스캔 점들의 분포와 수학적으로 0.2mm 오차가 있으니 가중치를 수정해야겠다\"라고 스스로 학습하는 경지에 이르렀다. (관련 대표 오픈소스: ParseNet, HPNet)\n레퍼런스\nfz-20/BGPSeg: BGPSeg: Boundary-Guided Primitive Instance Segmentation of Point Clouds",
        "id": "tag:blogger.com,1999:blog-5201956450461596914.post-3574650709079284629",
        "isoDate": "2026-02-24T10:32:25.942Z"
      },
      {
        "title": "ParSeNet, HPNet 딥러닝 모델 구조 조사 분석",
        "link": "http://daddynkidsmakers.blogspot.com/2026/02/parsenet-hpnet.html",
        "pubDate": "2026-02-20T10:11:00.000Z",
        "author": "Daddy Maker",
        "content": "ParSeNet이나 HPNet과 같은 역설계(Scan-to-CAD) 모델의 핵심 '눈(Eye)' 역할을 하는 백본(Backbone) 아키텍처는 주로 3D 점군(Point Cloud)의 기하학적 특징을 추출하는 딥러닝 네트워크로 구성된다.<br /><br /><div style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEguzQEMGxVwNAYt23vBGhT7CzW6u5er5TjrrAqIvbf4z7IPFy6g580sXtG-JpIocojesAWN4HpTZNMh5BG4Jp0xbCBXt_4vxgGKJITyixUcrUnos-zhtvHBHGWi1xIZ-4Gb8XeA4qWHcuWjEhYwLyCqHE3mUSi2HPRbASIFBmELzd3WISBGFcrecfl9oop2\"><img src=\"https://blogger.googleusercontent.com/img/a/AVvXsEguzQEMGxVwNAYt23vBGhT7CzW6u5er5TjrrAqIvbf4z7IPFy6g580sXtG-JpIocojesAWN4HpTZNMh5BG4Jp0xbCBXt_4vxgGKJITyixUcrUnos-zhtvHBHGWi1xIZ-4Gb8XeA4qWHcuWjEhYwLyCqHE3mUSi2HPRbASIFBmELzd3WISBGFcrecfl9oop2=w578-h156\" /></a></div><br /><b>1. 백본 모델(Backbone Model)의 구조와 기능<br /></b>이러한 파이프라인의 백본은 2D 이미지의 픽셀을 처리하는 CNN(ResNet 등)과 달리, 순서가 없고 불규칙하게 흩어진 3차원 좌표의 집합을 처리해야 한다. 이를 위해 주로 다음과 같은 3가지 아키텍처가 백본으로 결합되어 사용된다.<br /><br />PointNet++ (가장 표준적인 백본)<br /><blockquote style=\"border: none; margin: 0px 0px 0px 40px; padding: 0px; text-align: left;\">구조: 다층 퍼셉트론(MLP)을 각 점에 독립적으로 적용한 뒤, 최대 풀링(Max Pooling)을 통해 입력 순서에 구애받지 않는 대칭 함수(Symmetric Function)를 구성한다. 여기에 계층적 샘플링(Furthest Point Sampling)과 지역 군집화(Ball Query) 기법을 더해 지역적 기하학(Local Geometry)을 캡처한다.<br />기능: 단순한 좌표들을 엮어, \"이 점 주변은 평평하다\", \"이 점은 날카로운 모서리에 있다\"는 정보를 담은 고차원 특징 벡터로 변환한다.</blockquote><br />DGCNN (Dynamic Graph CNN)<br /><blockquote style=\"border: none; margin: 0px 0px 0px 40px; padding: 0px; text-align: left;\">구조: 점들 사이의 k-최근접 이웃(k-NN) 그래프를 구성하고, 네트워크 계층이 깊어질수록 특징 공간(Feature Space) 상에서 그래프의 연결을 동적으로 다시 계산하는 EdgeConv 연산을 수행한다.<br />기능: 점과 점 사이의 '관계'를 학습하는 데 특화되어 있다. 곡률이 변하는 경계면(Boundary)이나 서로 맞닿아 있는 직교 평면의 특징을 뚜렷하게 잡아낸다.</blockquote><br />Point Transformer (최신 SOTA 백본)<br /><blockquote style=\"border: none; margin: 0px 0px 0px 40px; padding: 0px; text-align: left;\">구조: 자연어 처리에서 쓰이는 셀프 어텐션(Self-attention) 메커니즘을 3D 점군에 맞게 변형하여 적용했다.<br />기능: 모델 전체의 전역적 맥락(Global Context)을 파악한다. CAD 모델 특유의 대칭성(Symmetry)이나 반복되는 구멍(Hole) 패턴의 특징을 매우 효과적으로 추출한다.</blockquote><br /><div><b>2. 입력 데이터 (Input Data) 예시<br /></b>백본 모델에 들어가는 입력값은 위상(Topology)이나 크기 정보가 없는 순수한 3D 좌표의 배열이다. 라이다(LiDAR) 스캐너나 가우시안 스플래팅 덩어리에서 추출된 표면 점 데이터가 이에 해당한다.<br /><br />형태: 실수 배열 (때로는 표면의 수직 방향을 나타내는 법선 벡터를 포함해 특징 배열로 입력됨).<br /><br /></div><div>예시 데이터 (기계 부품의 표면 점 10,000개 추출):<br /><br />[\n [0.12, 1.55, -0.42], \n [0.13, 1.55, -0.40],\n ...\n [5.00, 2.10, 1.11] // 총 10,000개의 [x, y, z] 배열\n]\n<br /><br /></div><div><b>3. 출력 데이터 (Output Data) 예시</b><br />백본에서 추출된 특징(Feature)은 여러 개의 서브 네트워크(Head)를 거쳐, 최종적으로 '분할 라벨(Segmentation Label)'과 수학적으로 정의된 '파라미터 수치(Parameter Vector)'로 나뉘어 출력된다.<br /><br />형태 1: 점 단위 분할 확률 (Point-wise Segmentation)<br /></div><blockquote style=\"border: none; margin: 0px 0px 0px 40px; padding: 0px; text-align: left;\"><div>각 점이 어떤 수학적 도형에 속하는지 분류한다.</div><div>예시: 점 $P_1$은 '원통(Cylinder)'일 확률 98%.</div></blockquote><div><br />형태 2: 도형 파라미터 회귀 (Primitive Parameters)<br /></div><blockquote style=\"border: none; margin: 0px 0px 0px 40px; padding: 0px; text-align: left;\"><div>분류된 덩어리에 대해 실수형 파라미터 방정식을 도출한다.</div><div>예시 A (평면 표면이 추출된 경우): Type: Plane</div><div>Normal_Vector (수직 벡터): [0.0, 0.0, 1.0]</div><div>Distance_from_Origin (원점 거리): 15.5</div></blockquote><div><br />예시 B (드릴로 뚫린 구멍이 추출된 경우):<br /></div><blockquote style=\"border: none; margin: 0px 0px 0px 40px; padding: 0px; text-align: left;\"><div>Type: Cylinder</div><div>Axis_Vector (중심축 방향): [0.0, 1.0, 0.0]</div><div>Center_Point (중심점): [10.0, 5.0, -2.0]</div><div>Radius (반지름 치수): 2.5 (이 수치가 설계 프로그램에서 조절 가능한 치수가 됨)</div></blockquote><div><br />이러한 출력값들이 모여서 최종적으로 솔리드웍스(SolidWorks)나 인벤터(Inventor)에서 읽을 수 있는 매크로 스크립트나 STEP 형식의 트리 구조로 조립된다.<br /><br /></div>",
        "contentSnippet": "ParSeNet이나 HPNet과 같은 역설계(Scan-to-CAD) 모델의 핵심 '눈(Eye)' 역할을 하는 백본(Backbone) 아키텍처는 주로 3D 점군(Point Cloud)의 기하학적 특징을 추출하는 딥러닝 네트워크로 구성된다.\n\n\n1. 백본 모델(Backbone Model)의 구조와 기능\n이러한 파이프라인의 백본은 2D 이미지의 픽셀을 처리하는 CNN(ResNet 등)과 달리, 순서가 없고 불규칙하게 흩어진 3차원 좌표의 집합을 처리해야 한다. 이를 위해 주로 다음과 같은 3가지 아키텍처가 백본으로 결합되어 사용된다.\nPointNet++ (가장 표준적인 백본)\n구조: 다층 퍼셉트론(MLP)을 각 점에 독립적으로 적용한 뒤, 최대 풀링(Max Pooling)을 통해 입력 순서에 구애받지 않는 대칭 함수(Symmetric Function)를 구성한다. 여기에 계층적 샘플링(Furthest Point Sampling)과 지역 군집화(Ball Query) 기법을 더해 지역적 기하학(Local Geometry)을 캡처한다.\n기능: 단순한 좌표들을 엮어, \"이 점 주변은 평평하다\", \"이 점은 날카로운 모서리에 있다\"는 정보를 담은 고차원 특징 벡터로 변환한다.\nDGCNN (Dynamic Graph CNN)\n구조: 점들 사이의 k-최근접 이웃(k-NN) 그래프를 구성하고, 네트워크 계층이 깊어질수록 특징 공간(Feature Space) 상에서 그래프의 연결을 동적으로 다시 계산하는 EdgeConv 연산을 수행한다.\n기능: 점과 점 사이의 '관계'를 학습하는 데 특화되어 있다. 곡률이 변하는 경계면(Boundary)이나 서로 맞닿아 있는 직교 평면의 특징을 뚜렷하게 잡아낸다.\nPoint Transformer (최신 SOTA 백본)\n구조: 자연어 처리에서 쓰이는 셀프 어텐션(Self-attention) 메커니즘을 3D 점군에 맞게 변형하여 적용했다.\n기능: 모델 전체의 전역적 맥락(Global Context)을 파악한다. CAD 모델 특유의 대칭성(Symmetry)이나 반복되는 구멍(Hole) 패턴의 특징을 매우 효과적으로 추출한다.\n\n2. 입력 데이터 (Input Data) 예시\n백본 모델에 들어가는 입력값은 위상(Topology)이나 크기 정보가 없는 순수한 3D 좌표의 배열이다. 라이다(LiDAR) 스캐너나 가우시안 스플래팅 덩어리에서 추출된 표면 점 데이터가 이에 해당한다.\n형태: 실수 배열 (때로는 표면의 수직 방향을 나타내는 법선 벡터를 포함해 특징 배열로 입력됨).\n\n예시 데이터 (기계 부품의 표면 점 10,000개 추출):\n[\n [0.12, 1.55, -0.42], \n [0.13, 1.55, -0.40],\n ...\n [5.00, 2.10, 1.11] // 총 10,000개의 [x, y, z] 배열\n]\n\n\n3. 출력 데이터 (Output Data) 예시\n백본에서 추출된 특징(Feature)은 여러 개의 서브 네트워크(Head)를 거쳐, 최종적으로 '분할 라벨(Segmentation Label)'과 수학적으로 정의된 '파라미터 수치(Parameter Vector)'로 나뉘어 출력된다.\n형태 1: 점 단위 분할 확률 (Point-wise Segmentation)\n\n각 점이 어떤 수학적 도형에 속하는지 분류한다.\n예시: 점 $P_1$은 '원통(Cylinder)'일 확률 98%.\n\n형태 2: 도형 파라미터 회귀 (Primitive Parameters)\n\n분류된 덩어리에 대해 실수형 파라미터 방정식을 도출한다.\n예시 A (평면 표면이 추출된 경우): Type: Plane\nNormal_Vector (수직 벡터): [0.0, 0.0, 1.0]\nDistance_from_Origin (원점 거리): 15.5\n\n예시 B (드릴로 뚫린 구멍이 추출된 경우):\n\nType: Cylinder\nAxis_Vector (중심축 방향): [0.0, 1.0, 0.0]\nCenter_Point (중심점): [10.0, 5.0, -2.0]\nRadius (반지름 치수): 2.5 (이 수치가 설계 프로그램에서 조절 가능한 치수가 됨)\n\n이러한 출력값들이 모여서 최종적으로 솔리드웍스(SolidWorks)나 인벤터(Inventor)에서 읽을 수 있는 매크로 스크립트나 STEP 형식의 트리 구조로 조립된다.",
        "id": "tag:blogger.com,1999:blog-5201956450461596914.post-7163984124488716879",
        "isoDate": "2026-02-20T10:11:00.000Z"
      },
      {
        "title": "월드랩과 오토데스크 협업을 통한 공간 AI 개발 동향",
        "link": "http://daddynkidsmakers.blogspot.com/2026/02/ai_20.html",
        "pubDate": "2026-02-20T09:06:00.000Z",
        "author": "Daddy Maker",
        "content": "<div style=\"text-align: left;\">이 글은&nbsp;월드랩과 오토데스크 협업을 통한 공간 AI 개발 동향을 조사한 글이다.<br /></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEgHqdy4fmI5H29460xcPdMtnmlED4gvfQgS-pSKNFDhrazqFH7vrHtedhj5g3d9QPlCHbLhs0VeyVmFQsypDWlp24ezE3CWU-8AjG5oM3kGeC0RlD8hyyEGuyuniLUL7kZ-fsV00H-cE_x6O-lNR3MYH6Hlsov16VFqUcKz2SZ7crvvX31ToANgYmhqHxGl\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"1209\" data-original-width=\"2490\" height=\"155\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEgHqdy4fmI5H29460xcPdMtnmlED4gvfQgS-pSKNFDhrazqFH7vrHtedhj5g3d9QPlCHbLhs0VeyVmFQsypDWlp24ezE3CWU-8AjG5oM3kGeC0RlD8hyyEGuyuniLUL7kZ-fsV00H-cE_x6O-lNR3MYH6Hlsov16VFqUcKz2SZ7crvvX31ToANgYmhqHxGl\" width=\"320\" /></a></div></div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEi3sMazDbR8uSir-DJCLpOnOOjsb_3mBsbpN6sg0p3RMJgoGmbV1TABzkWySInCK_vXzKGNsMSYS0tq77EV-Ryqul8FFvaNa7y5kx95XZQrIecaPxtWm46sO13BqSrNt1ss649dVjiMcXsz3RbXxEEdbZ4QRLA_2DyuUI6G4whHEu9YL4sDLpLsN_TBFv_r\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"1234\" data-original-width=\"2536\" height=\"156\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEi3sMazDbR8uSir-DJCLpOnOOjsb_3mBsbpN6sg0p3RMJgoGmbV1TABzkWySInCK_vXzKGNsMSYS0tq77EV-Ryqul8FFvaNa7y5kx95XZQrIecaPxtWm46sO13BqSrNt1ss649dVjiMcXsz3RbXxEEdbZ4QRLA_2DyuUI6G4whHEu9YL4sDLpLsN_TBFv_r\" width=\"320\" /></a></div></div><div style=\"text-align: left;\"><div _ngcontent-ng-c1427273380=\"\" aria-busy=\"false\" aria-live=\"polite\" class=\"markdown markdown-main-panel stronger enable-updated-hr-color preserve-whitespaces-in-response\" dir=\"ltr\" id=\"model-response-message-contentr_1589f2c64cadab00\" inline-copy-host=\"\" style=\"--animation-duration: 400ms; --fade-animation-function: linear; animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; color: #1f1f1f; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; font-family: &quot;Google Sans Text&quot;, sans-serif; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"></p><h3 data-path-to-node=\"2\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; font-family: &quot;Google Sans&quot;, sans-serif; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span style=\"font-size: small;\">오토데스크 마블(Autodesk Marble) 기술적 배경</span></h3><p data-path-to-node=\"3\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"28\" data-path-to-node=\"3\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">마블(Marble)은 오토데스크가 직접 개발한 제품이 아니다.</span> 이 모델은 'AI의 대모'라 불리는 페이페이 리(Fei-Fei Li) 교수가 설립한 AI 스타트업 월드랩스(World Labs)가 개발한 핵심 생성형 3D 월드 모델이다. 오토데스크는 2026년 2월, 월드랩스에 대규모 전략적 투자를 단행하며 자사 소프트웨어와의 통합 파트너십을 발표했다.</p><p data-path-to-node=\"4\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">마블의 구체적인 첫 코드 작성일이 공식적으로 공개되지는 않았으나, 회사의 설립과 주요 제품 마일스톤을 통해 개발 타임라인을 충분히 추론할 수 있다.</p><ul data-path-to-node=\"7\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding-inline-start: 32px; padding: 0px 0px 0px 27px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"7,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"7,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">초기 R&amp;D 및 시작 (2024년 1월):</span> 페이페이 리 교수를 비롯한 최고 수준의 AI 연구진들이 3D 환경 생성과 실시간 시뮬레이션을 목표로 2024년 1월에 월드랩스를 공동 창립했다. 마블의 근간이 되는 '공간 지능(Spatial Intelligence)' 연구와 코어 모델 개발은 이때부터 본격적으로 시작되었을 가능성이 높다.</p></li><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"7,1,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"7,1,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">프로토타입 및 베타 (2025년 9월):</span> 약 1년 8개월의 딥테크 연구 기간을 거쳐, 2025년 9월에 마블의 첫 번째 제한적 베타 버전이 세상에 공개되었다.</p></li><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"7,2,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"7,2,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">정식 출시 (2025년 11월):</span> 2025년 11월 12일, 텍스트, 이미지, 비디오 등을 입력받아 상호작용 가능한 3D 환경을 즉석에서 구축하는 마블 프론티어 모델이 일반 대중에게 정식으로 론칭되었다.</p></li></ul><h4 data-path-to-node=\"8\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; font-family: &quot;Google Sans&quot;, sans-serif; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">기술 스택</h4><p data-path-to-node=\"9\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">마블은 단순히 2D 이미지를 이어 붙이는 비디오 생성 AI가 아니라, 물리적 공간의 3차원 구조를 완벽히 이해하는 거대 월드 모델(LWM, Large World Models) 아키텍처를 채택하고 있다.</p><ul data-path-to-node=\"10\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding-inline-start: 32px; padding: 0px 0px 0px 27px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"10,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"10,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">3D 표현 포맷 (<a data-preview=\"\" href=\"https://www.google.com/search?ved=1t:260882&amp;q=3D+Gaussian+Splatting+images&amp;bbid=5201956450461596914&amp;bpid=4881800004379506588\" target=\"_blank\">3D Gaussian Splatting</a>):</span> 마블은 시점이 변하면 형태가 무너지는 기존 생성 모델들의 한계를 극복하고, 변형 없이 영구적으로 보존되는 3D 환경을 생성한다. 생성된 결과물은 3D 가우시안 스플랫(Gaussian Splats)이나 메쉬(Mesh) 형태로 다운로드하여 언리얼, 유니티 등 다른 게임 엔진으로 내보낼 수 있다.</p></li><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"10,1,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"10,1,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">실시간 프레임 모델 (RTFM, Real-Time Frame Model):</span> 2025년 10월에 도입된 핵심 렌더링 기술이다. 단일 GPU 환경에서도 실시간으로 월드를 생성하고 상호작용할 수 있도록, 기존 프레임들을 일종의 '공간 메모리'로 활용하여 높은 디테일을 유지한다.</p></li><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"10,2,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"10,2,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">웹 렌더링 엔진 (SparkJS.dev):</span> 별도의 무거운 클라이언트 없이 웹 브라우저 환경에서 매끄러운 3D 렌더링을 구현하기 위해 Three.js를 기반으로 한 독자적인 렌더러인 'SparkJS.dev'를 사용한다. 이는 가우시안 스플랫과 전통적인 WebGL 에셋(glTF 모델 등)을 한 화면에 자연스럽게 혼합해 준다.</p></li><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"10,3,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"10,3,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">공간 편집 도구 (Chisel):</span> 사용자가 직접 상자나 평면 같은 단순한 원시 도형(Primitive)으로 3D 뼈대를 잡으면, AI가 그 맥락을 파악해 그 위에 시각적 디테일과 텍스처를 입히는 하이브리드 3D 편집 도구를 지원한다.</p></li></ul><div><br /></div><div><p data-path-to-node=\"0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">기존의 스테이블 디퓨전 기반 3D 생성이 단일 '객체(Object)'를 깎아내는 데 집중했다면, 월드랩스의 '마블(Marble)'은 단일 이미지나 텍스트에서&nbsp;<span data-index-in-node=\"88\" data-path-to-node=\"8\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">거대한 3D 가상 세계(World) 전체를 생성해 내는 기술</span>입이다. 이를 오토데스크의 기존 생태계와 결합하는 것이 핵심이다.</p><h4 data-path-to-node=\"9\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; font-family: &quot;Google Sans&quot;, sans-serif; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span style=\"font-weight: normal;\">A. 백본 모델 (Backbone Models)</span></h4><ul data-path-to-node=\"10\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding-inline-start: 32px; padding: 0px 0px 0px 27px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"10,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"10,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">Large World Models (LWM) / 공간 지능(Spatial Intelligence):</span>&nbsp;단순 2D 픽셀의 패턴을 모방하는 것을 넘어, 3D 공간의 기하학(Geometry), 재질, 빛의 반사, 물리 법칙을 스스로 추론하는 거대 세계 모델을 백본으로 사용한다.</p></li><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"10,1,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"10,1,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><a data-preview=\"\" href=\"https://www.google.com/search?ved=1t:260882&amp;q=define+Neural+Radiance+Fields+NeRF&amp;bbid=5201956450461596914&amp;bpid=4881800004379506588\" target=\"_blank\">NeRF</a> 및 차세대 뉴럴 렌더링:</span>&nbsp;월드랩스의 핵심 개발진(NeRF의 창시자인 벤 밀든홀 등)의 기술적 배경을 고려할 때, 마블의 코어 엔진에는 고도화된 Neural Radiance Fields(NeRF) 기반 기술이나 가우시안 스플래팅 개념이 결합되어 시점 변화에 완벽히 대응하는 일관된 3D 씬을 연산한다.</p></li></ul><h4 data-path-to-node=\"11\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; font-family: &quot;Google Sans&quot;, sans-serif; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span style=\"font-weight: normal;\">B. 학습 데이터 종류 (Training Data)</span></h4><ul data-path-to-node=\"12\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding-inline-start: 32px; padding: 0px 0px 0px 27px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"12,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">일반적인 2D 이미지 쌍을 넘어서,&nbsp;<span data-index-in-node=\"20\" data-path-to-node=\"12,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">3D 레이아웃, 공간 깊이(Depth) 데이터, 카메라 트래킹(Pose)이 포함된 다중 시점 영상</span>, 그리고 오토데스크가 강점을 가진&nbsp;<span data-index-in-node=\"94\" data-path-to-node=\"12,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">기하학적/물리적 CAD 시뮬레이션 데이터</span>&nbsp;등 공간을 이해하기 위한 복합적인 고차원 데이터로 학습된다.</p></li></ul><h4 data-path-to-node=\"13\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; font-family: &quot;Google Sans&quot;, sans-serif; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span style=\"font-weight: normal;\">C. 오토데스크와의 통합 파이프라인 (Integration Workflow)</span></h4><ul data-path-to-node=\"14\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding-inline-start: 32px; padding: 0px 0px 0px 27px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"14,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"14,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">편집 가능한 3D 씬 (Editable 3D Environments):</span>&nbsp;마블은 단순한 비디오 영상(예: OpenAI Sora)을 생성하는 것이 아니라, 구조화되고 상호작용 가능한 3D 환경 자체를 출력한다.</p></li><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"14,1,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"14,1,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">라스트 마일 편집(Last-mile Editing) 생태계:</span>&nbsp;마블이 프롬프트로 전체 공간의 초안을 순식간에 생성하면, 이를 오토데스크의&nbsp;<span data-index-in-node=\"76\" data-path-to-node=\"14,1,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">Maya, 3ds Max, Revit</span>&nbsp;같은 전통적인 소프트웨어로 바로 넘길 수 있다. 여기서 아티스트나 엔지니어가 직접 폴리곤 토폴로지, 리깅, 정밀한 재질 수정을 거쳐 최종 결과물(M&amp;E 및 AEC 분야)을 완성하게 된다.</p></li></ul><div><br /></div></div><h4 data-path-to-node=\"11\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; font-family: &quot;Google Sans&quot;, sans-serif; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">유사한 오픈소스 3D/월드 생성 모델</h4><p data-path-to-node=\"12\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">마블과 같은 강력한 상용 월드 모델에 대항하여, 연구자들과 개발자들이 투명하게 활용할 수 있는 오픈소스 생태계의 3D 생성 기술들도 빠르게 발전하고 있다.</p><ul data-path-to-node=\"13\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding-inline-start: 32px; padding: 0px 0px 0px 27px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"13,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"13,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">DiamondWM:</span> 구글의 'Genie'나 마블과 유사한 성격을 지닌 대표적인 오픈소스 월드 모델이다. 대량의 FPS 게임 플레이 영상을 시각적으로 학습하여 개발되었으며, 사용자의 로컬 데스크톱 GPU에서도 직접 구동하며 실시간으로 상호작용할 수 있는 점이 특징이다.</p></li><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"13,1,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"13,1,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><a data-preview=\"\" href=\"https://www.google.com/search?ved=1t:260882&amp;q=NVIDIA+Isaac+Sim+robotics+simulation&amp;bbid=5201956450461596914&amp;bpid=4881800004379506588\" target=\"_blank\">NVIDIA Isaac Sim</a> (로보틱스 및 시뮬레이션):</span> 프롬프트 한 줄로 세상 전체를 즉석에서 그려내는 마법 같은 생성형 AI는 아니지만, 오픈소스 기반의 확장 가능한 레퍼런스 프레임워크 역할을 한다. 주로 AI 로봇 모델 훈련을 위한 합성 데이터를 대량으로 생성하고, 물리 법칙이 적용된 가상 환경을 정밀하게 시뮬레이션하는 데 핵심적으로 쓰인다.</p></li><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"13,2,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"13,2,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">Tencent Hunyuan 3D 시리즈:</span> 텍스트나 단일 이미지를 고품질 3D 에셋으로 변환하는 오픈 웨이트 기반의 생성 모델이다. 2025년 1월 버전 2.0 출시에 이어 최신 3.0 버전은 복잡한 건축물 생성 등에 폭넓게 활용되며 3D 아티스트들의 모델링 시간을 크게 단축시키고 있다.</p></li></ul><p data-path-to-node=\"14\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">아울러, 다음과 같은 백본 기술을 살펴볼 필요가 있다.</p><div _ngcontent-ng-c1427273380=\"\" aria-busy=\"false\" aria-live=\"polite\" class=\"markdown markdown-main-panel stronger enable-updated-hr-color preserve-whitespaces-in-response\" dir=\"ltr\" id=\"model-response-message-contentr_ea8e6c4e64240411\" inline-copy-host=\"\" style=\"--animation-duration: 400ms; --fade-animation-function: linear; animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"18\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"18\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">1. 가장 빠르고 완벽한 Image-to-3D Mesh</span></p><ul data-path-to-node=\"19\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding-inline-start: 32px; padding: 0px 0px 0px 27px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"19,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"19,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><a data-preview=\"\" href=\"https://www.google.com/search?ved=1t:260882&amp;q=Stable+Fast+3D+Stability+AI&amp;bbid=5201956450461596914&amp;bpid=4881800004379506588\" target=\"_blank\">Stable Fast 3D</a> (Stability AI):</span> 이미지를 넣으면 0.5초 만에 완벽한 텍스처와 UV 매핑이 완료된 3D 메시를 뽑아내는 오픈소스 모델이다.</p><ul data-path-to-node=\"19,0,1\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding-inline-start: 32px; padding: 0px 0px 0px 27px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"19,0,1,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><response-element ng-version=\"0.0.0-PLACEHOLDER\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><link-block _nghost-ng-c3052017089=\"\" class=\"ng-star-inserted\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><a _ngcontent-ng-c3052017089=\"\" _nghost-ng-c1563836749=\"\" class=\"ng-star-inserted\" data-hveid=\"0\" data-ved=\"0CAAQ_4QMahgKEwjZ46Dz3-aSAxUAAAAAHQAAAAAQwgs\" decode-data-ved=\"1\" externallink=\"\" href=\"https://github.com/Stability-AI/stable-fast-3d\" jslog=\"197247;track:generic_click,impression,attention;BardVeMetadataKey:[[&quot;r_ea8e6c4e64240411&quot;,&quot;c_85daeb557045aacb&quot;,null,&quot;rc_663b3dbc623fd94e&quot;,null,null,&quot;ko&quot;,null,1,null,null,1,0]]\" rel=\"noopener\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(11, 87, 208); clear: none; clip: auto; color: #0b57d0; columns: auto; contain: none; container: none; content: normal; cursor: pointer; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(11, 87, 208) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\" target=\"_blank\">Stability-AI/stable-fast-3d</a><!----></link-block><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></response-element></p></li></ul></li></ul><p data-path-to-node=\"20\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"20\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">2. 3D 가우시안 스플래팅 + 스테이블 디퓨전(생성형 AI)의 융합</span></p><ul data-path-to-node=\"21\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding-inline-start: 32px; padding: 0px 0px 0px 27px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"21,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"21,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">DreamGaussian:</span> 스테이블 디퓨전의 상상력과 3DGS를 결합해, 이미지를 먼저 가우시안으로 빠르게 만든 뒤 실질적으로 활용 가능한 Mesh로 변환하는 선구적인 프로젝트이다.</p><ul data-path-to-node=\"21,0,1\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding-inline-start: 32px; padding: 0px 0px 0px 27px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"21,0,1,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><response-element ng-version=\"0.0.0-PLACEHOLDER\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><link-block _nghost-ng-c3052017089=\"\" class=\"ng-star-inserted\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><a _ngcontent-ng-c3052017089=\"\" _nghost-ng-c1563836749=\"\" class=\"ng-star-inserted\" data-hveid=\"0\" data-ved=\"0CAAQ_4QMahgKEwjZ46Dz3-aSAxUAAAAAHQAAAAAQwws\" decode-data-ved=\"1\" externallink=\"\" href=\"https://github.com/dreamgaussian/dreamgaussian\" jslog=\"197247;track:generic_click,impression,attention;BardVeMetadataKey:[[&quot;r_ea8e6c4e64240411&quot;,&quot;c_85daeb557045aacb&quot;,null,&quot;rc_663b3dbc623fd94e&quot;,null,null,&quot;ko&quot;,null,1,null,null,1,0]]\" rel=\"noopener\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(11, 87, 208); clear: none; clip: auto; color: #0b57d0; columns: auto; contain: none; container: none; content: normal; cursor: pointer; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(11, 87, 208) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\" target=\"_blank\">dreamgaussian/dreamgaussian</a><!----></link-block><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></response-element></p></li></ul></li><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"21,1,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"21,1,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">Threestudio:</span> 네르프(NeRF), 3DGS, 스테이블 디퓨전을 이용한 3D 생성 연구를 한곳에 모아둔 텍스트-to-3D 통합 프레임워크이다.</p></li><ul data-path-to-node=\"21,1,1\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding-inline-start: 32px; padding: 0px 0px 0px 27px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"21,1,1,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><response-element ng-version=\"0.0.0-PLACEHOLDER\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><link-block _nghost-ng-c3052017089=\"\" class=\"ng-star-inserted\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><a _ngcontent-ng-c3052017089=\"\" _nghost-ng-c1563836749=\"\" class=\"ng-star-inserted\" data-hveid=\"0\" data-ved=\"0CAAQ_4QMahgKEwjZ46Dz3-aSAxUAAAAAHQAAAAAQxAs\" decode-data-ved=\"1\" externallink=\"\" href=\"https://github.com/threestudio-project/threestudio\" jslog=\"197247;track:generic_click,impression,attention;BardVeMetadataKey:[[&quot;r_ea8e6c4e64240411&quot;,&quot;c_85daeb557045aacb&quot;,null,&quot;rc_663b3dbc623fd94e&quot;,null,null,&quot;ko&quot;,null,1,null,null,1,0]]\" rel=\"noopener\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(11, 87, 208); clear: none; clip: auto; color: #0b57d0; columns: auto; contain: none; container: none; content: normal; cursor: pointer; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(11, 87, 208) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\" target=\"_blank\">threestudio-project/threestudio</a><!----></link-block><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></response-element></p></li></ul></ul><div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEhWTyG1mQE-AmueVG8ap-gOMLUKXs-8obuTn18UR8jVouOvUrWGNlPf3GhHnCdyr6Wkcq8ZxreqCjUnHgB2d3KdOiZz_buGlSaAr-8Gi7E5iwyJEPq5Wf3Kbj72E3mZgVUcwvK0E4G1ZZY-jXN0Pe_3MBvP10IcEPUdfyp8yXoBx_9xvA_a4jbkk-Xgbkq_\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"619\" data-original-width=\"1194\" height=\"166\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEhWTyG1mQE-AmueVG8ap-gOMLUKXs-8obuTn18UR8jVouOvUrWGNlPf3GhHnCdyr6Wkcq8ZxreqCjUnHgB2d3KdOiZz_buGlSaAr-8Gi7E5iwyJEPq5Wf3Kbj72E3mZgVUcwvK0E4G1ZZY-jXN0Pe_3MBvP10IcEPUdfyp8yXoBx_9xvA_a4jbkk-Xgbkq_\" width=\"320\" /></a></div><br /></div><p data-path-to-node=\"22\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"22\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">3. 원본 렌더링 기술</span></p><ul data-path-to-node=\"23\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding-inline-start: 32px; padding: 0px 0px 0px 27px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"23,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><span data-index-in-node=\"0\" data-path-to-node=\"23,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">3D Gaussian Splatting (Inria):</span> 실시간 렌더링 혁명을 일으킨 오리지널 소스코드이다.</p><ul data-path-to-node=\"23,0,1\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding-inline-start: 32px; padding: 0px 0px 0px 27px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><li style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><p data-path-to-node=\"23,0,1,0,0\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 8px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 8px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px 0px 0px 4px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><response-element ng-version=\"0.0.0-PLACEHOLDER\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><link-block _nghost-ng-c3052017089=\"\" class=\"ng-star-inserted\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\"><a _ngcontent-ng-c3052017089=\"\" _nghost-ng-c1563836749=\"\" class=\"ng-star-inserted\" data-hveid=\"0\" data-ved=\"0CAAQ_4QMahgKEwjZ46Dz3-aSAxUAAAAAHQAAAAAQxQs\" decode-data-ved=\"1\" externallink=\"\" href=\"https://github.com/graphdeco-inria/gaussian-splatting\" jslog=\"197247;track:generic_click,impression,attention;BardVeMetadataKey:[[&quot;r_ea8e6c4e64240411&quot;,&quot;c_85daeb557045aacb&quot;,null,&quot;rc_663b3dbc623fd94e&quot;,null,null,&quot;ko&quot;,null,1,null,null,1,0]]\" rel=\"noopener\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(11, 87, 208); clear: none; clip: auto; color: #0b57d0; columns: auto; contain: none; container: none; content: normal; cursor: pointer; cx: 0px; cy: 0px; d: none; direction: ltr; display: inline; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 0px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(11, 87, 208) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\" target=\"_blank\">graphdeco-inria/gaussian-splatting</a><!----></link-block><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></response-element></p></li></ul></li></ul><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEheEFpXwrAG3OaGMSrtwYEMXJfZzbiK-4yBpUl-PZfcuJA6BwC2eeE31Ug6GmpAR4CVc2fRphvznYY7j5VEjWBGBrXkcTvi2jEGSVRXAcmpqaG0ZsMEM3jhqHOtymq7kip6_DQTa-yuuZ41hFRkzUYJcZao7zEYC1dNlEgfvAxSYLQDVh1Eg9wr-V-gfdHd\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"\" data-original-height=\"340\" data-original-width=\"1485\" height=\"91\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEheEFpXwrAG3OaGMSrtwYEMXJfZzbiK-4yBpUl-PZfcuJA6BwC2eeE31Ug6GmpAR4CVc2fRphvznYY7j5VEjWBGBrXkcTvi2jEGSVRXAcmpqaG0ZsMEM3jhqHOtymq7kip6_DQTa-yuuZ41hFRkzUYJcZao7zEYC1dNlEgfvAxSYLQDVh1Eg9wr-V-gfdHd=w400-h91\" width=\"400\" /></a></div><br /><p data-path-to-node=\"25\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">최근 발표된 월드랩스의 마블과 오토데스크의 만남은 기존의 3D 제작 파이프라인(기획, 모델링, 렌더링)을 'AI 초안 생성, 디테일 모델 수정'이라는 차원으로 바꿔놓고 있다.</p><p data-path-to-node=\"26\" style=\"animation: auto ease 0s 1 normal none running none; appearance: none; background-attachment: scroll; background-clip: border-box; background-image: none; background-origin: padding-box; background-position: 0% 0%; background-repeat: repeat; background-size: auto; border: 0px none rgb(31, 31, 31); clear: none; clip: auto; columns: auto; contain: none; container: none; content: normal; cursor: auto; cx: 0px; cy: 0px; d: none; direction: ltr; fill: rgb(0, 0, 0); filter: none; flex: 0 1 auto; float: none; gap: normal; hyphens: manual; inset: auto; interactivity: auto; isolation: auto; line-height: 1.15; margin-bottom: 16px; margin-left: 0px; margin-right: 0px; margin-top: 0px !important; margin: 0px 0px 16px; marker: none; mask-clip: border-box; mask-composite: add; mask-image: none; mask-mode: match-source; mask-origin: border-box; mask-repeat: repeat; mask-size: auto; mask: none; offset: normal; opacity: 1; order: 0; outline: rgb(31, 31, 31) none 0px; overlay: none; padding: 0px; page: auto; perspective: none; position: static; quotes: auto; r: 0px; resize: none; rotate: none; rx: auto; ry: auto; scale: none; speak: normal; stroke: none; transform: none; transition: all; translate: none; visibility: visible; x: 0px; y: 0px; zoom: 1;\">결론적으로, 오토데스크가 왜 월드랩스에 그토록 막대한 자본을 투자했는지 그 전략적 배경은 명확하다. 수십 시간에 달하던 기존 CAD 및 3D 그래픽 설계자들의 수작업을 마블의 압도적인 '공간 지능'이 획기적으로 대체하고 보조할 수 있기 때문이다.</p></div></div></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b>레퍼런스</b></div><div style=\"text-align: left;\"><ul style=\"text-align: left;\"><li><a href=\"https://techcrunch.com/2026/02/18/world-labs-lands-200m-from-autodesk-to-bring-world-models-into-3d-workflows/\">World Labs lands $1B, with $200M from Autodesk, to bring world models into 3D workflows | TechCrunch</a></li><li><a href=\"https://techcrunch.com/2025/11/12/fei-fei-lis-world-labs-speeds-up-the-world-model-race-with-marble-its-first-commercial-product/\">Fei-Fei Li's World Labs speeds up the world model race with Marble, its first commercial product | TechCrunch</a></li><li><a href=\"https://marble.worldlabs.ai/projects\">Marble</a></li><li><a href=\"https://aecmag.com/ai/autodesks-200m-bet-on-spatial-ai/\">Autodesk's $200m bet on spatial AI - AEC Magazine</a></li><li><a href=\"https://biz.chosun.com/en/en-it/2026/02/19/EINCWCPG3NENVEFHNCX4WXXELY/\">Fei-Fei Li’s World Labs raises $1 billion to advance physical AI world model - CHOSUNBIZ</a></li><li><a href=\"https://develop3d.com/ai/autodesk-world-labs-ai-investment-200-million/\">Autodesk bets $200 million on World Labs' AI - DEVELOP3D</a></li></ul><div><br /></div></div>",
        "contentSnippet": "이 글은 월드랩과 오토데스크 협업을 통한 공간 AI 개발 동향을 조사한 글이다.\n\n\n\n\n\n\n\n\n\n오토데스크 마블(Autodesk Marble) 기술적 배경\n마블(Marble)은 오토데스크가 직접 개발한 제품이 아니다. 이 모델은 'AI의 대모'라 불리는 페이페이 리(Fei-Fei Li) 교수가 설립한 AI 스타트업 월드랩스(World Labs)가 개발한 핵심 생성형 3D 월드 모델이다. 오토데스크는 2026년 2월, 월드랩스에 대규모 전략적 투자를 단행하며 자사 소프트웨어와의 통합 파트너십을 발표했다.\n마블의 구체적인 첫 코드 작성일이 공식적으로 공개되지는 않았으나, 회사의 설립과 주요 제품 마일스톤을 통해 개발 타임라인을 충분히 추론할 수 있다.\n\n초기 R&D 및 시작 (2024년 1월): 페이페이 리 교수를 비롯한 최고 수준의 AI 연구진들이 3D 환경 생성과 실시간 시뮬레이션을 목표로 2024년 1월에 월드랩스를 공동 창립했다. 마블의 근간이 되는 '공간 지능(Spatial Intelligence)' 연구와 코어 모델 개발은 이때부터 본격적으로 시작되었을 가능성이 높다.\n\n프로토타입 및 베타 (2025년 9월): 약 1년 8개월의 딥테크 연구 기간을 거쳐, 2025년 9월에 마블의 첫 번째 제한적 베타 버전이 세상에 공개되었다.\n\n정식 출시 (2025년 11월): 2025년 11월 12일, 텍스트, 이미지, 비디오 등을 입력받아 상호작용 가능한 3D 환경을 즉석에서 구축하는 마블 프론티어 모델이 일반 대중에게 정식으로 론칭되었다.\n\n기술 스택\n마블은 단순히 2D 이미지를 이어 붙이는 비디오 생성 AI가 아니라, 물리적 공간의 3차원 구조를 완벽히 이해하는 거대 월드 모델(LWM, Large World Models) 아키텍처를 채택하고 있다.\n\n3D 표현 포맷 (3D Gaussian Splatting): 마블은 시점이 변하면 형태가 무너지는 기존 생성 모델들의 한계를 극복하고, 변형 없이 영구적으로 보존되는 3D 환경을 생성한다. 생성된 결과물은 3D 가우시안 스플랫(Gaussian Splats)이나 메쉬(Mesh) 형태로 다운로드하여 언리얼, 유니티 등 다른 게임 엔진으로 내보낼 수 있다.\n\n실시간 프레임 모델 (RTFM, Real-Time Frame Model): 2025년 10월에 도입된 핵심 렌더링 기술이다. 단일 GPU 환경에서도 실시간으로 월드를 생성하고 상호작용할 수 있도록, 기존 프레임들을 일종의 '공간 메모리'로 활용하여 높은 디테일을 유지한다.\n\n웹 렌더링 엔진 (SparkJS.dev): 별도의 무거운 클라이언트 없이 웹 브라우저 환경에서 매끄러운 3D 렌더링을 구현하기 위해 Three.js를 기반으로 한 독자적인 렌더러인 'SparkJS.dev'를 사용한다. 이는 가우시안 스플랫과 전통적인 WebGL 에셋(glTF 모델 등)을 한 화면에 자연스럽게 혼합해 준다.\n\n공간 편집 도구 (Chisel): 사용자가 직접 상자나 평면 같은 단순한 원시 도형(Primitive)으로 3D 뼈대를 잡으면, AI가 그 맥락을 파악해 그 위에 시각적 디테일과 텍스처를 입히는 하이브리드 3D 편집 도구를 지원한다.\n\n\n\n기존의 스테이블 디퓨전 기반 3D 생성이 단일 '객체(Object)'를 깎아내는 데 집중했다면, 월드랩스의 '마블(Marble)'은 단일 이미지나 텍스트에서 거대한 3D 가상 세계(World) 전체를 생성해 내는 기술입이다. 이를 오토데스크의 기존 생태계와 결합하는 것이 핵심이다.\nA. 백본 모델 (Backbone Models)\n\nLarge World Models (LWM) / 공간 지능(Spatial Intelligence): 단순 2D 픽셀의 패턴을 모방하는 것을 넘어, 3D 공간의 기하학(Geometry), 재질, 빛의 반사, 물리 법칙을 스스로 추론하는 거대 세계 모델을 백본으로 사용한다.\n\nNeRF 및 차세대 뉴럴 렌더링: 월드랩스의 핵심 개발진(NeRF의 창시자인 벤 밀든홀 등)의 기술적 배경을 고려할 때, 마블의 코어 엔진에는 고도화된 Neural Radiance Fields(NeRF) 기반 기술이나 가우시안 스플래팅 개념이 결합되어 시점 변화에 완벽히 대응하는 일관된 3D 씬을 연산한다.\n\nB. 학습 데이터 종류 (Training Data)\n\n일반적인 2D 이미지 쌍을 넘어서, 3D 레이아웃, 공간 깊이(Depth) 데이터, 카메라 트래킹(Pose)이 포함된 다중 시점 영상, 그리고 오토데스크가 강점을 가진 기하학적/물리적 CAD 시뮬레이션 데이터 등 공간을 이해하기 위한 복합적인 고차원 데이터로 학습된다.\n\nC. 오토데스크와의 통합 파이프라인 (Integration Workflow)\n\n편집 가능한 3D 씬 (Editable 3D Environments): 마블은 단순한 비디오 영상(예: OpenAI Sora)을 생성하는 것이 아니라, 구조화되고 상호작용 가능한 3D 환경 자체를 출력한다.\n\n라스트 마일 편집(Last-mile Editing) 생태계: 마블이 프롬프트로 전체 공간의 초안을 순식간에 생성하면, 이를 오토데스크의 Maya, 3ds Max, Revit 같은 전통적인 소프트웨어로 바로 넘길 수 있다. 여기서 아티스트나 엔지니어가 직접 폴리곤 토폴로지, 리깅, 정밀한 재질 수정을 거쳐 최종 결과물(M&E 및 AEC 분야)을 완성하게 된다.\n\n\n\n유사한 오픈소스 3D/월드 생성 모델\n마블과 같은 강력한 상용 월드 모델에 대항하여, 연구자들과 개발자들이 투명하게 활용할 수 있는 오픈소스 생태계의 3D 생성 기술들도 빠르게 발전하고 있다.\n\nDiamondWM: 구글의 'Genie'나 마블과 유사한 성격을 지닌 대표적인 오픈소스 월드 모델이다. 대량의 FPS 게임 플레이 영상을 시각적으로 학습하여 개발되었으며, 사용자의 로컬 데스크톱 GPU에서도 직접 구동하며 실시간으로 상호작용할 수 있는 점이 특징이다.\n\nNVIDIA Isaac Sim (로보틱스 및 시뮬레이션): 프롬프트 한 줄로 세상 전체를 즉석에서 그려내는 마법 같은 생성형 AI는 아니지만, 오픈소스 기반의 확장 가능한 레퍼런스 프레임워크 역할을 한다. 주로 AI 로봇 모델 훈련을 위한 합성 데이터를 대량으로 생성하고, 물리 법칙이 적용된 가상 환경을 정밀하게 시뮬레이션하는 데 핵심적으로 쓰인다.\n\nTencent Hunyuan 3D 시리즈: 텍스트나 단일 이미지를 고품질 3D 에셋으로 변환하는 오픈 웨이트 기반의 생성 모델이다. 2025년 1월 버전 2.0 출시에 이어 최신 3.0 버전은 복잡한 건축물 생성 등에 폭넓게 활용되며 3D 아티스트들의 모델링 시간을 크게 단축시키고 있다.\n\n아울러, 다음과 같은 백본 기술을 살펴볼 필요가 있다.\n\n1. 가장 빠르고 완벽한 Image-to-3D Mesh\n\nStable Fast 3D (Stability AI): 이미지를 넣으면 0.5초 만에 완벽한 텍스처와 UV 매핑이 완료된 3D 메시를 뽑아내는 오픈소스 모델이다.\n\n\nStability-AI/stable-fast-3d\n\n\n\n2. 3D 가우시안 스플래팅 + 스테이블 디퓨전(생성형 AI)의 융합\n\nDreamGaussian: 스테이블 디퓨전의 상상력과 3DGS를 결합해, 이미지를 먼저 가우시안으로 빠르게 만든 뒤 실질적으로 활용 가능한 Mesh로 변환하는 선구적인 프로젝트이다.\n\n\ndreamgaussian/dreamgaussian\n\n\n\nThreestudio: 네르프(NeRF), 3DGS, 스테이블 디퓨전을 이용한 3D 생성 연구를 한곳에 모아둔 텍스트-to-3D 통합 프레임워크이다.\n\n\n\nthreestudio-project/threestudio\n\n\n\n\n\n3. 원본 렌더링 기술\n\n3D Gaussian Splatting (Inria): 실시간 렌더링 혁명을 일으킨 오리지널 소스코드이다.\n\n\ngraphdeco-inria/gaussian-splatting\n\n\n\n\n\n최근 발표된 월드랩스의 마블과 오토데스크의 만남은 기존의 3D 제작 파이프라인(기획, 모델링, 렌더링)을 'AI 초안 생성, 디테일 모델 수정'이라는 차원으로 바꿔놓고 있다.\n결론적으로, 오토데스크가 왜 월드랩스에 그토록 막대한 자본을 투자했는지 그 전략적 배경은 명확하다. 수십 시간에 달하던 기존 CAD 및 3D 그래픽 설계자들의 수작업을 마블의 압도적인 '공간 지능'이 획기적으로 대체하고 보조할 수 있기 때문이다.\n\n\n\n레퍼런스\n\nWorld Labs lands $1B, with $200M from Autodesk, to bring world models into 3D workflows | TechCrunch\nFei-Fei Li's World Labs speeds up the world model race with Marble, its first commercial product | TechCrunch\nMarble\nAutodesk's $200m bet on spatial AI - AEC Magazine\nFei-Fei Li’s World Labs raises $1 billion to advance physical AI world model - CHOSUNBIZ\nAutodesk bets $200 million on World Labs' AI - DEVELOP3D",
        "id": "tag:blogger.com,1999:blog-5201956450461596914.post-4881800004379506588",
        "isoDate": "2026-02-20T09:06:00.000Z"
      }
    ]
  },
  {
    "name": "권용진",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "RSS feed for boyism Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성준의 린스타트업과 디자인씽킹",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권혁우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김준형",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강동혁",
    "category": "개인",
    "posts": []
  },
  {
    "name": "고종범",
    "category": "개인",
    "posts": []
  },
  {
    "name": "cheese10yun",
    "category": "개인",
    "posts": []
  },
  {
    "name": "구자철",
    "category": "개인",
    "posts": []
  },
  {
    "name": "FSS",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권동준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김용일",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김도균",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김민석",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권윤학",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강성훈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김만수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "엘키",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김슬기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김광현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성빈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강명훈",
    "category": "개인",
    "posts": [
      {
        "title": "SQL 대시보드 shaper",
        "link": "https://kangmyounghun.blogspot.com/2026/02/sql-shaper.html",
        "pubDate": "2026-02-21T13:15:00.002Z",
        "author": "강명훈",
        "content": "<div>SQL을 입력하면 차트를 그려주는 데이터 시각화 툴 <a href=\"https://taleshape.com/shaper/docs/\" target=\"_blank\">shaper</a>.</div>\n<div><pre><code><div>C:\\Users\\Administrator&gt;pip install shaper-bin</div><div>Collecting shaper-bin</div><div>&nbsp; Downloading shaper_bin-0.14.0-py3-none-any.whl.metadata (2.3 kB)</div><div>Requirement already satisfied: requests&gt;=2.31.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from shaper-bin) (2.32.5)</div><div>Requirement already satisfied: charset_normalizer&lt;4,&gt;=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests&gt;=2.31.0-&gt;shaper-bin) (3.4.4)</div><div>Requirement already satisfied: idna&lt;4,&gt;=2.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests&gt;=2.31.0-&gt;shaper-bin) (3.11)</div><div>Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests&gt;=2.31.0-&gt;shaper-bin) (1.26.20)</div><div>Requirement already satisfied: certifi&gt;=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests&gt;=2.31.0-&gt;shaper-bin) (2025.11.12)</div><div>Downloading shaper_bin-0.14.0-py3-none-any.whl (12 kB)</div><div>Installing collected packages: shaper-bin</div><div>Successfully installed shaper-bin-0.14.0</div><div><br /></div><div><span><a name='more'></a></span>[notice] A new release of pip is available: 25.3 -&gt; 26.0.1</div><div>[notice] To update, run: python.exe -m pip install --upgrade pip</div><div><br /></div><div>C:\\Users\\Administrator&gt;shaper</div><div>Fetching Shaper binary...</div><div>Installation failed: Unsupported platform: windows-x86_64</div></code></pre></div>\n<div><br /></div><div>윈도우는 아직 지원하지 않아서 WSL 우분투에 설치.</div>\n<div><pre><code class=\"java\"><div>root@easyDATA:~# pip install shaper-bin</div><div>error: externally-managed-environment</div><div><br /></div><div>× This environment is externally managed</div><div>╰─&gt; To install Python packages system-wide, try apt install</div><div>&nbsp; &nbsp; python3-xyz, where xyz is the package you are trying to</div><div>&nbsp; &nbsp; install.</div><div><br /></div><div>&nbsp; &nbsp; If you wish to install a non-Debian-packaged Python package,</div><div>&nbsp; &nbsp; create a virtual environment using python3 -m venv path/to/venv.</div><div>&nbsp; &nbsp; Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make</div><div>&nbsp; &nbsp; sure you have python3-full installed.</div><div><br /></div><div>&nbsp; &nbsp; If you wish to install a non-Debian packaged Python application,</div><div>&nbsp; &nbsp; it may be easiest to use pipx install xyz, which will manage a</div><div>&nbsp; &nbsp; virtual environment for you. Make sure you have pipx installed.</div><div><br /></div><div>&nbsp; &nbsp; See /usr/share/doc/python3.12/README.venv for more information.</div><div><br /></div><div>note: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.</div><div>hint: See PEP 668 for the detailed specification.</div></code></pre></div>\n<div><br /></div>\n<div>시스템 안전성을 위해 가상환경을 추천하는 듯. 그냥 강제 설치.</div>\n<div><pre><code class=\"java\"><div>root@easyDATA:~# pip install shaper-bin --break-system-packages</div><div>Collecting shaper-bin</div><div>&nbsp; Downloading shaper_bin-0.14.0-py3-none-any.whl.metadata (2.3 kB)</div><div>Requirement already satisfied: requests&gt;=2.31.0 in /usr/lib/python3/dist-packages (from shaper-bin) (2.31.0)</div><div>Downloading shaper_bin-0.14.0-py3-none-any.whl (12 kB)</div><div>Installing collected packages: shaper-bin</div><div>Successfully installed shaper-bin-0.14.0</div><div>WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv</div></code></pre></div>\n<div><br /></div><div>실행하면 localhost:5454 포트로 접속할 수 있는 웹 UI를 제공한다.</div>\n<div><pre><code class=\"java\"><div>root@easyDATA:~# shaper</div><div>Fetching Shaper binary...</div><div>Loading checksums...</div><div>Downloading shaper v0.14.0 for linux-x86_64...</div><div>Verifying checksum...</div><div>Checksum verified successfully</div><div>Installation complete!</div><div>time=2026-02-20T15:11:08.349+09:00 level=INFO msg=\"Starting Shaper\" version=0.14.0</div><div>time=2026-02-20T15:11:08.349+09:00 level=INFO msg=\"For configuration options see --help or visit https://taleshape.com/shaper/docs for more\"</div><div>time=2026-02-20T15:11:08.349+09:00 level=INFO msg=\"Created data directory\" path=/root/.shaper</div><div>time=2026-02-20T15:11:08.349+09:00 level=INFO msg=\"SQLite opened\" file=/root/.shaper/shaper_internal.sqlite</div><div>time=2026-02-20T15:11:08.361+09:00 level=INFO msg=\"DuckDB opened\" file=/root/.shaper/shaper.duckdb</div><div>time=2026-02-20T15:11:08.361+09:00 level=INFO msg=\"Loading init-sql-file\" path=/root/.shaper/init.sql</div><div>time=2026-02-20T15:11:08.361+09:00 level=INFO msg=\"init-sql-file does not exist, skipping\" path=/root/.shaper/init.sql</div><div>time=2026-02-20T15:11:08.472+09:00 level=WARN msg=\"No users found. Authentication is disabled until first user is created. Make sure you don't expose sensitive data publicly.\"</div><div>time=2026-02-20T15:11:08.472+09:00 level=INFO msg=\"nats: Not listening on any network interfaces. Specify a port to make NATS available on the network.\"</div><div>time=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats: Starting nats-server\"</div><div>time=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:&nbsp; &nbsp;Version:&nbsp; 2.12.3\"</div><div>time=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:&nbsp; &nbsp;Git:&nbsp; &nbsp; &nbsp; [not set]\"</div><div>time=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:&nbsp; &nbsp;Name:&nbsp; &nbsp; &nbsp;ND3IDHLHXLSNSXSYIMZM6UG5ZXVHKSZZG7RRPD67ETIFFYQDC2BMIVSZ\"</div><div>time=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:&nbsp; &nbsp;Node:&nbsp; &nbsp; &nbsp;CKVn21lV\"</div><div>time=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:&nbsp; &nbsp;ID:&nbsp; &nbsp; &nbsp; &nbsp;ND3IDHLHXLSNSXSYIMZM6UG5ZXVHKSZZG7RRPD67ETIFFYQDC2BMIVSZ\"</div><div>time=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats: Starting JetStream\"</div><div>time=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats: ---------------- JETSTREAM ----------------\"</div><div>time=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:&nbsp; &nbsp;Strict:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; true\"</div><div>time=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:&nbsp; &nbsp;Max Memory:&nbsp; &nbsp; &nbsp; 11.50 GB\"</div><div>time=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:&nbsp; &nbsp;Max Storage:&nbsp; &nbsp; &nbsp;715.04 GB\"</div><div>time=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:&nbsp; &nbsp;Store Directory: \\\"/root/.shaper/nats/jetstream\\\"\"</div><div>time=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:&nbsp; &nbsp;API Level:&nbsp; &nbsp; &nbsp; &nbsp;2\"</div><div>time=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats: -------------------------------------------\"</div><div>time=2026-02-20T15:11:08.474+09:00 level=INFO msg=\"nats: Took 596.612µs to start JetStream\"</div><div>time=2026-02-20T15:11:08.474+09:00 level=INFO msg=\"nats: Server is ready\"</div><div>time=2026-02-20T15:11:08.481+09:00 level=INFO msg=\"Loaded scheduled tasks\"</div><div>time=2026-02-20T15:11:08.481+09:00 level=INFO msg=\"Snapshots disabled\"</div><div>time=2026-02-20T15:11:08.482+09:00 level=INFO msg=\"Web server is listening at localhost:5454\"</div><div>time=2026-02-20T15:11:08.482+09:00 level=INFO msg=\"Open http://localhost:5454 in your browser\"</div></code></pre></div>\n<div><br /></div><div>간단한 데이터 분석 시 주로 사용하는 Logparser는 차트 기능이 단순해서 엑셀 병용이 필수였는데, shaper는 쿼리를 바로바로 차트로 그릴 수 있어서 매우 편하다. 다이렉트로 윈도우 이벤트 접근까지 가능해지면 대박인데 해줄래나<span style=\"font-size: x-small;\">(..)</span></div><div><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBbw5zilX7gX9seWc1eq5zrejiCQ4WflfbD_GfwR_UYCbWcfnoZQ6eeaVuPWsCky5eZPI3pjJE_uA_-4KoBudAqjNusjpDOegLCNy6evqgKOrLzcPmaOjN37Wia7sO2hsJZCFaVXaB5QcPmaLQPurTkpLoRmhWby3Qk_ujjkcAInu3wxg09b1RpwDjyQv5/s1145/shaper.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"720\" data-original-width=\"1145\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBbw5zilX7gX9seWc1eq5zrejiCQ4WflfbD_GfwR_UYCbWcfnoZQ6eeaVuPWsCky5eZPI3pjJE_uA_-4KoBudAqjNusjpDOegLCNy6evqgKOrLzcPmaOjN37Wia7sO2hsJZCFaVXaB5QcPmaLQPurTkpLoRmhWby3Qk_ujjkcAInu3wxg09b1RpwDjyQv5/s16000/shaper.png\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimTivd2LjLtinwPzpiSWikIGxKFf9-4OfjhmIUNsw9V5tmZ6p0OaWEx_Xd7I33XDnu2MGD9gg9XDseiRaLrz2JJA0PDCMXtD62iXlASln5IZVAzi-_d26TS4FXzYp525hBbZEsB9rcVT18eQwUA2J6P5B3E0PEgLxY1gRGahfuXEc59ddRnXl3m-o59jDf/s1145/shaper2.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"720\" data-original-width=\"1145\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimTivd2LjLtinwPzpiSWikIGxKFf9-4OfjhmIUNsw9V5tmZ6p0OaWEx_Xd7I33XDnu2MGD9gg9XDseiRaLrz2JJA0PDCMXtD62iXlASln5IZVAzi-_d26TS4FXzYp525hBbZEsB9rcVT18eQwUA2J6P5B3E0PEgLxY1gRGahfuXEc59ddRnXl3m-o59jDf/s16000/shaper2.png\" /></a></div><br /><div><b>관련 글</b><br /><ul><li><a href=\"http://kangmyounghun.blogspot.kr/2016/03/logparser-web-log.html\" target=\"\">LogParser 활용(Web Log 이상징후 분석)</a></li></ul></div>",
        "contentSnippet": "SQL을 입력하면 차트를 그려주는 데이터 시각화 툴 shaper.\n\nC:\\Users\\Administrator>pip install shaper-bin\nCollecting shaper-bin\n  Downloading shaper_bin-0.14.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: requests>=2.31.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from shaper-bin) (2.32.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests>=2.31.0->shaper-bin) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests>=2.31.0->shaper-bin) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests>=2.31.0->shaper-bin) (1.26.20)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests>=2.31.0->shaper-bin) (2025.11.12)\nDownloading shaper_bin-0.14.0-py3-none-any.whl (12 kB)\nInstalling collected packages: shaper-bin\nSuccessfully installed shaper-bin-0.14.0\n\n\n[notice] A new release of pip is available: 25.3 -> 26.0.1\n[notice] To update, run: python.exe -m pip install --upgrade pip\n\n\nC:\\Users\\Administrator>shaper\nFetching Shaper binary...\nInstallation failed: Unsupported platform: windows-x86_64\n\n\n\n\n윈도우는 아직 지원하지 않아서 WSL 우분투에 설치.\n\nroot@easyDATA:~# pip install shaper-bin\nerror: externally-managed-environment\n\n\n× This environment is externally managed\n╰─> To install Python packages system-wide, try apt install\n    python3-xyz, where xyz is the package you are trying to\n    install.\n\n\n    If you wish to install a non-Debian-packaged Python package,\n    create a virtual environment using python3 -m venv path/to/venv.\n    Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n    sure you have python3-full installed.\n\n\n    If you wish to install a non-Debian packaged Python application,\n    it may be easiest to use pipx install xyz, which will manage a\n    virtual environment for you. Make sure you have pipx installed.\n\n\n    See /usr/share/doc/python3.12/README.venv for more information.\n\n\nnote: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\nhint: See PEP 668 for the detailed specification.\n\n\n\n\n시스템 안전성을 위해 가상환경을 추천하는 듯. 그냥 강제 설치.\n\nroot@easyDATA:~# pip install shaper-bin --break-system-packages\nCollecting shaper-bin\n  Downloading shaper_bin-0.14.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: requests>=2.31.0 in /usr/lib/python3/dist-packages (from shaper-bin) (2.31.0)\nDownloading shaper_bin-0.14.0-py3-none-any.whl (12 kB)\nInstalling collected packages: shaper-bin\nSuccessfully installed shaper-bin-0.14.0\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\n\n실행하면 localhost:5454 포트로 접속할 수 있는 웹 UI를 제공한다.\n\nroot@easyDATA:~# shaper\nFetching Shaper binary...\nLoading checksums...\nDownloading shaper v0.14.0 for linux-x86_64...\nVerifying checksum...\nChecksum verified successfully\nInstallation complete!\ntime=2026-02-20T15:11:08.349+09:00 level=INFO msg=\"Starting Shaper\" version=0.14.0\ntime=2026-02-20T15:11:08.349+09:00 level=INFO msg=\"For configuration options see --help or visit https://taleshape.com/shaper/docs for more\"\ntime=2026-02-20T15:11:08.349+09:00 level=INFO msg=\"Created data directory\" path=/root/.shaper\ntime=2026-02-20T15:11:08.349+09:00 level=INFO msg=\"SQLite opened\" file=/root/.shaper/shaper_internal.sqlite\ntime=2026-02-20T15:11:08.361+09:00 level=INFO msg=\"DuckDB opened\" file=/root/.shaper/shaper.duckdb\ntime=2026-02-20T15:11:08.361+09:00 level=INFO msg=\"Loading init-sql-file\" path=/root/.shaper/init.sql\ntime=2026-02-20T15:11:08.361+09:00 level=INFO msg=\"init-sql-file does not exist, skipping\" path=/root/.shaper/init.sql\ntime=2026-02-20T15:11:08.472+09:00 level=WARN msg=\"No users found. Authentication is disabled until first user is created. Make sure you don't expose sensitive data publicly.\"\ntime=2026-02-20T15:11:08.472+09:00 level=INFO msg=\"nats: Not listening on any network interfaces. Specify a port to make NATS available on the network.\"\ntime=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats: Starting nats-server\"\ntime=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:   Version:  2.12.3\"\ntime=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:   Git:      [not set]\"\ntime=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:   Name:     ND3IDHLHXLSNSXSYIMZM6UG5ZXVHKSZZG7RRPD67ETIFFYQDC2BMIVSZ\"\ntime=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:   Node:     CKVn21lV\"\ntime=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:   ID:       ND3IDHLHXLSNSXSYIMZM6UG5ZXVHKSZZG7RRPD67ETIFFYQDC2BMIVSZ\"\ntime=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats: Starting JetStream\"\ntime=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats: ---------------- JETSTREAM ----------------\"\ntime=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:   Strict:          true\"\ntime=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:   Max Memory:      11.50 GB\"\ntime=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:   Max Storage:     715.04 GB\"\ntime=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:   Store Directory: \\\"/root/.shaper/nats/jetstream\\\"\"\ntime=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats:   API Level:       2\"\ntime=2026-02-20T15:11:08.473+09:00 level=INFO msg=\"nats: -------------------------------------------\"\ntime=2026-02-20T15:11:08.474+09:00 level=INFO msg=\"nats: Took 596.612µs to start JetStream\"\ntime=2026-02-20T15:11:08.474+09:00 level=INFO msg=\"nats: Server is ready\"\ntime=2026-02-20T15:11:08.481+09:00 level=INFO msg=\"Loaded scheduled tasks\"\ntime=2026-02-20T15:11:08.481+09:00 level=INFO msg=\"Snapshots disabled\"\ntime=2026-02-20T15:11:08.482+09:00 level=INFO msg=\"Web server is listening at localhost:5454\"\ntime=2026-02-20T15:11:08.482+09:00 level=INFO msg=\"Open http://localhost:5454 in your browser\"\n\n\n\n\n간단한 데이터 분석 시 주로 사용하는 Logparser는 차트 기능이 단순해서 엑셀 병용이 필수였는데, shaper는 쿼리를 바로바로 차트로 그릴 수 있어서 매우 편하다. 다이렉트로 윈도우 이벤트 접근까지 가능해지면 대박인데 해줄래나(..)\n\n\n\n\n\n관련 글\n\nLogParser 활용(Web Log 이상징후 분석)",
        "id": "tag:blogger.com,1999:blog-2597780270996323853.post-602682453519397977",
        "isoDate": "2026-02-21T13:15:00.002Z"
      }
    ]
  },
  {
    "name": "김민장",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성중",
    "category": "개인",
    "posts": []
  },
  {
    "name": "구교준",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김덕기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "고명환",
    "category": "개인",
    "posts": [
      {
        "creator": "고명환",
        "title": "글로벌 고객이 찾아오는 구글맵 리뷰 전략 - 창업(소상공인)",
        "link": "https://brunch.co.kr/@@LOc/332",
        "pubDate": "Tue, 24 Feb 2026 06:43:37 GMT",
        "author": "고명환",
        "content": "1. 배경 : 왜 지금 구글맵에 주목해야 할까요?  과거에는 구글맵이 주로 해외여행이나 외국인 관광객을 위한 지도였다면, 최근 분위기는 완전히 달라졌습니다. 국내에서도 안드로이드폰 기본 탑재와 더불어, 광고성 정보가 적고 신뢰할 수 있다는 인식 덕분에 MZ세대를 중심으로 '진짜 맛집' 검색 도구로 자리 잡고 있습니다. 특히 엔데믹 이후 급증한 외국인 관광객<img src= \"https://img1.kakaocdn.net/thumb/R1280x0/?fname=http%3A%2F%2Ft1.kakaocdn.net%2Fbrunch%2Fservice%2Fuser%2FLOc%2Fimage%2FgQUA5hftHFIU-HFApDXYcAbYEgg.png\" width=\"500\" />",
        "contentSnippet": "1. 배경 : 왜 지금 구글맵에 주목해야 할까요?  과거에는 구글맵이 주로 해외여행이나 외국인 관광객을 위한 지도였다면, 최근 분위기는 완전히 달라졌습니다. 국내에서도 안드로이드폰 기본 탑재와 더불어, 광고성 정보가 적고 신뢰할 수 있다는 인식 덕분에 MZ세대를 중심으로 '진짜 맛집' 검색 도구로 자리 잡고 있습니다. 특히 엔데믹 이후 급증한 외국인 관광객",
        "guid": "https://brunch.co.kr/@@LOc/332",
        "isoDate": "2026-02-24T06:43:37.000Z"
      },
      {
        "creator": "고명환",
        "title": "수익 구조가 바뀌는 실전 외식업 메뉴 엔지니어링 - 창업(소상공인)",
        "link": "https://brunch.co.kr/@@LOc/331",
        "pubDate": "Mon, 23 Feb 2026 04:23:43 GMT",
        "author": "고명환",
        "content": "1. 배경 : 왜 메뉴 구성이 중요한가  음식점 경영에서 메뉴판은 단순한 안내판이 아니라 가장 강력한 마케팅 도구이자 설계도입니다. 많은 사장님이 '내가 잘하는 음식'을 모두 나열하기만 하지만, 이는 재고 관리의 어려움과 수익성 저하로 이어집니다. 메뉴 구성 전략이 잘 짜여 있으면 손님의 주문 고민 시간은 짧아지고, 주방의 조리 효율은 높아지며, 최종적으로<img src= \"https://img1.kakaocdn.net/thumb/R1280x0/?fname=http%3A%2F%2Ft1.kakaocdn.net%2Fbrunch%2Fservice%2Fuser%2FLOc%2Fimage%2FKuf8342ugCJ6AT-PNVk8XCTX8Nw.png\" width=\"500\" />",
        "contentSnippet": "1. 배경 : 왜 메뉴 구성이 중요한가  음식점 경영에서 메뉴판은 단순한 안내판이 아니라 가장 강력한 마케팅 도구이자 설계도입니다. 많은 사장님이 '내가 잘하는 음식'을 모두 나열하기만 하지만, 이는 재고 관리의 어려움과 수익성 저하로 이어집니다. 메뉴 구성 전략이 잘 짜여 있으면 손님의 주문 고민 시간은 짧아지고, 주방의 조리 효율은 높아지며, 최종적으로",
        "guid": "https://brunch.co.kr/@@LOc/331",
        "isoDate": "2026-02-23T04:23:43.000Z"
      },
      {
        "creator": "고명환",
        "title": "식당 수익 개선, 5가지 실무 전략 - 창업(소상공인)",
        "link": "https://brunch.co.kr/@@LOc/330",
        "pubDate": "Fri, 20 Feb 2026 07:46:31 GMT",
        "author": "고명환",
        "content": "1. 배경 : 왜 지금 수익 구조를 점검해야 하는가?  최근 외식업계는 '3고(고물가, 고금리, 고환율)' 현상과 가파른 인건비 상승으로 인해 유례없는 위기를 맞고 있습니다. 단순히 매출액이 높다고 해서 안심할 수 있는 시대는 지났습니다. 열심히 음식을 팔아도 임대료, 인건비, 재료비를 내고 나면 사장님 손에 쥐어지는 돈이 처참한 경우가 많기 때문입니다. <img src= \"https://img1.kakaocdn.net/thumb/R1280x0/?fname=http%3A%2F%2Ft1.kakaocdn.net%2Fbrunch%2Fservice%2Fuser%2FLOc%2Fimage%2FCjS2eXcPBoJZGAJBPIlff2qI8Zc.png\" width=\"500\" />",
        "contentSnippet": "1. 배경 : 왜 지금 수익 구조를 점검해야 하는가?  최근 외식업계는 '3고(고물가, 고금리, 고환율)' 현상과 가파른 인건비 상승으로 인해 유례없는 위기를 맞고 있습니다. 단순히 매출액이 높다고 해서 안심할 수 있는 시대는 지났습니다. 열심히 음식을 팔아도 임대료, 인건비, 재료비를 내고 나면 사장님 손에 쥐어지는 돈이 처참한 경우가 많기 때문입니다.",
        "guid": "https://brunch.co.kr/@@LOc/330",
        "isoDate": "2026-02-20T07:46:31.000Z"
      },
      {
        "creator": "고명환",
        "title": "2026 카페 창업 트렌드에 따른 창업 준비 전략 - 창업(소상공인)",
        "link": "https://brunch.co.kr/@@LOc/329",
        "pubDate": "Thu, 19 Feb 2026 09:26:51 GMT",
        "author": "고명환",
        "content": "1. 도입 : 커피가 맛있는 카페? 이제는 당연히 기본값입니다.  &quot;우리 동네에 카페가 벌써 5개인데 내가 차려도 될까?&quot; 예비 사장님들의 가장 큰 고민일 것입니다. 2026년 현재, 단순히 커피가 맛있는 것만으로는 고객을 부를 수 없습니다. 골목 깊숙이 있는 우리 매장까지 손님이 찾아오게 만들려면 '목적성 메뉴(디저트)'와 '압도적 운영 효율(스마트 시스<img src= \"https://img1.kakaocdn.net/thumb/R1280x0/?fname=http%3A%2F%2Ft1.kakaocdn.net%2Fbrunch%2Fservice%2Fuser%2FLOc%2Fimage%2FixPjP8D4MNrPAM90ANfeft3qV7k.jpg\" width=\"500\" />",
        "contentSnippet": "1. 도입 : 커피가 맛있는 카페? 이제는 당연히 기본값입니다.  \"우리 동네에 카페가 벌써 5개인데 내가 차려도 될까?\" 예비 사장님들의 가장 큰 고민일 것입니다. 2026년 현재, 단순히 커피가 맛있는 것만으로는 고객을 부를 수 없습니다. 골목 깊숙이 있는 우리 매장까지 손님이 찾아오게 만들려면 '목적성 메뉴(디저트)'와 '압도적 운영 효율(스마트 시스",
        "guid": "https://brunch.co.kr/@@LOc/329",
        "isoDate": "2026-02-19T09:26:51.000Z"
      }
    ]
  },
  {
    "name": "강성희",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강병수",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김봉현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강형석",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김수로",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강미경",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김성현",
    "category": "개인",
    "posts": []
  },
  {
    "name": "강진우",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권민재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "권태관",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김도곤",
    "category": "개인",
    "posts": []
  },
  {
    "name": "칡토스의 게임 개발",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김선철",
    "category": "개인",
    "posts": []
  },
  {
    "name": "프리웨어 이야기",
    "category": "개인",
    "posts": [
      {
        "creator": "어떤오후의 프리웨어 이야기",
        "title": "ChatGPT, Gemini보다 강할 때? Claude가 빛을 발하는 5가지 핵심 업무",
        "link": "https://muzbox.tistory.com/483710",
        "pubDate": "Mon, 23 Feb 2026 11:35:25 +0900",
        "author": "어떤오후의 프리웨어 이야기",
        "comments": "https://muzbox.tistory.com/483710#entry483710comment",
        "content": "<div style=\"font-family: 'Noto Sans KR', sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; font-size: 16px; box-sizing: border-box; color: #3c4043;\">\n<div style=\"background-color: #e8f4fd; padding: 15px; border-radius: 8px; font-style: italic; margin-bottom: 25px; font-size: 15px;\">안녕하세요! 생성형 AI를 업무에 활용하는 분들이라면 늘 고민하실 거예요. \"어떤 AI를 써야 가장 효율적일까?\" 특히 ChatGPT와 Gemini가 워낙 유명하다 보니, Claude는 과연 어떤 특별한 강점이 있을지 궁금해하시는 분들이 많죠. 오늘은 2026년 최신 벤치마크 데이터를 바탕으로, Claude가 다른 모델들보다 확실히 빛을 발하는 5가지 핵심 업무를 깊이 있게 분석해 보려 해요. 제 경험을 섞어 가면서, 여러분의 AI 선택에 실질적인 도움을 드릴 수 있도록 노력해 보겠습니다!</div>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"프로그래머의 손이 코드를 입력하고 금융 차트와 디지털 문서가 보이는 미래형 작업 공간. Claude의 코딩, 금융 분석, 문서 처리 강점을 나타내는 이미지..jpeg\" data-origin-width=\"1200\" data-origin-height=\"1200\"><span data-url=\"https://blog.kakaocdn.net/dn/CGe2u/dJMcadntMtA/PTmf5YHBxy3t3EiALHqthk/img.jpg\" data-phocus=\"https://blog.kakaocdn.net/dn/CGe2u/dJMcadntMtA/PTmf5YHBxy3t3EiALHqthk/img.jpg\"><img src=\"https://blog.kakaocdn.net/dn/CGe2u/dJMcadntMtA/PTmf5YHBxy3t3EiALHqthk/img.jpg\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FCGe2u%2FdJMcadntMtA%2FPTmf5YHBxy3t3EiALHqthk%2Fimg.jpg\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"프로그래머의 손이 코드를 입력하고 금융 차트와 디지털 문서가 보이는 미래형 작업 공간. Claude의 코딩, 금융 분석, 문서 처리 강점을 나타내는 이미지.\" loading=\"lazy\" width=\"500\" height=\"500\" data-filename=\"프로그래머의 손이 코드를 입력하고 금융 차트와 디지털 문서가 보이는 미래형 작업 공간. Claude의 코딩, 금융 분석, 문서 처리 강점을 나타내는 이미지..jpeg\" data-origin-width=\"1200\" data-origin-height=\"1200\"/></span></figure>\n\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">생성형 AI 시장은 이제 단순히 '더 좋은 AI'를 찾는 단계를 넘어섰어요. 이제는 <b>'특정 상황에서 어떤 모델이 최적인가'</b>를 고민해야 하는 시대가 된 거죠. 2026년 2월 현재, 주요 AI 모델들의 발전 속도를 보면 정말 놀랍습니다.</p>\n<p style=\"margin-bottom: 20;\" data-ke-size=\"size16\">간단히 주요 플레이어와 최신 모델들을 살펴볼까요?</p>\n<table style=\"width: 100%; border-collapse: collapse; margin-bottom: 25px; border: 1px solid #dadce0;\" data-ke-align=\"alignLeft\">\n<thead style=\"background-color: #e8eaed;\">\n<tr>\n<th style=\"padding: 12px; border: 1px solid #dadce0; text-align: left; color: #3c4043;\">회사</th>\n<th style=\"padding: 12px; border: 1px solid #dadce0; text-align: left; color: #3c4043;\">최신 모델 (2026년 기준)</th>\n<th style=\"padding: 12px; border: 1px solid #dadce0; text-align: left; color: #3c4043;\">주요 특징</th>\n</tr>\n</thead>\n<tbody>\n<tr style=\"background-color: white;\">\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\"><b>OpenAI</b></td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">GPT-5.2 / GPT-5.3 Codex</td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">균형 잡힌 성능, 빠른 응답 속도</td>\n</tr>\n<tr style=\"background-color: #f1f3f4;\">\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\"><b>Google</b></td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">Gemini 3.1 Pro</td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">추상 추론 강화, 과학 문제 해결 특화</td>\n</tr>\n<tr style=\"background-color: white;\">\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\"><b>Anthropic</b></td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">Claude Opus 4.6 / Sonnet 4.5</td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">코딩 최강, 금융 분석 최고</td>\n</tr>\n</tbody>\n</table>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">이 강력한 AI 라인업 속에서, 과연 <b>Claude가 특별히 빛을 발하는 순간은 언제일까요?</b> 제가 직접 여러 프로젝트에서 사용해 본 경험과 함께, 최신 벤치마크 데이터를 토대로 Claude의 진정한 강점을 파헤쳐 보겠습니다.</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>1.   코딩 및 소프트웨어 개발: Claude의 독주</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">개발자라면 이 부분에 특히 주목해야 합니다. 2026년 현재, 코딩 분야에서 Claude의 성능은 정말 압도적이라고 해도 과언이 아니에요. 특히 <b>Claude Sonnet 4.5</b>와 <b>Opus 4.6</b>은 개발 워크플로우를 혁신적으로 바꿔놓을 잠재력을 가지고 있습니다.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; border-bottom: 2px solid #e8f0fe; padding-bottom: 5px;\" data-ke-size=\"size23\"><b>  최신 성능 지표가 말해주는 Claude의 힘</b></h3>\n<ul style=\"margin-bottom: 20px; padding-left: 20px;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 8px;\"><b>SWE-Bench Verified: 77.2%</b> &mdash; 이는 실제 소프트웨어 엔지니어링 벤치마크에서 높은 수준의 문제 해결 능력을 보여준다는 의미입니다.</li>\n<li style=\"margin-bottom: 8px;\">실제 GitHub 이슈 해결 성공률 <b>77% 향상</b> &mdash; 실제 오픈소스 프로젝트에 기여하는 데 얼마나 유용한지 보여주는 지표죠.</li>\n<li style=\"margin-bottom: 8px;\">복잡한 코드 리팩토링 작업 최대 <b>10배 빠름</b> &mdash; 개발 생산성에 엄청난 영향을 미칩니다.</li>\n<li style=\"margin-bottom: 8px;\"><b>Claude Code 2.0 지원</b> &mdash; 자동 체크포인트, 실행 취소(Undo), IDE 통합 등 개발 편의성을 대폭 개선했습니다.</li>\n</ul>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">대규모 코드베이스를 다루거나, 복잡한 시스템의 아키텍처를 이해하고 수정해야 할 때 Claude의 진가가 발휘되곤 합니다. 제가 직접 경험해본 바로는, 특히 수십만 라인에 달하는 레거시 코드를 분석하고 리팩토링할 때 Claude만큼 똑똑하고 끈기 있는 비서가 없었어요.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; border-bottom: 2px solid #e8f0fe; padding-bottom: 5px;\" data-ke-size=\"size23\"><b>  왜 Claude가 코딩에 유리한가?</b></h3>\n<ol style=\"margin-bottom: 20px; padding-left: 20px;\" data-ke-list-type=\"decimal\">\n<li style=\"margin-bottom: 8px;\"><b>긴 컨텍스트 처리 능력:</b> 방대한 코드 파일을 한 번에 읽고 맥락을 이해하는 능력이 탁월합니다.</li>\n<li style=\"margin-bottom: 8px;\"><b>코드 구조 전체 파악 및 리팩토링 능력:</b> 단순히 오류를 수정하는 것을 넘어, 전체적인 코드 품질과 설계를 개선하는 데 도움을 줍니다.</li>\n<li style=\"margin-bottom: 8px;\"><b>테스트 케이스 생성 정확도:</b> 버그를 찾고 예방하는 데 필수적인 고품질 테스트 코드를 효과적으로 생성해요.</li>\n<li style=\"margin-bottom: 8px;\"><b>버그 재현 및 원인 추적 정밀도:</b> 복잡한 버그의 발생 조건을 분석하고 근본 원인을 찾아내는 데 뛰어납니다.</li>\n</ol>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"개발자의 손이 파란색과 회색 테마의 코드 편집기에서 복잡한 코드를 입력하는 모습. Claude의 강력한 코딩 능력을 시각화.jpeg\" data-origin-width=\"1344\" data-origin-height=\"768\"><span data-url=\"https://blog.kakaocdn.net/dn/rq6Wo/dJMcaaj18Ze/L1I80hk3GLStNE6yZ187Y0/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/rq6Wo/dJMcaaj18Ze/L1I80hk3GLStNE6yZ187Y0/img.png\"><img src=\"https://blog.kakaocdn.net/dn/rq6Wo/dJMcaaj18Ze/L1I80hk3GLStNE6yZ187Y0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Frq6Wo%2FdJMcaaj18Ze%2FL1I80hk3GLStNE6yZ187Y0%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"개발자의 손이 파란색과 회색 테마의 코드 편집기에서 복잡한 코드를 입력하는 모습. Claude의 강력한 코딩 능력을 시각화.\" loading=\"lazy\" width=\"1344\" height=\"768\" data-filename=\"개발자의 손이 파란색과 회색 테마의 코드 편집기에서 복잡한 코드를 입력하는 모습. Claude의 강력한 코딩 능력을 시각화.jpeg\" data-origin-width=\"1344\" data-origin-height=\"768\"/></span></figure>\n\n<div style=\"background-color: #e8f4fd; border-left: 4px solid #1a73e8; padding: 15px; margin: 20px 0; border-radius: 0 8px 8px 0;\">  <b>팁:</b> GPT-5.3 Codex도 코딩 벤치마크에서 77.3%라는 높은 점수를 기록하며 강력한 보조 선택지로 꼽히지만, 대형 프로젝트에서 긴 맥락을 안정적으로 유지하며 작업하는 측면에서는 Claude가 더 신뢰할 만하다는 평가가 지배적입니다. 복잡하고 큰 규모의 프로젝트라면 Claude를 먼저 고려해 보세요!</div>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>2.   금융 분석 및 비즈니스 인텔리전스: Claude Opus 4.6의 특화 영역</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">금융 분야는 정말 섬세하고 복잡한 데이터 처리가 필요한 영역이잖아요. 그런데 Claude는 이 분야에서 <b>독보적인 성능</b>을 보여주고 있습니다. 특히 <b>Claude Opus 4.6</b>은 금융 전문가들에게 강력한 도구가 될 것이라고 생각해요. 저도 최근에 재무 보고서 분석에 Claude를 활용해봤는데, 정말 놀라웠어요.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; border-bottom: 2px solid #e8f0fe; padding-bottom: 5px;\" data-ke-size=\"size23\"><b>  Finance Agent 벤치마크 1위!</b></h3>\n<ul style=\"margin-bottom: 20px; padding-left: 20px;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 8px;\">복잡한 재무 문서(IR 자료, 애널리스트 보고서 등)의 완벽한 처리 능력</li>\n<li style=\"margin-bottom: 8px;\">다중 보고서 간의 심층적인 비교 분석</li>\n<li style=\"margin-bottom: 8px;\">현금흐름표, 손익계산서, 대차대조표의 <b>교차 해석</b> 및 인사이트 도출</li>\n<li style=\"margin-bottom: 8px;\"><b>Claude for Excel beta 지원:</b> Excel 통합으로 재무 데이터 작업 효율 극대화</li>\n</ul>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">다른 모델들도 물론 재무 데이터를 다룰 수 있지만, <b>대규모 재무 보고서를 통째로 업로드해서 수많은 세부 조항들 간의 논리적 연결을 추적하고, 나아가 정성적인 리스크 분석까지 수행하는 능력</b>은 Claude Opus 4.6이 단연 최고였습니다. 저는 복잡한 기업공개(IPO) 문서 분석에 활용하면서 시간을 정말 많이 절약할 수 있었어요.</p>\n<div style=\"background-color: #e8f4fd; border-left: 4px solid #1a73e8; padding: 15px; margin: 20px 0; border-radius: 0 8px 8px 0;\">  <b>주목:</b> Gemini 3.1 Pro가 추상 추론에 강하고, GPT-5.2도 균형 잡힌 분석을 제공하지만, 금융 특화 벤치마크에서는 Claude Opus 4.6이 <b>가장 높은 점수</b>를 기록하며 이 분야의 강자로 자리매김했습니다. 투자를 하시거나 기업 전략을 세우시는 분들이라면 꼭 활용해보세요!</div>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>3.   장문 분석 및 대규모 문서 처리: Claude의 전통적 강점</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">Claude는 초기 모델부터 <b>압도적인 컨텍스트 창(context window)</b>이 가장 큰 장점으로 꼽혔습니다. 2026년에도 이 강점은 여전히 유효하고, 오히려 더욱 강력해졌습니다. 방대한 양의 텍스트를 한 번에 처리해야 하는 작업이 많다면 Claude가 아주 좋은 선택지가 될 거예요.</p>\n<figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-filename=\"여러 화면에 금융 보고서와 법률 문서가 가득한 디지털 문서를 분석하는 사람. Claude의 장문 문서 처리 능력을 상징..jpeg\" data-origin-width=\"1344\" data-origin-height=\"768\"><span data-url=\"https://blog.kakaocdn.net/dn/bkm4ta/dJMcagduEOK/uTcK151dD1mgODgeHO3i3K/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bkm4ta/dJMcagduEOK/uTcK151dD1mgODgeHO3i3K/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bkm4ta/dJMcagduEOK/uTcK151dD1mgODgeHO3i3K/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbkm4ta%2FdJMcagduEOK%2FuTcK151dD1mgODgeHO3i3K%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" alt=\"여러 화면에 금융 보고서와 법률 문서가 가득한 디지털 문서를 분석하는 사람. Claude의 장문 문서 처리 능력을 상징.\" loading=\"lazy\" width=\"1344\" height=\"768\" data-filename=\"여러 화면에 금융 보고서와 법률 문서가 가득한 디지털 문서를 분석하는 사람. Claude의 장문 문서 처리 능력을 상징..jpeg\" data-origin-width=\"1344\" data-origin-height=\"768\"/></span></figure>\n\n<ul style=\"margin-bottom: 20px; padding-left: 20px;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 8px;\">대규모 연구 논문 수십 편을 한 번에 분석하고 핵심 요약</li>\n<li style=\"margin-bottom: 8px;\">100페이지가 넘는 법률 계약서 또는 정책 문서 검토</li>\n<li style=\"margin-bottom: 8px;\">수십 개의 보고서를 동시에 비교하고 공통점 및 차이점 도출</li>\n<li style=\"margin-bottom: 8px;\">기업 내부 문서들을 기반으로 한 지식베이스 구축</li>\n</ul>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">ChatGPT와 Gemini도 물론 문서 처리가 가능하지만, <b>'이 10개 문서에서 공통 리스크 요소만 추출하고, 연도별 변화까지 분석해줘'</b>와 같이 복합적이고 심층적인 질의를 할 때는 Claude가 훨씬 더 안정적이고 정확한 결과를 내놓는다는 것을 저도 여러 번 경험했습니다. 특히 중요한 문서일수록 AI의 안정성은 정말 중요하죠.</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>4. ✍️ 창의적 작업: 정교함이 필요할 땐 Claude Sonnet 4.5</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">창의적 작업이라고 하면 GPT가 먼저 떠오르실 수도 있습니다. 실제로 GPT-5.2는 빠른 응답 속도, 자연스러운 문체, 실시간 음성 기능, 그리고 Prism 협업 리서치 도구 등을 통해 초안 작성에 매우 유리한 강점을 가지고 있죠. 하지만 <b>'정교함'과 '안정적인 논리 구조'</b>가 핵심이라면 이야기가 달라집니다. 여기서 Claude Sonnet 4.5가 빛을 발합니다.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; border-bottom: 2px solid #e8f0fe; padding-bottom: 5px;\" data-ke-size=\"size23\"><b>  Claude의 창의성, 어떤 면이 특별할까요?</b></h3>\n<ul style=\"margin-bottom: 20px; padding-left: 20px;\" data-ke-list-type=\"disc\">\n<li style=\"margin-bottom: 8px;\">더 <b>정교한 창작물</b> 생성: 특히 기술 블로그, 전문 리포트, 백서와 같은 형식을 요구하는 작업에서 탁월합니다.</li>\n<li style=\"margin-bottom: 8px;\"><b>전문 문서 작성에 강점:</b> 복잡한 정보를 체계적으로 정리하고, 설득력 있는 논리를 전개하는 데 강해요.</li>\n<li style=\"margin-bottom: 8px;\">슬라이드 및 애니메이션 생성 최적화: 시각적인 자료 제작을 위한 아이디어 구상과 내용 구성에 도움을 줍니다.</li>\n<li style=\"margin-bottom: 8px;\"><b>논리적 구조가 안정적:</b> 길고 복잡한 글에서도 일관된 논리 흐름을 유지합니다.</li>\n</ul>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">제 경험상, <b>빠른 아이데이션이나 초안 작성은 GPT-5.2</b>에 맡기고, <b>정밀하고 완성도 높은 기술 문서나 보고서 작성은 Claude Sonnet 4.5</b>에 맡기는 것이 가장 효율적인 조합이었습니다. 마치 빠른 스케치에는 연필을, 최종 작품에는 유화를 쓰는 것과 비슷한 느낌이랄까요?</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>5. ⚡ 속도와 비용 효율: 경량 모델의 활약</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">모든 작업에 최고 성능의 모델만 사용할 수는 없죠. 특히 속도가 중요하거나 비용 절감이 핵심이라면, 경량화된 모델들을 고려하는 것이 현명합니다. 이 부분에서는 Claude보다는 다른 모델들이 강점을 보입니다.</p>\n<table style=\"width: 100%; border-collapse: collapse; margin-bottom: 25px; border: 1px solid #dadce0;\" data-ke-align=\"alignLeft\">\n<thead style=\"background-color: #e8eaed;\">\n<tr>\n<th style=\"padding: 12px; border: 1px solid #dadce0; text-align: left; color: #3c4043;\">모델</th>\n<th style=\"padding: 12px; border: 1px solid #dadce0; text-align: left; color: #3c4043;\">특징</th>\n</tr>\n</thead>\n<tbody>\n<tr style=\"background-color: white;\">\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">GPT-5.2 Instant</td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">빠른 응답 속도, 저비용으로 일반적인 작업에 효율적</td>\n</tr>\n<tr style=\"background-color: #f1f3f4;\">\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">Gemini Flash</td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">가성비 최고, 빠르고 경제적인 선택지</td>\n</tr>\n<tr style=\"background-color: white;\">\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">Claude Haiku</td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">경량 모델, 빠른 응답과 효율성 추구</td>\n</tr>\n</tbody>\n</table>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">Claude도 Haiku라는 경량 모델을 가지고 있지만, 범용적인 빠른 처리와 비용 효율성 측면에서는 아직 GPT-5.2 Instant나 Gemini Flash가 더 경쟁력이 있다는 것이 제 생각입니다. 정말 급하거나 가볍게 테스트할 때는 이 모델들을 활용하는 것이 좋아요.</p>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>  번외: 수학 및 추상적 논리 문제: Gemini의 영역</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">이 부분은 Claude가 강하기보다는 Gemini가 압도적인 영역이라 잠시 짚고 넘어가고 싶었어요. 복잡한 알고리즘 문제, 수학 증명, 추상 패턴 인식과 같은 순수 추론 영역에서는 <b>Gemini 3.1 Pro</b>가 가장 높은 평균 점수를 기록하고 있습니다. Claude도 물론 강력하지만, 이 분야에서는 Gemini가 근소하게 앞서고 있다는 점을 기억해두시면 좋습니다.</p>\n<div style=\"background-color: #fce8e6; border-left: 4px solid #d93025; padding: 15px; margin: 20px 0; border-radius: 0 8px 8px 0;\">⚠️ <b>주의:</b> 복잡한 수학 문제나 고도의 추상적 논리 추론이 필요한 경우, Claude보다는 Gemini 3.1 Pro가 더 신뢰할 수 있는 결과를 제공할 가능성이 높습니다. 목적에 맞는 도구를 선택하는 것이 중요하니까요!</div>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>  종합 벤치마크 요약: 어떤 AI를 선택해야 할까?</b></h2>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">각 모델의 강점을 정리해 봤는데요, 전체적인 그림을 한눈에 보실 수 있도록 요약 표를 만들어봤어요.</p>\n<h3 style=\"font-size: 19px; color: #1a73e8; margin: 25px 0 10px; border-bottom: 2px solid #e8f0fe; padding-bottom: 5px;\" data-ke-size=\"size23\"><b>분야별 1위 모델</b></h3>\n<table style=\"width: 100%; border-collapse: collapse; margin-bottom: 25px; border: 1px solid #dadce0;\" data-ke-align=\"alignLeft\">\n<thead style=\"background-color: #e8eaed;\">\n<tr>\n<th style=\"padding: 12px; border: 1px solid #dadce0; text-align: left; color: #3c4043;\">분야</th>\n<th style=\"padding: 12px; border: 1px solid #dadce0; text-align: left; color: #3c4043;\">1위 모델</th>\n</tr>\n</thead>\n<tbody>\n<tr style=\"background-color: white;\">\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">코딩</td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\"><b>Claude Sonnet 4.5</b></td>\n</tr>\n<tr style=\"background-color: #f1f3f4;\">\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">금융 분석</td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\"><b>Claude Opus 4.6</b></td>\n</tr>\n<tr style=\"background-color: white;\">\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">장문 문서 처리</td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\"><b>Claude Opus 4.6</b></td>\n</tr>\n<tr style=\"background-color: #f1f3f4;\">\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">정교한 창의적 문서</td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\"><b>Claude Sonnet 4.5</b></td>\n</tr>\n<tr style=\"background-color: white;\">\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">추상 추론</td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">Gemini 3.1 Pro</td>\n</tr>\n<tr style=\"background-color: #f1f3f4;\">\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">속도</td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">GPT-5.2 Instant</td>\n</tr>\n<tr style=\"background-color: white;\">\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">비용 효율</td>\n<td style=\"padding: 12px; border: 1px solid #dadce0; color: #3c4043;\">Gemini Flash</td>\n</tr>\n</tbody>\n</table>\n<div style=\"background-color: #f8f9fa; border: 1px solid #dadce0; border-radius: 8px; padding: 25px; margin: 30px 0; box-shadow: 0 4px 12px rgba(0,0,0,0.1);\">\n<div style=\"font-size: 26px; color: #1a73e8; font-weight: bold; margin-bottom: 15px; border-bottom: 2px solid #1a73e8; padding-bottom: 10px;\">  핵심 요약</div>\n<p style=\"font-size: 17px; margin-bottom: 15px;\" data-ke-size=\"size16\"><b>1.  ️ 복잡한 코드 개발 및 리팩토링:</b> Claude는 긴 컨텍스트와 심층적인 코드 이해력으로 개발 생산성을 극대화합니다.</p>\n<p style=\"font-size: 17px; margin-bottom: 15px;\" data-ke-size=\"size16\"><b>2.   전문 금융 데이터 분석:</b> 대용량 재무 보고서 교차 분석, 리스크 평가 등 금융 분야에 특화된 독보적인 성능을 보여줍니다.</p>\n<p style=\"font-size: 17px; margin-bottom: 15px;\" data-ke-size=\"size16\"><b>3.   방대한 장문 문서 처리:</b> 수백 페이지 분량의 논문이나 계약서를 안정적으로 처리하고 복합 질의에 정확히 답변합니다.</p>\n<p style=\"font-size: 17px; margin-bottom: 15px;\" data-ke-size=\"size16\"><b>4.   정교하고 논리적인 문서 창작:</b> 기술 블로그, 전문 리포트 등 완성도 높은 글쓰기에 탁월합니다.</p>\n<div style=\"font-size: 14px; color: #5f6368; margin-top: 20px; border-top: 1px dashed #dadce0; padding-top: 15px;\">결론적으로 AI 선택은 '무엇을 할 것인가'에 따라 달라집니다. Claude는 특정 전문 영역에서 '전문 작업용 엔진'으로서 진정한 가치를 발휘합니다.</div>\n</div>\n<h2 style=\"font-size: 22px; color: white; background: linear-gradient(to right, #1a73e8, #004d99); margin: 30px 0 15px; border-radius: 10px; padding: 10px 25px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2); font-weight: bold; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\" data-ke-size=\"size26\"><b>❓ 자주 묻는 질문 (FAQ)</b></h2>\n<div style=\"margin-bottom: 15px;\">\n<h3 style=\"font-size: 18px; color: #1a73e8; margin-bottom: 5px;\" data-ke-size=\"size23\">Q1: Claude는 모든 면에서 ChatGPT나 Gemini보다 뒤떨어지나요?</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">아니요, 그렇지 않습니다. Claude는 특정 전문 분야, 예를 들어 코딩, 금융 분석, 장문 문서 처리, 그리고 정교한 전문 문서 작성에서 ChatGPT나 Gemini보다 훨씬 더 뛰어난 성능과 안정성을 보여줍니다. '모든 것을 평균 이상으로 잘하는 모델'이라기보다는, '특정 전문 영역에서 압도적인 완성도를 제공하는 모델'이라고 이해하는 것이 정확합니다.</p>\n</div>\n<div style=\"margin-bottom: 15px;\">\n<h3 style=\"font-size: 18px; color: #1a73e8; margin-bottom: 5px;\" data-ke-size=\"size23\">Q2: 개발자라면 무조건 Claude를 사용해야 하나요?</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">'무조건'은 아니지만, 복잡한 코드베이스를 다루거나, 대규모 리팩토링, 상세한 테스트 케이스 생성이 필요한 경우에는 Claude Sonnet 4.5나 Opus 4.6이 매우 강력한 선택이 될 것입니다. GPT-5.3 Codex도 훌륭하지만, 긴 코드 맥락 유지나 구조적 이해 측면에서 Claude가 더 안정적이라는 평가가 많습니다. 프로젝트의 규모와 복잡성에 따라 선택하는 것이 좋습니다.</p>\n</div>\n<div style=\"margin-bottom: 15px;\">\n<h3 style=\"font-size: 18px; color: #1a73e8; margin-bottom: 5px;\" data-ke-size=\"size23\">Q3: 비용 효율적인 작업에는 어떤 모델이 가장 좋나요?</h3>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">속도와 비용 효율성이 최우선이라면, GPT-5.2 Instant나 Gemini Flash가 더 좋은 선택이 될 수 있습니다. Claude도 Haiku라는 경량 모델이 있지만, 범용적인 빠른 처리 및 비용 경쟁력 면에서는 아직 앞선 두 모델이 유리하다고 평가됩니다. 가벼운 작업이나 빠른 테스트에는 이러한 경량 모델들을 활용해보세요.</p>\n</div>\n<script type=\"application/ld+json\">\n  {\n    \"@context\": \"https://schema.org\",\n    \"@type\": \"FAQPage\",\n    \"mainEntity\": [\n      {\n        \"@type\": \"Question\",\n        \"name\": \"Claude는 모든 면에서 ChatGPT나 Gemini보다 뒤떨어지나요?\",\n        \"acceptedAnswer\": {\n          \"@type\": \"Answer\",\n          \"text\": \"아니요, 그렇지 않습니다. Claude는 특정 전문 분야, 예를 들어 코딩, 금융 분석, 장문 문서 처리, 그리고 정교한 전문 문서 작성에서 ChatGPT나 Gemini보다 훨씬 더 뛰어난 성능과 안정성을 보여줍니다. '모든 것을 평균 이상으로 잘하는 모델'이라기보다는, '특정 전문 영역에서 압도적인 완성도를 제공하는 모델'이라고 이해하는 것이 정확합니다.\"\n        }\n      },\n      {\n        \"@type\": \"Question\",\n        \"name\": \"개발자라면 무조건 Claude를 사용해야 하나요?\",\n        \"acceptedAnswer\": {\n          \"@type\": \"Answer\",\n          \"text\": \"'무조건'은 아니지만, 복잡한 코드베이스를 다루거나, 대규모 리팩토링, 상세한 테스트 케이스 생성이 필요한 경우에는 Claude Sonnet 4.5나 Opus 4.6이 매우 강력한 선택이 될 것입니다. GPT-5.3 Codex도 훌륭하지만, 긴 코드 맥락 유지나 구조적 이해 측면에서 Claude가 더 안정적이라는 평가가 많습니다. 프로젝트의 규모와 복잡성에 따라 선택하는 것이 좋습니다.\"\n        }\n      },\n      {\n        \"@type\": \"Question\",\n        \"name\": \"비용 효율적인 작업에는 어떤 모델이 가장 좋나요?\",\n        \"acceptedAnswer\": {\n          \"@type\": \"Answer\",\n          \"text\": \"속도와 비용 효율성이 최우선이라면, GPT-5.2 Instant나 Gemini Flash가 더 좋은 선택이 될 수 있습니다. Claude도 Haiku라는 경량 모델이 있지만, 범용적인 빠른 처리 및 비용 경쟁력 면에서는 아직 앞선 두 모델이 유리하다고 평가됩니다. 가벼운 작업이나 빠른 테스트에는 이러한 경량 모델들을 활용해보세요.\"\n        }\n      }\n    ]\n  }\n  </script>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">AI 선택은 결국 <b>여러분의 업무 구조와 목적에 달려 있습니다.</b> 모든 AI가 모든 작업에 완벽할 수는 없어요. 하지만 적절한 도구를 올바른 상황에 사용하는 것이 스마트한 워크플로우를 만드는 핵심이죠.</p>\n<p style=\"margin-bottom: 20px;\" data-ke-size=\"size16\">만약 여러분이 개발자, 금융 분석가, 기술 블로거, 또는 방대한 자료를 다루는 연구자라면, Claude는 단순한 챗봇을 넘어 <b>'전문 작업용 엔진'</b>에 가깝습니다. 각자의 니즈에 맞춰 AI를 현명하게 활용하시길 바라며, 궁금한 점이 있다면 언제든지 댓글로 남겨주세요! 다음에 더 유익한 정보로 찾아뵙겠습니다.</p>\n</div>",
        "contentSnippet": "안녕하세요! 생성형 AI를 업무에 활용하는 분들이라면 늘 고민하실 거예요. \"어떤 AI를 써야 가장 효율적일까?\" 특히 ChatGPT와 Gemini가 워낙 유명하다 보니, Claude는 과연 어떤 특별한 강점이 있을지 궁금해하시는 분들이 많죠. 오늘은 2026년 최신 벤치마크 데이터를 바탕으로, Claude가 다른 모델들보다 확실히 빛을 발하는 5가지 핵심 업무를 깊이 있게 분석해 보려 해요. 제 경험을 섞어 가면서, 여러분의 AI 선택에 실질적인 도움을 드릴 수 있도록 노력해 보겠습니다!\n\n\n생성형 AI 시장은 이제 단순히 '더 좋은 AI'를 찾는 단계를 넘어섰어요. 이제는 '특정 상황에서 어떤 모델이 최적인가'를 고민해야 하는 시대가 된 거죠. 2026년 2월 현재, 주요 AI 모델들의 발전 속도를 보면 정말 놀랍습니다.\n간단히 주요 플레이어와 최신 모델들을 살펴볼까요?\n회사\n최신 모델 (2026년 기준)\n주요 특징\n\n\n\n\nOpenAI\nGPT-5.2 / GPT-5.3 Codex\n균형 잡힌 성능, 빠른 응답 속도\n\n\nGoogle\nGemini 3.1 Pro\n추상 추론 강화, 과학 문제 해결 특화\n\n\nAnthropic\nClaude Opus 4.6 / Sonnet 4.5\n코딩 최강, 금융 분석 최고\n\n\n\n이 강력한 AI 라인업 속에서, 과연 Claude가 특별히 빛을 발하는 순간은 언제일까요? 제가 직접 여러 프로젝트에서 사용해 본 경험과 함께, 최신 벤치마크 데이터를 토대로 Claude의 진정한 강점을 파헤쳐 보겠습니다.\n1.   코딩 및 소프트웨어 개발: Claude의 독주\n개발자라면 이 부분에 특히 주목해야 합니다. 2026년 현재, 코딩 분야에서 Claude의 성능은 정말 압도적이라고 해도 과언이 아니에요. 특히 Claude Sonnet 4.5와 Opus 4.6은 개발 워크플로우를 혁신적으로 바꿔놓을 잠재력을 가지고 있습니다.\n  최신 성능 지표가 말해주는 Claude의 힘\nSWE-Bench Verified: 77.2% — 이는 실제 소프트웨어 엔지니어링 벤치마크에서 높은 수준의 문제 해결 능력을 보여준다는 의미입니다.\n실제 GitHub 이슈 해결 성공률 77% 향상 — 실제 오픈소스 프로젝트에 기여하는 데 얼마나 유용한지 보여주는 지표죠.\n복잡한 코드 리팩토링 작업 최대 10배 빠름 — 개발 생산성에 엄청난 영향을 미칩니다.\nClaude Code 2.0 지원 — 자동 체크포인트, 실행 취소(Undo), IDE 통합 등 개발 편의성을 대폭 개선했습니다.\n대규모 코드베이스를 다루거나, 복잡한 시스템의 아키텍처를 이해하고 수정해야 할 때 Claude의 진가가 발휘되곤 합니다. 제가 직접 경험해본 바로는, 특히 수십만 라인에 달하는 레거시 코드를 분석하고 리팩토링할 때 Claude만큼 똑똑하고 끈기 있는 비서가 없었어요.\n  왜 Claude가 코딩에 유리한가?\n긴 컨텍스트 처리 능력: 방대한 코드 파일을 한 번에 읽고 맥락을 이해하는 능력이 탁월합니다.\n코드 구조 전체 파악 및 리팩토링 능력: 단순히 오류를 수정하는 것을 넘어, 전체적인 코드 품질과 설계를 개선하는 데 도움을 줍니다.\n테스트 케이스 생성 정확도: 버그를 찾고 예방하는 데 필수적인 고품질 테스트 코드를 효과적으로 생성해요.\n버그 재현 및 원인 추적 정밀도: 복잡한 버그의 발생 조건을 분석하고 근본 원인을 찾아내는 데 뛰어납니다.\n\n\n  팁: GPT-5.3 Codex도 코딩 벤치마크에서 77.3%라는 높은 점수를 기록하며 강력한 보조 선택지로 꼽히지만, 대형 프로젝트에서 긴 맥락을 안정적으로 유지하며 작업하는 측면에서는 Claude가 더 신뢰할 만하다는 평가가 지배적입니다. 복잡하고 큰 규모의 프로젝트라면 Claude를 먼저 고려해 보세요!\n2.   금융 분석 및 비즈니스 인텔리전스: Claude Opus 4.6의 특화 영역\n금융 분야는 정말 섬세하고 복잡한 데이터 처리가 필요한 영역이잖아요. 그런데 Claude는 이 분야에서 독보적인 성능을 보여주고 있습니다. 특히 Claude Opus 4.6은 금융 전문가들에게 강력한 도구가 될 것이라고 생각해요. 저도 최근에 재무 보고서 분석에 Claude를 활용해봤는데, 정말 놀라웠어요.\n  Finance Agent 벤치마크 1위!\n복잡한 재무 문서(IR 자료, 애널리스트 보고서 등)의 완벽한 처리 능력\n다중 보고서 간의 심층적인 비교 분석\n현금흐름표, 손익계산서, 대차대조표의 교차 해석 및 인사이트 도출\nClaude for Excel beta 지원: Excel 통합으로 재무 데이터 작업 효율 극대화\n다른 모델들도 물론 재무 데이터를 다룰 수 있지만, 대규모 재무 보고서를 통째로 업로드해서 수많은 세부 조항들 간의 논리적 연결을 추적하고, 나아가 정성적인 리스크 분석까지 수행하는 능력은 Claude Opus 4.6이 단연 최고였습니다. 저는 복잡한 기업공개(IPO) 문서 분석에 활용하면서 시간을 정말 많이 절약할 수 있었어요.\n  주목: Gemini 3.1 Pro가 추상 추론에 강하고, GPT-5.2도 균형 잡힌 분석을 제공하지만, 금융 특화 벤치마크에서는 Claude Opus 4.6이 가장 높은 점수를 기록하며 이 분야의 강자로 자리매김했습니다. 투자를 하시거나 기업 전략을 세우시는 분들이라면 꼭 활용해보세요!\n3.   장문 분석 및 대규모 문서 처리: Claude의 전통적 강점\nClaude는 초기 모델부터 압도적인 컨텍스트 창(context window)이 가장 큰 장점으로 꼽혔습니다. 2026년에도 이 강점은 여전히 유효하고, 오히려 더욱 강력해졌습니다. 방대한 양의 텍스트를 한 번에 처리해야 하는 작업이 많다면 Claude가 아주 좋은 선택지가 될 거예요.\n\n\n\n대규모 연구 논문 수십 편을 한 번에 분석하고 핵심 요약\n100페이지가 넘는 법률 계약서 또는 정책 문서 검토\n수십 개의 보고서를 동시에 비교하고 공통점 및 차이점 도출\n기업 내부 문서들을 기반으로 한 지식베이스 구축\nChatGPT와 Gemini도 물론 문서 처리가 가능하지만, '이 10개 문서에서 공통 리스크 요소만 추출하고, 연도별 변화까지 분석해줘'와 같이 복합적이고 심층적인 질의를 할 때는 Claude가 훨씬 더 안정적이고 정확한 결과를 내놓는다는 것을 저도 여러 번 경험했습니다. 특히 중요한 문서일수록 AI의 안정성은 정말 중요하죠.\n4. ✍️ 창의적 작업: 정교함이 필요할 땐 Claude Sonnet 4.5\n창의적 작업이라고 하면 GPT가 먼저 떠오르실 수도 있습니다. 실제로 GPT-5.2는 빠른 응답 속도, 자연스러운 문체, 실시간 음성 기능, 그리고 Prism 협업 리서치 도구 등을 통해 초안 작성에 매우 유리한 강점을 가지고 있죠. 하지만 '정교함'과 '안정적인 논리 구조'가 핵심이라면 이야기가 달라집니다. 여기서 Claude Sonnet 4.5가 빛을 발합니다.\n  Claude의 창의성, 어떤 면이 특별할까요?\n더 정교한 창작물 생성: 특히 기술 블로그, 전문 리포트, 백서와 같은 형식을 요구하는 작업에서 탁월합니다.\n전문 문서 작성에 강점: 복잡한 정보를 체계적으로 정리하고, 설득력 있는 논리를 전개하는 데 강해요.\n슬라이드 및 애니메이션 생성 최적화: 시각적인 자료 제작을 위한 아이디어 구상과 내용 구성에 도움을 줍니다.\n논리적 구조가 안정적: 길고 복잡한 글에서도 일관된 논리 흐름을 유지합니다.\n제 경험상, 빠른 아이데이션이나 초안 작성은 GPT-5.2에 맡기고, 정밀하고 완성도 높은 기술 문서나 보고서 작성은 Claude Sonnet 4.5에 맡기는 것이 가장 효율적인 조합이었습니다. 마치 빠른 스케치에는 연필을, 최종 작품에는 유화를 쓰는 것과 비슷한 느낌이랄까요?\n5. ⚡ 속도와 비용 효율: 경량 모델의 활약\n모든 작업에 최고 성능의 모델만 사용할 수는 없죠. 특히 속도가 중요하거나 비용 절감이 핵심이라면, 경량화된 모델들을 고려하는 것이 현명합니다. 이 부분에서는 Claude보다는 다른 모델들이 강점을 보입니다.\n모델\n특징\n\n\n\n\nGPT-5.2 Instant\n빠른 응답 속도, 저비용으로 일반적인 작업에 효율적\n\n\nGemini Flash\n가성비 최고, 빠르고 경제적인 선택지\n\n\nClaude Haiku\n경량 모델, 빠른 응답과 효율성 추구\n\n\n\nClaude도 Haiku라는 경량 모델을 가지고 있지만, 범용적인 빠른 처리와 비용 효율성 측면에서는 아직 GPT-5.2 Instant나 Gemini Flash가 더 경쟁력이 있다는 것이 제 생각입니다. 정말 급하거나 가볍게 테스트할 때는 이 모델들을 활용하는 것이 좋아요.\n  번외: 수학 및 추상적 논리 문제: Gemini의 영역\n이 부분은 Claude가 강하기보다는 Gemini가 압도적인 영역이라 잠시 짚고 넘어가고 싶었어요. 복잡한 알고리즘 문제, 수학 증명, 추상 패턴 인식과 같은 순수 추론 영역에서는 Gemini 3.1 Pro가 가장 높은 평균 점수를 기록하고 있습니다. Claude도 물론 강력하지만, 이 분야에서는 Gemini가 근소하게 앞서고 있다는 점을 기억해두시면 좋습니다.\n⚠️ 주의: 복잡한 수학 문제나 고도의 추상적 논리 추론이 필요한 경우, Claude보다는 Gemini 3.1 Pro가 더 신뢰할 수 있는 결과를 제공할 가능성이 높습니다. 목적에 맞는 도구를 선택하는 것이 중요하니까요!\n  종합 벤치마크 요약: 어떤 AI를 선택해야 할까?\n각 모델의 강점을 정리해 봤는데요, 전체적인 그림을 한눈에 보실 수 있도록 요약 표를 만들어봤어요.\n분야별 1위 모델\n분야\n1위 모델\n\n\n\n\n코딩\nClaude Sonnet 4.5\n\n\n금융 분석\nClaude Opus 4.6\n\n\n장문 문서 처리\nClaude Opus 4.6\n\n\n정교한 창의적 문서\nClaude Sonnet 4.5\n\n\n추상 추론\nGemini 3.1 Pro\n\n\n속도\nGPT-5.2 Instant\n\n\n비용 효율\nGemini Flash\n\n\n\n\n  핵심 요약\n1.  ️ 복잡한 코드 개발 및 리팩토링: Claude는 긴 컨텍스트와 심층적인 코드 이해력으로 개발 생산성을 극대화합니다.\n2.   전문 금융 데이터 분석: 대용량 재무 보고서 교차 분석, 리스크 평가 등 금융 분야에 특화된 독보적인 성능을 보여줍니다.\n3.   방대한 장문 문서 처리: 수백 페이지 분량의 논문이나 계약서를 안정적으로 처리하고 복합 질의에 정확히 답변합니다.\n4.   정교하고 논리적인 문서 창작: 기술 블로그, 전문 리포트 등 완성도 높은 글쓰기에 탁월합니다.\n결론적으로 AI 선택은 '무엇을 할 것인가'에 따라 달라집니다. Claude는 특정 전문 영역에서 '전문 작업용 엔진'으로서 진정한 가치를 발휘합니다.\n❓ 자주 묻는 질문 (FAQ)\nQ1: Claude는 모든 면에서 ChatGPT나 Gemini보다 뒤떨어지나요?\n아니요, 그렇지 않습니다. Claude는 특정 전문 분야, 예를 들어 코딩, 금융 분석, 장문 문서 처리, 그리고 정교한 전문 문서 작성에서 ChatGPT나 Gemini보다 훨씬 더 뛰어난 성능과 안정성을 보여줍니다. '모든 것을 평균 이상으로 잘하는 모델'이라기보다는, '특정 전문 영역에서 압도적인 완성도를 제공하는 모델'이라고 이해하는 것이 정확합니다.\nQ2: 개발자라면 무조건 Claude를 사용해야 하나요?\n'무조건'은 아니지만, 복잡한 코드베이스를 다루거나, 대규모 리팩토링, 상세한 테스트 케이스 생성이 필요한 경우에는 Claude Sonnet 4.5나 Opus 4.6이 매우 강력한 선택이 될 것입니다. GPT-5.3 Codex도 훌륭하지만, 긴 코드 맥락 유지나 구조적 이해 측면에서 Claude가 더 안정적이라는 평가가 많습니다. 프로젝트의 규모와 복잡성에 따라 선택하는 것이 좋습니다.\nQ3: 비용 효율적인 작업에는 어떤 모델이 가장 좋나요?\n속도와 비용 효율성이 최우선이라면, GPT-5.2 Instant나 Gemini Flash가 더 좋은 선택이 될 수 있습니다. Claude도 Haiku라는 경량 모델이 있지만, 범용적인 빠른 처리 및 비용 경쟁력 면에서는 아직 앞선 두 모델이 유리하다고 평가됩니다. 가벼운 작업이나 빠른 테스트에는 이러한 경량 모델들을 활용해보세요.\nAI 선택은 결국 여러분의 업무 구조와 목적에 달려 있습니다. 모든 AI가 모든 작업에 완벽할 수는 없어요. 하지만 적절한 도구를 올바른 상황에 사용하는 것이 스마트한 워크플로우를 만드는 핵심이죠.\n만약 여러분이 개발자, 금융 분석가, 기술 블로거, 또는 방대한 자료를 다루는 연구자라면, Claude는 단순한 챗봇을 넘어 '전문 작업용 엔진'에 가깝습니다. 각자의 니즈에 맞춰 AI를 현명하게 활용하시길 바라며, 궁금한 점이 있다면 언제든지 댓글로 남겨주세요! 다음에 더 유익한 정보로 찾아뵙겠습니다.",
        "guid": "https://muzbox.tistory.com/483710",
        "categories": [
          "AI, 미래기술/AI 인사이트",
          "2026 AI 벤치마크",
          "ai 모델 선택",
          "ai 생산성",
          "chatgpt 비교",
          "Claude 강점",
          "Claude 금융 분석",
          "Claude 코딩",
          "Gemini 비교",
          "대규모 문서 처리 AI",
          "생성형 ai 활용"
        ],
        "isoDate": "2026-02-23T02:35:25.000Z"
      }
    ]
  },
  {
    "name": "동우리의 블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "하테나",
    "category": "개인",
    "posts": []
  },
  {
    "name": "늑돌이네 라지온",
    "category": "개인",
    "posts": []
  },
  {
    "name": "루리웹 - 루리웹 리뷰 게시판",
    "category": "게임",
    "posts": [
      {
        "creator": "｜RULIWEB｜",
        "title": "악역영애 4컷 만화는 한 주 쉬어갑니다.",
        "link": "https://bbs.ruliweb.com/news/board/11/read/2422",
        "pubDate": "Wed, 18 Feb 2026 15:04:52 +0900",
        "author": "｜RULIWEB｜",
        "content": "<img width=\"236\" height=\"177\" src=\"https://i3.ruliweb.com/thumb/26/02/18/19c6f59806751ad6b.webp\">",
        "contentSnippet": "",
        "categories": [
          "웹툰"
        ],
        "isoDate": "2026-02-18T06:04:52.000Z"
      }
    ]
  },
  {
    "name": "Reasontobe",
    "category": "개인",
    "posts": []
  },
  {
    "name": "에스티마의 인터넷이야기 EstimaStory.com",
    "category": "개인",
    "posts": []
  },
  {
    "name": "나긋한 개발 - 데비안 리눅스와 프로그램 언어",
    "category": "개인",
    "posts": [
      {
        "creator": "summerandwinter",
        "title": "미국 직구 배송과 관세 이야기",
        "link": "https://sacstory.tistory.com/entry/%EB%AF%B8%EA%B5%AD-%EC%A7%81%EA%B5%AC-%EB%B0%B0%EC%86%A1-%EA%B1%B8%EB%A6%AC%EB%8A%94-%EC%8B%9C%EA%B0%84",
        "pubDate": "Sun, 22 Feb 2026 13:50:43 +0900",
        "author": "summerandwinter",
        "comments": "https://sacstory.tistory.com/entry/%EB%AF%B8%EA%B5%AD-%EC%A7%81%EA%B5%AC-%EB%B0%B0%EC%86%A1-%EA%B1%B8%EB%A6%AC%EB%8A%94-%EC%8B%9C%EA%B0%84#entry399comment",
        "content": "2025년 12월 5일\n미국 켄터키주에서 생산하는 작은 제품 2개를 구매했다.\n&nbsp;\n구매한 제품은 알리익스프레스와 아마존 등 종합 쇼핑몰에서 판매를 하지 않는 제품이라 본사 홈페이지에서 직접 구매했다.\n종합 쇼핑몰에서만 해외 제품을 사봤지, 직접 본사에서 구매하는건 처음이었다.\n&nbsp;\n알리의 배송 옵션처럼 이 제품에도 배송 옵션을 제공했다.\n가장 저렴한 배송도 길어야 14일 걸린다고 표기되어 있었다. 알리와 비슷하겠지... 라고 생각한게 큰..",
        "contentSnippet": "2025년 12월 5일\n미국 켄터키주에서 생산하는 작은 제품 2개를 구매했다.\n \n구매한 제품은 알리익스프레스와 아마존 등 종합 쇼핑몰에서 판매를 하지 않는 제품이라 본사 홈페이지에서 직접 구매했다.\n종합 쇼핑몰에서만 해외 제품을 사봤지, 직접 본사에서 구매하는건 처음이었다.\n \n알리의 배송 옵션처럼 이 제품에도 배송 옵션을 제공했다.\n가장 저렴한 배송도 길어야 14일 걸린다고 표기되어 있었다. 알리와 비슷하겠지... 라고 생각한게 큰..",
        "guid": "https://sacstory.tistory.com/399",
        "categories": [
          "Review"
        ],
        "isoDate": "2026-02-22T04:50:43.000Z"
      }
    ]
  },
  {
    "name": "일상을 여행처럼...",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Just hack'em",
    "category": "개인",
    "posts": []
  },
  {
    "name": "C++ Truths",
    "category": "개인",
    "posts": []
  },
  {
    "name": "jacking75",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Joel on Software",
    "category": "개인",
    "posts": []
  },
  {
    "name": "벤자민로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "악보쓰는 프로그래머",
    "category": "개인",
    "posts": []
  },
  {
    "name": "쭌안아빠",
    "category": "개인",
    "posts": []
  },
  {
    "name": "A Gangster World",
    "category": "개인",
    "posts": []
  },
  {
    "name": "요우의 내맘대로 블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "개발자스럽다",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "Against All Odds.",
    "category": "개인",
    "posts": []
  },
  {
    "name": "움직이는 게임서버",
    "category": "개인",
    "posts": []
  },
  {
    "name": "이상욱",
    "category": "개인",
    "posts": []
  },
  {
    "name": "임철재",
    "category": "개인",
    "posts": []
  },
  {
    "name": "어쩐지 오늘은",
    "category": "개인",
    "posts": []
  },
  {
    "name": "oddpoet’s étude",
    "category": "개인",
    "posts": []
  },
  {
    "name": "0x00 - NULL",
    "category": "개인",
    "posts": []
  },
  {
    "name": "퇴근 후 서버다운",
    "category": "개인",
    "posts": [
      {
        "creator": "SIDNFT",
        "title": "울티마 1 플레이하시는 유튜버 / 미믹",
        "link": "https://serverdown.tistory.com/1576",
        "pubDate": "Wed, 25 Feb 2026 02:32:25 +0900",
        "author": "SIDNFT",
        "comments": "https://serverdown.tistory.com/1576#entry1576comment",
        "content": "<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"431\" data-origin-height=\"261\"><span data-url=\"https://blog.kakaocdn.net/dn/b8Syis/dJMcaaxBceM/urFohtR3hUFN3OgzQrNc51/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/b8Syis/dJMcaaxBceM/urFohtR3hUFN3OgzQrNc51/img.png\"><img src=\"https://blog.kakaocdn.net/dn/b8Syis/dJMcaaxBceM/urFohtR3hUFN3OgzQrNc51/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb8Syis%2FdJMcaaxBceM%2FurFohtR3hUFN3OgzQrNc51%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"431\" height=\"261\" data-origin-width=\"431\" data-origin-height=\"261\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">영상: <a href=\"https://www.youtube.com/watch?v=yGCITMDeGf4\" target=\"_blank\" rel=\"noopener&nbsp;noreferrer\">https://www.youtube.com/watch?v=yGCITMDeGf4</a></p>\n<figure data-ke-type=\"video\" data-ke-style=\"alignCenter\" data-video-host=\"youtube\" data-video-url=\"https://www.youtube.com/watch?v=yGCITMDeGf4\" data-video-thumbnail=\"https://scrap.kakaocdn.net/dn/zaKZH/dJMb8XR2Dkn/SztKM2N4ZCXAfks4SnOYaK/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720,https://scrap.kakaocdn.net/dn/p26p4/dJMb8Zvys0U/ZolsYqPT4JhW5Dke5cXOok/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720,https://scrap.kakaocdn.net/dn/fMROa/dJMb8XkcoV2/EkIf8QKwuDaIQXpoq5AA60/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720\" data-video-width=\"860\" data-video-height=\"484\" data-video-origin-width=\"860\" data-video-origin-height=\"484\" data-ke-mobilestyle=\"widthContent\" data-video-title=\"RPG의 시작 울티마1 공략 #CRPG\" data-original-url=\"\"><iframe src=\"https://www.youtube.com/embed/yGCITMDeGf4\" width=\"860\" height=\"484\" frameborder=\"\" allowfullscreen=\"true\"></iframe>\n<figcaption style=\"display: none;\"></figcaption>\n</figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">와 멋지다.</p>\n<p data-ke-size=\"size16\">모험을 지켜보겠습니다.</p>\n<p data-ke-size=\"size16\">일단 사운드 는 잡음 수준이군요</p>",
        "contentSnippet": "영상: https://www.youtube.com/watch?v=yGCITMDeGf4\n\n\n\n \n와 멋지다.\n모험을 지켜보겠습니다.\n일단 사운드 는 잡음 수준이군요",
        "guid": "https://serverdown.tistory.com/1576",
        "categories": [
          "게임",
          "고전게임",
          "울티마"
        ],
        "isoDate": "2026-02-24T17:32:25.000Z"
      },
      {
        "creator": "SIDNFT",
        "title": "비트코인과 양자 컴퓨터 내성 암호 그리고 둟려버린 SIKE 암호",
        "link": "https://serverdown.tistory.com/1575",
        "pubDate": "Sat, 21 Feb 2026 20:13:36 +0900",
        "author": "SIDNFT",
        "comments": "https://serverdown.tistory.com/1575#entry1575comment",
        "content": "<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"512\" data-origin-height=\"512\"><span data-url=\"https://blog.kakaocdn.net/dn/bFrBOr/dJMcacIUpAz/Hfe3LRQ4B3gnz1YVYBi1l1/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/bFrBOr/dJMcacIUpAz/Hfe3LRQ4B3gnz1YVYBi1l1/img.png\"><img src=\"https://blog.kakaocdn.net/dn/bFrBOr/dJMcacIUpAz/Hfe3LRQ4B3gnz1YVYBi1l1/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbFrBOr%2FdJMcacIUpAz%2FHfe3LRQ4B3gnz1YVYBi1l1%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"512\" height=\"512\" data-origin-width=\"512\" data-origin-height=\"512\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">(비트코인 뚫렸넹)</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">일단 궁금하신 비트코인은 양자 컴퓨터 내성 암호화 방식으로 변경해서 양자컴퓨터를 대응할 계획입니다.</p>\n<p data-ke-size=\"size16\">이전에도 여러번의 비트코인 업데이트가 있었고 이번에도 이 방식으로 해결 할 것입니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">왜 당장 안바꾸나요</h2>\n<p data-ke-size=\"size16\">아래 SIKE 암호과 뚫린 이야기에도 있지만 표준 양자내성암호화가 직정되지 않았습니다.</p>\n<p data-ke-size=\"size16\">섯불리 하나 도입했다가 표준이 다른걸로 선정되면 비트코인의 신뢰도가 하락 합니다.</p>\n<p data-ke-size=\"size16\">그냥 표준이 정해질때 까지 기다린 후에 그걸로 적용하는게 좋습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">양자내성암화로 바꿔도 해야할 일</h2>\n<p data-ke-size=\"size16\">현재의 암호와 방식으로는 양자 컴퓨터가 발전하게 되면 뚫릴 가능성이 있습니다.</p>\n<p data-ke-size=\"size16\">아마도 지갑 주소를 보고 지갑 키를 만들어내는 방식으로 공격하지 않을까 예상됩니다.</p>\n<p data-ke-size=\"size16\">이미 저장된 지갑 주소는 오랜기간 투자하면 결국 찾아낼 것입니다.</p>\n<p data-ke-size=\"size16\">(양자 컴퓨터가 20년 정도 발전한다면 말이죠)</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">양자 내성 암호화가 되더라도 결국 지감을 주기적으로 교체하는 방식은 계속 사용해야 할 것입니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">뚫려버린 SIKE 사태에 대해 알아보자</h2>\n<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"598\" data-origin-height=\"318\"><span data-url=\"https://blog.kakaocdn.net/dn/VGab5/dJMcaducIP9/VMyLnf99SlJUDd6oI7KA21/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/VGab5/dJMcaducIP9/VMyLnf99SlJUDd6oI7KA21/img.png\"><img src=\"https://blog.kakaocdn.net/dn/VGab5/dJMcaducIP9/VMyLnf99SlJUDd6oI7KA21/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FVGab5%2FdJMcaducIP9%2FVMyLnf99SlJUDd6oI7KA21%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"598\" height=\"318\" data-origin-width=\"598\" data-origin-height=\"318\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">(국민 연금 공단은 NPS 다 햇갈리지 말자)</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">참 허무한 방식으로 뚫려버려서 충격이 더 컸으며</p>\n<p data-ke-size=\"size16\">앞으로도 이런일은 벌어지기 어려울 것입니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">사이키(SIKE, Supersingular Isogeny Key Encapsulation) 암호화 방식은</p>\n<p data-ke-size=\"size16\">2022년 7월 5일에 NIST(미국 국립표준기술연구소) 의 양자내성암호화 사업을 시작할때 3개가 선정되었는데 그중에 한개 입니다.</p>\n<p data-ke-size=\"size16\">그리고 2022년 7월 22일 (17일후)에 이 암호화는 못쓴다는 결론이 났습니다.</p>\n<p data-ke-size=\"size16\">이걸로 양자컴퓨터가 유명해졌는데</p>\n<p data-ke-size=\"size16\">알려진 사실로는 이 암호화 방식은 뚫리기 위해 선정된 함정 카드였고</p>\n<p data-ke-size=\"size16\">수악적 허점으로 양자 컴퓨터가 아닌 일반 컴퓨터로 1시간 연산으로 키를 계산할 수 있었다고 합니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">애초에 전성되어선 안될 암호화 방식이였다.</h2>\n<p data-ke-size=\"size16\">제 생각입니다. 양자내성암호화의 중요성을 알리기위해 뚫릴꺼 알고 선정한거 같습니다.</p>\n<p data-ke-size=\"size16\">실제로 이 이슈로 양자내성암호화는 유명해 졌고 암호화 교체를 준비하던 프로젝트들도 암호화를 더 쉽게 변경할 수 있도록 준비하게 되었습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<h2 data-ke-size=\"size26\">나머지 두개는 ?</h2>\n<p data-ke-size=\"size16\">Kyber(ML-KEM)</p>\n<p data-ke-size=\"size16\">Dilithium(ML-DSA)</p>\n<p data-ke-size=\"size16\">두가지 방식이 남아있습니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">두 암호화는 뚫릴꺼 같으면 차원을 증가시켜 필요 양자 큐비트 수를 증가시키는 방식으로 대응하고 있습니다.</p>\n<p data-ke-size=\"size16\">양자컴퓨터 발전으로 큐비트가 늘어나는 속도보다 차원을 더 많이 늘려버리면 방어가 된다는 논리입니다.</p>\n<p data-ke-size=\"size16\">큐피트가 창이라면 방패 수를 훨씬 늘려 버린다는 뜻입니다.<br />(방패 늘리기 쉬운 방식이라는거죠)</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "(비트코인 뚫렸넹)\n \n일단 궁금하신 비트코인은 양자 컴퓨터 내성 암호화 방식으로 변경해서 양자컴퓨터를 대응할 계획입니다.\n이전에도 여러번의 비트코인 업데이트가 있었고 이번에도 이 방식으로 해결 할 것입니다.\n \n왜 당장 안바꾸나요\n아래 SIKE 암호과 뚫린 이야기에도 있지만 표준 양자내성암호화가 직정되지 않았습니다.\n섯불리 하나 도입했다가 표준이 다른걸로 선정되면 비트코인의 신뢰도가 하락 합니다.\n그냥 표준이 정해질때 까지 기다린 후에 그걸로 적용하는게 좋습니다.\n \n \n \n양자내성암화로 바꿔도 해야할 일\n현재의 암호와 방식으로는 양자 컴퓨터가 발전하게 되면 뚫릴 가능성이 있습니다.\n아마도 지갑 주소를 보고 지갑 키를 만들어내는 방식으로 공격하지 않을까 예상됩니다.\n이미 저장된 지갑 주소는 오랜기간 투자하면 결국 찾아낼 것입니다.\n(양자 컴퓨터가 20년 정도 발전한다면 말이죠)\n \n양자 내성 암호화가 되더라도 결국 지감을 주기적으로 교체하는 방식은 계속 사용해야 할 것입니다.\n \n뚫려버린 SIKE 사태에 대해 알아보자\n\n\n(국민 연금 공단은 NPS 다 햇갈리지 말자)\n \n참 허무한 방식으로 뚫려버려서 충격이 더 컸으며\n앞으로도 이런일은 벌어지기 어려울 것입니다.\n \n사이키(SIKE, Supersingular Isogeny Key Encapsulation) 암호화 방식은\n2022년 7월 5일에 NIST(미국 국립표준기술연구소) 의 양자내성암호화 사업을 시작할때 3개가 선정되었는데 그중에 한개 입니다.\n그리고 2022년 7월 22일 (17일후)에 이 암호화는 못쓴다는 결론이 났습니다.\n이걸로 양자컴퓨터가 유명해졌는데\n알려진 사실로는 이 암호화 방식은 뚫리기 위해 선정된 함정 카드였고\n수악적 허점으로 양자 컴퓨터가 아닌 일반 컴퓨터로 1시간 연산으로 키를 계산할 수 있었다고 합니다.\n \n애초에 전성되어선 안될 암호화 방식이였다.\n제 생각입니다. 양자내성암호화의 중요성을 알리기위해 뚫릴꺼 알고 선정한거 같습니다.\n실제로 이 이슈로 양자내성암호화는 유명해 졌고 암호화 교체를 준비하던 프로젝트들도 암호화를 더 쉽게 변경할 수 있도록 준비하게 되었습니다.\n \n나머지 두개는 ?\nKyber(ML-KEM)\nDilithium(ML-DSA)\n두가지 방식이 남아있습니다.\n \n두 암호화는 뚫릴꺼 같으면 차원을 증가시켜 필요 양자 큐비트 수를 증가시키는 방식으로 대응하고 있습니다.\n양자컴퓨터 발전으로 큐비트가 늘어나는 속도보다 차원을 더 많이 늘려버리면 방어가 된다는 논리입니다.\n큐피트가 창이라면 방패 수를 훨씬 늘려 버린다는 뜻입니다.\n(방패 늘리기 쉬운 방식이라는거죠)",
        "guid": "https://serverdown.tistory.com/1575",
        "categories": [
          "코인",
          "비트코인",
          "양자내성암호화",
          "양자컴퓨터"
        ],
        "isoDate": "2026-02-21T11:13:36.000Z"
      },
      {
        "creator": "SIDNFT",
        "title": "유니티 셰이더 그래프 사용해보자 / 강의시장을 어지럽힐 유튜브 영상",
        "link": "https://serverdown.tistory.com/1574",
        "pubDate": "Thu, 19 Feb 2026 15:26:33 +0900",
        "author": "SIDNFT",
        "comments": "https://serverdown.tistory.com/1574#entry1574comment",
        "content": "<p><figure class=\"imageblock alignCenter\" data-ke-mobileStyle=\"widthOrigin\" data-origin-width=\"245\" data-origin-height=\"223\"><span data-url=\"https://blog.kakaocdn.net/dn/GKQqy/dJMcafr2j65/OdP5kQEewPhMjPnXzIQEdK/img.png\" data-phocus=\"https://blog.kakaocdn.net/dn/GKQqy/dJMcafr2j65/OdP5kQEewPhMjPnXzIQEdK/img.png\"><img src=\"https://blog.kakaocdn.net/dn/GKQqy/dJMcafr2j65/OdP5kQEewPhMjPnXzIQEdK/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FGKQqy%2FdJMcafr2j65%2FOdP5kQEewPhMjPnXzIQEdK%2Fimg.png\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\" loading=\"lazy\" width=\"245\" height=\"223\" data-origin-width=\"245\" data-origin-height=\"223\"/></span></figure>\n</p>\n<p data-ke-size=\"size16\">결국 선 넘어버렸군요</p>\n<p data-ke-size=\"size16\">유튜브에 그냥 올라와 버렸습니다.</p>\n<p data-ke-size=\"size16\">강의 시장이 치열하다는 증거 입니다.</p>\n<p data-ke-size=\"size16\">배우기 좋은 타이밍입니다.</p>\n<p data-ke-size=\"size16\">셰이더를 배워봅시다.</p>\n<p data-ke-size=\"size16\">영상: <a href=\"https://www.youtube.com/watch?v=KnueAgpUL3Y&amp;t=237s\" target=\"_blank\" rel=\"noopener&nbsp;noreferrer\">https://www.youtube.com/watch?v=KnueAgpUL3Y&amp;t=237s</a></p>\n<figure data-ke-type=\"video\" data-ke-style=\"alignCenter\" data-video-host=\"youtube\" data-video-url=\"https://www.youtube.com/watch?v=KnueAgpUL3Y\" data-video-thumbnail=\"https://scrap.kakaocdn.net/dn/cVgten/dJMb8T9VYpF/9i9HwfkVnUVWgV7VYQs1Ek/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720,https://scrap.kakaocdn.net/dn/Zw4Ce/dJMb83Sfl35/8WEeZybGHQfqmkqIODP5IK/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720,https://scrap.kakaocdn.net/dn/1kIPc/dJMb8YpRW4I/ibHbB3odbNnze8dWXqpV8k/img.jpg?width=1280&amp;height=720&amp;face=0_0_1280_720\" data-video-width=\"860\" data-video-height=\"484\" data-video-origin-width=\"860\" data-video-origin-height=\"484\" data-ke-mobilestyle=\"widthContent\" data-video-title=\"기적의 셰이더 그래프 \" data-original-url=\"\"><iframe src=\"https://www.youtube.com/embed/KnueAgpUL3Y\" width=\"860\" height=\"484\" frameborder=\"\" allowfullscreen=\"true\"></iframe>\n<figcaption style=\"display: none;\"></figcaption>\n</figure>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">어렵다고 피하지 말고&nbsp;</p>\n<p data-ke-size=\"size16\">봐두세요&nbsp;</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">온갖 용여를 다 설명해줍니다.</p>\n<p data-ke-size=\"size16\">메탈릭, 에미션, 등등</p>\n<p data-ke-size=\"size16\">&nbsp;</p>\n<p data-ke-size=\"size16\">당장은 이해가 안가도 언젠가 갑자기 머리속에서 정리될 것입니다.</p>\n<p data-ke-size=\"size16\">&nbsp;</p>",
        "contentSnippet": "결국 선 넘어버렸군요\n유튜브에 그냥 올라와 버렸습니다.\n강의 시장이 치열하다는 증거 입니다.\n배우기 좋은 타이밍입니다.\n셰이더를 배워봅시다.\n영상: https://www.youtube.com/watch?v=KnueAgpUL3Y&t=237s\n\n\n\n \n어렵다고 피하지 말고 \n봐두세요 \n \n온갖 용여를 다 설명해줍니다.\n메탈릭, 에미션, 등등\n \n당장은 이해가 안가도 언젠가 갑자기 머리속에서 정리될 것입니다.",
        "guid": "https://serverdown.tistory.com/1574",
        "categories": [
          "프로그래밍/개발메모",
          "셰이더",
          "유니티"
        ],
        "isoDate": "2026-02-19T06:26:33.000Z"
      }
    ]
  },
  {
    "name": "coolspeed",
    "category": "개인",
    "posts": []
  },
  {
    "name": "오늘도 끄적끄적",
    "category": "개인",
    "posts": []
  },
  {
    "name": "dx11 Vanica's Lifelog - 夢が夢で終わらないように",
    "category": "개인",
    "posts": []
  },
  {
    "name": "초코사랑",
    "category": "개인",
    "posts": []
  },
  {
    "name": "ZeroCho Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "imays게임엔진개발자",
    "category": "개인",
    "posts": []
  },
  {
    "name": "RSS feed for hurinmon Blog",
    "category": "개인",
    "posts": []
  },
  {
    "name": "기억보단 기록을",
    "category": "개인",
    "posts": [
      {
        "creator": "향로 (기억보단 기록을)",
        "title": "벽돌 쌓기",
        "link": "https://jojoldu.tistory.com/866",
        "pubDate": "Mon, 23 Feb 2026 08:13:00 +0900",
        "author": "향로 (기억보단 기록을)",
        "comments": "https://jojoldu.tistory.com/866#entry866comment",
        "content": "<h1>보이지 않는 벽돌</h1>\n<p>얼마 전에 올린 <a href=\"https://www.youtube.com/watch?v=ewO9thSbbvs\">유튜브 영상</a>에 이런 댓글이 달렸다.</p>\n<blockquote data-ke-style=\"style1\"><p data-ke-size=\"size16\"><span style=\"font-family: 'Noto Serif KR';\"><p>&quot;침대에 누워서 영상을 보다가, 울컥 한 것도 처음이네요.<br>결국 못 참고 집 밖에서 &#39;끅끅&#39; 애써 참아가며 울다가 들어왔습니다.&quot;</p>\n</span></p></blockquote><p>요즘 개발자들 사이에서 <code>불안</code> 이라는 단어가 자주 보인다.<br>공부를 하고는 있는데 이게 맞는 건지 모르겠고, 열심히 하고 있는데 성장하고 있는 건지 느껴지지 않는다.<br>밖에서는 AI 때문에 점점 더 개발자의 자리가 없을거라고, 지금 하고 있는 노력들이 무용지물이 될 거라는 이야기가 계속 돈다.  </p>\n<hr>\n<p><code>빵집</code> 압축 프로그램을 만드신 양병규 개발자님이 이스터에그로 자신의 생각을 숨겨놓은 내용이 있다. </p>\n<p>마침 이 내용이 <a href=\"https://www.clien.net/service/board/park/3518580\">클리앙에도 올라와 있어서</a> 공유하고 싶었다.</p>\n<blockquote data-ke-style=\"style1\"><p data-ke-size=\"size16\"><span style=\"font-family: 'Noto Serif KR';\"><p>한강 강바닥에 벽돌을 쌓는다.<br>강변이 아니라 강바닥이다.<br>헤엄쳐서 내려가서, 벽돌 한 장 놓고 올라온다.<br>나와서 보면 당연히 아무것도 안 보인다.<br>다음 날도 한 장, 그 다음 날도 한 장.<br>사흘이고 열흘이고 한 달이고, 매일매일 벽돌을 쌓아도 수면 위에서는 아무 변화가 없다.</p>\n<p>이쯤 되면 의심이 생긴다.<br>이걸 계속 해야 하나.<br>의미가 있긴 한 건가.<br>그래서 많은 사람들이 포기한다.</p>\n<p>근데 포기하지 않고 계속 쌓으면, 열흘이고 한 달이고 1년이고 2년이고, 언젠가는 그 벽돌이 수면 위로 올라온다.<br><strong>그때부터는 벽돌 한 장을 쌓으면 한 장이 쌓인 게 보인다.</strong><br>내 눈에만 보이는 게 아니라 남들 눈에도 보인다.<br>그리고 더 중요한 건, 그때부터 재밌어진다는 거다.<br>쌓는 대로 올라가는 게 보이니까.</p>\n</span></p></blockquote><p>대학교 4학년 2학기부터 5학년 2학기까지 1년 반을 취업 준비했다.<br>SI 회사에 들어가서도 매일 6시에 일어나서 8시 전에 출근해서 한 시간이라도 공부했다.<br>이직한 뒤에도 계속 공부했다.<br>회사에서 잘하지 못했으니까.<br>그때는 내가 성장하고 있는 건지 아닌 건지 알 수가 없었다.<br>방향도 못 잡겠고, 옆에 있는 개발자들에 비해서 부족한 건 분명했고.  </p>\n<p>근데 계속 쌓았다.<br>그러다 어느 날부터 사람들이 나보고 잘한다고 하더라.<br><strong>그때부터 공부한 만큼 실력이 올라가는 게 느껴지기 시작했다</strong>.<br><strong>한번 그게 느껴지니까, 안 해도 될 때도 하게 됐다</strong>.<br>재밌으니까.</p>\n<p>댓글 중에 공감되는 이야기가 있었다.</p>\n<blockquote data-ke-style=\"style1\"><p data-ke-size=\"size16\"><span style=\"font-family: 'Noto Serif KR';\"><p>&quot;사람마다 수심이 다르다는 말 듣자마자 마음 속에 울림이 생기고 다시 용기가 생겼습니다.&quot;</p>\n</span></p></blockquote><p>우리가 회빙환 웹툰 주인공처럼 눈앞에 스탯 창이 떠서 경험치가 쌓이는 게 보이면 아무도 포기 안 할 거다.<br>올라가는 게 보이니까.<br>근데 현실은 그렇지 않다.<br>눈에 보이지 않는다.<br>그럼에도 <strong>보이지 않는데 쌓이는 건 분명히 있다</strong>.  </p>\n<p>각자가 빠져 있는 수심이 다르다.<br>누군가는 3년 만에 벽돌이 수면 위로 올라오고, 누군가는 5년, 누군가는 7년이 걸릴 수도 있다.</p>\n<p><strong>지금 쌓고 있는 벽돌은 보이지 않지만, 사라지지도 않는다.</strong></p>\n<p>그러니까 오늘도 벽돌 한 장 쌓으면 된다.<br>안 보여도 괜찮다.<br>분명 쌓이고 있다.</p>",
        "contentSnippet": "보이지 않는 벽돌\n얼마 전에 올린 유튜브 영상에 이런 댓글이 달렸다.\n\n\"침대에 누워서 영상을 보다가, 울컥 한 것도 처음이네요.\n결국 못 참고 집 밖에서 '끅끅' 애써 참아가며 울다가 들어왔습니다.\"\n\n요즘 개발자들 사이에서 불안 이라는 단어가 자주 보인다.\n공부를 하고는 있는데 이게 맞는 건지 모르겠고, 열심히 하고 있는데 성장하고 있는 건지 느껴지지 않는다.\n밖에서는 AI 때문에 점점 더 개발자의 자리가 없을거라고, 지금 하고 있는 노력들이 무용지물이 될 거라는 이야기가 계속 돈다.  \n빵집 압축 프로그램을 만드신 양병규 개발자님이 이스터에그로 자신의 생각을 숨겨놓은 내용이 있다. \n마침 이 내용이 클리앙에도 올라와 있어서 공유하고 싶었다.\n\n한강 강바닥에 벽돌을 쌓는다.\n강변이 아니라 강바닥이다.\n헤엄쳐서 내려가서, 벽돌 한 장 놓고 올라온다.\n나와서 보면 당연히 아무것도 안 보인다.\n다음 날도 한 장, 그 다음 날도 한 장.\n사흘이고 열흘이고 한 달이고, 매일매일 벽돌을 쌓아도 수면 위에서는 아무 변화가 없다.\n이쯤 되면 의심이 생긴다.\n이걸 계속 해야 하나.\n의미가 있긴 한 건가.\n그래서 많은 사람들이 포기한다.\n근데 포기하지 않고 계속 쌓으면, 열흘이고 한 달이고 1년이고 2년이고, 언젠가는 그 벽돌이 수면 위로 올라온다.\n그때부터는 벽돌 한 장을 쌓으면 한 장이 쌓인 게 보인다.\n내 눈에만 보이는 게 아니라 남들 눈에도 보인다.\n그리고 더 중요한 건, 그때부터 재밌어진다는 거다.\n쌓는 대로 올라가는 게 보이니까.\n\n대학교 4학년 2학기부터 5학년 2학기까지 1년 반을 취업 준비했다.\nSI 회사에 들어가서도 매일 6시에 일어나서 8시 전에 출근해서 한 시간이라도 공부했다.\n이직한 뒤에도 계속 공부했다.\n회사에서 잘하지 못했으니까.\n그때는 내가 성장하고 있는 건지 아닌 건지 알 수가 없었다.\n방향도 못 잡겠고, 옆에 있는 개발자들에 비해서 부족한 건 분명했고.  \n근데 계속 쌓았다.\n그러다 어느 날부터 사람들이 나보고 잘한다고 하더라.\n그때부터 공부한 만큼 실력이 올라가는 게 느껴지기 시작했다.\n한번 그게 느껴지니까, 안 해도 될 때도 하게 됐다.\n재밌으니까.\n댓글 중에 공감되는 이야기가 있었다.\n\n\"사람마다 수심이 다르다는 말 듣자마자 마음 속에 울림이 생기고 다시 용기가 생겼습니다.\"\n\n우리가 회빙환 웹툰 주인공처럼 눈앞에 스탯 창이 떠서 경험치가 쌓이는 게 보이면 아무도 포기 안 할 거다.\n올라가는 게 보이니까.\n근데 현실은 그렇지 않다.\n눈에 보이지 않는다.\n그럼에도 보이지 않는데 쌓이는 건 분명히 있다.  \n각자가 빠져 있는 수심이 다르다.\n누군가는 3년 만에 벽돌이 수면 위로 올라오고, 누군가는 5년, 누군가는 7년이 걸릴 수도 있다.\n지금 쌓고 있는 벽돌은 보이지 않지만, 사라지지도 않는다.\n그러니까 오늘도 벽돌 한 장 쌓으면 된다.\n안 보여도 괜찮다.\n분명 쌓이고 있다.",
        "guid": "https://jojoldu.tistory.com/866",
        "categories": [
          "생각정리",
          "개발자",
          "벽돌쌓기",
          "빵집",
          "양병규",
          "취업",
          "커리어"
        ],
        "isoDate": "2026-02-22T23:13:00.000Z"
      }
    ]
  },
  {
    "name": "WestwoodForever's Dev Log",
    "category": "개인",
    "posts": []
  },
  {
    "name": "허니몬(Honeymon)의 자바guru",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Wolf Loves Fox :: 일상",
    "category": "개인",
    "posts": []
  },
  {
    "name": "Game Programmer Life",
    "category": "개인",
    "posts": []
  },
  {
    "name": "yuchi's dev",
    "category": "개인",
    "posts": []
  },
  {
    "name": "만화로 나누는 자유/오픈소스 소프트웨어 이야기",
    "category": "개인",
    "posts": []
  },
  {
    "name": "신현석(Hyeonseok Shin)",
    "category": "개인",
    "posts": []
  },
  {
    "name": "즐거운 개발자 :: 네이버  블로그",
    "category": "개인",
    "posts": []
  },
  {
    "name": "황제펭귄의 게임개발이야기 [여기는 한국]",
    "category": "개인",
    "posts": []
  },
  {
    "name": "LINE ENGINEERING",
    "category": "기업",
    "posts": [
      {
        "title": "LINE DEV AI 리포터즈의 여정을 공유합니다!",
        "link": "https://techblog.lycorp.co.jp/ko/introducing-the-journey-of-the-line-dev-ai-reporters",
        "pubDate": "Mon, 23 Feb 2026 01:30:00 GMT",
        "content": "요즘은 \"AI 써보셨어요?\"라는 질문이 더 이상 특별하게 느껴지지 않습니다. 이미 많은 개발자들이 각자의 방식으로 ChatGPT나 Claude Code 같은 AI 도구를 업무에 활...",
        "contentSnippet": "요즘은 \"AI 써보셨어요?\"라는 질문이 더 이상 특별하게 느껴지지 않습니다. 이미 많은 개발자들이 각자의 방식으로 ChatGPT나 Claude Code 같은 AI 도구를 업무에 활...",
        "guid": "https://techblog.lycorp.co.jp/ko/introducing-the-journey-of-the-line-dev-ai-reporters",
        "isoDate": "2026-02-23T01:30:00.000Z"
      }
    ]
  },
  {
    "name": "뱅크샐러드 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "우아한형제들 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "TOAST Meetup",
    "category": "기업",
    "posts": [
      {
        "title": "읽지 않는 코드의 시대",
        "link": "https://meetup.nhncloud.com/posts/408",
        "pubDate": "Mon, 23 Feb 2026 00:20:28 GMT",
        "content": "[![NHN Cloud_meetup banner_coding_202602_900.png](https://image.toast.com/aaaadh/real/2026/techblog/NHN%20Cloudmeetup%20bannercoding202602900.png)](https://www.nhncloud.com/kr)\r\r\n\r\r\n>  본 콘텐츠는 작성자가 사내 게시판에 공유한 글을 가공한 것으로, 작성자의 의도와 맥락을 충실히 전달하기 위해 원문의 문장 스타일을 그대로 유지하였습니다.\r\r\n\r\r\n\r\r\n---\r\r\n<br>\r\r\n\r\r\n나는 오랫동안 아름다운 코드를 꿈꿔 왔다.\r\r\n\r\r\n순수 함수들이 물 흐르듯 합성되고, 타입 시스템이 버그를 원천적으로 불가능하게 만들며, 수학적 증명처럼 견고한 로직이 펼쳐지는 그런 코드. 우리는 그것을 '장인 정신'이라 불렀고, 그 경지에 도달하기 위해 범주론을 공부하고, 타입 레벨 프로그래밍을 이해하려 씨름했다.\r\r\n\r\r\n그러나 어느 날 문득 깨달았다. 더 이상 아무도 코드를 읽지 않는다는 것을.\r\r\n<br>\r\r\n## 가독성의 정의가 바뀌다\r\r\n'가독성'이라는 단어의 의미가 조용히, 그러나 근본적으로 변하고 있다.\r\r\n과거의 가독성은 **인간의 인지**를 위한 것이었다.\r\r\n\r\r\n* 개발자가 로직을 머릿속에서 추적할 수 있는가\r\r\n* 동료가 코드 리뷰에서 오류를 발견할 수 있는가\r\r\n* 6개월 후의 내가 이 코드를 이해할 수 있는가\r\r\n\r\r\n우리는 이 질문들에 답하기 위해 클린 코드(Clean Code)를 논했고, SOLID 원칙을 세웠으며, 디자인 패턴이라는 공통 언어를 만들었다. 모두 인간의 제한된 작업 기억 용량 안에서 복잡성을 다루기 위한 몸부림이었다.\r\r\n그러나 이제 가독성은 기계의 패턴 인식을 위한 것이 되어 가고 있다.\r\r\n\r\r\n* AI가 이 코드의 패턴을 학습 데이터에서 본 적 있는가\r\r\n* AI가 수정 요청을 받았을 때 정확한 위치를 찾을 수 있는가\r\r\n* AI가 로컬 변경을 가했을 때 전체 시스템이 깨지지 않는가\r\r\n\r\r\n이 두 가지 가독성은 때로 겹치지만, 본질적으로 다른 것을 최적화한다. 인간을 위한 가독성은 **추상화와 압축**을 추구한다. 반복을 제거하고, 패턴을 이름 붙이며, 복잡성을 캡슐화한다. 기계를 위한 가독성은 **명시성과 예측 가능성**을 추구한다. 관습을 따르고, 구조를 일정하게 유지하며, 장황하고 지저분해지더라도 암묵적인 것을 명시적으로 드러낸다.\r\r\n<br>\r\r\n## 장인 정신의 비극적 위치\r\r\n\r\r\n함수형 프로그래밍이 약속했던 것은 명확했다. 인간의 인지적 한계를 코드로 극복하는 것.\r\r\n참조 투명성은 코드의 어떤 부분이든 독립적으로 추론할 수 있게 해주었다. 불변성은 시간에 따른 상태 변화를 머릿속에서 추적하는 부담을 덜어주었다. 강력한 타입 시스템은 컴파일러가 우리 대신 오류를 잡아주었다. 이 모든 것이 **인간의 한계를 보완**하기 위한 도구였다.\r\r\n\r\r\n그러나 AI에게는 보완할 한계가 없다.\r\r\n\r\r\nAI는 수백만 개의 코드베이스를 학습했다. 패턴 매칭의 원시적 힘으로 무장한 AI에게, 인간의 인지 부하를 줄여주는 우아한 추상화는 그저 노이즈에 가깝다. AI는 똑같은 CRUD 작업의 만 가지 변형을 보았다. 심혈을 기울여 작성한 우아한 모나드 트랜스포머 스택보다 평범하고 반복적인 명령형 코드가 AI에게는 더 익숙하다.\r\r\n여기서 장인 정신의 딜레마가 시작된다.\r\r\n\r\r\n함수형 프로그래밍의 지지자들은 수십 년간 같은 말을 해왔다.\r\r\n\r\r\n\"배우기는 어렵지만, 장기적 이점이 있습니다.\"\r\r\n\r\r\n그런데 AI가 명령형 프로그래밍의 진입 장벽을 무너뜨려버렸다. 이제 누구나 AI의 도움으로 명령형 코드를 빠르게 작성할 수 있다. 반면 함수형 프로그래밍의 학습 곡선은 여전히 가파르다.\r\r\n더욱 쓸쓸한 것은 함수형 프로그래밍의 아름다움을 감상할 줄 아는 사람들마저 더 이상 코드를 직접 읽지 않는다는 사실이다. 그들의 Claude Code가 대신 읽는다. 감상할 눈이 사라진 예술이 무슨 의미가 있을까.\r\r\n\r\r\n```\r\r\nClaude >> @PureFunctional OrderService::doStuff의 로직을 분석해서 설명해 줘\r\r\n```\r\r\n<br>\r\r\n## 매개체로 전락한 코드\r\r\n\r\r\n이제 코드의 존재론적 지위가 바뀌고 있다.\r\r\n과거에 코드는 인간의 영역이었다. 우리는 그 안에서 살았다. 매일 읽고, 고치고, 확장했다. 변수명 하나에 고민하고, 함수의 위치를 두고 토론했다. 코드는 우리의 생각이 물질화된 것이었고, 그래서 우리는 그것의 아름다움에 신경 썼다.\r\r\n이제 코드는 인간의 의도와 기계의 실행 사이를 잇는 매개체가 되어 가고 있다.\r\r\n\r\r\n```\r\r\n인간의 의도 → 자연어 명세 → AI → 코드(누가 신경이나 쓰는가) → 실행\r\r\n```\r\r\n\r\r\n이 모델에서 함수형 vs 객체지향 논쟁은 x86 vs ARM 논쟁과 비슷해진다. 특정 상황에서 성능 차이가 있을 수 있다. 그러나 그것은 더 이상 인간이 거주하는 층위가 아니다.\r\r\n우리는 집을 짓는 목수에서 집을 주문하는 건축주가 되어 가고 있다. 목수에게 나뭇결은 단순한 무늬가 아니다. 그것은 나무의 강도와 방향을 말해주고, 어디를 깎고 어디를 살려야 할지 알려주며, 완성된 작품이 세월을 어떻게 견딜지를 예언한다. 그러나 건축주는 문이 제대로 닫히면 그만이다.\r\r\n<br>\r\r\n## 새로운 미학의 등장\r\r\n\r\r\n그렇다면 AI 시대의 '좋은 코드'란 무엇인가? 새로운 미학이 필요하다.\r\r\n\r\r\n### 관용적 표현이 최적화를 이긴다\r\r\n\r\r\n```\r\r\n// AI 친화적: 수백만 번 본 패턴\r\r\nusers.stream().filter(User::isActive).toList();\r\r\n\r\r\n// AI 비친화적: 이게 뭐 하는 코드지?\r\r\nusers.stream().reduce(new ArrayList<>(),\r\r\n    (acc, u) -> { if(u.isActive()) acc.add(u); return acc; },\r\r\n    (a, b) -> { a.addAll(b); return a; });\r\r\n```\r\r\n\r\r\nAI는 익숙한 것을 잘 다룬다. 창의적인 최적화보다 평범한 관용구가 더 안전하다.\r\r\n\r\r\n### 추론의 지역성\r\r\n\r\r\n함수를 이해하는 데 필요한 모든 것이 눈에 보이거나, 한 번의 점프로 도달 가능해야 한다. Action at a distance는 금물이다. Dependency injection 트릭, aspect weaving, runtime proxy—이 모든 암묵적 메커니즘이 AI의 추론을 방해한다.\r\r\n\r\r\n### 예측 가능한 구조\r\r\n\r\r\n```\r\r\n/order\r\r\n  OrderController.java\r\r\n  OrderService.java\r\r\n  OrderRepository.java\r\r\n  Order.java\r\r\n```\r\r\n\r\r\nAI는 관습에서 맥락을 추론한다. 파일이 어디 있을지 예측할 수 있으면, 탐색 비용이 줄어든다. 관습은 압축된 정보다.\r\r\n\r\r\n### 명시적 상태 전이\r\r\n\r\r\n```\r\r\n// AI 친화적: 가능한 상태가 명시적으로 열거됨\r\r\nenum Status { DRAFT, SUBMITTED, FULFILLED }\r\r\n\r\r\n// AI 비친화적: 플래그 조합으로 상태를 유추해야 함\r\r\nboolean isSubmitted;\r\r\nboolean isFulfilled;\r\r\nboolean isDraft;\r\r\n```\r\r\n\r\r\n상태가 열거형으로 선언되어 있으면 AI는 전체 그림을 한눈에 파악한다. 여러 boolean 플래그를 조합해서 상태를 유추해야 하는 코드는 AI도 인간처럼 길을 잃는다. `isSubmitted`와 `isFulfilled`가 동시에 true면 무슨 상태인가?\r\r\n\r\r\n### 장황하지만 명확한 이름\r\r\n\r\r\n```\r\r\n// AI 친화적: 이름만 봐도 의도가 명확\r\r\npublic boolean isEligibleForRefundBasedOnPurchaseDateAndMembershipStatus()\r\r\n\r\r\n// AI 비친화적: 주석에 의존\r\r\n/** 구매일과 멤버십 상태에 따라 환불 가능 여부를 판단한다 */\r\r\npublic boolean canRefund()\r\r\n```\r\r\n\r\r\n인간에게는 간결한 이름과 상세한 주석이 읽기 좋을 수 있다. 그러나 AI는 주석보다 코드를 신뢰한다. 함수명 자체가 의도를 담고 있으면, AI는 별도의 맥락 없이도 정확하게 해당 함수를 활용할 수 있다.\r\r\n\r\r\n### 작은 파일, 단일 책임\r\r\n\r\r\n100줄짜리 파일 10개가 1000줄짜리보다 낫다. AI의 컨텍스트 윈도우는 실질적 제약이다. 작은 파일은 전체를 교체하기도, 부분을 수정하기도 쉽다.\r\r\n\r\r\n### 명세로서의 테스트\r\r\n\r\r\n```\r\r\n@Test\r\r\nvoid shouldRejectOrderWhenInventoryInsufficient() { ... }\r\r\n```\r\r\n\r\r\nAI는 테스트를 읽고 코드의 의도를 역으로 파악한다. 테스트 이름이 곧 요구 사항이 되고, 테스트 본문이 곧 예제가 된다.\r\r\n<br>\r\r\n## 아이러니: 함수형의 귀환\r\r\n\r\r\n흥미로운 반전이 있다.\r\r\nAI 친화적 코드의 원칙들—불변성, 명시적 상태, 작고 순수한 함수—은 함수형 프로그래밍의 원칙과 상당 부분 겹친다.\r\r\n함수형 프로그래밍은 살아남을 것이다. 그것이 아름다워서가 아니라, **기계가 읽기 좋아서**.\r\r\n\r\r\n고차원적 추상화와 이론적 우아함은 사라질 것이다. 그러나 불변 데이터, 순수 함수, 명시적 타입은 남을 것이다. 미학은 바뀌었지만, 핵심 원칙들은 다른 이유로 생명을 얻었다. 이것이 함수형 프로그래밍 애호가들에게 위안이 될지는 모르겠다. 사랑하는 것이 살아남았지만, 사랑 받는 이유가 완전히 달라졌으니.\r\r\n<br>\r\r\n## 결론\r\r\n\r\r\n우리는 아름다운 시대의 끝자락에 서 있다.\r\r\n코드를 매개로 인간과 인간이 소통하던 시대. 변수명 하나에 의도를 담고, 함수의 구조로 사고의 흐름을 표현하던 시대. 동료의 코드를 읽으며 그의 사고방식을 이해하고, 때로는 감탄하던 시대.\r\r\n\r\r\n그 시대가 저물고 있다.\r\r\n코드는 점점 인간의 눈을 거치지 않는 영역으로 이동하고 있다. 우리가 코드 안에서 살았기 때문에 그 아름다움에 신경 썼듯이, 우리가 코드 밖으로 나가면 그 아름다움은 의미를 잃는다. 장인 정신은 LP 레코드나 기계식 시계처럼—효율보다 과정을 사랑하는 이들의 영역으로 남게 될 것이다. 시장은 우아함을 알아봐주지 않는다. 시장은 속도를 원한다. 그리고 AI가 명령형 코드의 속도를 거의 무한히 빠르게 만들어 버렸다.\r\r\n\r\r\n그래도 우리는 가끔, 퇴근 후 조용한 사무실에서 마침내 완성한 순수 함수의 체인이 테스트를 통과하며 초록불이 켜지던 순간을 기억할 것이다. 타입 시스템이 버그를 컴파일 타임에 잡아주었을 때의 희열을. 코드가 단지 작동하는 것을 넘어, 그 자체로 하나의 증명이 되었을 때의 만족을.\r\r\n\r\r\n아름다운 추억이다.\r\r\n\r\r\n---\r\r\n\r\r\n>  본 콘텐츠는 작성자가 사내 게시판에 공유한 글을 가공한 것으로, 작성자의 의도와 맥락을 충실히 전달하기 위해 원문의 문장 스타일을 그대로 유지하였습니다.\r\r\n\r\r\n\r\r\n[![NHN Cloud_meetup banner_footer_202507-01.png](https://image.toast.com/aaaadh/real/2026/techblog/NHN%20Cloudmeetup%20bannerfooter20250701.png)](https://www.nhncloud.com/kr)",
        "contentSnippet": "[![NHN Cloud_meetup banner_coding_202602_900.png](https://image.toast.com/aaaadh/real/2026/techblog/NHN%20Cloudmeetup%20bannercoding202602900.png)](https://www.nhncloud.com/kr)\r\r\n\r\r\n>  본 콘텐츠는 작성자가 사내 게시판에 공유한 글을 가공한 것으로, 작성자의 의도와 맥락을 충실히 전달하기 위해 원문의 문장 스타일을 그대로 유지하였습니다.\r\r\n\r\r\n\r\r\n---\r\r\n\r\r\n\r\r\n나는 오랫동안 아름다운 코드를 꿈꿔 왔다.\r\r\n\r\r\n순수 함수들이 물 흐르듯 합성되고, 타입 시스템이 버그를 원천적으로 불가능하게 만들며, 수학적 증명처럼 견고한 로직이 펼쳐지는 그런 코드. 우리는 그것을 '장인 정신'이라 불렀고, 그 경지에 도달하기 위해 범주론을 공부하고, 타입 레벨 프로그래밍을 이해하려 씨름했다.\r\r\n\r\r\n그러나 어느 날 문득 깨달았다. 더 이상 아무도 코드를 읽지 않는다는 것을.\r\r\n\r\r\n## 가독성의 정의가 바뀌다\r\r\n'가독성'이라는 단어의 의미가 조용히, 그러나 근본적으로 변하고 있다.\r\r\n과거의 가독성은 **인간의 인지**를 위한 것이었다.\r\r\n\r\r\n* 개발자가 로직을 머릿속에서 추적할 수 있는가\r\r\n* 동료가 코드 리뷰에서 오류를 발견할 수 있는가\r\r\n* 6개월 후의 내가 이 코드를 이해할 수 있는가\r\r\n\r\r\n우리는 이 질문들에 답하기 위해 클린 코드(Clean Code)를 논했고, SOLID 원칙을 세웠으며, 디자인 패턴이라는 공통 언어를 만들었다. 모두 인간의 제한된 작업 기억 용량 안에서 복잡성을 다루기 위한 몸부림이었다.\r\r\n그러나 이제 가독성은 기계의 패턴 인식을 위한 것이 되어 가고 있다.\r\r\n\r\r\n* AI가 이 코드의 패턴을 학습 데이터에서 본 적 있는가\r\r\n* AI가 수정 요청을 받았을 때 정확한 위치를 찾을 수 있는가\r\r\n* AI가 로컬 변경을 가했을 때 전체 시스템이 깨지지 않는가\r\r\n\r\r\n이 두 가지 가독성은 때로 겹치지만, 본질적으로 다른 것을 최적화한다. 인간을 위한 가독성은 **추상화와 압축**을 추구한다. 반복을 제거하고, 패턴을 이름 붙이며, 복잡성을 캡슐화한다. 기계를 위한 가독성은 **명시성과 예측 가능성**을 추구한다. 관습을 따르고, 구조를 일정하게 유지하며, 장황하고 지저분해지더라도 암묵적인 것을 명시적으로 드러낸다.\r\r\n\r\r\n## 장인 정신의 비극적 위치\r\r\n\r\r\n함수형 프로그래밍이 약속했던 것은 명확했다. 인간의 인지적 한계를 코드로 극복하는 것.\r\r\n참조 투명성은 코드의 어떤 부분이든 독립적으로 추론할 수 있게 해주었다. 불변성은 시간에 따른 상태 변화를 머릿속에서 추적하는 부담을 덜어주었다. 강력한 타입 시스템은 컴파일러가 우리 대신 오류를 잡아주었다. 이 모든 것이 **인간의 한계를 보완**하기 위한 도구였다.\r\r\n\r\r\n그러나 AI에게는 보완할 한계가 없다.\r\r\n\r\r\nAI는 수백만 개의 코드베이스를 학습했다. 패턴 매칭의 원시적 힘으로 무장한 AI에게, 인간의 인지 부하를 줄여주는 우아한 추상화는 그저 노이즈에 가깝다. AI는 똑같은 CRUD 작업의 만 가지 변형을 보았다. 심혈을 기울여 작성한 우아한 모나드 트랜스포머 스택보다 평범하고 반복적인 명령형 코드가 AI에게는 더 익숙하다.\r\r\n여기서 장인 정신의 딜레마가 시작된다.\r\r\n\r\r\n함수형 프로그래밍의 지지자들은 수십 년간 같은 말을 해왔다.\r\r\n\r\r\n\"배우기는 어렵지만, 장기적 이점이 있습니다.\"\r\r\n\r\r\n그런데 AI가 명령형 프로그래밍의 진입 장벽을 무너뜨려버렸다. 이제 누구나 AI의 도움으로 명령형 코드를 빠르게 작성할 수 있다. 반면 함수형 프로그래밍의 학습 곡선은 여전히 가파르다.\r\r\n더욱 쓸쓸한 것은 함수형 프로그래밍의 아름다움을 감상할 줄 아는 사람들마저 더 이상 코드를 직접 읽지 않는다는 사실이다. 그들의 Claude Code가 대신 읽는다. 감상할 눈이 사라진 예술이 무슨 의미가 있을까.\r\r\n\r\r\n```\r\r\nClaude >> @PureFunctional OrderService::doStuff의 로직을 분석해서 설명해 줘\r\r\n```\r\r\n\r\r\n## 매개체로 전락한 코드\r\r\n\r\r\n이제 코드의 존재론적 지위가 바뀌고 있다.\r\r\n과거에 코드는 인간의 영역이었다. 우리는 그 안에서 살았다. 매일 읽고, 고치고, 확장했다. 변수명 하나에 고민하고, 함수의 위치를 두고 토론했다. 코드는 우리의 생각이 물질화된 것이었고, 그래서 우리는 그것의 아름다움에 신경 썼다.\r\r\n이제 코드는 인간의 의도와 기계의 실행 사이를 잇는 매개체가 되어 가고 있다.\r\r\n\r\r\n```\r\r\n인간의 의도 → 자연어 명세 → AI → 코드(누가 신경이나 쓰는가) → 실행\r\r\n```\r\r\n\r\r\n이 모델에서 함수형 vs 객체지향 논쟁은 x86 vs ARM 논쟁과 비슷해진다. 특정 상황에서 성능 차이가 있을 수 있다. 그러나 그것은 더 이상 인간이 거주하는 층위가 아니다.\r\r\n우리는 집을 짓는 목수에서 집을 주문하는 건축주가 되어 가고 있다. 목수에게 나뭇결은 단순한 무늬가 아니다. 그것은 나무의 강도와 방향을 말해주고, 어디를 깎고 어디를 살려야 할지 알려주며, 완성된 작품이 세월을 어떻게 견딜지를 예언한다. 그러나 건축주는 문이 제대로 닫히면 그만이다.\r\r\n\r\r\n## 새로운 미학의 등장\r\r\n\r\r\n그렇다면 AI 시대의 '좋은 코드'란 무엇인가? 새로운 미학이 필요하다.\r\r\n\r\r\n### 관용적 표현이 최적화를 이긴다\r\r\n\r\r\n```\r\r\n// AI 친화적: 수백만 번 본 패턴\r\r\nusers.stream().filter(User::isActive).toList();\r\r\n\r\r\n// AI 비친화적: 이게 뭐 하는 코드지?\r\r\nusers.stream().reduce(new ArrayList(),\r\r\n    (acc, u) -> { if(u.isActive()) acc.add(u); return acc; },\r\r\n    (a, b) -> { a.addAll(b); return a; });\r\r\n```\r\r\n\r\r\nAI는 익숙한 것을 잘 다룬다. 창의적인 최적화보다 평범한 관용구가 더 안전하다.\r\r\n\r\r\n### 추론의 지역성\r\r\n\r\r\n함수를 이해하는 데 필요한 모든 것이 눈에 보이거나, 한 번의 점프로 도달 가능해야 한다. Action at a distance는 금물이다. Dependency injection 트릭, aspect weaving, runtime proxy—이 모든 암묵적 메커니즘이 AI의 추론을 방해한다.\r\r\n\r\r\n### 예측 가능한 구조\r\r\n\r\r\n```\r\r\n/order\r\r\n  OrderController.java\r\r\n  OrderService.java\r\r\n  OrderRepository.java\r\r\n  Order.java\r\r\n```\r\r\n\r\r\nAI는 관습에서 맥락을 추론한다. 파일이 어디 있을지 예측할 수 있으면, 탐색 비용이 줄어든다. 관습은 압축된 정보다.\r\r\n\r\r\n### 명시적 상태 전이\r\r\n\r\r\n```\r\r\n// AI 친화적: 가능한 상태가 명시적으로 열거됨\r\r\nenum Status { DRAFT, SUBMITTED, FULFILLED }\r\r\n\r\r\n// AI 비친화적: 플래그 조합으로 상태를 유추해야 함\r\r\nboolean isSubmitted;\r\r\nboolean isFulfilled;\r\r\nboolean isDraft;\r\r\n```\r\r\n\r\r\n상태가 열거형으로 선언되어 있으면 AI는 전체 그림을 한눈에 파악한다. 여러 boolean 플래그를 조합해서 상태를 유추해야 하는 코드는 AI도 인간처럼 길을 잃는다. `isSubmitted`와 `isFulfilled`가 동시에 true면 무슨 상태인가?\r\r\n\r\r\n### 장황하지만 명확한 이름\r\r\n\r\r\n```\r\r\n// AI 친화적: 이름만 봐도 의도가 명확\r\r\npublic boolean isEligibleForRefundBasedOnPurchaseDateAndMembershipStatus()\r\r\n\r\r\n// AI 비친화적: 주석에 의존\r\r\n/** 구매일과 멤버십 상태에 따라 환불 가능 여부를 판단한다 */\r\r\npublic boolean canRefund()\r\r\n```\r\r\n\r\r\n인간에게는 간결한 이름과 상세한 주석이 읽기 좋을 수 있다. 그러나 AI는 주석보다 코드를 신뢰한다. 함수명 자체가 의도를 담고 있으면, AI는 별도의 맥락 없이도 정확하게 해당 함수를 활용할 수 있다.\r\r\n\r\r\n### 작은 파일, 단일 책임\r\r\n\r\r\n100줄짜리 파일 10개가 1000줄짜리보다 낫다. AI의 컨텍스트 윈도우는 실질적 제약이다. 작은 파일은 전체를 교체하기도, 부분을 수정하기도 쉽다.\r\r\n\r\r\n### 명세로서의 테스트\r\r\n\r\r\n```\r\r\n@Test\r\r\nvoid shouldRejectOrderWhenInventoryInsufficient() { ... }\r\r\n```\r\r\n\r\r\nAI는 테스트를 읽고 코드의 의도를 역으로 파악한다. 테스트 이름이 곧 요구 사항이 되고, 테스트 본문이 곧 예제가 된다.\r\r\n\r\r\n## 아이러니: 함수형의 귀환\r\r\n\r\r\n흥미로운 반전이 있다.\r\r\nAI 친화적 코드의 원칙들—불변성, 명시적 상태, 작고 순수한 함수—은 함수형 프로그래밍의 원칙과 상당 부분 겹친다.\r\r\n함수형 프로그래밍은 살아남을 것이다. 그것이 아름다워서가 아니라, **기계가 읽기 좋아서**.\r\r\n\r\r\n고차원적 추상화와 이론적 우아함은 사라질 것이다. 그러나 불변 데이터, 순수 함수, 명시적 타입은 남을 것이다. 미학은 바뀌었지만, 핵심 원칙들은 다른 이유로 생명을 얻었다. 이것이 함수형 프로그래밍 애호가들에게 위안이 될지는 모르겠다. 사랑하는 것이 살아남았지만, 사랑 받는 이유가 완전히 달라졌으니.\r\r\n\r\r\n## 결론\r\r\n\r\r\n우리는 아름다운 시대의 끝자락에 서 있다.\r\r\n코드를 매개로 인간과 인간이 소통하던 시대. 변수명 하나에 의도를 담고, 함수의 구조로 사고의 흐름을 표현하던 시대. 동료의 코드를 읽으며 그의 사고방식을 이해하고, 때로는 감탄하던 시대.\r\r\n\r\r\n그 시대가 저물고 있다.\r\r\n코드는 점점 인간의 눈을 거치지 않는 영역으로 이동하고 있다. 우리가 코드 안에서 살았기 때문에 그 아름다움에 신경 썼듯이, 우리가 코드 밖으로 나가면 그 아름다움은 의미를 잃는다. 장인 정신은 LP 레코드나 기계식 시계처럼—효율보다 과정을 사랑하는 이들의 영역으로 남게 될 것이다. 시장은 우아함을 알아봐주지 않는다. 시장은 속도를 원한다. 그리고 AI가 명령형 코드의 속도를 거의 무한히 빠르게 만들어 버렸다.\r\r\n\r\r\n그래도 우리는 가끔, 퇴근 후 조용한 사무실에서 마침내 완성한 순수 함수의 체인이 테스트를 통과하며 초록불이 켜지던 순간을 기억할 것이다. 타입 시스템이 버그를 컴파일 타임에 잡아주었을 때의 희열을. 코드가 단지 작동하는 것을 넘어, 그 자체로 하나의 증명이 되었을 때의 만족을.\r\r\n\r\r\n아름다운 추억이다.\r\r\n\r\r\n---\r\r\n\r\r\n>  본 콘텐츠는 작성자가 사내 게시판에 공유한 글을 가공한 것으로, 작성자의 의도와 맥락을 충실히 전달하기 위해 원문의 문장 스타일을 그대로 유지하였습니다.\r\r\n\r\r\n\r\r\n[![NHN Cloud_meetup banner_footer_202507-01.png](https://image.toast.com/aaaadh/real/2026/techblog/NHN%20Cloudmeetup%20bannerfooter20250701.png)](https://www.nhncloud.com/kr)",
        "isoDate": "2026-02-23T00:20:28.000Z"
      }
    ]
  },
  {
    "name": "ZUM 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "SK Planet",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Spoqa tech blog",
    "category": "기업",
    "posts": []
  },
  {
    "name": "팀 왈도 번역팀",
    "category": "게임",
    "posts": []
  },
  {
    "name": "근원님",
    "category": "개인",
    "posts": []
  },
  {
    "name": "황의윤",
    "category": "개인",
    "posts": []
  },
  {
    "name": "호돌맨",
    "category": "개인",
    "posts": []
  },
  {
    "name": "박우빈",
    "category": "개인",
    "posts": []
  },
  {
    "name": "문다영",
    "category": "개인",
    "posts": []
  },
  {
    "name": "유수민",
    "category": "개인",
    "posts": []
  },
  {
    "name": "안건주",
    "category": "개인",
    "posts": []
  },
  {
    "name": "손현호",
    "category": "개인",
    "posts": []
  },
  {
    "name": "STARTUP BIBLE",
    "category": "개인",
    "posts": [
      {
        "creator": "Kihong Bae",
        "title": "용기가 복리처럼 불어날 때",
        "link": "https://www.thestartupbible.com/2026/02/when-courage-compounds.html",
        "pubDate": "Sun, 22 Feb 2026 21:24:00 +0000",
        "content:encodedSnippet": "지난 2년은 대부분의 창업가에겐 20년 같이 느껴졌을 정도로 길고, 힘들었을 것이다. 우리 투자사 중에서도 남들보다 월등하게 잘하는 곳도 있지만, 이런 회사들은 아웃라이어이고, 대부분 정말 배고프고, 춥고, 스트레스 가득 차고, 하루가 이틀이었으면 하는 바람으로 창업가라는 왕관의 무게를 버텼다. 이 중 이젠 사라진 스타트업도 꽤 있고, 잘 살아남아서 이제 다시 성장의 준비를 하는 곳들도 있지만, 대부분 아직 데미지에서 서서히 회복하고 있는 것 같다.\n하지만, 힘든 시기를 겪으면서 이제 바닥에서 서서히 회복하고 있는 창업가분들과 이야기를 해보면 여전히 전반적인 분위기와 기조는 긍정적이어서 나에게는 희망이 보인다. 원래 내가 아는 좋은 창업가들은 고비를 잘 참고, 포기를 잘 모르기 때문에 어려운 상황에서도 이런 초긍정 태도를 유지할 수 있다고 생각하는 반면 나 같으면 저렇게 어려운 상황에서 계속 고개를 들고 현실을 직시하면서, 매일매일 크고 작은 불을 태연하게 끌 수 있을지 스스로에게 항상 물어본다. 최근에 이런 창업가들을 보고 머릿속에서 떠오르는 하나의 단어는 바로 ‘용기’라는 단어다. 어떻게 이 사람들은 절망적인 순간에 이런 대단한 용기를 보여줄 수 있을까.\n이런 창업가 몇 분과 이야기를 해보고, 여러 가지 기사와 팟캐스트를 들으면서 나만의 개똥 답안이 완성되긴 했다. 이들에겐 몇 가지 공통점이 있었다. 사업을 하면서 힘들었던 순간들이 기억할 수 없을 정도로 많았지만, 그중 기억나는 최악의 상황이 대부분 몇 번 있었다고 한다. 이런 최악의 상황이 닥쳤을 때, 이들이 최악의 상황 속에서 봤던 또 다른 이면은, 바로 이 최악의 상황이 “모든 게 다 괜찮을 거야”라고 할 정도로 희망적이진 않았지만, 실제로 걱정했던 것처럼 정말로 죽을 정도는 아니었다는 점이다. 그리고 죽을 정도는 아니라는 것을 느끼는 순간, 항상 했던 것처럼 최선을 다해 열심히 하다 보면 또 어떻게 길을 찾고, 잘 극복해서 살아 남는 경우가 꽤 많았다는 것이다.\n최악이라고, 정말 망할 수 있겠다고, 정말 죽을 수 있겠다고 생각했던 그 순간을 어찌어찌해서 살아 남으면, 여기서 말콤 글래드웰이 정의한 용기가 작용하는 것 같다. 글래드웰이 정의한 용기에 대한 포스팅은 여기서 확인할 수 있는데, 요약하자면 다음과 같다.\n“힘든 상황에서 자신을 용감하게 만드는 용기는 선천적인 게 아니다. 굉장히 힘든 상황을 극복했는데, 되돌아보니 이 상황이 생각만큼 힘들지 않았다고 느낄 때, 그때 후천적으로 습득하는 게 용기이다.”\n\n\n\n\n스타트업을 하다보면, 위에서 말 한 최악의 상황이 계속 발생한다. 한 번의 죽을 고비를 넘기면, 또 다른 고비가 오고, 고비가 올 때마다 실은 그 심각함과 나쁜 정도는 배가된다. 고비가 올 때마다 이번엔 정말 끝이라는 걱정을 하지만, 어찌어찌 죽지 않고 그 상황을 극복하고, 상황을 극복하고 뒤 돌아보면 또 그렇게 죽을 정도는 아니었다고 생각한다. 바로 이 생각이 몸에 학습되면서 용기가 생긴다. 그리고 이 과정을 반복하다 보면 용기에도 복리가 적용된다.\n이렇게 용기에 복리가 적용되면 초인이 된다. 슈퍼맨 같은 초인이 아니라 어려움과 장애물에 초인적인 태도를 보일 수 있는, 그런 강하고 용기있는 초인 말이다.\n힘들다. 바쁘다. 피곤하다. 어쩔 땐 정말 죽을 것 같다. 나도 이런 기분이 드는데, 초기 스타트업 창업가는 오죽하랴. 하지만, 내일 하루 더 싸우기 위해서 오늘 죽지 말고 버티자. 버티면 죽지 않을 것이고, 그럴 때마다 용기가 복리로 쌓일 것이다.",
        "dc:creator": "Kihong Bae",
        "comments": "https://www.thestartupbible.com/2026/02/when-courage-compounds.html#respond",
        "content": "지난 2년은 대부분의 창업가에겐 20년 같이 느껴졌을 정도로 길고, 힘들었을 것이다. 우리 투자사 중에서도 남들보다 월등하게 잘하는 곳도 있지만, 이런 회사들은 아웃라이어이고, 대부분 정말 배고프고, 춥고, 스트레스 가득 차고, 하루가 이틀이었으면 하는 바람으로 창업가라는 왕관의 무게를 버텼다. 이 중 이젠 사라진 스타트업도 꽤 있고, 잘 살아남아서 이제 다시 성장의 준비를 하는 곳들도 있지만, 대부분 아직(...)",
        "contentSnippet": "지난 2년은 대부분의 창업가에겐 20년 같이 느껴졌을 정도로 길고, 힘들었을 것이다. 우리 투자사 중에서도 남들보다 월등하게 잘하는 곳도 있지만, 이런 회사들은 아웃라이어이고, 대부분 정말 배고프고, 춥고, 스트레스 가득 차고, 하루가 이틀이었으면 하는 바람으로 창업가라는 왕관의 무게를 버텼다. 이 중 이젠 사라진 스타트업도 꽤 있고, 잘 살아남아서 이제 다시 성장의 준비를 하는 곳들도 있지만, 대부분 아직(...)",
        "guid": "https://www.thestartupbible.com/?p=9693",
        "categories": [
          "Uncategorized",
          "compounding",
          "failure",
          "FoundersAtWork",
          "hustle",
          "inspiring",
          "Strong"
        ],
        "isoDate": "2026-02-22T21:24:00.000Z"
      },
      {
        "creator": "Kihong Bae",
        "title": "참 어려운 상장 시장",
        "link": "https://www.thestartupbible.com/2026/02/public-markets-are-hard-for-vcs.html",
        "pubDate": "Wed, 18 Feb 2026 21:29:00 +0000",
        "content:encodedSnippet": "우리는 쿠팡의 매우 작은 주주이다. 스트롱이 쿠팡에 직접 투자하진 않았지만, 스트로의 포트폴리오였던 Recomio라는 회사가 쿠팡에 주식교환으로 인수되면서 쿠팡의 주주가 됐고, 상장 주식을 대부분 팔았지만 아직 소량을 보유하고 있다. 우리 1호 펀드의 포트폴리오인데 이미 이 펀드의 나이는 14살이 되어 간다. 청산 시점이 지났기 때문에 우리의 LP들이 지난 몇 년 동안 꾸준히 물어봤던 질문이 남은 쿠팡 주식 처분 시점이다.\n이분들에게 나의 한결같은 답변은 “한국에 살면 직접 체감할 수 있는데, 쿠팡은 한국의 이커머스를 완전히 다 먹어버리고 있습니다. 제가 보기엔 현재 쿠팡 주가는(당시 $30 ~ $35 정도) 저평가되어 있기 때문에 가격이 더 오르면 팔 건데 그게 최소 $50 선이 되지 않을까 싶습니다.” 였다. 실은 매우 확신에 찬 자신 있는 답변이었다. 솔직히 겉으로는 $50라고 했지만, 내 마음속엔 쿠팡은 $100까지 간다고 믿고 있었다.\n그런데 이젠 솔직히 잘 모르겠다. 개인정보가 유출됐을 때만 해도 잘 해결하면 된다고 생각했는데, 이후에 쿠팡이 보여준 태도와 중요한 순간에 했던 결정들의 질이 상당히 실망스러웠고, 그 결과 때문에 현재 쿠팡 주식은 $20를 하회하고 있다. 역시 어떤 LP 분들은 나에게 과연 쿠팡 주식이 앞으로 $50 가 될 수 있을지 물어보고, 더 나아가서는 우리의 포트폴리오사가 상장하면, 스트롱은 어떤 기준으로 상장 주식을 처리할지 물어보는 분들도 있다.\n이 질문에 대해서 나는 몇 주 동안 시간 날 때마다 곰곰이 생각해 보고 있는데, 내 결론은 상장 시장은 참 어려운 시장이고, 비상장 회사에 투자하는 VC에겐 – 특히나 우리 같이 극초기 회사에 투자하는 – 제3외국어와 같이 이해하고 배우기 어렵다는 것이다. 엄밀히 말하면 쿠팡 사업 자체의 펀더멘탈이 무너져서 주가가 하락한다고 볼 순 없다. 쿠팡의 펀더멘탈은 실은 매우 강하다. 이 정도로 물류 인프라를 잘 운영하고, 이 정도로 세상 온갖 제품을 잘 판매하는 회사는 한국에 없고, 전 세계에도 몇 개 없을 정도로 사업은 잘 한다. 물론, 그 물류와 이커머스 사업의 인프라를 비인간적으로 운영하기도 하지만, 어쨌든 사업 자체만 봤을 땐 굉장히 탄탄한 회사다. 주가가 폭락하는 이유는 시장의 정서와 감정의 문제이고, 흔히 이 바닥에서 말하는 FUDGE(Fear, Uncertainty, Doubt, Greed, Emotion)가 크게 작용하고 있다.\n이번에 미국에서 만난 어떤 투자자는 중국의 VC에 오래전에 투자했는데, 이 VC가 초기 투자했던 중국 회사가 상장하면서 당시의 시총에 의하면 거의 2,000배의 돈을 (서류상으로) 벌었다고 했다. 하지만, 이 VC는 마치 내가 쿠팡에 대해서 확신했듯이, 그 회사의 펀더멘탈이 강하고 시장이 더 성장할 것이기 때문에 몇 년 후엔 더 많은 돈을 벌 수 있을 것이라는 자신감으로 상장 주식을 안 팔고 계속 보유했다고 한다. 그래서 4년을 더 보유했는데, 그동안 부침이 있었지만, 현재 가격은 4년 전과 똑같다고 한다. 서류상으로 돈을 크게 잃진 않았지만, 이 VC에 투자한 LP들은 4년 동안 실제 배분받은 건 한 푼도 없었고, 막대한 기회비용이 발생해서 굉장히 불만이 많은 것 같다. 실은 이 회사는 그동안 매출은 많이 증가했지만, 중국 정부의 규제와 방향에 대한 불확실성으로 인해서 주가는 제자리걸음을 하고 있었다.\n상장 시장은 비상장 시장과 많이 다르다. 남들은 비상장 시장이 비이성적이라고 하는데, 나는 오히려 상장 시장이 더 비이성적인 것 같다. 논리보단 감정, 두려움과 욕심이 주가를 움직이는 시장인데, 솔직히 생각해 보면 이 세상이 비이성적이라서 이게 고스란히 상장 시장에 전체적으로 반영되는 것 같다.\n쿠팡 사태로 내가 배운 점은 – 그리고 아직도 이 배움은 계속 되고 있다 – 우리가 투자한 회사가 상장하게 되면, 너무 욕심부리지 말고 적당한 시점에 파는 게 어쩌면 VC들에겐 더 맞는 전략일 수도 있다는 것이다. 계속 보유하면서 상장 시장을 공부하고 예측하는 방법도 있지만, 위에서 말한 여러 가지 요소 때문에 상장 시장을 분석하고 예측하는 건 우리 같은 초기 VC들이 잘할 수 있는 게임은 아닌 것 같다.",
        "dc:creator": "Kihong Bae",
        "comments": "https://www.thestartupbible.com/2026/02/public-markets-are-hard-for-vcs.html#respond",
        "content": "우리는 쿠팡의 매우 작은 주주이다. 스트롱이 쿠팡에 직접 투자하진 않았지만, 스트로의 포트폴리오였던 Recomio라는 회사가 쿠팡에 주식교환으로 인수되면서 쿠팡의 주주가 됐고, 상장 주식을 대부분 팔았지만 아직 소량을 보유하고 있다. 우리 1호 펀드의 포트폴리오인데 이미 이 펀드의 나이는 14살이 되어 간다. 청산 시점이 지났기 때문에 우리의 LP들이 지난 몇 년 동안 꾸준히 물어봤던 질문이 남은 쿠팡 주식(...)",
        "contentSnippet": "우리는 쿠팡의 매우 작은 주주이다. 스트롱이 쿠팡에 직접 투자하진 않았지만, 스트로의 포트폴리오였던 Recomio라는 회사가 쿠팡에 주식교환으로 인수되면서 쿠팡의 주주가 됐고, 상장 주식을 대부분 팔았지만 아직 소량을 보유하고 있다. 우리 1호 펀드의 포트폴리오인데 이미 이 펀드의 나이는 14살이 되어 간다. 청산 시점이 지났기 때문에 우리의 LP들이 지난 몇 년 동안 꾸준히 물어봤던 질문이 남은 쿠팡 주식(...)",
        "guid": "https://www.thestartupbible.com/?p=9690",
        "categories": [
          "Uncategorized",
          "exit",
          "general",
          "ipo",
          "korea",
          "strategy",
          "Strong"
        ],
        "isoDate": "2026-02-18T21:29:00.000Z"
      }
    ]
  },
  {
    "name": "매거진 입맛",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "요즘 IT",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "쿠팡 엔지니어링",
    "category": "기업",
    "posts": []
  },
  {
    "name": "지마켓 기술 블로그",
    "category": "기업",
    "posts": []
  },
  {
    "name": "리멤버 (드라마 앤 컴퍼니)",
    "category": "기업",
    "posts": []
  },
  {
    "name": "Kimchi hill",
    "category": "큐레이션",
    "posts": []
  },
  {
    "name": "Hudi.blog",
    "category": "개인",
    "posts": [
      {
        "title": "안정 해시 (Consistent Hash)",
        "link": "https://hudi.blog/consistent-hash/",
        "pubDate": "Sat, 21 Feb 2026 23:30:00 GMT",
        "content:encodedSnippet": "해결하고자 하는 문제 상황\n\n      \n    \n  \n  \n    \n데이터를 샤딩하는 상황을 생각해보자. 보편적으로 샤드키를 모듈로 연산하여 만들어낸 해시를 사용할 것이다. 예를 들자면, 노드가 총 N개인 상황에서 nodeIndex = hash(key) % N 와 같이 연산하여, 해당 데이터가 배치될 노드를 식별할 것이다. N이 고정된 상황에서는 문제가 없을 것이다.\n문제는 N값이 변경되는 상황이다. 분산 시스템을 설계하는 입장에서는 시스템이 실패하는 상황까지 반드시 고려해야한다. 즉, 일부 노드가 실패해 N이 기존보다 작아질 수 있다. 물론 성능 향상을 위해 노드를 추가로 투입해 N이 커질수도 있다.\nN값이 변경되면 어떤 문제가 발생할까? hash(key) % N 의 결과는 대부분 달라질 것이고, 대부분의 데이터의 노드를 재배치 해야하는 일이 발생할 것이다. 아래 표를 한번 보자.\nhash(key)\n% 5\n% 4\n변경 여부\n\n\n\n\n11\n1\n3\nO\n\n\n22\n2\n2\nX\n\n\n33\n3\n1\nO\n\n\n44\n4\n0\nO\n\n\n55\n0\n3\nO\n\n\n66\n1\n2\nO\n\n\n\nN = 5 일때와 N = 4 일때, 하나의 데이터를 제외하고는 모두 노드 재배치가 발생한 것을 볼 수 있다. 이는 아래와 같은 큰\b비용을 낳는다.\n대규모 데이터 이동 및 복제 비용 발생\n(캐시라면) 대규모 캐시 미스 발생\n즉, 한마디로 노드가 추가/삭제 되었을 때 불필요하게 대부분의 키를 재배치 하게 되는 문제가 발생하는 것이다. 이번 포스팅에서 알아볼 안정 해시 (Consistent Hash) 는 노드가 추가/삭제 되었을 때, 일부 키만 이동하도록 만들어 재샤딩 비용과 대규모 캐시 미스 문제를 해결하기 위한 방법 중 하나이다.\n\n안정 해시 (Consistent Hash)\n안정 해시는 위와 다르게, 노드가 추가/삭제 되었을 때 평균적으로 k/nk/nk/n 개의 키만 재배치하는 기술이다 (이때, kkk 는 키의 개수, nnn 은 노드 수).\n\n해시 링 (Hash Ring)\n해시 링은 안정 해시에서 사용되는 핵심 자료구조이다. 해시 링을 이해하기 위해서는 먼저 해시 공간 (Hash Space) 에 대해 이해하고 넘어가야한다. 해시 함수로 SHA-1 을 사용한다면, SHA-1 에 임의 데이터를 입력한다면 출력값 범위는 0부터 2160−12^{160} -12160−1 이다. 이 범위가 SHA-1 해시 함수의 해시 공간이라고 할 수 있다. 좀 더 일반화 하자면, 해시 공간은 해시 함수의 출력값 범위인 x0 부터 xn 까지의 범위를 말한다고 할 수 있다.\n\n      \n    \n  \n  \n    \n이제 해시 공간을 둥글게 말아 양 끝이 만나는 모양으로 만들어보자. 아래와 같이 해시 공간의 양 끝인 x0 와 xn 가 만나는 원형 모양이 만들어진다. 이를 해시 링 (Hash Ring) 이라고 한다.\n\n      \n    \n  \n  \n    \n\n해시 링 위에 서버와 키 배치하기\n자, 이제 이 준비된 해시 링 위에 서버를 배치할 것이다. 해시 함수를 사용하여 서버를 해싱한 값에 해당하는 해시 링 위치에 서버를 배치한다. \nserverHash = hash(nodeId)\n\n그리고 이어서 키(Key)도 링 위에 배치해보자. 마찬가지로 아래와 같이 해시 함수를 사용하여 반환된 값에 해당하는 해시 링 위치에 키를 배치한다.\nkeyHash = hash(key)\n\n\n      \n    \n  \n  \n    \n그럼 이런 형태로 서버(s0 ~ s3)와 키(k0 ~ k3) 가 해시 링 위에 배치된 모습을 상상해볼 수 있다. 그렇다면 키는 어떤 서버에 배치되는 것일까? \n\n      \n    \n  \n  \n    \n키는 자신으로부터 시계 방향으로 링을 탐색해 나가면서 만나는 첫번째 서버에 저장된다. 즉, 위 그림처럼 k0 -> s0 , k1 -> s1 , k2 -> s2 , k3 -> s3 와 같이 키가 배치되는 것이다.\n\n서버가 추가/제거 되었을 때\n\n      \n    \n  \n  \n    \n이렇게 해시 링 위에 서버와 키를 배치하고, 각 키는 시계방향으로 탐색해서 처음 마주하게 되는 서버에 배치하면 어떤 점이 유리할까? 위 그림처럼 해시 링 위에서 (1) 노드가 추가되거나 (2) 노드가 제거될 때, 해당 노드와 연관된 최소한의 키만 재배치 된다는 점이다. \n위 그림처럼 s5 서버가 추가되더라도 기존에 s0 에 배치된 k0 만 재배치 되었고, s2 서버가 제거되더라도 기존에 s2 에 배치된 k2 키만 s3 로 재배치 되었다. 나머지 k1 과 k3 키는 재배치 없이 그대로 유지된 모습을 확인할 수 있다.\n\n한계와 보완\n위 설계에는 2가지 한계점이 있다.\n파티션 크기가 균등하지 않다.\n키의 분포가 균등하지 않다.\n이 문제를 하나씩 살펴보자.\n\n한계점 1. 파티션 크기가 분등하지 않다\n\n      \n    \n  \n  \n    \n파티션이란 인접한 서버 사이의 해시 공간을 의미한다. 해시 링 위에 서버를 배치하는 것은 서버 식별자의 해시값 기준으로 수행되기 때문에, 위와 같이 파티션 크기의 불균등이 발생할 수 있다. 위 상황에서는 s0 에 배치되는 키보다 s1 에 배치되는 키의 수가 훨씬 많을 것이다. 즉 s1 의 부하가 훨씬 클 것이다.\n\n한계점 2. 키의 분포가 균등하지 않다\n\n      \n    \n  \n  \n    \n파티션 크기만 균등하다고 문제가 해결되지는 않는다. 해시 링 위의 키 위치 또한 해시값 기준으로 배치되기 때문에, 위와 같이 해시 공간의 한쪽으로 키 위치가 쏠리게 된다면, 특정 서버가 부하를 과도하게 부담할 수 있다. 위의 경우에는 s1 이 모든 부하를 감당하게 될 것이다.\n\n보완 : 가상 노드 (Virtual Node) 도입\n이 문제를 해결하기 위해 가상 노드를 도입할 수 있다. 가상 노드란 실제 노드를 가리키는 노드이다. 일종의 원본 노드에 대한 심볼릭 링크라고 생각하면 이해가 쉬울 것이다. 만일 특정 키가 실제 노드가 아닌 가상 노드를 가리키게 된다면, 그 키는 실제로는 가상 노드가 가리키는 원본 서버에 배치된다. 예를 들어, 아래 그림에서 s0_1 은 실제로는 s0 를 가리키는 가상 노드이며, k0 이 s0_1 가상 노드에 배치되었으므로, 실제로는 s0 서버에 배치되는 것이다.\n\n      \n    \n  \n  \n    \n이런 가상 노드를 무수히 늘리게 되면, 앞서 살펴본 파티션 크기와 키의 분포의 불균등을 해소할 수 있는데, 그 원리는 아래와 같다.\n가상 노드를 도입하기 이전에는 하나의 노드가 하나의 큰 파티션을 담당했지만, 가상 노드를 도입하면 여러개의 노드가 작은 파티션 구간 여러곳을 분산하여 담당하게 된다.\n그 결과 각 가상 노드가 담당하는 파티션 크기의 분산이 줄어들게 되며, 키 분포의 편차도 평균화된다.\n즉, 노드 별 담당 파티션과 노드별 키 개수의 표준 편차가 작아져, 데이터가 고르게 분포된다.\n단, 이는 Trade-off 인데, 가상 노드의 개수가 늘어날수록 표준 편차는 줄어들 수 있으나, 그만큼 가상 노드를 저장할 공간이 더 많이 필요하게 되기 때문이다.\n\nReferences\n가상 면접 사례로 배우는 대규모 시스템 설계 기초",
        "content": "해결하고자 하는 문제 상황  데이터를 샤딩하는 상황을 생각해보자. 보편적으로 샤드키를 모듈로 연산하여 만들어낸 해시를 사용할 것이다. 예를 들자면, 노드가 총 N개인 상황에서  와 같이 연산하여, 해당 데이터가 배치될 노드를 식별할 것이다. N…",
        "contentSnippet": "해결하고자 하는 문제 상황  데이터를 샤딩하는 상황을 생각해보자. 보편적으로 샤드키를 모듈로 연산하여 만들어낸 해시를 사용할 것이다. 예를 들자면, 노드가 총 N개인 상황에서  와 같이 연산하여, 해당 데이터가 배치될 노드를 식별할 것이다. N…",
        "guid": "https://hudi.blog/consistent-hash/",
        "isoDate": "2026-02-21T23:30:00.000Z"
      }
    ]
  },
  {
    "name": "토스",
    "category": "기업",
    "posts": [
      {
        "title": "토스, ‘2026년 인디게임 데브캠프’ 협력기업 참여",
        "link": "https://toss.im/tossfeed/article/45501",
        "pubDate": "Mon, 23 Feb 2026 11:10:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-uswsmm{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;margin:24px 0 8px;padding:0;list-style:none;}.css-uswsmm ul,.css-uswsmm ol{margin:16px 0 0;}.css-uswsmm>li{margin-bottom:16px;padding-left:24px;}.css-uswsmm>li:last-of-type{margin-bottom:0;}.css-uswsmm>li>span{position:relative;}.css-uswsmm>li>span>:first-child::before{content:'•';font-weight:500;color:var(--adaptiveGrey800);position:absolute;left:-24px;}\n.css-1hwiibq{font-size:17px;line-height:1.6;word-break:keep-all;letter-spacing:0em;font-weight:400;color:var(--adaptiveGrey800);}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}.css-1kxrhf3{white-space:pre-wrap;}콘진원과 함께 인디게임 사업화글로벌 진출 지원\n‘앱인토스’ 기반 HTML5 게임 성장 생태계 강화\nIT 업계 상생 모델 주도…“창작자와 함께 성장하는 건강한 디지털 생태계 만들 것”\n.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n– 토스(운영사 비바리퍼블리카, 대표 이승건)가 유망 인디게임 프로젝트 발굴 및 지원을 위한 단계별 경쟁 선발 프로그램 ‘2026년 인디게임 데브캠프’에 협력기업으로 참여한다고 밝혔다. 초기 창업 기업과 예비창업자의 사업화 난관을 해소하고, 국내 인디게임의 경쟁력 강화를 지원하기 위해서다.\n‘2026년 인디게임 데브캠프’는 문화체육관광부(장관 최휘영)와 한국콘텐츠진흥원(원장직무대행 유현석, 이하 콘진원)이 인디게임 산업의 경쟁력 강화와 대한민국 게임산업의 성장 체력 확보를 목표로 추진하는 사업이다. 창의적 아이디어를 보유한 초기 창업 기업과 예비창업자를 발굴해 단계별 경쟁 선발을 거쳐 개발, 사업화, 투자 연계까지 체계적으로 지원한다.\n게임업계에서는 참신한 기획력과 기술력을 갖추고도 자금 부족, 마케팅 역량 미비, 글로벌 네트워크 한계 등으로 인해 실제 출시와 안정적인 서비스 운영 단계까지 이어지지 못하는 사례가 꾸준히 발생하고 있다. 이에 토스는 인디게임 기업이 직면한 구조적 제약을 보완하고, 실질적인 시장 안착을 지원하는 데 주력할 계획이다.\n특히 토스는 HTML5 기반 게임사의 기술적·경영적 한계 극복과 사업화 역량 강화를 핵심 과제로 삼는다. 지원사업 참여 기업을 대상으로 이용자 확보를 위한 마케팅을 체계적으로 지원하고, HTML5 기업 간 네트워킹을 통해 산업 내 협업 기반을 확대할 방침이다. 또한 사업성이 우수한 게임에 대해서는 투자 가능성도 적극 검토한다. 이를 통해 인디게임의 실질적인 사업화 성과 창출과 이용자 저변 확대를 견인한다는 전략이다.\n앞서 토스는 지난해 12월 열린 ‘코리아 인디게임 쇼케이스 2025’에서 국내 HTML5 게임 저변 확대를 지원하기 위해 콘진원과 MOU를 체결한 바 있다. 이번 참여는 당시 협약의 연장선에서 인디게임 생태계 지원을 한층 구체화한 행보다.\n토스는 IT 업계의 상생과 동반성장을 주도하며 실질적인 성과를 이어가고 있다. 토스의 미니앱 플랫폼 ‘앱인토스’는 지난 7월 정식 출시 이후 약 7개월 만에 제휴 미니앱 수 1,000개를 돌파했다. 토스 앱 내에서 첫 미니앱을 선보인 후 약 10개월 만에 거둔 성과다.\n특히 ‘게임’ 분야는 전체 미니앱의 약 50%를 차지하며 ‘앱인토스’ 초기 성장을 이끈 핵심 카테고리로 자리 잡았다. 별도의 앱 설치 없이 즉시 실행 가능한 구조 덕분에 이용자 편의성이 높고, 이는 게임 개발자의 수요와 맞물려 성공 사례로 이어지고 있다. 실제로 게임 ‘돌돌디’를 개발한 ‘마나바바’는 ‘앱인토스’ 제휴 이후 월 매출 2억 1,000만 원을 돌파하며 경영 위기를 극복하고 재도약의 발판을 마련했다.\n파트너사의 서비스 지속성 측면에서도 성과가 나타났다. 지난 10개월간 ‘앱인토스’와 제휴한 파트너사 중 95%가 현재까지 서비스를 운영 중인 것으로 집계됐다. 이는 초기 사용자 확보와 마케팅 부담 완화가 창업 초기 기업의 생존율 제고에 긍정적으로 작용한 결과로 분석된다. ‘앱인토스’에서는 게임 출시 이후 필요한 마케팅 솔루션을 무상으로 지원해 개발사가 콘텐츠 완성도 제고에 집중할 수 있도록 돕고 있다.\n토스 관계자는 “토스는 혁신적인 아이디어를 가진 인디게임 개발사가 시장에 안착하고 글로벌 무대로 확장할 수 있도록 실질적인 지원을 아끼지 않을 것”이라며 “앞으로도 사용자 중심 철학을 바탕으로 창작자와 함께 성장하는 건강한 디지털 생태계를 만들어가겠다”고 밝혔다.",
        "content": "문체부와 콘진원 사업 일환",
        "contentSnippet": "문체부와 콘진원 사업 일환",
        "guid": "https://toss.im/tossfeed/article/45501",
        "isoDate": "2026-02-23T11:10:00.000Z"
      },
      {
        "title": "Can a single word from a customer change a service?",
        "link": "https://toss.im/tossfeed/article/usercentric",
        "pubDate": "Thu, 19 Feb 2026 10:11:00 GMT",
        "content:encodedSnippet": ".css-1vn47db{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex:none;-ms-flex:none;flex:none;}\n.css-of5acw{margin:24px 0 8px;text-align:center;}.css-1pgssrp{max-width:100%;border-radius:16px;}.css-14on8x8{font-size:17px;letter-spacing:0em;line-height:1.6;font-weight:normal;color:var(--adaptiveGrey800);margin:24px 0 8px;}\n.css-1r3ko7u{line-height:0;display:block;}.css-1iisb9p{display:contents;line-height:1.6;}.css-1kxrhf3{white-space:pre-wrap;}Toss listens to hundreds of VOCs every month and applies user feedback to the product in no time—from small texts, screen layouts to new features. The principle Toss has always upheld, “User Centric”, isn’t just some grand slogan, but something that sits at the core of our daily conversations and questions.\n.css-1feg9au{font-size:24px;letter-spacing:0em;line-height:1.6;font-weight:bold;color:var(--adaptiveGrey900);margin:24px 0 4px;}\n.css-q3ktjb{white-space:pre-wrap;font-weight:bold;}Numbers Don’t Tell the Full Story\nData tells us “what” happened, but not “why.” That’s why we always listen to the users. When we set a goal, verify a hypothesis or identify a problem, real insights come from real conversations, not just numbers and assumptions. That is why our UX researchers build hypotheses based on various sources then validate them through tests and user interviews. The result isn’t guesswork, but insights rooted in real context that bring depth to every product decision we make.\nTools That Connect Us with Users\nBack in 2020, all research had to be done by the UX researchers. But as Toss expanded into banking, securities, insurance and more, research became broader and more complex, which led us to build our own tools to hear the voices of our users more clearly.\nThe first tool we built was the “Toss Form,” a simple way for anyone to create surveys, send push notifications to users and collect responses. This allowed not only UX researchers but also designers, product owners, and developers to run surveys themselves and get direct results. Next came the “Ask Me Anything” program. User research originally required recruiting participants and juggling schedules. Now, this program makes research effortless. Teams can instantly run online interviews with random users anytime, and uncover what data alone can’t tell us: When do people use Toss the most? What do users make of this phrase? Are our screens too complicated?\nConducting research became much simpler thanks to our internally developed tools and processes, but restrictions still remained. Usability Testing (UT) required preparing app prototypes and questionnaires, which took at least an hour for even the simplest tests. This kept us from conducting larger and more frequent tests. We needed to find a completely new way to check usability much more frequently and easily. That’s how “Huri-bot,” our AI-powered usability testing tool, was born. Trained on Toss user patterns, the bot can evaluate early prototypes and help refine designs and products with ease.\nUX Researchers, Drawing the Bigger Picture\nOnce the environment was set up for anyone to run tests easily, UX researchers were able to focus on more complex, in-depth problems. Their roles expanded beyond product features, into areas like analyzing business domains such as commerce and ads, and reviewing Toss’ overall UX flow to suggest improvements. They also dived deep into specific topics, such as studying the financial lives of senior users, testing accessibility for users with visual impairment, and validating interaction and graphic designs across the app.\nOn top of that, they find entirely new approaches to solve problems that existing methods could never uncover. Toss, which began with a single simple transfer service, now offers over 100 services. With so many services, it takes much more than a usability test to check if users are easily accessing their desired services. For that reason, we created TNS(Toss Navigation Score). TNS measures how easily users reach a service, and the results are used to improve entry points and information architecture across the app. For example, we might ask a user, “You can file a hospital bill with your insurer through Toss. Where would you go to do that?” We then track how they navigate the app and give a score. TNS helps us refine the overall service structure by analyzing what confuses users and what doesn’t.\nWe are continuously improving the way we collect VOCs and reflect them in our services. But every journey starts with the same question: Is this service truly for the users? The tools may change, but Toss principles remain the same. We keep our ears open to users every single day, because the best answers always start with them.\nResearch Story 1 :\nFinding answers where users get stuck\nThe “Personalized Loan Finder” service, launched in 2019, allows users to compare loan products and apply for the most favorable option when they are in genuine need of a loan. The TNS score for the service is 66, which is higher than the average of 59, but it still leaves room for improvement.\nA closer look at the user journey* through TNS revealed that many users struggled to find the entry point to the service.The reason was obvious. The entry button was labeled, “My Loan Limit,” which wasn’t enough to show that the users could actually compare loan products there. Some users even went straight to the “My Assets” page, thinking that loans were also part of their assets, only to find no path to the service. In other words, the user journey was cut off mid-flow.\n.css-18442ym{font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}.css-jfs1hr{white-space:pre-wrap;font-size:13px;line-height:1.7;word-break:keep-all;letter-spacing:0em;color:var(--adaptiveGrey600);white-space:pre-wrap;}*The end-to-end experience of a user interacting with a product to achieve a certain goal.\nFirst, we changed the label of the button. We tested 3 variations to replace the “My Loan Limit” label, and changed the label to “Compare New Loans,” which had the highest conversion rate. Second, we reinforced the link between the service and the “My Assets” page. By adding new content that displayed estimated interest rates and limits, users in need of a loan were directed more seamlessly into the “Personalized Loan Finder” service.\nThe impact was immediate. The internal data showed that the number of preliminary loan applications coming through the “My Assets” page jumped 144% MoM and 124% WoW.* This experience was a clear reminder of one simple truth: every answer can be found in the user.\n*As of July 2025\nResearch Story 2 :\nThe Toss Identity in the Eyes of Users\nIn 2023, Toss expanded its simple payment service, Toss Pay, into the offline world. Users could now pay with Toss not only online, but also at convenience stores, cafés, and restaurants. But this sparked a new challenge: How can we make Toss and Toss Pay known in the offline world?\nUntil now, Toss had thrived entirely in the digital world. The challenge was to find a way to present Toss in a consistent way in the unfamiliar offline world. We realized that a consistent graphic language was necessary, and we turned to user perception for the answer. When asked, “What comes to mind when you think of Toss?,” most users gave similar answers. They described Toss as simple, practical and convenient. One person said, “Toss feels like an app built in Excel by a genius engineer.” Funny as it sounded, it also revealed our brand’s essence in the most intuitive way.\nTo bring that abstract image into life, we conducted research on fonts, colors and logos. For fonts, most users said that English text in black felt the most like Toss. This was because that was the look users had grown familiar with from seeing it over and over in the press and marketing. When it came to colors and logos, the image most recalled by users was a blue logo on a white square background. This shows that users associated the brand more with the app icon, which is what they encounter on a daily basis, rather than a graphic symbol.\n\nToss’ blue logo alongside the English text “toss” in black, often seen in Toss’s press releases.\n\nToss app icon. The blue logo on a white square background is what most users recall when asked to picture the Toss logo from memory.\n\nThe Toss Pay payment icon, updated in the UI based on research findings.\n\nThese insights were quickly reflected. We applied the white background, blue logo, and black text across the Toss Pay UI and offline payment touchpoints, to reflect the image most familiar to our users.\n\n.css-1ifza5r{border:0;margin-top:0;margin-bottom:0;height:1px;opacity:1;background:var(--tHairlineBackground);margin:0;}\nWriter Myeonghwa Jung, Toss User Research Team Leader",
        "guid": "https://toss.im/tossfeed/article/usercentric",
        "isoDate": "2026-02-19T10:11:00.000Z"
      }
    ]
  },
  {
    "name": "모나미",
    "category": "개인",
    "posts": []
  },
  {
    "name": "김진홍",
    "category": "개인",
    "posts": []
  }
]